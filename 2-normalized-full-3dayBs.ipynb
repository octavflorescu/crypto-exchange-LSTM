{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %config IPCompleter.greedy=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular import *\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN_NAME = 'Close'\n",
    "DATE_COLUMN_NAME = 'Date'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "df = pd.read_csv(\"BTC-ETH_.csv\",\n",
    "                 #read dates as dates\n",
    "                 parse_dates=[DATE_COLUMN_NAME], date_parser=lambda x: datetime.fromtimestamp(int(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Total Trade Quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-14 07:57:00</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.151851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-14 07:58:00</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>13.530307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-14 07:59:00</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>4.623229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-14 08:00:00</td>\n",
       "      <td>0.022004</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>15.558100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-14 08:01:00</td>\n",
       "      <td>0.022004</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>15.558100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date      Open      High       Low     Close  \\\n",
       "0 2019-10-14 07:57:00  0.022033  0.022033  0.022033  0.022033   \n",
       "1 2019-10-14 07:58:00  0.022033  0.022033  0.022030  0.022030   \n",
       "2 2019-10-14 07:59:00  0.022030  0.022030  0.022030  0.022030   \n",
       "3 2019-10-14 08:00:00  0.022004  0.022008  0.022000  0.022008   \n",
       "4 2019-10-14 08:01:00  0.022004  0.022008  0.022000  0.022008   \n",
       "\n",
       "   Total Trade Quantity  \n",
       "0              0.151851  \n",
       "1             13.530307  \n",
       "2              4.623229  \n",
       "3             15.558100  \n",
       "4             15.558100  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Total Trade Quantity</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Dayofweek</th>\n",
       "      <th>...</th>\n",
       "      <th>Is_month_end</th>\n",
       "      <th>Is_month_start</th>\n",
       "      <th>Is_quarter_end</th>\n",
       "      <th>Is_quarter_start</th>\n",
       "      <th>Is_year_end</th>\n",
       "      <th>Is_year_start</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Second</th>\n",
       "      <th>Elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.151851</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1571039820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>13.530307</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>1571039880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>4.623229</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>1571039940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022004</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>15.558100</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1571040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022004</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>15.558100</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1571040060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Open      High       Low     Close  Total Trade Quantity  Year  Month  \\\n",
       "0  0.022033  0.022033  0.022033  0.022033              0.151851  2019     10   \n",
       "1  0.022033  0.022033  0.022030  0.022030             13.530307  2019     10   \n",
       "2  0.022030  0.022030  0.022030  0.022030              4.623229  2019     10   \n",
       "3  0.022004  0.022008  0.022000  0.022008             15.558100  2019     10   \n",
       "4  0.022004  0.022008  0.022000  0.022008             15.558100  2019     10   \n",
       "\n",
       "   Week  Day  Dayofweek  ...  Is_month_end  Is_month_start  Is_quarter_end  \\\n",
       "0    42   14          0  ...         False           False           False   \n",
       "1    42   14          0  ...         False           False           False   \n",
       "2    42   14          0  ...         False           False           False   \n",
       "3    42   14          0  ...         False           False           False   \n",
       "4    42   14          0  ...         False           False           False   \n",
       "\n",
       "   Is_quarter_start  Is_year_end  Is_year_start  Hour  Minute  Second  \\\n",
       "0             False        False          False     7      57       0   \n",
       "1             False        False          False     7      58       0   \n",
       "2             False        False          False     7      59       0   \n",
       "3             False        False          False     8       0       0   \n",
       "4             False        False          False     8       1       0   \n",
       "\n",
       "      Elapsed  \n",
       "0  1571039820  \n",
       "1  1571039880  \n",
       "2  1571039940  \n",
       "3  1571040000  \n",
       "4  1571040060  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_datepart(df, DATE_COLUMN_NAME, time=True);\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Total Trade Quantity</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Dayofweek</th>\n",
       "      <th>...</th>\n",
       "      <th>Is_month_end</th>\n",
       "      <th>Is_month_start</th>\n",
       "      <th>Is_quarter_end</th>\n",
       "      <th>Is_quarter_start</th>\n",
       "      <th>Is_year_end</th>\n",
       "      <th>Is_year_start</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Second</th>\n",
       "      <th>Elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.151851</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>13.530307</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>4.623229</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022004</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>15.558100</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022004</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>15.558100</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Open      High       Low     Close  Total Trade Quantity    Year  \\\n",
       "0  0.022033  0.022033  0.022033  0.022033              0.151851  2019.0   \n",
       "1  0.022033  0.022033  0.022030  0.022030             13.530307  2019.0   \n",
       "2  0.022030  0.022030  0.022030  0.022030              4.623229  2019.0   \n",
       "3  0.022004  0.022008  0.022000  0.022008             15.558100  2019.0   \n",
       "4  0.022004  0.022008  0.022000  0.022008             15.558100  2019.0   \n",
       "\n",
       "   Month  Week   Day  Dayofweek  ...  Is_month_end  Is_month_start  \\\n",
       "0   10.0  42.0  14.0        0.0  ...           0.0             0.0   \n",
       "1   10.0  42.0  14.0        0.0  ...           0.0             0.0   \n",
       "2   10.0  42.0  14.0        0.0  ...           0.0             0.0   \n",
       "3   10.0  42.0  14.0        0.0  ...           0.0             0.0   \n",
       "4   10.0  42.0  14.0        0.0  ...           0.0             0.0   \n",
       "\n",
       "   Is_quarter_end  Is_quarter_start  Is_year_end  Is_year_start  Hour  Minute  \\\n",
       "0             0.0               0.0          0.0            0.0   7.0    57.0   \n",
       "1             0.0               0.0          0.0            0.0   7.0    58.0   \n",
       "2             0.0               0.0          0.0            0.0   7.0    59.0   \n",
       "3             0.0               0.0          0.0            0.0   8.0     0.0   \n",
       "4             0.0               0.0          0.0            0.0   8.0     1.0   \n",
       "\n",
       "   Second       Elapsed  \n",
       "0     0.0  1.571040e+09  \n",
       "1     0.0  1.571040e+09  \n",
       "2     0.0  1.571040e+09  \n",
       "3     0.0  1.571040e+09  \n",
       "4     0.0  1.571040e+09  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.astype(float)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Open', 0),\n",
       " ('High', 0),\n",
       " ('Low', 0),\n",
       " ('Close', 0),\n",
       " ('Total Trade Quantity', 0),\n",
       " ('Year', 0),\n",
       " ('Month', 0),\n",
       " ('Week', 0),\n",
       " ('Day', 0),\n",
       " ('Dayofweek', 0),\n",
       " ('Dayofyear', 0),\n",
       " ('Is_month_end', 0),\n",
       " ('Is_month_start', 0),\n",
       " ('Is_quarter_end', 0),\n",
       " ('Is_quarter_start', 0),\n",
       " ('Is_year_end', 0),\n",
       " ('Is_year_start', 0),\n",
       " ('Hour', 0),\n",
       " ('Minute', 0),\n",
       " ('Second', 0),\n",
       " ('Elapsed', 0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(column, sum(df[column].isna())) for column in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Open          High           Low         Close  \\\n",
      "count  27788.000000  27788.000000  27788.000000  27788.000000   \n",
      "mean       0.020890      0.020892      0.020887      0.020889   \n",
      "std        0.000990      0.000990      0.000991      0.000991   \n",
      "min        0.018971      0.019005      0.018897      0.019005   \n",
      "25%        0.019844      0.019845      0.019840      0.019843   \n",
      "50%        0.021233      0.021236      0.021229      0.021234   \n",
      "75%        0.021777      0.021778      0.021776      0.021776   \n",
      "max        0.022477      0.022500      0.022470      0.022484   \n",
      "\n",
      "       Total Trade Quantity     Year         Month          Week  \\\n",
      "count          2.778800e+04  27788.0  27788.000000  27788.000000   \n",
      "mean           4.433066e+00   2019.0     10.082230     42.943933   \n",
      "std            1.985220e+01      0.0      0.274719      0.794968   \n",
      "min            1.000000e-08   2019.0     10.000000     42.000000   \n",
      "25%            2.063824e-01   2019.0     10.000000     42.000000   \n",
      "50%            9.691496e-01   2019.0     10.000000     43.000000   \n",
      "75%            3.221354e+00   2019.0     10.000000     44.000000   \n",
      "max            1.450456e+03   2019.0     11.000000     44.000000   \n",
      "\n",
      "                Day     Dayofweek  ...  Is_month_end  Is_month_start  \\\n",
      "count  27788.000000  27788.000000  ...  27788.000000    27788.000000   \n",
      "mean      20.918094      2.859688  ...      0.051821        0.051821   \n",
      "std        7.625217      1.909530  ...      0.221669        0.221669   \n",
      "min        1.000000      0.000000  ...      0.000000        0.000000   \n",
      "25%       17.000000      1.000000  ...      0.000000        0.000000   \n",
      "50%       22.000000      3.000000  ...      0.000000        0.000000   \n",
      "75%       27.000000      4.000000  ...      0.000000        0.000000   \n",
      "max       31.000000      6.000000  ...      1.000000        1.000000   \n",
      "\n",
      "       Is_quarter_end  Is_quarter_start  Is_year_end  Is_year_start  \\\n",
      "count         27788.0           27788.0      27788.0        27788.0   \n",
      "mean              0.0               0.0          0.0            0.0   \n",
      "std               0.0               0.0          0.0            0.0   \n",
      "min               0.0               0.0          0.0            0.0   \n",
      "25%               0.0               0.0          0.0            0.0   \n",
      "50%               0.0               0.0          0.0            0.0   \n",
      "75%               0.0               0.0          0.0            0.0   \n",
      "max               0.0               0.0          0.0            0.0   \n",
      "\n",
      "               Hour        Minute   Second       Elapsed  \n",
      "count  27788.000000  27788.000000  27788.0  2.778800e+04  \n",
      "mean      11.468656     29.498129      0.0  1.571872e+09  \n",
      "std        6.883997     17.322393      0.0  4.799231e+05  \n",
      "min        0.000000      0.000000      0.0  1.571040e+09  \n",
      "25%        6.000000     14.000000      0.0  1.571457e+09  \n",
      "50%       11.000000     29.000000      0.0  1.571873e+09  \n",
      "75%       17.000000     45.000000      0.0  1.572287e+09  \n",
      "max       23.000000     59.000000      0.0  1.572703e+09  \n",
      "\n",
      "[8 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27788, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['Second'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "True_target_column_name = TARGET_COLUMN_NAME+'_future'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[True_target_column_name] = df[TARGET_COLUMN_NAME].shift(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Total Trade Quantity</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Dayofweek</th>\n",
       "      <th>...</th>\n",
       "      <th>Is_month_end</th>\n",
       "      <th>Is_month_start</th>\n",
       "      <th>Is_quarter_end</th>\n",
       "      <th>Is_quarter_start</th>\n",
       "      <th>Is_year_end</th>\n",
       "      <th>Is_year_start</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Elapsed</th>\n",
       "      <th>Close_future</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.151851</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "      <td>0.022008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>13.530307</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "      <td>0.022008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>4.623229</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022004</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>15.558100</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022004</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>15.558100</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "      <td>0.022013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>10.862016</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "      <td>0.022024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>10.862016</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "      <td>0.022036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.022015</td>\n",
       "      <td>0.022015</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>2.339544</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "      <td>0.022034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.022015</td>\n",
       "      <td>0.022024</td>\n",
       "      <td>0.022015</td>\n",
       "      <td>0.022024</td>\n",
       "      <td>11.265395</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "      <td>0.022033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.022036</td>\n",
       "      <td>0.022036</td>\n",
       "      <td>0.022036</td>\n",
       "      <td>0.022036</td>\n",
       "      <td>0.887427</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "      <td>0.022033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Open      High       Low     Close  Total Trade Quantity    Year  \\\n",
       "0  0.022033  0.022033  0.022033  0.022033              0.151851  2019.0   \n",
       "1  0.022033  0.022033  0.022030  0.022030             13.530307  2019.0   \n",
       "2  0.022030  0.022030  0.022030  0.022030              4.623229  2019.0   \n",
       "3  0.022004  0.022008  0.022000  0.022008             15.558100  2019.0   \n",
       "4  0.022004  0.022008  0.022000  0.022008             15.558100  2019.0   \n",
       "5  0.022000  0.022000  0.022000  0.022000             10.862016  2019.0   \n",
       "6  0.022000  0.022000  0.022000  0.022000             10.862016  2019.0   \n",
       "7  0.022015  0.022015  0.022013  0.022013              2.339544  2019.0   \n",
       "8  0.022015  0.022024  0.022015  0.022024             11.265395  2019.0   \n",
       "9  0.022036  0.022036  0.022036  0.022036              0.887427  2019.0   \n",
       "\n",
       "   Month  Week   Day  Dayofweek  ...  Is_month_end  Is_month_start  \\\n",
       "0   10.0  42.0  14.0        0.0  ...           0.0             0.0   \n",
       "1   10.0  42.0  14.0        0.0  ...           0.0             0.0   \n",
       "2   10.0  42.0  14.0        0.0  ...           0.0             0.0   \n",
       "3   10.0  42.0  14.0        0.0  ...           0.0             0.0   \n",
       "4   10.0  42.0  14.0        0.0  ...           0.0             0.0   \n",
       "5   10.0  42.0  14.0        0.0  ...           0.0             0.0   \n",
       "6   10.0  42.0  14.0        0.0  ...           0.0             0.0   \n",
       "7   10.0  42.0  14.0        0.0  ...           0.0             0.0   \n",
       "8   10.0  42.0  14.0        0.0  ...           0.0             0.0   \n",
       "9   10.0  42.0  14.0        0.0  ...           0.0             0.0   \n",
       "\n",
       "   Is_quarter_end  Is_quarter_start  Is_year_end  Is_year_start  Hour  Minute  \\\n",
       "0             0.0               0.0          0.0            0.0   7.0    57.0   \n",
       "1             0.0               0.0          0.0            0.0   7.0    58.0   \n",
       "2             0.0               0.0          0.0            0.0   7.0    59.0   \n",
       "3             0.0               0.0          0.0            0.0   8.0     0.0   \n",
       "4             0.0               0.0          0.0            0.0   8.0     1.0   \n",
       "5             0.0               0.0          0.0            0.0   8.0     2.0   \n",
       "6             0.0               0.0          0.0            0.0   8.0     3.0   \n",
       "7             0.0               0.0          0.0            0.0   8.0     4.0   \n",
       "8             0.0               0.0          0.0            0.0   8.0     5.0   \n",
       "9             0.0               0.0          0.0            0.0   8.0     6.0   \n",
       "\n",
       "        Elapsed  Close_future  \n",
       "0  1.571040e+09      0.022008  \n",
       "1  1.571040e+09      0.022008  \n",
       "2  1.571040e+09      0.022000  \n",
       "3  1.571040e+09      0.022000  \n",
       "4  1.571040e+09      0.022013  \n",
       "5  1.571040e+09      0.022024  \n",
       "6  1.571040e+09      0.022036  \n",
       "7  1.571040e+09      0.022034  \n",
       "8  1.571040e+09      0.022033  \n",
       "9  1.571040e+09      0.022033  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train valid test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20841, 4168, 2778]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20841, 25009, 27788)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAIN_VALID_TEST_SPLIT_RATIOS\n",
    "trvate_split = tuple(int(x * len(df)) for x in (0.75, 0.9, 1.0))\n",
    "print([int(len(df)* x) for x in (0.75, 0.15, 0.1)])\n",
    "trvate_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for batch size using http://www.alcula.com/calculators/math/gcd/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20760, 4080, 2880)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_train_limit = trvate_split[0]-trvate_split[0]%BATCH_SIZE\n",
    "train_df = df[:tmp_train_limit]\n",
    "\n",
    "tmp_valid_limit = trvate_split[1] -trvate_split[0]%BATCH_SIZE -(trvate_split[1]-trvate_split[0])%BATCH_SIZE\n",
    "valid_df = df[tmp_train_limit:tmp_valid_limit]\n",
    "\n",
    "test_df = df[tmp_valid_limit:(len(df)- len(df)%BATCH_SIZE)]\n",
    "\n",
    "len(train_df), len(valid_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data(self: pd.DataFrame):\n",
    "    return self.drop(columns=[True_target_column_name])\n",
    "def target(self: pd.DataFrame):\n",
    "    return self[[True_target_column_name]]\n",
    "\n",
    "pd.DataFrame.input_data = input_data\n",
    "pd.DataFrame.target = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Total Trade Quantity</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Dayofweek</th>\n",
       "      <th>Dayofyear</th>\n",
       "      <th>Is_month_end</th>\n",
       "      <th>Is_month_start</th>\n",
       "      <th>Is_quarter_end</th>\n",
       "      <th>Is_quarter_start</th>\n",
       "      <th>Is_year_end</th>\n",
       "      <th>Is_year_start</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.151851</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>13.530307</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>4.623229</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022004</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>15.558100</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022004</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>15.558100</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.571040e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Open      High       Low     Close  Total Trade Quantity    Year  \\\n",
       "0  0.022033  0.022033  0.022033  0.022033              0.151851  2019.0   \n",
       "1  0.022033  0.022033  0.022030  0.022030             13.530307  2019.0   \n",
       "2  0.022030  0.022030  0.022030  0.022030              4.623229  2019.0   \n",
       "3  0.022004  0.022008  0.022000  0.022008             15.558100  2019.0   \n",
       "4  0.022004  0.022008  0.022000  0.022008             15.558100  2019.0   \n",
       "\n",
       "   Month  Week   Day  Dayofweek  Dayofyear  Is_month_end  Is_month_start  \\\n",
       "0   10.0  42.0  14.0        0.0      287.0           0.0             0.0   \n",
       "1   10.0  42.0  14.0        0.0      287.0           0.0             0.0   \n",
       "2   10.0  42.0  14.0        0.0      287.0           0.0             0.0   \n",
       "3   10.0  42.0  14.0        0.0      287.0           0.0             0.0   \n",
       "4   10.0  42.0  14.0        0.0      287.0           0.0             0.0   \n",
       "\n",
       "   Is_quarter_end  Is_quarter_start  Is_year_end  Is_year_start  Hour  Minute  \\\n",
       "0             0.0               0.0          0.0            0.0   7.0    57.0   \n",
       "1             0.0               0.0          0.0            0.0   7.0    58.0   \n",
       "2             0.0               0.0          0.0            0.0   7.0    59.0   \n",
       "3             0.0               0.0          0.0            0.0   8.0     0.0   \n",
       "4             0.0               0.0          0.0            0.0   8.0     1.0   \n",
       "\n",
       "        Elapsed  \n",
       "0  1.571040e+09  \n",
       "1  1.571040e+09  \n",
       "2  1.571040e+09  \n",
       "3  1.571040e+09  \n",
       "4  1.571040e+09  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.input_data().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape (27788, 21)\n",
      "train_df.shape (20760, 21)\n",
      "valid_df.shape (4080, 21)\n",
      "test_df.shape (2880, 21)\n"
     ]
    }
   ],
   "source": [
    "print('df.shape', df.shape)\n",
    "print('train_df.shape', train_df.shape)\n",
    "print('valid_df.shape', valid_df.shape)\n",
    "print('test_df.shape', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler#, PolynomialFeatures\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# preprocessing_pipeline = Pipeline([\n",
    "#     (\"normalizer\", StandardScaler())\n",
    "#     (\"poli-feature\", PolynomialFeatures(degree=2))\n",
    "# ]).fit(train_df.input_data(), train_df.target())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(df_to_transform, transformer):\n",
    "    return pd.DataFrame(transformer.transform(df_to_transform),\n",
    "                        columns=df_to_transform.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tranformer = StandardScaler()\n",
    "input_tranformer.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Total Trade Quantity</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Dayofweek</th>\n",
       "      <th>...</th>\n",
       "      <th>Is_month_end</th>\n",
       "      <th>Is_month_start</th>\n",
       "      <th>Is_quarter_end</th>\n",
       "      <th>Is_quarter_start</th>\n",
       "      <th>Is_year_end</th>\n",
       "      <th>Is_year_start</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Elapsed</th>\n",
       "      <th>Close_future</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.875639</td>\n",
       "      <td>0.873760</td>\n",
       "      <td>0.877658</td>\n",
       "      <td>0.875750</td>\n",
       "      <td>-0.213800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.004943</td>\n",
       "      <td>-1.686621</td>\n",
       "      <td>-1.440382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.655072</td>\n",
       "      <td>1.587934</td>\n",
       "      <td>-1.733799</td>\n",
       "      <td>0.848229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.875628</td>\n",
       "      <td>0.873749</td>\n",
       "      <td>0.874176</td>\n",
       "      <td>0.872266</td>\n",
       "      <td>0.405407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.004943</td>\n",
       "      <td>-1.686621</td>\n",
       "      <td>-1.440382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.655072</td>\n",
       "      <td>1.645677</td>\n",
       "      <td>-1.733632</td>\n",
       "      <td>0.848229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.872154</td>\n",
       "      <td>0.870273</td>\n",
       "      <td>0.874176</td>\n",
       "      <td>0.872266</td>\n",
       "      <td>-0.006847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.004943</td>\n",
       "      <td>-1.686621</td>\n",
       "      <td>-1.440382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.655072</td>\n",
       "      <td>1.703420</td>\n",
       "      <td>-1.733465</td>\n",
       "      <td>0.839926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.844087</td>\n",
       "      <td>0.845992</td>\n",
       "      <td>0.841696</td>\n",
       "      <td>0.848005</td>\n",
       "      <td>0.499261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.004943</td>\n",
       "      <td>-1.686621</td>\n",
       "      <td>-1.440382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.509083</td>\n",
       "      <td>-1.703420</td>\n",
       "      <td>-1.733298</td>\n",
       "      <td>0.839926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.844087</td>\n",
       "      <td>0.845992</td>\n",
       "      <td>0.841696</td>\n",
       "      <td>0.848005</td>\n",
       "      <td>0.499261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.004943</td>\n",
       "      <td>-1.686621</td>\n",
       "      <td>-1.440382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.509083</td>\n",
       "      <td>-1.645677</td>\n",
       "      <td>-1.733131</td>\n",
       "      <td>0.854415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Open      High       Low     Close  Total Trade Quantity  Year  Month  \\\n",
       "0  0.875639  0.873760  0.877658  0.875750             -0.213800   0.0    0.0   \n",
       "1  0.875628  0.873749  0.874176  0.872266              0.405407   0.0    0.0   \n",
       "2  0.872154  0.870273  0.874176  0.872266             -0.006847   0.0    0.0   \n",
       "3  0.844087  0.845992  0.841696  0.848005              0.499261   0.0    0.0   \n",
       "4  0.844087  0.845992  0.841696  0.848005              0.499261   0.0    0.0   \n",
       "\n",
       "       Week       Day  Dayofweek  ...  Is_month_end  Is_month_start  \\\n",
       "0 -1.004943 -1.686621  -1.440382  ...           0.0             0.0   \n",
       "1 -1.004943 -1.686621  -1.440382  ...           0.0             0.0   \n",
       "2 -1.004943 -1.686621  -1.440382  ...           0.0             0.0   \n",
       "3 -1.004943 -1.686621  -1.440382  ...           0.0             0.0   \n",
       "4 -1.004943 -1.686621  -1.440382  ...           0.0             0.0   \n",
       "\n",
       "   Is_quarter_end  Is_quarter_start  Is_year_end  Is_year_start      Hour  \\\n",
       "0             0.0               0.0          0.0            0.0 -0.655072   \n",
       "1             0.0               0.0          0.0            0.0 -0.655072   \n",
       "2             0.0               0.0          0.0            0.0 -0.655072   \n",
       "3             0.0               0.0          0.0            0.0 -0.509083   \n",
       "4             0.0               0.0          0.0            0.0 -0.509083   \n",
       "\n",
       "     Minute   Elapsed  Close_future  \n",
       "0  1.587934 -1.733799      0.848229  \n",
       "1  1.645677 -1.733632      0.848229  \n",
       "2  1.703420 -1.733465      0.839926  \n",
       "3 -1.703420 -1.733298      0.839926  \n",
       "4 -1.645677 -1.733131      0.854415  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = transform_df(train_df, input_tranformer)\n",
    "valid_data = transform_df(valid_df, input_tranformer)\n",
    "test_data = transform_df(test_df, input_tranformer)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(train_data.input_data().values),\n",
    "                                        torch.from_numpy(train_data.target().values)),\n",
    "                          shuffle=False, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(TensorDataset(torch.from_numpy(valid_data.input_data().values),\n",
    "                                      torch.from_numpy(valid_data.target().values)),\n",
    "                        shuffle=False, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(TensorDataset(torch.from_numpy(test_data.input_data().values),\n",
    "                                       torch.from_numpy(test_data.target().values)),\n",
    "                         shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, features, output_size=1,\n",
    "#                  embedding_dim=10,\n",
    "#                  hidden_dim=50,\n",
    "                 n_layers=1,\n",
    "                 drop_prob=0.5, device=torch.device(\"cpu\")):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        \n",
    "        embedding_dim=features*2\n",
    "        hidden_dim=features*8\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        \n",
    "        self.liniar = nn.Linear(features, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=n_layers, dropout=drop_prob, batch_first=True)\n",
    "#         self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_dim, hidden_dim//2),\n",
    "                                nn.Dropout(drop_prob),\n",
    "                                nn.Tanh(),\n",
    "                                nn.Linear(hidden_dim//2, output_size))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.float()\n",
    "        \n",
    "        liniar = self.liniar(x)\n",
    "        lstm_out, hidden = self.lstm(liniar.view(batch_size, -1, liniar.shape[1]),\n",
    "                                     (hidden[0][:,-batch_size:,:],# hidden\n",
    "                                      hidden[1][:,-batch_size:,:]))# cell state\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "#         out = self.dropout(lstm_out)\n",
    "        out = self.fc(lstm_out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "#         out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(self.device), # hidden\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(self.device)) # cell state\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_count = train_data.input_data().shape[1]\n",
    "# output_size = 1\n",
    "# embedding_dim = 400\n",
    "# hidden_dim = 512\n",
    "# n_layers = 2\n",
    "\n",
    "model = SentimentNet(features_count, device=DEVICE)\n",
    "model.to(DEVICE)\n",
    "\n",
    "lr=0.0003\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/6... Step: 1... Loss: 0.679237... Val Loss: 2.276038\n",
      "Validation loss decreased (inf --> 2.276038).  Saving model ...\n",
      "Epoch: 1/6... Step: 2... Loss: 0.658726... Val Loss: 2.270478\n",
      "Validation loss decreased (2.276038 --> 2.270478).  Saving model ...\n",
      "Epoch: 1/6... Step: 3... Loss: 0.725427... Val Loss: 2.264896\n",
      "Validation loss decreased (2.270478 --> 2.264896).  Saving model ...\n",
      "Epoch: 1/6... Step: 4... Loss: 0.712062... Val Loss: 2.259309\n",
      "Validation loss decreased (2.264896 --> 2.259309).  Saving model ...\n",
      "Epoch: 1/6... Step: 5... Loss: 1.032497... Val Loss: 2.253734\n",
      "Validation loss decreased (2.259309 --> 2.253734).  Saving model ...\n",
      "Epoch: 1/6... Step: 6... Loss: 1.222427... Val Loss: 2.248096\n",
      "Validation loss decreased (2.253734 --> 2.248096).  Saving model ...\n",
      "Epoch: 1/6... Step: 7... Loss: 1.359423... Val Loss: 2.242397\n",
      "Validation loss decreased (2.248096 --> 2.242397).  Saving model ...\n",
      "Epoch: 1/6... Step: 8... Loss: 1.243900... Val Loss: 2.236656\n",
      "Validation loss decreased (2.242397 --> 2.236656).  Saving model ...\n",
      "Epoch: 1/6... Step: 9... Loss: 1.189671... Val Loss: 2.230895\n",
      "Validation loss decreased (2.236656 --> 2.230895).  Saving model ...\n",
      "Epoch: 1/6... Step: 10... Loss: 1.182773... Val Loss: 2.225076\n",
      "Validation loss decreased (2.230895 --> 2.225076).  Saving model ...\n",
      "Epoch: 1/6... Step: 11... Loss: 0.912662... Val Loss: 2.219187\n",
      "Validation loss decreased (2.225076 --> 2.219187).  Saving model ...\n",
      "Epoch: 1/6... Step: 12... Loss: 0.900679... Val Loss: 2.213225\n",
      "Validation loss decreased (2.219187 --> 2.213225).  Saving model ...\n",
      "Epoch: 1/6... Step: 13... Loss: 0.655366... Val Loss: 2.207228\n",
      "Validation loss decreased (2.213225 --> 2.207228).  Saving model ...\n",
      "Epoch: 1/6... Step: 14... Loss: 0.763206... Val Loss: 2.201168\n",
      "Validation loss decreased (2.207228 --> 2.201168).  Saving model ...\n",
      "Epoch: 1/6... Step: 15... Loss: 0.699215... Val Loss: 2.195037\n",
      "Validation loss decreased (2.201168 --> 2.195037).  Saving model ...\n",
      "Epoch: 1/6... Step: 16... Loss: 0.597987... Val Loss: 2.188864\n",
      "Validation loss decreased (2.195037 --> 2.188864).  Saving model ...\n",
      "Epoch: 1/6... Step: 17... Loss: 0.551628... Val Loss: 2.182654\n",
      "Validation loss decreased (2.188864 --> 2.182654).  Saving model ...\n",
      "Epoch: 1/6... Step: 18... Loss: 0.553265... Val Loss: 2.176404\n",
      "Validation loss decreased (2.182654 --> 2.176404).  Saving model ...\n",
      "Epoch: 1/6... Step: 19... Loss: 0.443523... Val Loss: 2.170142\n",
      "Validation loss decreased (2.176404 --> 2.170142).  Saving model ...\n",
      "Epoch: 1/6... Step: 20... Loss: 0.538167... Val Loss: 2.163848\n",
      "Validation loss decreased (2.170142 --> 2.163848).  Saving model ...\n",
      "Epoch: 1/6... Step: 21... Loss: 0.606041... Val Loss: 2.157474\n",
      "Validation loss decreased (2.163848 --> 2.157474).  Saving model ...\n",
      "Epoch: 1/6... Step: 22... Loss: 0.458876... Val Loss: 2.151024\n",
      "Validation loss decreased (2.157474 --> 2.151024).  Saving model ...\n",
      "Epoch: 1/6... Step: 23... Loss: 0.470092... Val Loss: 2.144484\n",
      "Validation loss decreased (2.151024 --> 2.144484).  Saving model ...\n",
      "Epoch: 1/6... Step: 24... Loss: 0.412122... Val Loss: 2.137867\n",
      "Validation loss decreased (2.144484 --> 2.137867).  Saving model ...\n",
      "Epoch: 1/6... Step: 25... Loss: 0.418095... Val Loss: 2.131165\n",
      "Validation loss decreased (2.137867 --> 2.131165).  Saving model ...\n",
      "Epoch: 1/6... Step: 26... Loss: 0.401361... Val Loss: 2.124366\n",
      "Validation loss decreased (2.131165 --> 2.124366).  Saving model ...\n",
      "Epoch: 1/6... Step: 27... Loss: 0.242823... Val Loss: 2.117541\n",
      "Validation loss decreased (2.124366 --> 2.117541).  Saving model ...\n",
      "Epoch: 1/6... Step: 28... Loss: 0.206676... Val Loss: 2.110711\n",
      "Validation loss decreased (2.117541 --> 2.110711).  Saving model ...\n",
      "Epoch: 1/6... Step: 29... Loss: 0.214909... Val Loss: 2.103871\n",
      "Validation loss decreased (2.110711 --> 2.103871).  Saving model ...\n",
      "Epoch: 1/6... Step: 30... Loss: 0.189108... Val Loss: 2.097022\n",
      "Validation loss decreased (2.103871 --> 2.097022).  Saving model ...\n",
      "Epoch: 1/6... Step: 31... Loss: 0.202535... Val Loss: 2.090163\n",
      "Validation loss decreased (2.097022 --> 2.090163).  Saving model ...\n",
      "Epoch: 1/6... Step: 32... Loss: 0.267863... Val Loss: 2.083262\n",
      "Validation loss decreased (2.090163 --> 2.083262).  Saving model ...\n",
      "Epoch: 1/6... Step: 33... Loss: 0.218921... Val Loss: 2.076306\n",
      "Validation loss decreased (2.083262 --> 2.076306).  Saving model ...\n",
      "Epoch: 1/6... Step: 34... Loss: 0.201102... Val Loss: 2.069285\n",
      "Validation loss decreased (2.076306 --> 2.069285).  Saving model ...\n",
      "Epoch: 1/6... Step: 35... Loss: 0.179299... Val Loss: 2.062193\n",
      "Validation loss decreased (2.069285 --> 2.062193).  Saving model ...\n",
      "Epoch: 1/6... Step: 36... Loss: 0.213617... Val Loss: 2.055014\n",
      "Validation loss decreased (2.062193 --> 2.055014).  Saving model ...\n",
      "Epoch: 1/6... Step: 37... Loss: 0.283218... Val Loss: 2.047700\n",
      "Validation loss decreased (2.055014 --> 2.047700).  Saving model ...\n",
      "Epoch: 1/6... Step: 38... Loss: 0.238714... Val Loss: 2.040258\n",
      "Validation loss decreased (2.047700 --> 2.040258).  Saving model ...\n",
      "Epoch: 1/6... Step: 39... Loss: 0.299914... Val Loss: 2.032654\n",
      "Validation loss decreased (2.040258 --> 2.032654).  Saving model ...\n",
      "Epoch: 1/6... Step: 40... Loss: 0.310095... Val Loss: 2.024871\n",
      "Validation loss decreased (2.032654 --> 2.024871).  Saving model ...\n",
      "Epoch: 1/6... Step: 41... Loss: 0.344416... Val Loss: 2.016888\n",
      "Validation loss decreased (2.024871 --> 2.016888).  Saving model ...\n",
      "Epoch: 1/6... Step: 42... Loss: 0.327683... Val Loss: 2.008717\n",
      "Validation loss decreased (2.016888 --> 2.008717).  Saving model ...\n",
      "Epoch: 1/6... Step: 43... Loss: 0.232202... Val Loss: 2.000419\n",
      "Validation loss decreased (2.008717 --> 2.000419).  Saving model ...\n",
      "Epoch: 1/6... Step: 44... Loss: 0.248425... Val Loss: 1.992001\n",
      "Validation loss decreased (2.000419 --> 1.992001).  Saving model ...\n",
      "Epoch: 1/6... Step: 45... Loss: 0.228426... Val Loss: 1.983412\n",
      "Validation loss decreased (1.992001 --> 1.983412).  Saving model ...\n",
      "Epoch: 1/6... Step: 46... Loss: 0.164697... Val Loss: 1.974692\n",
      "Validation loss decreased (1.983412 --> 1.974692).  Saving model ...\n",
      "Epoch: 1/6... Step: 47... Loss: 0.182083... Val Loss: 1.965812\n",
      "Validation loss decreased (1.974692 --> 1.965812).  Saving model ...\n",
      "Epoch: 1/6... Step: 48... Loss: 0.253646... Val Loss: 1.956694\n",
      "Validation loss decreased (1.965812 --> 1.956694).  Saving model ...\n",
      "Epoch: 1/6... Step: 49... Loss: 0.070583... Val Loss: 1.947563\n",
      "Validation loss decreased (1.956694 --> 1.947563).  Saving model ...\n",
      "Epoch: 1/6... Step: 50... Loss: 0.088187... Val Loss: 1.938396\n",
      "Validation loss decreased (1.947563 --> 1.938396).  Saving model ...\n",
      "Epoch: 1/6... Step: 51... Loss: 0.060458... Val Loss: 1.929245\n",
      "Validation loss decreased (1.938396 --> 1.929245).  Saving model ...\n",
      "Epoch: 1/6... Step: 52... Loss: 0.076099... Val Loss: 1.920086\n",
      "Validation loss decreased (1.929245 --> 1.920086).  Saving model ...\n",
      "Epoch: 1/6... Step: 53... Loss: 0.127422... Val Loss: 1.910850\n",
      "Validation loss decreased (1.920086 --> 1.910850).  Saving model ...\n",
      "Epoch: 1/6... Step: 54... Loss: 0.195740... Val Loss: 1.901444\n",
      "Validation loss decreased (1.910850 --> 1.901444).  Saving model ...\n",
      "Epoch: 1/6... Step: 55... Loss: 0.208076... Val Loss: 1.891851\n",
      "Validation loss decreased (1.901444 --> 1.891851).  Saving model ...\n",
      "Epoch: 1/6... Step: 56... Loss: 0.117889... Val Loss: 1.882181\n",
      "Validation loss decreased (1.891851 --> 1.882181).  Saving model ...\n",
      "Epoch: 1/6... Step: 57... Loss: 0.062910... Val Loss: 1.872480\n",
      "Validation loss decreased (1.882181 --> 1.872480).  Saving model ...\n",
      "Epoch: 1/6... Step: 58... Loss: 0.081263... Val Loss: 1.862689\n",
      "Validation loss decreased (1.872480 --> 1.862689).  Saving model ...\n",
      "Epoch: 1/6... Step: 59... Loss: 0.123660... Val Loss: 1.852701\n",
      "Validation loss decreased (1.862689 --> 1.852701).  Saving model ...\n",
      "Epoch: 1/6... Step: 60... Loss: 0.120187... Val Loss: 1.842519\n",
      "Validation loss decreased (1.852701 --> 1.842519).  Saving model ...\n",
      "Epoch: 1/6... Step: 61... Loss: 0.141584... Val Loss: 1.832084\n",
      "Validation loss decreased (1.842519 --> 1.832084).  Saving model ...\n",
      "Epoch: 1/6... Step: 62... Loss: 0.095753... Val Loss: 1.821478\n",
      "Validation loss decreased (1.832084 --> 1.821478).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/6... Step: 63... Loss: 0.041361... Val Loss: 1.810888\n",
      "Validation loss decreased (1.821478 --> 1.810888).  Saving model ...\n",
      "Epoch: 1/6... Step: 64... Loss: 0.029131... Val Loss: 1.800359\n",
      "Validation loss decreased (1.810888 --> 1.800359).  Saving model ...\n",
      "Epoch: 1/6... Step: 65... Loss: 0.011317... Val Loss: 1.790044\n",
      "Validation loss decreased (1.800359 --> 1.790044).  Saving model ...\n",
      "Epoch: 1/6... Step: 66... Loss: 0.006453... Val Loss: 1.779997\n",
      "Validation loss decreased (1.790044 --> 1.779997).  Saving model ...\n",
      "Epoch: 1/6... Step: 67... Loss: 0.010504... Val Loss: 1.770178\n",
      "Validation loss decreased (1.779997 --> 1.770178).  Saving model ...\n",
      "Epoch: 1/6... Step: 68... Loss: 0.007407... Val Loss: 1.760579\n",
      "Validation loss decreased (1.770178 --> 1.760579).  Saving model ...\n",
      "Epoch: 1/6... Step: 69... Loss: 0.005240... Val Loss: 1.751234\n",
      "Validation loss decreased (1.760579 --> 1.751234).  Saving model ...\n",
      "Epoch: 1/6... Step: 70... Loss: 0.004929... Val Loss: 1.742256\n",
      "Validation loss decreased (1.751234 --> 1.742256).  Saving model ...\n",
      "Epoch: 1/6... Step: 71... Loss: 0.005979... Val Loss: 1.733660\n",
      "Validation loss decreased (1.742256 --> 1.733660).  Saving model ...\n",
      "Epoch: 1/6... Step: 72... Loss: 0.003365... Val Loss: 1.725374\n",
      "Validation loss decreased (1.733660 --> 1.725374).  Saving model ...\n",
      "Epoch: 1/6... Step: 73... Loss: 0.006882... Val Loss: 1.717404\n",
      "Validation loss decreased (1.725374 --> 1.717404).  Saving model ...\n",
      "Epoch: 1/6... Step: 74... Loss: 0.006126... Val Loss: 1.709665\n",
      "Validation loss decreased (1.717404 --> 1.709665).  Saving model ...\n",
      "Epoch: 1/6... Step: 75... Loss: 0.006319... Val Loss: 1.702157\n",
      "Validation loss decreased (1.709665 --> 1.702157).  Saving model ...\n",
      "Epoch: 1/6... Step: 76... Loss: 0.010027... Val Loss: 1.694802\n",
      "Validation loss decreased (1.702157 --> 1.694802).  Saving model ...\n",
      "Epoch: 1/6... Step: 77... Loss: 0.005292... Val Loss: 1.687748\n",
      "Validation loss decreased (1.694802 --> 1.687748).  Saving model ...\n",
      "Epoch: 1/6... Step: 78... Loss: 0.018372... Val Loss: 1.681173\n",
      "Validation loss decreased (1.687748 --> 1.681173).  Saving model ...\n",
      "Epoch: 1/6... Step: 79... Loss: 0.095749... Val Loss: 1.675246\n",
      "Validation loss decreased (1.681173 --> 1.675246).  Saving model ...\n",
      "Epoch: 1/6... Step: 80... Loss: 0.058660... Val Loss: 1.669782\n",
      "Validation loss decreased (1.675246 --> 1.669782).  Saving model ...\n",
      "Epoch: 1/6... Step: 81... Loss: 0.086355... Val Loss: 1.663518\n",
      "Validation loss decreased (1.669782 --> 1.663518).  Saving model ...\n",
      "Epoch: 1/6... Step: 82... Loss: 0.092550... Val Loss: 1.655864\n",
      "Validation loss decreased (1.663518 --> 1.655864).  Saving model ...\n",
      "Epoch: 1/6... Step: 83... Loss: 0.121440... Val Loss: 1.646344\n",
      "Validation loss decreased (1.655864 --> 1.646344).  Saving model ...\n",
      "Epoch: 1/6... Step: 84... Loss: 0.128059... Val Loss: 1.634811\n",
      "Validation loss decreased (1.646344 --> 1.634811).  Saving model ...\n",
      "Epoch: 1/6... Step: 85... Loss: 0.107853... Val Loss: 1.621502\n",
      "Validation loss decreased (1.634811 --> 1.621502).  Saving model ...\n",
      "Epoch: 1/6... Step: 86... Loss: 0.027963... Val Loss: 1.607974\n",
      "Validation loss decreased (1.621502 --> 1.607974).  Saving model ...\n",
      "Epoch: 1/6... Step: 87... Loss: 0.003384... Val Loss: 1.594926\n",
      "Validation loss decreased (1.607974 --> 1.594926).  Saving model ...\n",
      "Epoch: 1/6... Step: 88... Loss: 0.058090... Val Loss: 1.581020\n",
      "Validation loss decreased (1.594926 --> 1.581020).  Saving model ...\n",
      "Epoch: 1/6... Step: 89... Loss: 0.117805... Val Loss: 1.564956\n",
      "Validation loss decreased (1.581020 --> 1.564956).  Saving model ...\n",
      "Epoch: 1/6... Step: 90... Loss: 0.072474... Val Loss: 1.547417\n",
      "Validation loss decreased (1.564956 --> 1.547417).  Saving model ...\n",
      "Epoch: 1/6... Step: 91... Loss: 0.068227... Val Loss: 1.528533\n",
      "Validation loss decreased (1.547417 --> 1.528533).  Saving model ...\n",
      "Epoch: 1/6... Step: 92... Loss: 0.042371... Val Loss: 1.508759\n",
      "Validation loss decreased (1.528533 --> 1.508759).  Saving model ...\n",
      "Epoch: 1/6... Step: 93... Loss: 0.029814... Val Loss: 1.488585\n",
      "Validation loss decreased (1.508759 --> 1.488585).  Saving model ...\n",
      "Epoch: 1/6... Step: 94... Loss: 0.013529... Val Loss: 1.468662\n",
      "Validation loss decreased (1.488585 --> 1.468662).  Saving model ...\n",
      "Epoch: 1/6... Step: 95... Loss: 0.019520... Val Loss: 1.448619\n",
      "Validation loss decreased (1.468662 --> 1.448619).  Saving model ...\n",
      "Epoch: 1/6... Step: 96... Loss: 0.044113... Val Loss: 1.427562\n",
      "Validation loss decreased (1.448619 --> 1.427562).  Saving model ...\n",
      "Epoch: 1/6... Step: 97... Loss: 0.074527... Val Loss: 1.404722\n",
      "Validation loss decreased (1.427562 --> 1.404722).  Saving model ...\n",
      "Epoch: 1/6... Step: 98... Loss: 0.140405... Val Loss: 1.378602\n",
      "Validation loss decreased (1.404722 --> 1.378602).  Saving model ...\n",
      "Epoch: 1/6... Step: 99... Loss: 0.107648... Val Loss: 1.349640\n",
      "Validation loss decreased (1.378602 --> 1.349640).  Saving model ...\n",
      "Epoch: 1/6... Step: 100... Loss: 0.092958... Val Loss: 1.317995\n",
      "Validation loss decreased (1.349640 --> 1.317995).  Saving model ...\n",
      "Epoch: 1/6... Step: 101... Loss: 0.067158... Val Loss: 1.284329\n",
      "Validation loss decreased (1.317995 --> 1.284329).  Saving model ...\n",
      "Epoch: 1/6... Step: 102... Loss: 0.057308... Val Loss: 1.248952\n",
      "Validation loss decreased (1.284329 --> 1.248952).  Saving model ...\n",
      "Epoch: 1/6... Step: 103... Loss: 0.004912... Val Loss: 1.215263\n",
      "Validation loss decreased (1.248952 --> 1.215263).  Saving model ...\n",
      "Epoch: 1/6... Step: 104... Loss: 0.003927... Val Loss: 1.184455\n",
      "Validation loss decreased (1.215263 --> 1.184455).  Saving model ...\n",
      "Epoch: 1/6... Step: 105... Loss: 0.004179... Val Loss: 1.156819\n",
      "Validation loss decreased (1.184455 --> 1.156819).  Saving model ...\n",
      "Epoch: 1/6... Step: 106... Loss: 0.042438... Val Loss: 1.129751\n",
      "Validation loss decreased (1.156819 --> 1.129751).  Saving model ...\n",
      "Epoch: 1/6... Step: 107... Loss: 0.101628... Val Loss: 1.097571\n",
      "Validation loss decreased (1.129751 --> 1.097571).  Saving model ...\n",
      "Epoch: 1/6... Step: 108... Loss: 0.116118... Val Loss: 1.059686\n",
      "Validation loss decreased (1.097571 --> 1.059686).  Saving model ...\n",
      "Epoch: 1/6... Step: 109... Loss: 0.097327... Val Loss: 1.016409\n",
      "Validation loss decreased (1.059686 --> 1.016409).  Saving model ...\n",
      "Epoch: 1/6... Step: 110... Loss: 0.092635... Val Loss: 0.967917\n",
      "Validation loss decreased (1.016409 --> 0.967917).  Saving model ...\n",
      "Epoch: 1/6... Step: 111... Loss: 0.039147... Val Loss: 0.923204\n",
      "Validation loss decreased (0.967917 --> 0.923204).  Saving model ...\n",
      "Epoch: 1/6... Step: 112... Loss: 0.030741... Val Loss: 0.887146\n",
      "Validation loss decreased (0.923204 --> 0.887146).  Saving model ...\n",
      "Epoch: 1/6... Step: 113... Loss: 0.035938... Val Loss: 0.859396\n",
      "Validation loss decreased (0.887146 --> 0.859396).  Saving model ...\n",
      "Epoch: 1/6... Step: 114... Loss: 0.090889... Val Loss: 0.843962\n",
      "Validation loss decreased (0.859396 --> 0.843962).  Saving model ...\n",
      "Epoch: 1/6... Step: 115... Loss: 0.093216... Val Loss: 0.839299\n",
      "Validation loss decreased (0.843962 --> 0.839299).  Saving model ...\n",
      "Epoch: 1/6... Step: 116... Loss: 0.162456... Val Loss: 0.846636\n",
      "Epoch: 1/6... Step: 117... Loss: 0.262697... Val Loss: 0.866339\n",
      "Epoch: 1/6... Step: 118... Loss: 0.231563... Val Loss: 0.895910\n",
      "Epoch: 1/6... Step: 119... Loss: 0.243366... Val Loss: 0.933702\n",
      "Epoch: 1/6... Step: 120... Loss: 0.208904... Val Loss: 0.978013\n",
      "Epoch: 1/6... Step: 121... Loss: 0.158671... Val Loss: 1.027035\n",
      "Epoch: 1/6... Step: 122... Loss: 0.181219... Val Loss: 1.080120\n",
      "Epoch: 1/6... Step: 123... Loss: 0.194697... Val Loss: 1.136607\n",
      "Epoch: 1/6... Step: 124... Loss: 0.238127... Val Loss: 1.196167\n",
      "Epoch: 1/6... Step: 125... Loss: 0.194042... Val Loss: 1.257395\n",
      "Epoch: 1/6... Step: 126... Loss: 0.124901... Val Loss: 1.319243\n",
      "Epoch: 1/6... Step: 127... Loss: 0.057533... Val Loss: 1.380598\n",
      "Epoch: 1/6... Step: 128... Loss: 0.040466... Val Loss: 1.441366\n",
      "Epoch: 1/6... Step: 129... Loss: 0.038564... Val Loss: 1.501590\n",
      "Epoch: 1/6... Step: 130... Loss: 0.042685... Val Loss: 1.561446\n",
      "Epoch: 1/6... Step: 131... Loss: 0.022641... Val Loss: 1.620600\n",
      "Epoch: 1/6... Step: 132... Loss: 0.014875... Val Loss: 1.678945\n",
      "Epoch: 1/6... Step: 133... Loss: 0.031526... Val Loss: 1.737556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/6... Step: 134... Loss: 0.024805... Val Loss: 1.796618\n",
      "Epoch: 1/6... Step: 135... Loss: 0.023223... Val Loss: 1.856605\n",
      "Epoch: 1/6... Step: 136... Loss: 0.050273... Val Loss: 1.915454\n",
      "Epoch: 1/6... Step: 137... Loss: 0.240547... Val Loss: 1.951670\n",
      "Epoch: 1/6... Step: 138... Loss: 0.468308... Val Loss: 1.956239\n",
      "Epoch: 1/6... Step: 139... Loss: 0.420459... Val Loss: 1.936221\n",
      "Epoch: 1/6... Step: 140... Loss: 0.501309... Val Loss: 1.895988\n",
      "Epoch: 1/6... Step: 141... Loss: 2.053659... Val Loss: 1.823253\n",
      "Epoch: 1/6... Step: 142... Loss: 3.878632... Val Loss: 1.708761\n",
      "Epoch: 1/6... Step: 143... Loss: 3.188229... Val Loss: 1.559974\n",
      "Epoch: 1/6... Step: 144... Loss: 2.875305... Val Loss: 1.380721\n",
      "Epoch: 1/6... Step: 145... Loss: 2.585359... Val Loss: 1.171479\n",
      "Epoch: 1/6... Step: 146... Loss: 1.963796... Val Loss: 0.937128\n",
      "Epoch: 1/6... Step: 147... Loss: 1.749370... Val Loss: 0.683099\n",
      "Validation loss decreased (0.839299 --> 0.683099).  Saving model ...\n",
      "Epoch: 1/6... Step: 148... Loss: 1.399945... Val Loss: 0.426648\n",
      "Validation loss decreased (0.683099 --> 0.426648).  Saving model ...\n",
      "Epoch: 1/6... Step: 149... Loss: 1.267451... Val Loss: 0.209079\n",
      "Validation loss decreased (0.426648 --> 0.209079).  Saving model ...\n",
      "Epoch: 1/6... Step: 150... Loss: 0.958031... Val Loss: 0.127955\n",
      "Validation loss decreased (0.209079 --> 0.127955).  Saving model ...\n",
      "Epoch: 1/6... Step: 151... Loss: 0.446663... Val Loss: 0.366811\n",
      "Epoch: 1/6... Step: 152... Loss: 0.402714... Val Loss: 1.172288\n",
      "Epoch: 1/6... Step: 153... Loss: 0.098876... Val Loss: 2.531615\n",
      "Epoch: 1/6... Step: 154... Loss: 0.064125... Val Loss: 4.040737\n",
      "Epoch: 1/6... Step: 155... Loss: 0.091963... Val Loss: 5.343651\n",
      "Epoch: 1/6... Step: 156... Loss: 0.207800... Val Loss: 6.288155\n",
      "Epoch: 1/6... Step: 157... Loss: 0.278136... Val Loss: 6.913574\n",
      "Epoch: 1/6... Step: 158... Loss: 0.316971... Val Loss: 7.280597\n",
      "Epoch: 1/6... Step: 159... Loss: 0.177166... Val Loss: 7.494929\n",
      "Epoch: 1/6... Step: 160... Loss: 0.145116... Val Loss: 7.605989\n",
      "Epoch: 1/6... Step: 161... Loss: 0.111096... Val Loss: 7.638158\n",
      "Epoch: 1/6... Step: 162... Loss: 0.122062... Val Loss: 7.604087\n",
      "Epoch: 1/6... Step: 163... Loss: 0.143601... Val Loss: 7.502840\n",
      "Epoch: 1/6... Step: 164... Loss: 0.145919... Val Loss: 7.341619\n",
      "Epoch: 1/6... Step: 165... Loss: 0.145442... Val Loss: 7.141113\n",
      "Epoch: 1/6... Step: 166... Loss: 0.090442... Val Loss: 6.965577\n",
      "Epoch: 1/6... Step: 167... Loss: 0.069755... Val Loss: 6.807834\n",
      "Epoch: 1/6... Step: 168... Loss: 0.083089... Val Loss: 6.657400\n",
      "Epoch: 1/6... Step: 169... Loss: 0.116622... Val Loss: 6.476910\n",
      "Epoch: 1/6... Step: 170... Loss: 0.128936... Val Loss: 6.253659\n",
      "Epoch: 1/6... Step: 171... Loss: 0.207807... Val Loss: 5.961334\n",
      "Epoch: 1/6... Step: 172... Loss: 0.120558... Val Loss: 5.650058\n",
      "Epoch: 1/6... Step: 173... Loss: 0.103737... Val Loss: 5.337213\n",
      "Epoch: 2/6... Step: 174... Loss: 0.024810... Val Loss: 5.071441\n",
      "Epoch: 2/6... Step: 175... Loss: 0.055723... Val Loss: 4.826204\n",
      "Epoch: 2/6... Step: 176... Loss: 0.093896... Val Loss: 4.595526\n",
      "Epoch: 2/6... Step: 177... Loss: 0.114905... Val Loss: 4.375232\n",
      "Epoch: 2/6... Step: 178... Loss: 0.058884... Val Loss: 4.174808\n",
      "Epoch: 2/6... Step: 179... Loss: 0.026063... Val Loss: 4.001279\n",
      "Epoch: 2/6... Step: 180... Loss: 0.043074... Val Loss: 3.855084\n",
      "Epoch: 2/6... Step: 181... Loss: 0.033274... Val Loss: 3.732773\n",
      "Epoch: 2/6... Step: 182... Loss: 0.031019... Val Loss: 3.630652\n",
      "Epoch: 2/6... Step: 183... Loss: 0.036421... Val Loss: 3.548030\n",
      "Epoch: 2/6... Step: 184... Loss: 0.020447... Val Loss: 3.474254\n",
      "Epoch: 2/6... Step: 185... Loss: 0.021085... Val Loss: 3.408723\n",
      "Epoch: 2/6... Step: 186... Loss: 0.036420... Val Loss: 3.343042\n",
      "Epoch: 2/6... Step: 187... Loss: 0.021666... Val Loss: 3.282574\n",
      "Epoch: 2/6... Step: 188... Loss: 0.021205... Val Loss: 3.225412\n",
      "Epoch: 2/6... Step: 189... Loss: 0.028280... Val Loss: 3.168600\n",
      "Epoch: 2/6... Step: 190... Loss: 0.030562... Val Loss: 3.110596\n",
      "Epoch: 2/6... Step: 191... Loss: 0.017654... Val Loss: 3.055499\n",
      "Epoch: 2/6... Step: 192... Loss: 0.025817... Val Loss: 3.000003\n",
      "Epoch: 2/6... Step: 193... Loss: 0.016423... Val Loss: 2.950616\n",
      "Epoch: 2/6... Step: 194... Loss: 0.021733... Val Loss: 2.910184\n",
      "Epoch: 2/6... Step: 195... Loss: 0.013032... Val Loss: 2.874002\n",
      "Epoch: 2/6... Step: 196... Loss: 0.014857... Val Loss: 2.843018\n",
      "Epoch: 2/6... Step: 197... Loss: 0.013233... Val Loss: 2.814612\n",
      "Epoch: 2/6... Step: 198... Loss: 0.013334... Val Loss: 2.790558\n",
      "Epoch: 2/6... Step: 199... Loss: 0.015678... Val Loss: 2.772252\n",
      "Epoch: 2/6... Step: 200... Loss: 0.019615... Val Loss: 2.750051\n",
      "Epoch: 2/6... Step: 201... Loss: 0.017506... Val Loss: 2.724767\n",
      "Epoch: 2/6... Step: 202... Loss: 0.012646... Val Loss: 2.698083\n",
      "Epoch: 2/6... Step: 203... Loss: 0.010646... Val Loss: 2.671220\n",
      "Epoch: 2/6... Step: 204... Loss: 0.007157... Val Loss: 2.646837\n",
      "Epoch: 2/6... Step: 205... Loss: 0.013609... Val Loss: 2.630206\n",
      "Epoch: 2/6... Step: 206... Loss: 0.007410... Val Loss: 2.616762\n",
      "Epoch: 2/6... Step: 207... Loss: 0.008319... Val Loss: 2.604724\n",
      "Epoch: 2/6... Step: 208... Loss: 0.006333... Val Loss: 2.594008\n",
      "Epoch: 2/6... Step: 209... Loss: 0.008646... Val Loss: 2.586203\n",
      "Epoch: 2/6... Step: 210... Loss: 0.021329... Val Loss: 2.586481\n",
      "Epoch: 2/6... Step: 211... Loss: 0.013974... Val Loss: 2.592158\n",
      "Epoch: 2/6... Step: 212... Loss: 0.028206... Val Loss: 2.606755\n",
      "Epoch: 2/6... Step: 213... Loss: 0.022775... Val Loss: 2.628304\n",
      "Epoch: 2/6... Step: 214... Loss: 0.037232... Val Loss: 2.659207\n",
      "Epoch: 2/6... Step: 215... Loss: 0.027855... Val Loss: 2.695450\n",
      "Epoch: 2/6... Step: 216... Loss: 0.013468... Val Loss: 2.728588\n",
      "Epoch: 2/6... Step: 217... Loss: 0.011792... Val Loss: 2.759722\n",
      "Epoch: 2/6... Step: 218... Loss: 0.013128... Val Loss: 2.784009\n",
      "Epoch: 2/6... Step: 219... Loss: 0.031826... Val Loss: 2.791926\n",
      "Epoch: 2/6... Step: 220... Loss: 0.031385... Val Loss: 2.787664\n",
      "Epoch: 2/6... Step: 221... Loss: 0.012682... Val Loss: 2.780028\n",
      "Epoch: 2/6... Step: 222... Loss: 0.090075... Val Loss: 2.748190\n",
      "Epoch: 2/6... Step: 223... Loss: 0.045189... Val Loss: 2.702869\n",
      "Epoch: 2/6... Step: 224... Loss: 0.043812... Val Loss: 2.646708\n",
      "Epoch: 2/6... Step: 225... Loss: 0.018924... Val Loss: 2.588689\n",
      "Epoch: 2/6... Step: 226... Loss: 0.006449... Val Loss: 2.537229\n",
      "Epoch: 2/6... Step: 227... Loss: 0.021869... Val Loss: 2.501124\n",
      "Epoch: 2/6... Step: 228... Loss: 0.026422... Val Loss: 2.479793\n",
      "Epoch: 2/6... Step: 229... Loss: 0.010868... Val Loss: 2.464553\n",
      "Epoch: 2/6... Step: 230... Loss: 0.006100... Val Loss: 2.449859\n",
      "Epoch: 2/6... Step: 231... Loss: 0.010415... Val Loss: 2.440383\n",
      "Epoch: 2/6... Step: 232... Loss: 0.016063... Val Loss: 2.439824\n",
      "Epoch: 2/6... Step: 233... Loss: 0.016219... Val Loss: 2.447012\n",
      "Epoch: 2/6... Step: 234... Loss: 0.022696... Val Loss: 2.463217\n",
      "Epoch: 2/6... Step: 235... Loss: 0.012692... Val Loss: 2.483712\n",
      "Epoch: 2/6... Step: 236... Loss: 0.008644... Val Loss: 2.499126\n",
      "Epoch: 2/6... Step: 237... Loss: 0.005711... Val Loss: 2.509506\n",
      "Epoch: 2/6... Step: 238... Loss: 0.018825... Val Loss: 2.510778\n",
      "Epoch: 2/6... Step: 239... Loss: 0.020079... Val Loss: 2.504293\n",
      "Epoch: 2/6... Step: 240... Loss: 0.009693... Val Loss: 2.495208\n",
      "Epoch: 2/6... Step: 241... Loss: 0.004132... Val Loss: 2.485758\n",
      "Epoch: 2/6... Step: 242... Loss: 0.006290... Val Loss: 2.474767\n",
      "Epoch: 2/6... Step: 243... Loss: 0.009187... Val Loss: 2.459982\n",
      "Epoch: 2/6... Step: 244... Loss: 0.006065... Val Loss: 2.443484\n",
      "Epoch: 2/6... Step: 245... Loss: 0.002325... Val Loss: 2.428761\n",
      "Epoch: 2/6... Step: 246... Loss: 0.005598... Val Loss: 2.417188\n",
      "Epoch: 2/6... Step: 247... Loss: 0.009308... Val Loss: 2.411412\n",
      "Epoch: 2/6... Step: 248... Loss: 0.010960... Val Loss: 2.411093\n",
      "Epoch: 2/6... Step: 249... Loss: 0.016685... Val Loss: 2.417242\n",
      "Epoch: 2/6... Step: 250... Loss: 0.009530... Val Loss: 2.426031\n",
      "Epoch: 2/6... Step: 251... Loss: 0.007895... Val Loss: 2.433815\n",
      "Epoch: 2/6... Step: 252... Loss: 0.004631... Val Loss: 2.439743\n",
      "Epoch: 2/6... Step: 253... Loss: 0.010559... Val Loss: 2.443774\n",
      "Epoch: 2/6... Step: 254... Loss: 0.016634... Val Loss: 2.442738\n",
      "Epoch: 2/6... Step: 255... Loss: 0.031292... Val Loss: 2.432487\n",
      "Epoch: 2/6... Step: 256... Loss: 0.041417... Val Loss: 2.409025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/6... Step: 257... Loss: 0.063483... Val Loss: 2.365421\n",
      "Epoch: 2/6... Step: 258... Loss: 0.090584... Val Loss: 2.296728\n",
      "Epoch: 2/6... Step: 259... Loss: 0.154963... Val Loss: 2.205458\n",
      "Epoch: 2/6... Step: 260... Loss: 0.083800... Val Loss: 2.111629\n",
      "Epoch: 2/6... Step: 261... Loss: 0.026875... Val Loss: 2.021126\n",
      "Epoch: 2/6... Step: 262... Loss: 0.012754... Val Loss: 1.932680\n",
      "Epoch: 2/6... Step: 263... Loss: 0.005202... Val Loss: 1.843319\n",
      "Epoch: 2/6... Step: 264... Loss: 0.007449... Val Loss: 1.763855\n",
      "Epoch: 2/6... Step: 265... Loss: 0.004024... Val Loss: 1.702574\n",
      "Epoch: 2/6... Step: 266... Loss: 0.016563... Val Loss: 1.654430\n",
      "Epoch: 2/6... Step: 267... Loss: 0.028422... Val Loss: 1.609744\n",
      "Epoch: 2/6... Step: 268... Loss: 0.050383... Val Loss: 1.562348\n",
      "Epoch: 2/6... Step: 269... Loss: 0.070062... Val Loss: 1.511600\n",
      "Epoch: 2/6... Step: 270... Loss: 0.073965... Val Loss: 1.462826\n",
      "Epoch: 2/6... Step: 271... Loss: 0.066462... Val Loss: 1.425322\n",
      "Epoch: 2/6... Step: 272... Loss: 0.017142... Val Loss: 1.399758\n",
      "Epoch: 2/6... Step: 273... Loss: 0.005607... Val Loss: 1.379283\n",
      "Epoch: 2/6... Step: 274... Loss: 0.009151... Val Loss: 1.351743\n",
      "Epoch: 2/6... Step: 275... Loss: 0.026727... Val Loss: 1.303589\n",
      "Epoch: 2/6... Step: 276... Loss: 0.097378... Val Loss: 1.210643\n",
      "Epoch: 2/6... Step: 277... Loss: 0.076300... Val Loss: 1.087065\n",
      "Epoch: 2/6... Step: 278... Loss: 0.041348... Val Loss: 0.946618\n",
      "Epoch: 2/6... Step: 279... Loss: 0.048625... Val Loss: 0.805926\n",
      "Epoch: 2/6... Step: 280... Loss: 0.014873... Val Loss: 0.688589\n",
      "Epoch: 2/6... Step: 281... Loss: 0.011332... Val Loss: 0.585042\n",
      "Epoch: 2/6... Step: 282... Loss: 0.007139... Val Loss: 0.482518\n",
      "Epoch: 2/6... Step: 283... Loss: 0.006526... Val Loss: 0.381817\n",
      "Epoch: 2/6... Step: 284... Loss: 0.103796... Val Loss: 0.229756\n",
      "Epoch: 2/6... Step: 285... Loss: 0.035048... Val Loss: 0.107542\n",
      "Validation loss decreased (0.127955 --> 0.107542).  Saving model ...\n",
      "Epoch: 2/6... Step: 286... Loss: 0.013078... Val Loss: 0.055470\n",
      "Validation loss decreased (0.107542 --> 0.055470).  Saving model ...\n",
      "Epoch: 2/6... Step: 287... Loss: 0.012408... Val Loss: 0.056918\n",
      "Epoch: 2/6... Step: 288... Loss: 0.005008... Val Loss: 0.091976\n",
      "Epoch: 2/6... Step: 289... Loss: 0.020274... Val Loss: 0.144173\n",
      "Epoch: 2/6... Step: 290... Loss: 0.053966... Val Loss: 0.201550\n",
      "Epoch: 2/6... Step: 291... Loss: 0.043119... Val Loss: 0.257598\n",
      "Epoch: 2/6... Step: 292... Loss: 0.052403... Val Loss: 0.308438\n",
      "Epoch: 2/6... Step: 293... Loss: 0.038232... Val Loss: 0.353318\n",
      "Epoch: 2/6... Step: 294... Loss: 0.020454... Val Loss: 0.393321\n",
      "Epoch: 2/6... Step: 295... Loss: 0.024800... Val Loss: 0.428267\n",
      "Epoch: 2/6... Step: 296... Loss: 0.023148... Val Loss: 0.457808\n",
      "Epoch: 2/6... Step: 297... Loss: 0.025604... Val Loss: 0.481146\n",
      "Epoch: 2/6... Step: 298... Loss: 0.016448... Val Loss: 0.499873\n",
      "Epoch: 2/6... Step: 299... Loss: 0.006192... Val Loss: 0.518414\n",
      "Epoch: 2/6... Step: 300... Loss: 0.011960... Val Loss: 0.538817\n",
      "Epoch: 2/6... Step: 301... Loss: 0.015063... Val Loss: 0.559517\n",
      "Epoch: 2/6... Step: 302... Loss: 0.011244... Val Loss: 0.579094\n",
      "Epoch: 2/6... Step: 303... Loss: 0.009696... Val Loss: 0.596913\n",
      "Epoch: 2/6... Step: 304... Loss: 0.010937... Val Loss: 0.613480\n",
      "Epoch: 2/6... Step: 305... Loss: 0.011032... Val Loss: 0.628823\n",
      "Epoch: 2/6... Step: 306... Loss: 0.007613... Val Loss: 0.642003\n",
      "Epoch: 2/6... Step: 307... Loss: 0.008415... Val Loss: 0.652937\n",
      "Epoch: 2/6... Step: 308... Loss: 0.011831... Val Loss: 0.660343\n",
      "Epoch: 2/6... Step: 309... Loss: 0.055952... Val Loss: 0.669696\n",
      "Epoch: 2/6... Step: 310... Loss: 0.121996... Val Loss: 0.675915\n",
      "Epoch: 2/6... Step: 311... Loss: 0.118453... Val Loss: 0.655240\n",
      "Epoch: 2/6... Step: 312... Loss: 0.035436... Val Loss: 0.612513\n",
      "Epoch: 2/6... Step: 313... Loss: 0.023953... Val Loss: 0.550720\n",
      "Epoch: 2/6... Step: 314... Loss: 0.679996... Val Loss: 0.381906\n",
      "Epoch: 2/6... Step: 315... Loss: 0.679937... Val Loss: 0.141977\n",
      "Epoch: 2/6... Step: 316... Loss: 0.339691... Val Loss: 0.097974\n",
      "Epoch: 2/6... Step: 317... Loss: 0.216678... Val Loss: 0.461119\n",
      "Epoch: 2/6... Step: 318... Loss: 0.147206... Val Loss: 1.310903\n",
      "Epoch: 2/6... Step: 319... Loss: 0.053973... Val Loss: 2.368491\n",
      "Epoch: 2/6... Step: 320... Loss: 0.069630... Val Loss: 3.195736\n",
      "Epoch: 2/6... Step: 321... Loss: 0.062042... Val Loss: 3.781683\n",
      "Epoch: 2/6... Step: 322... Loss: 0.081271... Val Loss: 4.224429\n",
      "Epoch: 2/6... Step: 323... Loss: 0.075529... Val Loss: 4.594702\n",
      "Epoch: 2/6... Step: 324... Loss: 0.102633... Val Loss: 4.848028\n",
      "Epoch: 2/6... Step: 325... Loss: 0.081146... Val Loss: 5.085551\n",
      "Epoch: 2/6... Step: 326... Loss: 0.081753... Val Loss: 5.207451\n",
      "Epoch: 2/6... Step: 327... Loss: 0.080674... Val Loss: 5.255870\n",
      "Epoch: 2/6... Step: 328... Loss: 0.054144... Val Loss: 5.257182\n",
      "Epoch: 2/6... Step: 329... Loss: 0.099304... Val Loss: 5.150478\n",
      "Epoch: 2/6... Step: 330... Loss: 0.091055... Val Loss: 5.005762\n",
      "Epoch: 2/6... Step: 331... Loss: 0.079982... Val Loss: 4.847309\n",
      "Epoch: 2/6... Step: 332... Loss: 0.109893... Val Loss: 4.801181\n",
      "Epoch: 2/6... Step: 333... Loss: 0.125662... Val Loss: 4.866852\n",
      "Epoch: 2/6... Step: 334... Loss: 0.133841... Val Loss: 5.041934\n",
      "Epoch: 2/6... Step: 335... Loss: 0.124773... Val Loss: 5.276729\n",
      "Epoch: 2/6... Step: 336... Loss: 0.129454... Val Loss: 5.555884\n",
      "Epoch: 2/6... Step: 337... Loss: 0.084517... Val Loss: 5.835335\n",
      "Epoch: 2/6... Step: 338... Loss: 0.066392... Val Loss: 6.116942\n",
      "Epoch: 2/6... Step: 339... Loss: 0.104991... Val Loss: 6.448038\n",
      "Epoch: 2/6... Step: 340... Loss: 0.128917... Val Loss: 6.793901\n",
      "Epoch: 2/6... Step: 341... Loss: 0.091787... Val Loss: 7.117441\n",
      "Epoch: 2/6... Step: 342... Loss: 0.090509... Val Loss: 7.316773\n",
      "Epoch: 2/6... Step: 343... Loss: 0.139569... Val Loss: 7.317528\n",
      "Epoch: 2/6... Step: 344... Loss: 0.253631... Val Loss: 7.084926\n",
      "Epoch: 2/6... Step: 345... Loss: 0.175338... Val Loss: 6.732054\n",
      "Epoch: 2/6... Step: 346... Loss: 0.106302... Val Loss: 6.340434\n",
      "Epoch: 3/6... Step: 347... Loss: 0.035309... Val Loss: 6.047073\n",
      "Epoch: 3/6... Step: 348... Loss: 0.022668... Val Loss: 5.821977\n",
      "Epoch: 3/6... Step: 349... Loss: 0.026964... Val Loss: 5.657139\n",
      "Epoch: 3/6... Step: 350... Loss: 0.023490... Val Loss: 5.536825\n",
      "Epoch: 3/6... Step: 351... Loss: 0.090774... Val Loss: 5.476254\n",
      "Epoch: 3/6... Step: 352... Loss: 0.112187... Val Loss: 5.478090\n",
      "Epoch: 3/6... Step: 353... Loss: 0.081204... Val Loss: 5.522243\n",
      "Epoch: 3/6... Step: 354... Loss: 0.030385... Val Loss: 5.572377\n",
      "Epoch: 3/6... Step: 355... Loss: 0.027936... Val Loss: 5.595064\n",
      "Epoch: 3/6... Step: 356... Loss: 0.039208... Val Loss: 5.584126\n",
      "Epoch: 3/6... Step: 357... Loss: 0.099443... Val Loss: 5.512264\n",
      "Epoch: 3/6... Step: 358... Loss: 0.086929... Val Loss: 5.393206\n",
      "Epoch: 3/6... Step: 359... Loss: 0.143472... Val Loss: 5.209055\n",
      "Epoch: 3/6... Step: 360... Loss: 0.055055... Val Loss: 5.000279\n",
      "Epoch: 3/6... Step: 361... Loss: 0.030880... Val Loss: 4.779882\n",
      "Epoch: 3/6... Step: 362... Loss: 0.024604... Val Loss: 4.544785\n",
      "Epoch: 3/6... Step: 363... Loss: 0.016447... Val Loss: 4.311101\n",
      "Epoch: 3/6... Step: 364... Loss: 0.011418... Val Loss: 4.106037\n",
      "Epoch: 3/6... Step: 365... Loss: 0.013966... Val Loss: 3.925697\n",
      "Epoch: 3/6... Step: 366... Loss: 0.017954... Val Loss: 3.790512\n",
      "Epoch: 3/6... Step: 367... Loss: 0.051376... Val Loss: 3.720270\n",
      "Epoch: 3/6... Step: 368... Loss: 0.023231... Val Loss: 3.686586\n",
      "Epoch: 3/6... Step: 369... Loss: 0.023393... Val Loss: 3.685249\n",
      "Epoch: 3/6... Step: 370... Loss: 0.016423... Val Loss: 3.701115\n",
      "Epoch: 3/6... Step: 371... Loss: 0.017335... Val Loss: 3.731845\n",
      "Epoch: 3/6... Step: 372... Loss: 0.011579... Val Loss: 3.769143\n",
      "Epoch: 3/6... Step: 373... Loss: 0.021158... Val Loss: 3.776613\n",
      "Epoch: 3/6... Step: 374... Loss: 0.023983... Val Loss: 3.752793\n",
      "Epoch: 3/6... Step: 375... Loss: 0.020408... Val Loss: 3.704198\n",
      "Epoch: 3/6... Step: 376... Loss: 0.019697... Val Loss: 3.634110\n",
      "Epoch: 3/6... Step: 377... Loss: 0.016670... Val Loss: 3.543816\n",
      "Epoch: 3/6... Step: 378... Loss: 0.009360... Val Loss: 3.455041\n",
      "Epoch: 3/6... Step: 379... Loss: 0.006651... Val Loss: 3.370024\n",
      "Epoch: 3/6... Step: 380... Loss: 0.007423... Val Loss: 3.283835\n",
      "Epoch: 3/6... Step: 381... Loss: 0.005786... Val Loss: 3.199875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/6... Step: 382... Loss: 0.006466... Val Loss: 3.132466\n",
      "Epoch: 3/6... Step: 383... Loss: 0.014486... Val Loss: 3.098898\n",
      "Epoch: 3/6... Step: 384... Loss: 0.011108... Val Loss: 3.083906\n",
      "Epoch: 3/6... Step: 385... Loss: 0.019801... Val Loss: 3.099102\n",
      "Epoch: 3/6... Step: 386... Loss: 0.021824... Val Loss: 3.145172\n",
      "Epoch: 3/6... Step: 387... Loss: 0.029529... Val Loss: 3.224532\n",
      "Epoch: 3/6... Step: 388... Loss: 0.023676... Val Loss: 3.327161\n",
      "Epoch: 3/6... Step: 389... Loss: 0.012063... Val Loss: 3.422649\n",
      "Epoch: 3/6... Step: 390... Loss: 0.010255... Val Loss: 3.504313\n",
      "Epoch: 3/6... Step: 391... Loss: 0.010046... Val Loss: 3.577338\n",
      "Epoch: 3/6... Step: 392... Loss: 0.007893... Val Loss: 3.626006\n",
      "Epoch: 3/6... Step: 393... Loss: 0.014861... Val Loss: 3.658948\n",
      "Epoch: 3/6... Step: 394... Loss: 0.009643... Val Loss: 3.694812\n",
      "Epoch: 3/6... Step: 395... Loss: 0.034788... Val Loss: 3.685258\n",
      "Epoch: 3/6... Step: 396... Loss: 0.018634... Val Loss: 3.647840\n",
      "Epoch: 3/6... Step: 397... Loss: 0.024849... Val Loss: 3.575991\n",
      "Epoch: 3/6... Step: 398... Loss: 0.018769... Val Loss: 3.470546\n",
      "Epoch: 3/6... Step: 399... Loss: 0.005590... Val Loss: 3.360853\n",
      "Epoch: 3/6... Step: 400... Loss: 0.010947... Val Loss: 3.279712\n",
      "Epoch: 3/6... Step: 401... Loss: 0.011888... Val Loss: 3.226528\n",
      "Epoch: 3/6... Step: 402... Loss: 0.007827... Val Loss: 3.176721\n",
      "Epoch: 3/6... Step: 403... Loss: 0.005338... Val Loss: 3.129421\n",
      "Epoch: 3/6... Step: 404... Loss: 0.008738... Val Loss: 3.099844\n",
      "Epoch: 3/6... Step: 405... Loss: 0.017176... Val Loss: 3.104916\n",
      "Epoch: 3/6... Step: 406... Loss: 0.013166... Val Loss: 3.135709\n",
      "Epoch: 3/6... Step: 407... Loss: 0.017606... Val Loss: 3.193410\n",
      "Epoch: 3/6... Step: 408... Loss: 0.008383... Val Loss: 3.261676\n",
      "Epoch: 3/6... Step: 409... Loss: 0.006954... Val Loss: 3.312779\n",
      "Epoch: 3/6... Step: 410... Loss: 0.003797... Val Loss: 3.347715\n",
      "Epoch: 3/6... Step: 411... Loss: 0.009153... Val Loss: 3.358592\n",
      "Epoch: 3/6... Step: 412... Loss: 0.009248... Val Loss: 3.352102\n",
      "Epoch: 3/6... Step: 413... Loss: 0.007162... Val Loss: 3.341533\n",
      "Epoch: 3/6... Step: 414... Loss: 0.003214... Val Loss: 3.333027\n",
      "Epoch: 3/6... Step: 415... Loss: 0.004925... Val Loss: 3.333810\n",
      "Epoch: 3/6... Step: 416... Loss: 0.002552... Val Loss: 3.340903\n",
      "Epoch: 3/6... Step: 417... Loss: 0.004780... Val Loss: 3.358787\n",
      "Epoch: 3/6... Step: 418... Loss: 0.008092... Val Loss: 3.390997\n",
      "Epoch: 3/6... Step: 419... Loss: 0.011277... Val Loss: 3.434681\n",
      "Epoch: 3/6... Step: 420... Loss: 0.009841... Val Loss: 3.488064\n",
      "Epoch: 3/6... Step: 421... Loss: 0.006018... Val Loss: 3.540194\n",
      "Epoch: 3/6... Step: 422... Loss: 0.007254... Val Loss: 3.590278\n",
      "Epoch: 3/6... Step: 423... Loss: 0.004879... Val Loss: 3.632400\n",
      "Epoch: 3/6... Step: 424... Loss: 0.007974... Val Loss: 3.664440\n",
      "Epoch: 3/6... Step: 425... Loss: 0.004749... Val Loss: 3.690434\n",
      "Epoch: 3/6... Step: 426... Loss: 0.028278... Val Loss: 3.710436\n",
      "Epoch: 3/6... Step: 427... Loss: 0.028483... Val Loss: 3.726648\n",
      "Epoch: 3/6... Step: 428... Loss: 0.082070... Val Loss: 3.725151\n",
      "Epoch: 3/6... Step: 429... Loss: 0.113788... Val Loss: 3.702422\n",
      "Epoch: 3/6... Step: 430... Loss: 0.090333... Val Loss: 3.668548\n",
      "Epoch: 3/6... Step: 431... Loss: 0.055220... Val Loss: 3.632059\n",
      "Epoch: 3/6... Step: 432... Loss: 0.030255... Val Loss: 3.592872\n",
      "Epoch: 3/6... Step: 433... Loss: 0.009632... Val Loss: 3.551807\n",
      "Epoch: 3/6... Step: 434... Loss: 0.045384... Val Loss: 3.507009\n",
      "Epoch: 3/6... Step: 435... Loss: 0.015436... Val Loss: 3.468740\n",
      "Epoch: 3/6... Step: 436... Loss: 0.005465... Val Loss: 3.432529\n",
      "Epoch: 3/6... Step: 437... Loss: 0.010967... Val Loss: 3.392175\n",
      "Epoch: 3/6... Step: 438... Loss: 0.017814... Val Loss: 3.342532\n",
      "Epoch: 3/6... Step: 439... Loss: 0.073154... Val Loss: 3.259207\n",
      "Epoch: 3/6... Step: 440... Loss: 0.102873... Val Loss: 3.122571\n",
      "Epoch: 3/6... Step: 441... Loss: 0.082427... Val Loss: 2.939449\n",
      "Epoch: 3/6... Step: 442... Loss: 0.054021... Val Loss: 2.738383\n",
      "Epoch: 3/6... Step: 443... Loss: 0.037863... Val Loss: 2.519243\n",
      "Epoch: 3/6... Step: 444... Loss: 0.033978... Val Loss: 2.216575\n",
      "Epoch: 3/6... Step: 445... Loss: 0.040438... Val Loss: 1.845677\n",
      "Epoch: 3/6... Step: 446... Loss: 0.019755... Val Loss: 1.471139\n",
      "Epoch: 3/6... Step: 447... Loss: 0.005106... Val Loss: 1.072068\n",
      "Epoch: 3/6... Step: 448... Loss: 0.004172... Val Loss: 0.783122\n",
      "Epoch: 3/6... Step: 449... Loss: 0.006060... Val Loss: 0.599596\n",
      "Epoch: 3/6... Step: 450... Loss: 0.009920... Val Loss: 0.478721\n",
      "Epoch: 3/6... Step: 451... Loss: 0.002651... Val Loss: 0.391868\n",
      "Epoch: 3/6... Step: 452... Loss: 0.058954... Val Loss: 0.328509\n",
      "Epoch: 3/6... Step: 453... Loss: 0.021958... Val Loss: 0.289515\n",
      "Epoch: 3/6... Step: 454... Loss: 0.009961... Val Loss: 0.263170\n",
      "Epoch: 3/6... Step: 455... Loss: 0.003946... Val Loss: 0.241406\n",
      "Epoch: 3/6... Step: 456... Loss: 0.005886... Val Loss: 0.226365\n",
      "Epoch: 3/6... Step: 457... Loss: 0.065660... Val Loss: 0.202714\n",
      "Epoch: 3/6... Step: 458... Loss: 0.004890... Val Loss: 0.185646\n",
      "Epoch: 3/6... Step: 459... Loss: 0.010407... Val Loss: 0.173580\n",
      "Epoch: 3/6... Step: 460... Loss: 0.004964... Val Loss: 0.162604\n",
      "Epoch: 3/6... Step: 461... Loss: 0.002674... Val Loss: 0.153305\n",
      "Epoch: 3/6... Step: 462... Loss: 0.015743... Val Loss: 0.145610\n",
      "Epoch: 3/6... Step: 463... Loss: 0.039564... Val Loss: 0.141889\n",
      "Epoch: 3/6... Step: 464... Loss: 0.031023... Val Loss: 0.141864\n",
      "Epoch: 3/6... Step: 465... Loss: 0.034778... Val Loss: 0.146090\n",
      "Epoch: 3/6... Step: 466... Loss: 0.025350... Val Loss: 0.153880\n",
      "Epoch: 3/6... Step: 467... Loss: 0.016462... Val Loss: 0.164320\n",
      "Epoch: 3/6... Step: 468... Loss: 0.021013... Val Loss: 0.178230\n",
      "Epoch: 3/6... Step: 469... Loss: 0.022135... Val Loss: 0.196580\n",
      "Epoch: 3/6... Step: 470... Loss: 0.029587... Val Loss: 0.221767\n",
      "Epoch: 3/6... Step: 471... Loss: 0.016680... Val Loss: 0.252733\n",
      "Epoch: 3/6... Step: 472... Loss: 0.009006... Val Loss: 0.285955\n",
      "Epoch: 3/6... Step: 473... Loss: 0.006402... Val Loss: 0.315834\n",
      "Epoch: 3/6... Step: 474... Loss: 0.003664... Val Loss: 0.342176\n",
      "Epoch: 3/6... Step: 475... Loss: 0.002720... Val Loss: 0.365397\n",
      "Epoch: 3/6... Step: 476... Loss: 0.005727... Val Loss: 0.385264\n",
      "Epoch: 3/6... Step: 477... Loss: 0.006933... Val Loss: 0.397871\n",
      "Epoch: 3/6... Step: 478... Loss: 0.006427... Val Loss: 0.404632\n",
      "Epoch: 3/6... Step: 479... Loss: 0.005698... Val Loss: 0.408244\n",
      "Epoch: 3/6... Step: 480... Loss: 0.007553... Val Loss: 0.412259\n",
      "Epoch: 3/6... Step: 481... Loss: 0.008336... Val Loss: 0.417576\n",
      "Epoch: 3/6... Step: 482... Loss: 0.063930... Val Loss: 0.412728\n",
      "Epoch: 3/6... Step: 483... Loss: 0.076883... Val Loss: 0.387384\n",
      "Epoch: 3/6... Step: 484... Loss: 0.027298... Val Loss: 0.367062\n",
      "Epoch: 3/6... Step: 485... Loss: 0.023864... Val Loss: 0.328148\n",
      "Epoch: 3/6... Step: 486... Loss: 0.059922... Val Loss: 0.243574\n",
      "Epoch: 3/6... Step: 487... Loss: 0.521339... Val Loss: 0.204915\n",
      "Epoch: 3/6... Step: 488... Loss: 0.255633... Val Loss: 0.212563\n",
      "Epoch: 3/6... Step: 489... Loss: 0.115263... Val Loss: 0.240268\n",
      "Epoch: 3/6... Step: 490... Loss: 0.147217... Val Loss: 0.295919\n",
      "Epoch: 3/6... Step: 491... Loss: 0.116617... Val Loss: 0.379994\n",
      "Epoch: 3/6... Step: 492... Loss: 0.063922... Val Loss: 0.479571\n",
      "Epoch: 3/6... Step: 493... Loss: 0.067336... Val Loss: 0.609392\n",
      "Epoch: 3/6... Step: 494... Loss: 0.063457... Val Loss: 0.756027\n",
      "Epoch: 3/6... Step: 495... Loss: 0.076824... Val Loss: 0.957064\n",
      "Epoch: 3/6... Step: 496... Loss: 0.085817... Val Loss: 1.215793\n",
      "Epoch: 3/6... Step: 497... Loss: 0.078316... Val Loss: 1.442265\n",
      "Epoch: 3/6... Step: 498... Loss: 0.081120... Val Loss: 1.721616\n",
      "Epoch: 3/6... Step: 499... Loss: 0.069984... Val Loss: 1.966258\n",
      "Epoch: 3/6... Step: 500... Loss: 0.043423... Val Loss: 2.187754\n",
      "Epoch: 3/6... Step: 501... Loss: 0.057452... Val Loss: 2.386224\n",
      "Epoch: 3/6... Step: 502... Loss: 0.073207... Val Loss: 2.523564\n",
      "Epoch: 3/6... Step: 503... Loss: 0.082061... Val Loss: 2.604152\n",
      "Epoch: 3/6... Step: 504... Loss: 0.074512... Val Loss: 2.640230\n",
      "Epoch: 3/6... Step: 505... Loss: 0.099469... Val Loss: 2.716885\n",
      "Epoch: 3/6... Step: 506... Loss: 0.104797... Val Loss: 2.829485\n",
      "Epoch: 3/6... Step: 507... Loss: 0.098948... Val Loss: 2.979238\n",
      "Epoch: 3/6... Step: 508... Loss: 0.102632... Val Loss: 3.142080\n",
      "Epoch: 3/6... Step: 509... Loss: 0.085357... Val Loss: 3.309342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/6... Step: 510... Loss: 0.077166... Val Loss: 3.448900\n",
      "Epoch: 3/6... Step: 511... Loss: 0.072962... Val Loss: 3.590691\n",
      "Epoch: 3/6... Step: 512... Loss: 0.134715... Val Loss: 3.773839\n",
      "Epoch: 3/6... Step: 513... Loss: 0.101245... Val Loss: 3.978736\n",
      "Epoch: 3/6... Step: 514... Loss: 0.087754... Val Loss: 4.184352\n",
      "Epoch: 3/6... Step: 515... Loss: 0.099325... Val Loss: 4.317513\n",
      "Epoch: 3/6... Step: 516... Loss: 0.135303... Val Loss: 4.345933\n",
      "Epoch: 3/6... Step: 517... Loss: 0.260969... Val Loss: 4.223079\n",
      "Epoch: 3/6... Step: 518... Loss: 0.170371... Val Loss: 4.016653\n",
      "Epoch: 3/6... Step: 519... Loss: 0.090579... Val Loss: 3.786461\n",
      "Epoch: 4/6... Step: 520... Loss: 0.029524... Val Loss: 3.638009\n",
      "Epoch: 4/6... Step: 521... Loss: 0.038961... Val Loss: 3.575088\n",
      "Epoch: 4/6... Step: 522... Loss: 0.039018... Val Loss: 3.585376\n",
      "Epoch: 4/6... Step: 523... Loss: 0.030939... Val Loss: 3.649883\n",
      "Epoch: 4/6... Step: 524... Loss: 0.114496... Val Loss: 3.808599\n",
      "Epoch: 4/6... Step: 525... Loss: 0.135374... Val Loss: 4.046287\n",
      "Epoch: 4/6... Step: 526... Loss: 0.124468... Val Loss: 4.321063\n",
      "Epoch: 4/6... Step: 527... Loss: 0.056074... Val Loss: 4.579763\n",
      "Epoch: 4/6... Step: 528... Loss: 0.046946... Val Loss: 4.831594\n",
      "Epoch: 4/6... Step: 529... Loss: 0.020169... Val Loss: 5.044295\n",
      "Epoch: 4/6... Step: 530... Loss: 0.032062... Val Loss: 5.168368\n",
      "Epoch: 4/6... Step: 531... Loss: 0.057418... Val Loss: 5.203870\n",
      "Epoch: 4/6... Step: 532... Loss: 0.129348... Val Loss: 5.111536\n",
      "Epoch: 4/6... Step: 533... Loss: 0.099766... Val Loss: 4.921220\n",
      "Epoch: 4/6... Step: 534... Loss: 0.089473... Val Loss: 4.645334\n",
      "Epoch: 4/6... Step: 535... Loss: 0.077680... Val Loss: 4.298426\n",
      "Epoch: 4/6... Step: 536... Loss: 0.085719... Val Loss: 3.838120\n",
      "Epoch: 4/6... Step: 537... Loss: 0.046315... Val Loss: 3.270976\n",
      "Epoch: 4/6... Step: 538... Loss: 0.045994... Val Loss: 2.675566\n",
      "Epoch: 4/6... Step: 539... Loss: 0.018182... Val Loss: 2.171253\n",
      "Epoch: 4/6... Step: 540... Loss: 0.015142... Val Loss: 1.809735\n",
      "Epoch: 4/6... Step: 541... Loss: 0.010100... Val Loss: 1.540232\n",
      "Epoch: 4/6... Step: 542... Loss: 0.014058... Val Loss: 1.345685\n",
      "Epoch: 4/6... Step: 543... Loss: 0.012384... Val Loss: 1.206661\n",
      "Epoch: 4/6... Step: 544... Loss: 0.015393... Val Loss: 1.114046\n",
      "Epoch: 4/6... Step: 545... Loss: 0.015986... Val Loss: 1.057805\n",
      "Epoch: 4/6... Step: 546... Loss: 0.010688... Val Loss: 1.006437\n",
      "Epoch: 4/6... Step: 547... Loss: 0.009418... Val Loss: 0.947672\n",
      "Epoch: 4/6... Step: 548... Loss: 0.007845... Val Loss: 0.890862\n",
      "Epoch: 4/6... Step: 549... Loss: 0.007409... Val Loss: 0.832867\n",
      "Epoch: 4/6... Step: 550... Loss: 0.005357... Val Loss: 0.776573\n",
      "Epoch: 4/6... Step: 551... Loss: 0.008075... Val Loss: 0.734514\n",
      "Epoch: 4/6... Step: 552... Loss: 0.006087... Val Loss: 0.704191\n",
      "Epoch: 4/6... Step: 553... Loss: 0.007339... Val Loss: 0.684749\n",
      "Epoch: 4/6... Step: 554... Loss: 0.004986... Val Loss: 0.672275\n",
      "Epoch: 4/6... Step: 555... Loss: 0.007627... Val Loss: 0.669666\n",
      "Epoch: 4/6... Step: 556... Loss: 0.018293... Val Loss: 0.683853\n",
      "Epoch: 4/6... Step: 557... Loss: 0.010519... Val Loss: 0.706209\n",
      "Epoch: 4/6... Step: 558... Loss: 0.017875... Val Loss: 0.744544\n",
      "Epoch: 4/6... Step: 559... Loss: 0.020193... Val Loss: 0.801633\n",
      "Epoch: 4/6... Step: 560... Loss: 0.022799... Val Loss: 0.878432\n",
      "Epoch: 4/6... Step: 561... Loss: 0.015389... Val Loss: 0.970460\n",
      "Epoch: 4/6... Step: 562... Loss: 0.008757... Val Loss: 1.060642\n",
      "Epoch: 4/6... Step: 563... Loss: 0.008335... Val Loss: 1.147038\n",
      "Epoch: 4/6... Step: 564... Loss: 0.008659... Val Loss: 1.231852\n",
      "Epoch: 4/6... Step: 565... Loss: 0.007417... Val Loss: 1.294857\n",
      "Epoch: 4/6... Step: 566... Loss: 0.014444... Val Loss: 1.340257\n",
      "Epoch: 4/6... Step: 567... Loss: 0.009453... Val Loss: 1.390087\n",
      "Epoch: 4/6... Step: 568... Loss: 0.033601... Val Loss: 1.379166\n",
      "Epoch: 4/6... Step: 569... Loss: 0.020719... Val Loss: 1.328041\n",
      "Epoch: 4/6... Step: 570... Loss: 0.029044... Val Loss: 1.232628\n",
      "Epoch: 4/6... Step: 571... Loss: 0.018494... Val Loss: 1.117719\n",
      "Epoch: 4/6... Step: 572... Loss: 0.008244... Val Loss: 1.005939\n",
      "Epoch: 4/6... Step: 573... Loss: 0.008375... Val Loss: 0.916402\n",
      "Epoch: 4/6... Step: 574... Loss: 0.009259... Val Loss: 0.849616\n",
      "Epoch: 4/6... Step: 575... Loss: 0.009751... Val Loss: 0.781849\n",
      "Epoch: 4/6... Step: 576... Loss: 0.009027... Val Loss: 0.710592\n",
      "Epoch: 4/6... Step: 577... Loss: 0.007967... Val Loss: 0.648739\n",
      "Epoch: 4/6... Step: 578... Loss: 0.006393... Val Loss: 0.604812\n",
      "Epoch: 4/6... Step: 579... Loss: 0.007364... Val Loss: 0.574063\n",
      "Epoch: 4/6... Step: 580... Loss: 0.011211... Val Loss: 0.558753\n",
      "Epoch: 4/6... Step: 581... Loss: 0.006011... Val Loss: 0.550915\n",
      "Epoch: 4/6... Step: 582... Loss: 0.008578... Val Loss: 0.534776\n",
      "Epoch: 4/6... Step: 583... Loss: 0.005366... Val Loss: 0.512463\n",
      "Epoch: 4/6... Step: 584... Loss: 0.014392... Val Loss: 0.476874\n",
      "Epoch: 4/6... Step: 585... Loss: 0.018728... Val Loss: 0.431199\n",
      "Epoch: 4/6... Step: 586... Loss: 0.014944... Val Loss: 0.382978\n",
      "Epoch: 4/6... Step: 587... Loss: 0.007388... Val Loss: 0.337509\n",
      "Epoch: 4/6... Step: 588... Loss: 0.004734... Val Loss: 0.299000\n",
      "Epoch: 4/6... Step: 589... Loss: 0.002737... Val Loss: 0.265617\n",
      "Epoch: 4/6... Step: 590... Loss: 0.002244... Val Loss: 0.238239\n",
      "Epoch: 4/6... Step: 591... Loss: 0.001928... Val Loss: 0.216893\n",
      "Epoch: 4/6... Step: 592... Loss: 0.005954... Val Loss: 0.200598\n",
      "Epoch: 4/6... Step: 593... Loss: 0.004548... Val Loss: 0.189580\n",
      "Epoch: 4/6... Step: 594... Loss: 0.004481... Val Loss: 0.182821\n",
      "Epoch: 4/6... Step: 595... Loss: 0.005474... Val Loss: 0.179993\n",
      "Epoch: 4/6... Step: 596... Loss: 0.004387... Val Loss: 0.179088\n",
      "Epoch: 4/6... Step: 597... Loss: 0.005240... Val Loss: 0.177489\n",
      "Epoch: 4/6... Step: 598... Loss: 0.003946... Val Loss: 0.174386\n",
      "Epoch: 4/6... Step: 599... Loss: 0.001504... Val Loss: 0.172215\n",
      "Epoch: 4/6... Step: 600... Loss: 0.032034... Val Loss: 0.168930\n",
      "Epoch: 4/6... Step: 601... Loss: 0.064830... Val Loss: 0.164623\n",
      "Epoch: 4/6... Step: 602... Loss: 0.092178... Val Loss: 0.159840\n",
      "Epoch: 4/6... Step: 603... Loss: 0.096153... Val Loss: 0.155589\n",
      "Epoch: 4/6... Step: 604... Loss: 0.084543... Val Loss: 0.152235\n",
      "Epoch: 4/6... Step: 605... Loss: 0.050725... Val Loss: 0.148392\n",
      "Epoch: 4/6... Step: 606... Loss: 0.022460... Val Loss: 0.143703\n",
      "Epoch: 4/6... Step: 607... Loss: 0.070051... Val Loss: 0.137770\n",
      "Epoch: 4/6... Step: 608... Loss: 0.066471... Val Loss: 0.132766\n",
      "Epoch: 4/6... Step: 609... Loss: 0.040219... Val Loss: 0.129557\n",
      "Epoch: 4/6... Step: 610... Loss: 0.035488... Val Loss: 0.128306\n",
      "Epoch: 4/6... Step: 611... Loss: 0.024090... Val Loss: 0.128642\n",
      "Epoch: 4/6... Step: 612... Loss: 0.004014... Val Loss: 0.129510\n",
      "Epoch: 4/6... Step: 613... Loss: 0.001686... Val Loss: 0.130514\n",
      "Epoch: 4/6... Step: 614... Loss: 0.001226... Val Loss: 0.131668\n",
      "Epoch: 4/6... Step: 615... Loss: 0.003405... Val Loss: 0.133156\n",
      "Epoch: 4/6... Step: 616... Loss: 0.004966... Val Loss: 0.135176\n",
      "Epoch: 4/6... Step: 617... Loss: 0.004659... Val Loss: 0.138399\n",
      "Epoch: 4/6... Step: 618... Loss: 0.004240... Val Loss: 0.141121\n",
      "Epoch: 4/6... Step: 619... Loss: 0.005230... Val Loss: 0.143116\n",
      "Epoch: 4/6... Step: 620... Loss: 0.002559... Val Loss: 0.144620\n",
      "Epoch: 4/6... Step: 621... Loss: 0.002157... Val Loss: 0.145840\n",
      "Epoch: 4/6... Step: 622... Loss: 0.007848... Val Loss: 0.144482\n",
      "Epoch: 4/6... Step: 623... Loss: 0.005502... Val Loss: 0.141809\n",
      "Epoch: 4/6... Step: 624... Loss: 0.011351... Val Loss: 0.137352\n",
      "Epoch: 4/6... Step: 625... Loss: 0.058361... Val Loss: 0.131574\n",
      "Epoch: 4/6... Step: 626... Loss: 0.006406... Val Loss: 0.125147\n",
      "Epoch: 4/6... Step: 627... Loss: 0.016185... Val Loss: 0.114867\n",
      "Epoch: 4/6... Step: 628... Loss: 0.013023... Val Loss: 0.100998\n",
      "Epoch: 4/6... Step: 629... Loss: 0.008502... Val Loss: 0.086447\n",
      "Epoch: 4/6... Step: 630... Loss: 0.079206... Val Loss: 0.071810\n",
      "Epoch: 4/6... Step: 631... Loss: 0.005987... Val Loss: 0.061502\n",
      "Epoch: 4/6... Step: 632... Loss: 0.010300... Val Loss: 0.055218\n",
      "Validation loss decreased (0.055470 --> 0.055218).  Saving model ...\n",
      "Epoch: 4/6... Step: 633... Loss: 0.015461... Val Loss: 0.052166\n",
      "Validation loss decreased (0.055218 --> 0.052166).  Saving model ...\n",
      "Epoch: 4/6... Step: 634... Loss: 0.012084... Val Loss: 0.051856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.052166 --> 0.051856).  Saving model ...\n",
      "Epoch: 4/6... Step: 635... Loss: 0.029236... Val Loss: 0.053672\n",
      "Epoch: 4/6... Step: 636... Loss: 0.054521... Val Loss: 0.057031\n",
      "Epoch: 4/6... Step: 637... Loss: 0.042994... Val Loss: 0.061124\n",
      "Epoch: 4/6... Step: 638... Loss: 0.049512... Val Loss: 0.065276\n",
      "Epoch: 4/6... Step: 639... Loss: 0.036642... Val Loss: 0.069133\n",
      "Epoch: 4/6... Step: 640... Loss: 0.025484... Val Loss: 0.072597\n",
      "Epoch: 4/6... Step: 641... Loss: 0.031957... Val Loss: 0.075531\n",
      "Epoch: 4/6... Step: 642... Loss: 0.032353... Val Loss: 0.077822\n",
      "Epoch: 4/6... Step: 643... Loss: 0.042948... Val Loss: 0.079307\n",
      "Epoch: 4/6... Step: 644... Loss: 0.033449... Val Loss: 0.080177\n",
      "Epoch: 4/6... Step: 645... Loss: 0.019033... Val Loss: 0.080806\n",
      "Epoch: 4/6... Step: 646... Loss: 0.007014... Val Loss: 0.081553\n",
      "Epoch: 4/6... Step: 647... Loss: 0.004605... Val Loss: 0.082419\n",
      "Epoch: 4/6... Step: 648... Loss: 0.003647... Val Loss: 0.083385\n",
      "Epoch: 4/6... Step: 649... Loss: 0.006461... Val Loss: 0.084433\n",
      "Epoch: 4/6... Step: 650... Loss: 0.002797... Val Loss: 0.085546\n",
      "Epoch: 4/6... Step: 651... Loss: 0.004802... Val Loss: 0.086685\n",
      "Epoch: 4/6... Step: 652... Loss: 0.004854... Val Loss: 0.087803\n",
      "Epoch: 4/6... Step: 653... Loss: 0.007871... Val Loss: 0.088883\n",
      "Epoch: 4/6... Step: 654... Loss: 0.009464... Val Loss: 0.089928\n",
      "Epoch: 4/6... Step: 655... Loss: 0.061709... Val Loss: 0.091028\n",
      "Epoch: 4/6... Step: 656... Loss: 0.088661... Val Loss: 0.091593\n",
      "Epoch: 4/6... Step: 657... Loss: 0.076557... Val Loss: 0.091050\n",
      "Epoch: 4/6... Step: 658... Loss: 0.033495... Val Loss: 0.089671\n",
      "Epoch: 4/6... Step: 659... Loss: 0.031013... Val Loss: 0.087642\n",
      "Epoch: 4/6... Step: 660... Loss: 0.684636... Val Loss: 0.084918\n",
      "Epoch: 4/6... Step: 661... Loss: 0.250578... Val Loss: 0.087152\n",
      "Epoch: 4/6... Step: 662... Loss: 0.079233... Val Loss: 0.096076\n",
      "Epoch: 4/6... Step: 663... Loss: 0.074003... Val Loss: 0.113229\n",
      "Epoch: 4/6... Step: 664... Loss: 0.093278... Val Loss: 0.144606\n",
      "Epoch: 4/6... Step: 665... Loss: 0.076261... Val Loss: 0.202975\n",
      "Epoch: 4/6... Step: 666... Loss: 0.089728... Val Loss: 0.308727\n",
      "Epoch: 4/6... Step: 667... Loss: 0.065992... Val Loss: 0.485942\n",
      "Epoch: 4/6... Step: 668... Loss: 0.106368... Val Loss: 0.768072\n",
      "Epoch: 4/6... Step: 669... Loss: 0.117828... Val Loss: 1.159605\n",
      "Epoch: 4/6... Step: 670... Loss: 0.091951... Val Loss: 1.601906\n",
      "Epoch: 4/6... Step: 671... Loss: 0.101078... Val Loss: 2.123343\n",
      "Epoch: 4/6... Step: 672... Loss: 0.056227... Val Loss: 2.680282\n",
      "Epoch: 4/6... Step: 673... Loss: 0.065198... Val Loss: 3.151307\n",
      "Epoch: 4/6... Step: 674... Loss: 0.060736... Val Loss: 3.532730\n",
      "Epoch: 4/6... Step: 675... Loss: 0.075097... Val Loss: 3.858098\n",
      "Epoch: 4/6... Step: 676... Loss: 0.067194... Val Loss: 4.120557\n",
      "Epoch: 4/6... Step: 677... Loss: 0.078664... Val Loss: 4.307272\n",
      "Epoch: 4/6... Step: 678... Loss: 0.078888... Val Loss: 4.464853\n",
      "Epoch: 4/6... Step: 679... Loss: 0.097462... Val Loss: 4.618053\n",
      "Epoch: 4/6... Step: 680... Loss: 0.105430... Val Loss: 4.781165\n",
      "Epoch: 4/6... Step: 681... Loss: 0.123456... Val Loss: 4.957272\n",
      "Epoch: 4/6... Step: 682... Loss: 0.095915... Val Loss: 5.127451\n",
      "Epoch: 4/6... Step: 683... Loss: 0.078828... Val Loss: 5.280299\n",
      "Epoch: 4/6... Step: 684... Loss: 0.076776... Val Loss: 5.422578\n",
      "Epoch: 4/6... Step: 685... Loss: 0.130791... Val Loss: 5.578364\n",
      "Epoch: 4/6... Step: 686... Loss: 0.086810... Val Loss: 5.745269\n",
      "Epoch: 4/6... Step: 687... Loss: 0.095177... Val Loss: 5.921037\n",
      "Epoch: 4/6... Step: 688... Loss: 0.072869... Val Loss: 6.056499\n",
      "Epoch: 4/6... Step: 689... Loss: 0.097712... Val Loss: 6.136685\n",
      "Epoch: 4/6... Step: 690... Loss: 0.233393... Val Loss: 6.128015\n",
      "Epoch: 4/6... Step: 691... Loss: 0.154069... Val Loss: 6.065977\n",
      "Epoch: 4/6... Step: 692... Loss: 0.082545... Val Loss: 5.979881\n",
      "Epoch: 5/6... Step: 693... Loss: 0.029391... Val Loss: 5.919162\n",
      "Epoch: 5/6... Step: 694... Loss: 0.059656... Val Loss: 5.888600\n",
      "Epoch: 5/6... Step: 695... Loss: 0.047143... Val Loss: 5.881550\n",
      "Epoch: 5/6... Step: 696... Loss: 0.043834... Val Loss: 5.894194\n",
      "Epoch: 5/6... Step: 697... Loss: 0.139638... Val Loss: 5.936985\n",
      "Epoch: 5/6... Step: 698... Loss: 0.177253... Val Loss: 6.013031\n",
      "Epoch: 5/6... Step: 699... Loss: 0.190474... Val Loss: 6.119840\n",
      "Epoch: 5/6... Step: 700... Loss: 0.128816... Val Loss: 6.246907\n",
      "Epoch: 5/6... Step: 701... Loss: 0.103142... Val Loss: 6.389560\n",
      "Epoch: 5/6... Step: 702... Loss: 0.065635... Val Loss: 6.539172\n",
      "Epoch: 5/6... Step: 703... Loss: 0.021730... Val Loss: 6.680461\n",
      "Epoch: 5/6... Step: 704... Loss: 0.016860... Val Loss: 6.810019\n",
      "Epoch: 5/6... Step: 705... Loss: 0.029543... Val Loss: 6.910057\n",
      "Epoch: 5/6... Step: 706... Loss: 0.020313... Val Loss: 6.987315\n",
      "Epoch: 5/6... Step: 707... Loss: 0.037689... Val Loss: 7.036980\n",
      "Epoch: 5/6... Step: 708... Loss: 0.048816... Val Loss: 7.056609\n",
      "Epoch: 5/6... Step: 709... Loss: 0.070555... Val Loss: 7.043937\n",
      "Epoch: 5/6... Step: 710... Loss: 0.069778... Val Loss: 7.001969\n",
      "Epoch: 5/6... Step: 711... Loss: 0.092674... Val Loss: 6.928126\n",
      "Epoch: 5/6... Step: 712... Loss: 0.066381... Val Loss: 6.831977\n",
      "Epoch: 5/6... Step: 713... Loss: 0.039478... Val Loss: 6.724621\n",
      "Epoch: 5/6... Step: 714... Loss: 0.068124... Val Loss: 6.596151\n",
      "Epoch: 5/6... Step: 715... Loss: 0.052564... Val Loss: 6.452588\n",
      "Epoch: 5/6... Step: 716... Loss: 0.055843... Val Loss: 6.292367\n",
      "Epoch: 5/6... Step: 717... Loss: 0.041520... Val Loss: 6.119652\n",
      "Epoch: 5/6... Step: 718... Loss: 0.033260... Val Loss: 5.938139\n",
      "Epoch: 5/6... Step: 719... Loss: 0.060808... Val Loss: 5.736355\n",
      "Epoch: 5/6... Step: 720... Loss: 0.047287... Val Loss: 5.518723\n",
      "Epoch: 5/6... Step: 721... Loss: 0.035675... Val Loss: 5.287557\n",
      "Epoch: 5/6... Step: 722... Loss: 0.030628... Val Loss: 5.038334\n",
      "Epoch: 5/6... Step: 723... Loss: 0.018187... Val Loss: 4.773914\n",
      "Epoch: 5/6... Step: 724... Loss: 0.009902... Val Loss: 4.498700\n",
      "Epoch: 5/6... Step: 725... Loss: 0.007214... Val Loss: 4.205131\n",
      "Epoch: 5/6... Step: 726... Loss: 0.006970... Val Loss: 3.889850\n",
      "Epoch: 5/6... Step: 727... Loss: 0.007818... Val Loss: 3.566805\n",
      "Epoch: 5/6... Step: 728... Loss: 0.005333... Val Loss: 3.271270\n",
      "Epoch: 5/6... Step: 729... Loss: 0.011558... Val Loss: 3.028354\n",
      "Epoch: 5/6... Step: 730... Loss: 0.010970... Val Loss: 2.832343\n",
      "Epoch: 5/6... Step: 731... Loss: 0.017957... Val Loss: 2.682090\n",
      "Epoch: 5/6... Step: 732... Loss: 0.019259... Val Loss: 2.571958\n",
      "Epoch: 5/6... Step: 733... Loss: 0.031690... Val Loss: 2.502524\n",
      "Epoch: 5/6... Step: 734... Loss: 0.028204... Val Loss: 2.465338\n",
      "Epoch: 5/6... Step: 735... Loss: 0.016023... Val Loss: 2.446215\n",
      "Epoch: 5/6... Step: 736... Loss: 0.015101... Val Loss: 2.445663\n",
      "Epoch: 5/6... Step: 737... Loss: 0.017392... Val Loss: 2.462570\n",
      "Epoch: 5/6... Step: 738... Loss: 0.007534... Val Loss: 2.486833\n",
      "Epoch: 5/6... Step: 739... Loss: 0.016905... Val Loss: 2.519237\n",
      "Epoch: 5/6... Step: 740... Loss: 0.017457... Val Loss: 2.566252\n",
      "Epoch: 5/6... Step: 741... Loss: 0.008497... Val Loss: 2.604232\n",
      "Epoch: 5/6... Step: 742... Loss: 0.003659... Val Loss: 2.636251\n",
      "Epoch: 5/6... Step: 743... Loss: 0.005405... Val Loss: 2.658696\n",
      "Epoch: 5/6... Step: 744... Loss: 0.006363... Val Loss: 2.674661\n",
      "Epoch: 5/6... Step: 745... Loss: 0.004317... Val Loss: 2.692283\n",
      "Epoch: 5/6... Step: 746... Loss: 0.015443... Val Loss: 2.721386\n",
      "Epoch: 5/6... Step: 747... Loss: 0.014973... Val Loss: 2.760956\n",
      "Epoch: 5/6... Step: 748... Loss: 0.005852... Val Loss: 2.798809\n",
      "Epoch: 5/6... Step: 749... Loss: 0.005691... Val Loss: 2.826215\n",
      "Epoch: 5/6... Step: 750... Loss: 0.009650... Val Loss: 2.846130\n",
      "Epoch: 5/6... Step: 751... Loss: 0.005202... Val Loss: 2.868573\n",
      "Epoch: 5/6... Step: 752... Loss: 0.005391... Val Loss: 2.891377\n",
      "Epoch: 5/6... Step: 753... Loss: 0.006334... Val Loss: 2.917239\n",
      "Epoch: 5/6... Step: 754... Loss: 0.003853... Val Loss: 2.940031\n",
      "Epoch: 5/6... Step: 755... Loss: 0.010307... Val Loss: 2.947220\n",
      "Epoch: 5/6... Step: 756... Loss: 0.006848... Val Loss: 2.941608\n",
      "Epoch: 5/6... Step: 757... Loss: 0.012396... Val Loss: 2.919496\n",
      "Epoch: 5/6... Step: 758... Loss: 0.017805... Val Loss: 2.878338\n",
      "Epoch: 5/6... Step: 759... Loss: 0.015242... Val Loss: 2.826323\n",
      "Epoch: 5/6... Step: 760... Loss: 0.010145... Val Loss: 2.765009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/6... Step: 761... Loss: 0.009480... Val Loss: 2.696949\n",
      "Epoch: 5/6... Step: 762... Loss: 0.011033... Val Loss: 2.619703\n",
      "Epoch: 5/6... Step: 763... Loss: 0.009661... Val Loss: 2.537090\n",
      "Epoch: 5/6... Step: 764... Loss: 0.007046... Val Loss: 2.453086\n",
      "Epoch: 5/6... Step: 765... Loss: 0.011145... Val Loss: 2.368673\n",
      "Epoch: 5/6... Step: 766... Loss: 0.005530... Val Loss: 2.286168\n",
      "Epoch: 5/6... Step: 767... Loss: 0.005236... Val Loss: 2.205125\n",
      "Epoch: 5/6... Step: 768... Loss: 0.003599... Val Loss: 2.128275\n",
      "Epoch: 5/6... Step: 769... Loss: 0.005486... Val Loss: 2.054234\n",
      "Epoch: 5/6... Step: 770... Loss: 0.008049... Val Loss: 1.981968\n",
      "Epoch: 5/6... Step: 771... Loss: 0.003545... Val Loss: 1.914936\n",
      "Epoch: 5/6... Step: 772... Loss: 0.003330... Val Loss: 1.857833\n",
      "Epoch: 5/6... Step: 773... Loss: 0.001295... Val Loss: 1.806795\n",
      "Epoch: 5/6... Step: 774... Loss: 0.003611... Val Loss: 1.761781\n",
      "Epoch: 5/6... Step: 775... Loss: 0.009367... Val Loss: 1.723219\n",
      "Epoch: 5/6... Step: 776... Loss: 0.008697... Val Loss: 1.691248\n",
      "Epoch: 5/6... Step: 777... Loss: 0.008679... Val Loss: 1.665312\n",
      "Epoch: 5/6... Step: 778... Loss: 0.026061... Val Loss: 1.641932\n",
      "Epoch: 5/6... Step: 779... Loss: 0.009151... Val Loss: 1.620040\n",
      "Epoch: 5/6... Step: 780... Loss: 0.045740... Val Loss: 1.598350\n",
      "Epoch: 5/6... Step: 781... Loss: 0.014586... Val Loss: 1.580124\n",
      "Epoch: 5/6... Step: 782... Loss: 0.005988... Val Loss: 1.565163\n",
      "Epoch: 5/6... Step: 783... Loss: 0.006140... Val Loss: 1.553585\n",
      "Epoch: 5/6... Step: 784... Loss: 0.004866... Val Loss: 1.545113\n",
      "Epoch: 5/6... Step: 785... Loss: 0.003713... Val Loss: 1.538787\n",
      "Epoch: 5/6... Step: 786... Loss: 0.004695... Val Loss: 1.533882\n",
      "Epoch: 5/6... Step: 787... Loss: 0.004561... Val Loss: 1.530232\n",
      "Epoch: 5/6... Step: 788... Loss: 0.005440... Val Loss: 1.527814\n",
      "Epoch: 5/6... Step: 789... Loss: 0.005465... Val Loss: 1.526225\n",
      "Epoch: 5/6... Step: 790... Loss: 0.004391... Val Loss: 1.522339\n",
      "Epoch: 5/6... Step: 791... Loss: 0.014632... Val Loss: 1.509456\n",
      "Epoch: 5/6... Step: 792... Loss: 0.014844... Val Loss: 1.487860\n",
      "Epoch: 5/6... Step: 793... Loss: 0.009700... Val Loss: 1.458657\n",
      "Epoch: 5/6... Step: 794... Loss: 0.006038... Val Loss: 1.425884\n",
      "Epoch: 5/6... Step: 795... Loss: 0.007900... Val Loss: 1.393388\n",
      "Epoch: 5/6... Step: 796... Loss: 0.005375... Val Loss: 1.364568\n",
      "Epoch: 5/6... Step: 797... Loss: 0.001356... Val Loss: 1.338761\n",
      "Epoch: 5/6... Step: 798... Loss: 0.069842... Val Loss: 1.314450\n",
      "Epoch: 5/6... Step: 799... Loss: 0.009240... Val Loss: 1.288648\n",
      "Epoch: 5/6... Step: 800... Loss: 0.049772... Val Loss: 1.245448\n",
      "Epoch: 5/6... Step: 801... Loss: 0.060572... Val Loss: 1.176558\n",
      "Epoch: 5/6... Step: 802... Loss: 0.052716... Val Loss: 1.086008\n",
      "Epoch: 5/6... Step: 803... Loss: 0.124999... Val Loss: 0.987368\n",
      "Epoch: 5/6... Step: 804... Loss: 0.005129... Val Loss: 0.902866\n",
      "Epoch: 5/6... Step: 805... Loss: 0.012442... Val Loss: 0.830302\n",
      "Epoch: 5/6... Step: 806... Loss: 0.006352... Val Loss: 0.767117\n",
      "Epoch: 5/6... Step: 807... Loss: 0.002304... Val Loss: 0.712103\n",
      "Epoch: 5/6... Step: 808... Loss: 0.008948... Val Loss: 0.663977\n",
      "Epoch: 5/6... Step: 809... Loss: 0.008795... Val Loss: 0.622286\n",
      "Epoch: 5/6... Step: 810... Loss: 0.006841... Val Loss: 0.586131\n",
      "Epoch: 5/6... Step: 811... Loss: 0.007633... Val Loss: 0.554856\n",
      "Epoch: 5/6... Step: 812... Loss: 0.004761... Val Loss: 0.527611\n",
      "Epoch: 5/6... Step: 813... Loss: 0.004006... Val Loss: 0.503842\n",
      "Epoch: 5/6... Step: 814... Loss: 0.006047... Val Loss: 0.483246\n",
      "Epoch: 5/6... Step: 815... Loss: 0.006625... Val Loss: 0.465601\n",
      "Epoch: 5/6... Step: 816... Loss: 0.014648... Val Loss: 0.450939\n",
      "Epoch: 5/6... Step: 817... Loss: 0.012832... Val Loss: 0.438903\n",
      "Epoch: 5/6... Step: 818... Loss: 0.009524... Val Loss: 0.428777\n",
      "Epoch: 5/6... Step: 819... Loss: 0.006414... Val Loss: 0.419869\n",
      "Epoch: 5/6... Step: 820... Loss: 0.005485... Val Loss: 0.412110\n",
      "Epoch: 5/6... Step: 821... Loss: 0.004714... Val Loss: 0.405398\n",
      "Epoch: 5/6... Step: 822... Loss: 0.008487... Val Loss: 0.399793\n",
      "Epoch: 5/6... Step: 823... Loss: 0.004537... Val Loss: 0.394960\n",
      "Epoch: 5/6... Step: 824... Loss: 0.007039... Val Loss: 0.390891\n",
      "Epoch: 5/6... Step: 825... Loss: 0.010197... Val Loss: 0.387844\n",
      "Epoch: 5/6... Step: 826... Loss: 0.013002... Val Loss: 0.385924\n",
      "Epoch: 5/6... Step: 827... Loss: 0.014842... Val Loss: 0.385202\n",
      "Epoch: 5/6... Step: 828... Loss: 0.063404... Val Loss: 0.384694\n",
      "Epoch: 5/6... Step: 829... Loss: 0.038614... Val Loss: 0.383298\n",
      "Epoch: 5/6... Step: 830... Loss: 0.022750... Val Loss: 0.381272\n",
      "Epoch: 5/6... Step: 831... Loss: 0.027439... Val Loss: 0.375610\n",
      "Epoch: 5/6... Step: 832... Loss: 0.044346... Val Loss: 0.363731\n",
      "Epoch: 5/6... Step: 833... Loss: 0.604132... Val Loss: 0.354636\n",
      "Epoch: 5/6... Step: 834... Loss: 0.100558... Val Loss: 0.348903\n",
      "Epoch: 5/6... Step: 835... Loss: 0.061614... Val Loss: 0.343247\n",
      "Epoch: 5/6... Step: 836... Loss: 0.056829... Val Loss: 0.338099\n",
      "Epoch: 5/6... Step: 837... Loss: 0.055363... Val Loss: 0.334240\n",
      "Epoch: 5/6... Step: 838... Loss: 0.058892... Val Loss: 0.328998\n",
      "Epoch: 5/6... Step: 839... Loss: 0.053278... Val Loss: 0.322157\n",
      "Epoch: 5/6... Step: 840... Loss: 0.068662... Val Loss: 0.313339\n",
      "Epoch: 5/6... Step: 841... Loss: 0.069088... Val Loss: 0.310252\n",
      "Epoch: 5/6... Step: 842... Loss: 0.089731... Val Loss: 0.314685\n",
      "Epoch: 5/6... Step: 843... Loss: 0.081633... Val Loss: 0.321168\n",
      "Epoch: 5/6... Step: 844... Loss: 0.148469... Val Loss: 0.338235\n",
      "Epoch: 5/6... Step: 845... Loss: 0.070222... Val Loss: 0.359278\n",
      "Epoch: 5/6... Step: 846... Loss: 0.069541... Val Loss: 0.383408\n",
      "Epoch: 5/6... Step: 847... Loss: 0.100395... Val Loss: 0.410810\n",
      "Epoch: 5/6... Step: 848... Loss: 0.065585... Val Loss: 0.438191\n",
      "Epoch: 5/6... Step: 849... Loss: 0.057495... Val Loss: 0.466181\n",
      "Epoch: 5/6... Step: 850... Loss: 0.073608... Val Loss: 0.495384\n",
      "Epoch: 5/6... Step: 851... Loss: 0.137085... Val Loss: 0.527720\n",
      "Epoch: 5/6... Step: 852... Loss: 0.172975... Val Loss: 0.563502\n",
      "Epoch: 5/6... Step: 853... Loss: 0.185982... Val Loss: 0.602479\n",
      "Epoch: 5/6... Step: 854... Loss: 0.149266... Val Loss: 0.644200\n",
      "Epoch: 5/6... Step: 855... Loss: 0.145199... Val Loss: 0.687972\n",
      "Epoch: 5/6... Step: 856... Loss: 0.061628... Val Loss: 0.731823\n",
      "Epoch: 5/6... Step: 857... Loss: 0.096132... Val Loss: 0.776529\n",
      "Epoch: 5/6... Step: 858... Loss: 0.171095... Val Loss: 0.823230\n",
      "Epoch: 5/6... Step: 859... Loss: 0.136175... Val Loss: 0.870834\n",
      "Epoch: 5/6... Step: 860... Loss: 0.136086... Val Loss: 0.918999\n",
      "Epoch: 5/6... Step: 861... Loss: 0.069239... Val Loss: 0.964997\n",
      "Epoch: 5/6... Step: 862... Loss: 0.056122... Val Loss: 1.007523\n",
      "Epoch: 5/6... Step: 863... Loss: 0.114861... Val Loss: 1.043817\n",
      "Epoch: 5/6... Step: 864... Loss: 0.076293... Val Loss: 1.075680\n",
      "Epoch: 5/6... Step: 865... Loss: 0.067865... Val Loss: 1.104041\n",
      "Epoch: 6/6... Step: 866... Loss: 0.072781... Val Loss: 1.135128\n",
      "Epoch: 6/6... Step: 867... Loss: 0.121904... Val Loss: 1.170538\n",
      "Epoch: 6/6... Step: 868... Loss: 0.111735... Val Loss: 1.210076\n",
      "Epoch: 6/6... Step: 869... Loss: 0.120064... Val Loss: 1.253942\n",
      "Epoch: 6/6... Step: 870... Loss: 0.229180... Val Loss: 1.303922\n",
      "Epoch: 6/6... Step: 871... Loss: 0.274891... Val Loss: 1.360678\n",
      "Epoch: 6/6... Step: 872... Loss: 0.301444... Val Loss: 1.424417\n",
      "Epoch: 6/6... Step: 873... Loss: 0.261593... Val Loss: 1.494510\n",
      "Epoch: 6/6... Step: 874... Loss: 0.259164... Val Loss: 1.571220\n",
      "Epoch: 6/6... Step: 875... Loss: 0.237181... Val Loss: 1.654365\n",
      "Epoch: 6/6... Step: 876... Loss: 0.154186... Val Loss: 1.742310\n",
      "Epoch: 6/6... Step: 877... Loss: 0.131647... Val Loss: 1.834466\n",
      "Epoch: 6/6... Step: 878... Loss: 0.068464... Val Loss: 1.928854\n",
      "Epoch: 6/6... Step: 879... Loss: 0.076729... Val Loss: 2.025674\n",
      "Epoch: 6/6... Step: 880... Loss: 0.070560... Val Loss: 2.124517\n",
      "Epoch: 6/6... Step: 881... Loss: 0.045404... Val Loss: 2.224118\n",
      "Epoch: 6/6... Step: 882... Loss: 0.024078... Val Loss: 2.322792\n",
      "Epoch: 6/6... Step: 883... Loss: 0.027342... Val Loss: 2.421071\n",
      "Epoch: 6/6... Step: 884... Loss: 0.014933... Val Loss: 2.517068\n",
      "Epoch: 6/6... Step: 885... Loss: 0.020327... Val Loss: 2.611468\n",
      "Epoch: 6/6... Step: 886... Loss: 0.035095... Val Loss: 2.705844\n",
      "Epoch: 6/6... Step: 887... Loss: 0.014379... Val Loss: 2.797598\n",
      "Epoch: 6/6... Step: 888... Loss: 0.012014... Val Loss: 2.886763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/6... Step: 889... Loss: 0.009263... Val Loss: 2.972268\n",
      "Epoch: 6/6... Step: 890... Loss: 0.009320... Val Loss: 3.053525\n",
      "Epoch: 6/6... Step: 891... Loss: 0.008460... Val Loss: 3.130094\n",
      "Epoch: 6/6... Step: 892... Loss: 0.017509... Val Loss: 3.199204\n",
      "Epoch: 6/6... Step: 893... Loss: 0.016109... Val Loss: 3.260397\n",
      "Epoch: 6/6... Step: 894... Loss: 0.016939... Val Loss: 3.313625\n",
      "Epoch: 6/6... Step: 895... Loss: 0.018100... Val Loss: 3.359027\n",
      "Epoch: 6/6... Step: 896... Loss: 0.019354... Val Loss: 3.396821\n",
      "Epoch: 6/6... Step: 897... Loss: 0.013812... Val Loss: 3.428848\n",
      "Epoch: 6/6... Step: 898... Loss: 0.011849... Val Loss: 3.455461\n",
      "Epoch: 6/6... Step: 899... Loss: 0.016064... Val Loss: 3.476112\n",
      "Epoch: 6/6... Step: 900... Loss: 0.017571... Val Loss: 3.490773\n",
      "Epoch: 6/6... Step: 901... Loss: 0.016253... Val Loss: 3.500349\n",
      "Epoch: 6/6... Step: 902... Loss: 0.014160... Val Loss: 3.506111\n",
      "Epoch: 6/6... Step: 903... Loss: 0.014522... Val Loss: 3.508133\n",
      "Epoch: 6/6... Step: 904... Loss: 0.017481... Val Loss: 3.506543\n",
      "Epoch: 6/6... Step: 905... Loss: 0.018740... Val Loss: 3.501487\n",
      "Epoch: 6/6... Step: 906... Loss: 0.010935... Val Loss: 3.494786\n",
      "Epoch: 6/6... Step: 907... Loss: 0.013640... Val Loss: 3.485983\n",
      "Epoch: 6/6... Step: 908... Loss: 0.020229... Val Loss: 3.474164\n",
      "Epoch: 6/6... Step: 909... Loss: 0.017879... Val Loss: 3.459949\n",
      "Epoch: 6/6... Step: 910... Loss: 0.017142... Val Loss: 3.443257\n",
      "Epoch: 6/6... Step: 911... Loss: 0.016596... Val Loss: 3.424264\n",
      "Epoch: 6/6... Step: 912... Loss: 0.026866... Val Loss: 3.402949\n",
      "Epoch: 6/6... Step: 913... Loss: 0.013681... Val Loss: 3.380441\n",
      "Epoch: 6/6... Step: 914... Loss: 0.035921... Val Loss: 3.354360\n",
      "Epoch: 6/6... Step: 915... Loss: 0.029769... Val Loss: 3.325151\n",
      "Epoch: 6/6... Step: 916... Loss: 0.029882... Val Loss: 3.293126\n",
      "Epoch: 6/6... Step: 917... Loss: 0.025652... Val Loss: 3.259374\n",
      "Epoch: 6/6... Step: 918... Loss: 0.018584... Val Loss: 3.224775\n",
      "Epoch: 6/6... Step: 919... Loss: 0.016399... Val Loss: 3.190539\n",
      "Epoch: 6/6... Step: 920... Loss: 0.018112... Val Loss: 3.156522\n",
      "Epoch: 6/6... Step: 921... Loss: 0.027847... Val Loss: 3.121082\n",
      "Epoch: 6/6... Step: 922... Loss: 0.019238... Val Loss: 3.085061\n",
      "Epoch: 6/6... Step: 923... Loss: 0.020533... Val Loss: 3.048993\n",
      "Epoch: 6/6... Step: 924... Loss: 0.015470... Val Loss: 3.013258\n",
      "Epoch: 6/6... Step: 925... Loss: 0.014581... Val Loss: 2.977872\n",
      "Epoch: 6/6... Step: 926... Loss: 0.014728... Val Loss: 2.943326\n",
      "Epoch: 6/6... Step: 927... Loss: 0.010815... Val Loss: 2.909566\n",
      "Epoch: 6/6... Step: 928... Loss: 0.025980... Val Loss: 2.875419\n",
      "Epoch: 6/6... Step: 929... Loss: 0.020399... Val Loss: 2.841077\n",
      "Epoch: 6/6... Step: 930... Loss: 0.023023... Val Loss: 2.806620\n",
      "Epoch: 6/6... Step: 931... Loss: 0.027404... Val Loss: 2.772089\n",
      "Epoch: 6/6... Step: 932... Loss: 0.022401... Val Loss: 2.738155\n",
      "Epoch: 6/6... Step: 933... Loss: 0.016451... Val Loss: 2.704879\n",
      "Epoch: 6/6... Step: 934... Loss: 0.011943... Val Loss: 2.672735\n",
      "Epoch: 6/6... Step: 935... Loss: 0.008676... Val Loss: 2.641742\n",
      "Epoch: 6/6... Step: 936... Loss: 0.008269... Val Loss: 2.611934\n",
      "Epoch: 6/6... Step: 937... Loss: 0.007633... Val Loss: 2.583280\n",
      "Epoch: 6/6... Step: 938... Loss: 0.013827... Val Loss: 2.555703\n",
      "Epoch: 6/6... Step: 939... Loss: 0.009785... Val Loss: 2.529082\n",
      "Epoch: 6/6... Step: 940... Loss: 0.008506... Val Loss: 2.503415\n",
      "Epoch: 6/6... Step: 941... Loss: 0.010657... Val Loss: 2.478422\n",
      "Epoch: 6/6... Step: 942... Loss: 0.011940... Val Loss: 2.454126\n",
      "Epoch: 6/6... Step: 943... Loss: 0.013909... Val Loss: 2.430679\n",
      "Epoch: 6/6... Step: 944... Loss: 0.003411... Val Loss: 2.408929\n",
      "Epoch: 6/6... Step: 945... Loss: 0.004164... Val Loss: 2.389202\n",
      "Epoch: 6/6... Step: 946... Loss: 0.004635... Val Loss: 2.371029\n",
      "Epoch: 6/6... Step: 947... Loss: 0.015314... Val Loss: 2.354463\n",
      "Epoch: 6/6... Step: 948... Loss: 0.026209... Val Loss: 2.339570\n",
      "Epoch: 6/6... Step: 949... Loss: 0.023473... Val Loss: 2.326360\n",
      "Epoch: 6/6... Step: 950... Loss: 0.025172... Val Loss: 2.314769\n",
      "Epoch: 6/6... Step: 951... Loss: 0.059065... Val Loss: 2.304216\n",
      "Epoch: 6/6... Step: 952... Loss: 0.049422... Val Loss: 2.294259\n",
      "Epoch: 6/6... Step: 953... Loss: 0.087132... Val Loss: 2.284749\n",
      "Epoch: 6/6... Step: 954... Loss: 0.039544... Val Loss: 2.276244\n",
      "Epoch: 6/6... Step: 955... Loss: 0.025346... Val Loss: 2.268850\n",
      "Epoch: 6/6... Step: 956... Loss: 0.027958... Val Loss: 2.262562\n",
      "Epoch: 6/6... Step: 957... Loss: 0.031140... Val Loss: 2.257305\n",
      "Epoch: 6/6... Step: 958... Loss: 0.022965... Val Loss: 2.252796\n",
      "Epoch: 6/6... Step: 959... Loss: 0.025828... Val Loss: 2.248850\n",
      "Epoch: 6/6... Step: 960... Loss: 0.027162... Val Loss: 2.245371\n",
      "Epoch: 6/6... Step: 961... Loss: 0.026132... Val Loss: 2.242406\n",
      "Epoch: 6/6... Step: 962... Loss: 0.017171... Val Loss: 2.239994\n",
      "Epoch: 6/6... Step: 963... Loss: 0.005886... Val Loss: 2.237990\n",
      "Epoch: 6/6... Step: 964... Loss: 0.006214... Val Loss: 2.236038\n",
      "Epoch: 6/6... Step: 965... Loss: 0.008447... Val Loss: 2.234176\n",
      "Epoch: 6/6... Step: 966... Loss: 0.003963... Val Loss: 2.232566\n",
      "Epoch: 6/6... Step: 967... Loss: 0.003985... Val Loss: 2.231318\n",
      "Epoch: 6/6... Step: 968... Loss: 0.018095... Val Loss: 2.230737\n",
      "Epoch: 6/6... Step: 969... Loss: 0.027883... Val Loss: 2.230660\n",
      "Epoch: 6/6... Step: 970... Loss: 0.017443... Val Loss: 2.230800\n",
      "Epoch: 6/6... Step: 971... Loss: 0.097223... Val Loss: 2.230959\n",
      "Epoch: 6/6... Step: 972... Loss: 0.008478... Val Loss: 2.230949\n",
      "Epoch: 6/6... Step: 973... Loss: 0.045646... Val Loss: 2.229431\n",
      "Epoch: 6/6... Step: 974... Loss: 0.060069... Val Loss: 2.225557\n",
      "Epoch: 6/6... Step: 975... Loss: 0.057491... Val Loss: 2.219406\n",
      "Epoch: 6/6... Step: 976... Loss: 0.139006... Val Loss: 2.212638\n",
      "Epoch: 6/6... Step: 977... Loss: 0.018818... Val Loss: 2.206982\n",
      "Epoch: 6/6... Step: 978... Loss: 0.035219... Val Loss: 2.202073\n",
      "Epoch: 6/6... Step: 979... Loss: 0.026152... Val Loss: 2.197664\n",
      "Epoch: 6/6... Step: 980... Loss: 0.025815... Val Loss: 2.193632\n",
      "Epoch: 6/6... Step: 981... Loss: 0.033715... Val Loss: 2.189830\n",
      "Epoch: 6/6... Step: 982... Loss: 0.024004... Val Loss: 2.186160\n",
      "Epoch: 6/6... Step: 983... Loss: 0.028176... Val Loss: 2.182528\n",
      "Epoch: 6/6... Step: 984... Loss: 0.022330... Val Loss: 2.178959\n",
      "Epoch: 6/6... Step: 985... Loss: 0.022610... Val Loss: 2.175436\n",
      "Epoch: 6/6... Step: 986... Loss: 0.022479... Val Loss: 2.171977\n",
      "Epoch: 6/6... Step: 987... Loss: 0.022020... Val Loss: 2.168600\n",
      "Epoch: 6/6... Step: 988... Loss: 0.020832... Val Loss: 2.165292\n",
      "Epoch: 6/6... Step: 989... Loss: 0.017070... Val Loss: 2.162087\n",
      "Epoch: 6/6... Step: 990... Loss: 0.017489... Val Loss: 2.158963\n",
      "Epoch: 6/6... Step: 991... Loss: 0.021226... Val Loss: 2.155919\n",
      "Epoch: 6/6... Step: 992... Loss: 0.024466... Val Loss: 2.152953\n",
      "Epoch: 6/6... Step: 993... Loss: 0.020981... Val Loss: 2.150114\n",
      "Epoch: 6/6... Step: 994... Loss: 0.013179... Val Loss: 2.147423\n",
      "Epoch: 6/6... Step: 995... Loss: 0.017984... Val Loss: 2.144850\n",
      "Epoch: 6/6... Step: 996... Loss: 0.017484... Val Loss: 2.142377\n",
      "Epoch: 6/6... Step: 997... Loss: 0.016975... Val Loss: 2.140013\n",
      "Epoch: 6/6... Step: 998... Loss: 0.016098... Val Loss: 2.137752\n",
      "Epoch: 6/6... Step: 999... Loss: 0.018200... Val Loss: 2.135585\n",
      "Epoch: 6/6... Step: 1000... Loss: 0.017459... Val Loss: 2.133521\n",
      "Epoch: 6/6... Step: 1001... Loss: 0.079820... Val Loss: 2.131504\n",
      "Epoch: 6/6... Step: 1002... Loss: 0.055903... Val Loss: 2.129584\n",
      "Epoch: 6/6... Step: 1003... Loss: 0.030556... Val Loss: 2.127763\n",
      "Epoch: 6/6... Step: 1004... Loss: 0.082387... Val Loss: 2.125586\n",
      "Epoch: 6/6... Step: 1005... Loss: 0.145647... Val Loss: 2.122725\n",
      "Epoch: 6/6... Step: 1006... Loss: 0.717887... Val Loss: 2.119850\n",
      "Epoch: 6/6... Step: 1007... Loss: 0.092402... Val Loss: 2.117232\n",
      "Epoch: 6/6... Step: 1008... Loss: 0.142047... Val Loss: 2.114759\n",
      "Epoch: 6/6... Step: 1009... Loss: 0.117916... Val Loss: 2.112456\n",
      "Epoch: 6/6... Step: 1010... Loss: 0.140658... Val Loss: 2.110301\n",
      "Epoch: 6/6... Step: 1011... Loss: 0.172502... Val Loss: 2.108252\n",
      "Epoch: 6/6... Step: 1012... Loss: 0.180648... Val Loss: 2.106311\n",
      "Epoch: 6/6... Step: 1013... Loss: 0.176939... Val Loss: 2.104457\n",
      "Epoch: 6/6... Step: 1014... Loss: 0.133828... Val Loss: 2.102727\n",
      "Epoch: 6/6... Step: 1015... Loss: 0.137073... Val Loss: 2.101133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/6... Step: 1016... Loss: 0.200139... Val Loss: 2.099636\n",
      "Epoch: 6/6... Step: 1017... Loss: 0.087851... Val Loss: 2.098318\n",
      "Epoch: 6/6... Step: 1018... Loss: 0.102218... Val Loss: 2.097134\n",
      "Epoch: 6/6... Step: 1019... Loss: 0.085708... Val Loss: 2.096076\n",
      "Epoch: 6/6... Step: 1020... Loss: 0.079524... Val Loss: 2.095145\n",
      "Epoch: 6/6... Step: 1021... Loss: 0.109998... Val Loss: 2.094313\n",
      "Epoch: 6/6... Step: 1022... Loss: 0.112367... Val Loss: 2.093580\n",
      "Epoch: 6/6... Step: 1023... Loss: 0.100073... Val Loss: 2.092942\n",
      "Epoch: 6/6... Step: 1024... Loss: 0.082524... Val Loss: 2.092414\n",
      "Epoch: 6/6... Step: 1025... Loss: 0.086252... Val Loss: 2.091984\n",
      "Epoch: 6/6... Step: 1026... Loss: 0.087686... Val Loss: 2.091640\n",
      "Epoch: 6/6... Step: 1027... Loss: 0.078465... Val Loss: 2.091366\n",
      "Epoch: 6/6... Step: 1028... Loss: 0.084112... Val Loss: 2.091152\n",
      "Epoch: 6/6... Step: 1029... Loss: 0.070805... Val Loss: 2.090984\n",
      "Epoch: 6/6... Step: 1030... Loss: 0.065074... Val Loss: 2.090858\n",
      "Epoch: 6/6... Step: 1031... Loss: 0.099829... Val Loss: 2.090770\n",
      "Epoch: 6/6... Step: 1032... Loss: 0.096563... Val Loss: 2.090710\n",
      "Epoch: 6/6... Step: 1033... Loss: 0.090892... Val Loss: 2.090671\n",
      "Epoch: 6/6... Step: 1034... Loss: 0.081131... Val Loss: 2.090646\n",
      "Epoch: 6/6... Step: 1035... Loss: 0.090801... Val Loss: 2.090631\n",
      "Epoch: 6/6... Step: 1036... Loss: 0.157998... Val Loss: 2.090623\n",
      "Epoch: 6/6... Step: 1037... Loss: 0.126887... Val Loss: 2.090621\n",
      "Epoch: 6/6... Step: 1038... Loss: 0.091090... Val Loss: 2.090620\n"
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "counter = 0\n",
    "print_every = 1\n",
    "# clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                   steps_per_epoch=len(train_loader),\n",
    "                                                   epochs=epochs)\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(BATCH_SIZE)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(BATCH_SIZE)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(DEVICE), lab.to(DEVICE)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) < valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr=0.0001\n",
    "# optimizer.lr = lr\n",
    "# # when doing this, commetn out the valid_loss_min = np.Inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Visualizer:\n",
    "    colors=['blue','black', 'red']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.date_pred_targ_dict: dict = dict()\n",
    "        \n",
    "    def add(self, timestamp, pred, targ, color='red'):\n",
    "        self.date_pred_targ_dict[color] = pd.concat([self.date_pred_targ_dict.get(color, pd.DataFrame()),\n",
    "                                                     pd.concat([pd.DataFrame(timestamp),\n",
    "                                                       pd.DataFrame(pred),\n",
    "                                                       pd.DataFrame(targ)], axis=1)])\n",
    "    \n",
    "    def plot(self):\n",
    "        for color in self.colors:\n",
    "            for row in tqdm(self.date_pred_targ_dict.get(color, pd.DataFrame()).itertuples()):\n",
    "                plt.scatter(row[1], row[-2], color=color)\n",
    "                plt.scatter(row[1], row[-1], color='g')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 173/173 [00:09<00:00, 17.74it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 34/34 [00:01<00:00, 17.66it/s]\n",
      "  0%|                                                                                           | 0/24 [00:00<?, ?it/s]C:\\Users\\octav\\Miniconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\loss.py:88: UserWarning: Using a target size (torch.Size([120, 1])) that is different to the input size (torch.Size([120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\octav\\Miniconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\functional.py:1946: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 24/24 [00:01<00:00, 16.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE loss: 0.03238\n",
      "MAE loss: 0.15001\n",
      "KLDiv loss: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('./state_dict.pt'))\n",
    "visualizer = Visualizer()\n",
    "\n",
    "mse_losses = []\n",
    "mae_losses = []\n",
    "kldiv_losses = []\n",
    "num_correct = 0\n",
    "custom_batch = BATCH_SIZE\n",
    "\n",
    "h = model.init_hidden(custom_batch)\n",
    "\n",
    "model.eval()\n",
    "for inputs, labels in tqdm(train_loader):\n",
    "    if inputs.shape[0] != custom_batch:\n",
    "        continue\n",
    "    inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "    output, h = model(inputs, h)\n",
    "    visualizer.add(inputs[:,-1], output, labels[:,-1], color='black')\n",
    "    \n",
    "for inputs, labels in tqdm(val_loader):\n",
    "    if inputs.shape[0] != custom_batch:\n",
    "        continue\n",
    "    inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "    output, h = model(inputs, h)\n",
    "    visualizer.add(inputs[:,-1], output, labels[:,-1], color='blue')\n",
    "    \n",
    "for inputs, labels in tqdm(test_loader):\n",
    "    if inputs.shape[0] != custom_batch:\n",
    "        continue\n",
    "    inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "    output, h = model(inputs, h)\n",
    "    visualizer.add(inputs[:,-1], output, labels[:,-1])\n",
    "    mse_losses.append(nn.MSELoss()(output.squeeze(), labels.float()).item())\n",
    "    mae_losses.append(nn.L1Loss()(output.squeeze(), labels.float()).item())\n",
    "    kldiv_losses.append(nn.KLDivLoss()(output.squeeze(), labels.float()).item())\n",
    "    \n",
    "\n",
    "print(\"MSE loss: {:.5f}\".format(np.mean(mse_losses)))\n",
    "print(\"MAE loss: {:.5f}\".format(np.mean(mae_losses)))\n",
    "print(\"KLDiv loss: {:.5f}\".format(np.mean(kldiv_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:05, 176.78it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "# visualizer.plot()\n",
    "for color in ['blue']:\n",
    "    for row in tqdm(visualizer2.date_pred_targ_dict.get(color, pd.DataFrame())[:1000].itertuples()):\n",
    "        plt.scatter(row[1], row[-2], color=color)\n",
    "        plt.scatter(row[1], row[-1], color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4080it [00:53, 76.40it/s] \n",
      "6885it [04:24, 19.40it/s]"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "visualizer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 235.92it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJMAAAJCCAYAAAB0wYY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3W2MpF1+H+T/KU+TUFbSjvETs8uqqwBZiiWPsljFipUlY7vXIXE8rHfFokiVyJFAJSCgPCC/qj6sB1Fg3uRGfIgoDLElbgmQRWTGa9nJdlgt/mKrB9butTaWQXS11jbexyhpIUqKx+nDh+qe6Z7pnjo1dXe93HVd0qiePn266vRMT88zvznnd1LOOQAAAACgRGvdCwAAAABgewiTAAAAACgmTAIAAACgmDAJAAAAgGLCJAAAAACKCZMAAAAAKCZMAgAAAKCYMAkAAACAYsIkAAAAAIo9WvcC3sU3fdM35W63u+5lAAAAADTG8+fP/yDn/N68eVsZJnW73Tg5OVn3MgAAAAAaI6U0KZnnmBsAAAAAxYRJAAAAABQTJgEAAABQTJgEAAAAQDFhEgAAAADFhEkAAAAAFBMmAQAAAFBMmAQAAABAMWESAAAAAMWESQAAAAAUEyYBAAAAUEyYBAAAAEAxYRIAAAAAxYRJAAAAABQTJgEAAABQTJgEAAAAQDFhEgAAAADFhEkAAAAAFBMmAQAAAFBMmAQAAABAMWESAAAAAMWESQAAAAAUEyYBAAAAUEyYBAAAADReVUV0uxGt1uyxqta9ou31aN0LAAAAAHhIVRUxGERMp7O3J5PZ2xER/f761rWt7EwCAAAAGm04fBUkXZtOZ+MsTpgEAAAANNr5+WLjvJ0wCQAAAGi0g4PFxnk7YRIAAADQaKNRRLt9e6zdno2zOGESAAAA0Gj9fsR4HNHpRKQ0exyPlW+/K7e5AQAAAI3X7wuP6mJnEgAAAADFhEkAAADAzquqiG43otWaPVbVule0uRxzAwAAAHZaVUUMBhHT6eztyWT2doSjcXexMwkAAADYacPhqyDp2nQ6G+dNwiQAAABgqy17RO38fLHxXSdMAgAAALbW9RG1ySQi51dH1BYJlA4OFhvfdcIkAAAAYGvVcURtNIpot2+Ptduzcd4kTAIAAAC2Vh1H1Pr9iPE4otOJSGn2OB4r376P29wAAACArXVwMDvadtf4Ivp94VEpO5MAAACArbXKI2rLFn03hTAJAAB4cP4CBjyUVR1Rq6PouylSznnda1hYr9fLJycn614GAABQ4PovYDcLctttfSTAdul27z5O1+lEnJ2tejUPI6X0POfcmzfPziQAAOBB1XHTEsC61VH03RTCJAAA4EH5CxjQBPcVei9a9N0EwiQAAOBB+QsY0ASrLPredMIkAADgQfkLGNAEqyr63gbCJAAA4EH5CxiwjE26DbLfn5VtX17OHnf1+9ijdS8AAABovn5/d//SBby712+DnExmb0f4nrJOdiYBAAAAG8ltkJtJmAQAAABsJLdBbiZhEgAAALCR3Aa5mYRJAAAAwEZyG+RmEiYBAAAAG8ltkJtJmAQAAAAsrKoiut2IVmv2WFUP8zr9fsTZWcTl5exRkLR+wiQAAABgIVUVMRhETCYROc8eB4PFA6VVBVLUS5gEAAAA3DIv5BkOI6bT22PT6Wx8kdeoI5Bi9VLOed1rWFiv18snJyfrXgYAAAA0znXIczMsardvdxW1WrMA6HUpzY6jleh2ZwHS6zqd2XE2Vi+l9Dzn3Js3z84kAABg7UqOujgOA6tRsuvo4ODuj71v/C7n54uNszmESQAAwFqVHHVxHAZWpyTkGY1mu5Vuardn46XqCKRYD2ESAACwlGV3DJXsgqijnwUoUxLy9PuzY2+dzuxoW6dz+xhciToCKdZDZxIAAPDOSrpV5inpXqmjnwUoU8fv60Veazic7Xo6OJgFSXW/BuV0JgEAAA+ujh1DJbsgSo/D6FWC5dWx62iR1zo7m4XCZ2eCpG2xVJiUUvpMSuk3U0qXKaU7k6uU0h9PKf1aSunXr+Y+vfG+n0kp/V8ppS9d/fjoMusBAABWq44C3ZKjLiVz9CpBfeoIeYS7zbXszqQvR8SnI+KLb5nzDyPie3LOfzYiPhoRfz6l9C/ceP8P55w/evXjS0uuBwAAWKE6CnRLdkGUzNGrBJtDuNtstXQmpZS+EBE/lHN+a5FRSqkdEb8SEf9mzvlXU0o/ExG/kHP+uUVeT2cSAABshlV2q8yjVwk2R7c7C5Be1+nMdjqxmTaqMyml9HUppS9FxNci4u/knH/1xrtHKaXfSCn9VErpj61iPQAAQD1W2a0yj2vGYXPUcQSWzTU3TEopfT6l9OU7fnyy9EVyzv8o5/zRiPhIRHwspfRtV+/68Yj4MxHxz0fEN0bEj75lHYOU0klK6eSDDz4ofWkAAOCBbUqBrmvGYXMId5ttbpiUc/5Ezvnb7vjx84u+WM75H0TEFyLiz1+9/Xt55h9GxN+MiI+95WPHOedezrn33nvvLfrSAABAw23SLinYdcLdZnvwY24ppfdSSt9w9d//eER8IiL+3tXbH7p6TBHxAzEr9AYAAHgnm7JLCnadcLfZHi3zwSmlT0XEfxkR70XE51JKX8o5/0sppQ9HxE/nnL8vIj4UET+bUvq6mIVX/2PO+ReunqJKKb0XESkivhQR/8Yy6wEAAAA2Q78vPGqqWm5zWzW3uQEAAMC7qaqI4XBWhn1wMDt6JvQhovw2t6V2JgEAAADbo6oiBoOI6XT29mQyeztCoES5B+9MAgAAADbDcPgqSLo2nc7GoZQwCQAAAHbE+fli43AXYRIAAADco6oiut2IVmv2WFXrXtFyDg4WG4e7CJMAAADgDtf9QpNJRM6v+oW2OVAajSLa7dtj7fZsHEoJkwAAAOAOTewX6vcjxuOITicipdnjeKx8m8WknPO617CwXq+XT05O1r0MAAAAGqzVmu1Iel1KEZeXq18PPLSU0vOcc2/ePDuTAAAA4A76heBuwiQAAIDXNK10mXezif1CvjbZBI/WvQAAAIBNcl26fN2Vc126HKFXZtdc/3oPhxHn57MdSaPR+r4OfG2yKXQmAQAA3NDtzv6S/rpOJ+LsbNWrgVd8bfLQdCYBAAC8g/PzxcZhVXxtsimESQAAADcoXV6OTp+H42uTTSFMAgAAuGETS5ffpq7wpo7nue70mUwicn7V6SNQqse2fW3SXMIkAACAG/r9iPF41kOT0uxxPN7MguO6wpu6nmc4fFUOfW06nY2zvG362qTZFHADAABssKq6/zaxugqZS5/nbWuJmO1quuuvmClFXF6WrwdYDwXcAAAAW27ejqG6CplLnqdk91JJp49OJdh+wiQAAGCnbFOYMe/YWGkh87zPueR5So6wzev00akEzSBMAgAAdsa2hRnzdgyVFDKXfM4lz1Oye2lep49OJWgGnUkAAMDOqKtjaFVK1juvx6iuPqQ6fu50KsFm05kEAADwmro6hlalZMdQvz8Lcy4vZ4+v3+xV+jnPe546rqWv61gesF7CJAAAYGeUhhmboo6r4Ov6nOtYS13H8q7nbUrgZC3sGsfcAACAnXEdVNzs7Wm3Fw9Ftsmmfc51HKfbpM/JWmiS0mNuwiQAAGCnzAszmmibPueSXqVN6r6yFppEZxIAAMAd5nUDNdE2fc4lx/I2qfuqdC0lx8+WPaK2ST8vNJswCQAAgI1R0qu0Sd1XJWsp6YEq7Ypadi1QB2ESAADAmihLflNJ0XcdN8vVpWQtw+HtHqOI2dvD4WJz6lgL1EGYBAAA3EvY8XDq2InSVPOO5dVxs1yda523lpLjZ3UcUduknxeaTQE3AABwJzdDPSxlybuj5Nfa1wObQAE3AACwlDqO3XA/Zcm7o+T4mSNqbBNhEgAAcCdhx8NSlrwZVnGUs+T4mSNqbBNhEgAAcCdhx/3qCCDsRHl4836dVtlbNa8HqnQObAJhEgAAcCdhx93qCiDsRHlYJb9OjnLCu1HADQAA3KuqZn+xPj+f7UgajYQdipK3Q8mvU6s1C5pel9JsdxDsGgXcAADA0hy7eVNpl9Qquni4X8mvk6Oc8G6ESQAAAAsoCSBW2cXD3Up+nRzlhHcjTAIAAFhASQChi2f9Sn6d9FbBu9GZBAAAsKB5XVK6eDaDzi9YTGlnkjAJAACgZkq6gW2kgBsAAGBNdPEATSZMAgAAqJkuHmim6rSK7lE3Wk9b0T3qRnW6m636j9a9AAAAgCbq94VH0CTVaRWDZ4OYvpi1608uJjF4NoiIiP7j3frNbmcSAAAAwBzD4+HLIOna9MU0hse7d02jMAkAAABgjvOL84XGm0yYBAAAADDHwf7BQuNNJkwCAAAAmGN0OIr23u1rGtt77Rgd7t41jcIkAAAAgDn6j/sxfjKOzn4nUqTo7Hdi/GS8c+XbEREp57zuNSys1+vlk5OTdS8DAAAAoDFSSs9zzr158+xMAgCAAlUV0e1GtFqzx6pa94oAYD0erXsBAACw6aoqYjCImF7dCD2ZzN6OiOjv3ukGAHacnUkAADDHcPgqSLo2nc7G18VOKQDWxc4kAACY4/x8sfGHZqcUAOtkZxIAAMTbd/ocHNz9MfeNP7RN3CkFwO4QJgEAsPOud/pMJhE5v9rpcx0ojUYR7fbtj2m3Z+Pv8lrLHk/btJ1SAOwWYRIAADtv3k6ffj9iPI7odCJSmj2Ox4sfKZsXWpWqa6eU3iUA3oUwCQCAxpsXmpTs9On3I87OIi4vZ4/v0k1U1/G0OnZK1RVsAbB7hEkAADRaSWiyqk6kuo6n1bFTSu8SwG3VaRXdo260nraie9SN6lS6fh9hEgAAjVYSmtTZifQ2dYZW83ZK1bEbC2BXVKdVDJ4NYnIxiRw5JheTGDwbCJTuIUwCAGBt6ujsqesIWx2dSPOsKrTapN1YANtgeDyM6Yvb//IwfTGN4bHtmncRJgEAsBZ1dPbUGZrU0Yk0z6pCq03ajQWwDc4v7v6Xh/vGd50wCQCAtaijs2cbQ5OS0GrZHVubtBsLYBsc7N/9Lw/3je86YRIAALes6rr4ksBjm46w1aWOHVubtBsLYBuMDkfR3rv9Lw/tvXaMDm3XvIswCQCAl1Z5Xfy8wGPbjrDVpY4dW5u2Gwtg0/Uf92P8ZByd/U6kSNHZ78T4yTj6jzf4D4w1Sjnn5Z4gpc9ExE9ExLdGxMdyzidvmft1EXESEb+Tc/7+q7F/OiL++4j4xoj43yLir+Sc//Btr9nr9fLJyb0vAwDAO+p2Z6HN6zqdWQhTp+uw6GZw0m6/2jVUspZ5z7GNWq1ZePa6lGZhWKmqmgVQ5+ezcG002t6fEwBWI6X0POfcmzevjp1JX46IT0fEFwvm/vWI+MprY/9xRPxUzvlbIuLvR8S/VsOaAAB4B6XXxddxFG7e8bMmHmErUdcta9u0GwuA7bJ0mJRz/krO+bfmzUspfSQi/mJE/PSNsRQR3xMRP3c19LMR8QPLrgkAgHdTEmTUeRTubYFHE4+wlXBEDYBNt8rOpKOI+JGIuLk595+IiH+Qc/6jq7e/GhH/1F0fnFIapJROUkonH3zwwcOuFABgR5UEGXV0+tS1liZq4m4rAJqlKExKKX0+pfTlO358svDjvz8ivpZzfv76u+6YfmeJU855nHPu5Zx77733XsnLAgCwoJIgo/Qo3CrW0lRN220FQLMUhUk550/knL/tjh8/X/g63xER/3JK6SxmZdvfk1L67yLiDyLiG1JKj67mfSQifnfBzwEAgBrNCzJKj5/V1askVAHYTtVpFd2jbrSetqJ71I3q9AGuBmUtVnLMLef84znnj+ScuxHxlyLi7+ac/3KeXSX3v0TEv3I19QcjojSgAgBgDUqOn9XZqwTA9qlOqxg8G8TkYhI5ckwuJjF4NhAoNcTSYVJK6VMppa9GxMcj4nMppV++Gv9wSukXC57iRyPi30sp/R8x61D6b5ZdEwAAD6fk+NmqepUA2EzD42FMX9z+g2D6YhrDY38QNEGabQ7aLr1eL5+cnKx7GQAAG6eqZoHN+fns2NlotJ6jYa3WbEfS61KaHVkDoNlaT1uR76hETpHi8rP+INhUKaXnOefevHmrvM0NAIAHVHK0rI4eoxKlvUoANNPB/t3f8O8bZ7sIkwAAGmLe0bJV9hiV9CoB0Fyjw1G0927/QdDea8fo0B8ETSBMAgBoiPPzt4+vsseopFcJgObqP+7H+Mk4OvudSJGis9+J8ZNx9B/7g6AJhEkAAA0x72jZvLCpbv1+xNnZrCPp7EyQBLBr+o/7cfb+WVx+9jLO3j+7M0iqTqvoHnWj9bQV3aPunbe9rXIOZYRJAAANMe9omR4jADZJdVrF4NkgJheTyJFjcjGJwbPBrZBnlXMoJ0wCAGiIeUfL9BgBsEmGx8OYvrh9/nr6YhrD4+Fa5lDu0boXAABAffr9+4+TXY8Ph7OjbQcHsyDJ8TMA1uH84u5z1jfHVzmHcnYmAQDsED1GAGyKg/27z1nfHF/lHMoJkwAAAICVGx2Oor13+/x1e68do8PRWuZQTpgEAAAArFz/cT/GT8bR2e9EihSd/U6Mn4xv3fq2yjmUSznnda9hYb1eL5+cnKx7GQAAAACNkVJ6nnPuzZtnZxIAAAAAxYRJAAAAABQTJgEAAABQTJgEAAAALKw6raJ71I3W01Z0j7pRnVbrXhIr8mjdCwAAAAC2S3VaxeDZIKYvphERMbmYxODZICLCDWk7wM4kAIANUFUR3W5EqzV7rO74x92SObAOJbsT7GCAZhkeD18GSdemL6YxPB6uaUWskp1JAABrVlURg0HE9Or/ySeT2dsREf1++RxYh5LdCXYwQPOcX5wvNE6zpJzzutewsF6vl09OTta9DACAWnS7s3DodZ1OxNlZ+RxYh+5RNyYXb35xdvY7cfb+WfEcYLv4fd1MKaXnOefevHmOuQEAPLB5x9PO7/lH3JvjJXNgHUp2J9jBAM0zOhxFe699a6y9147R4WhNK2KVhEkAAA/o+njaZBKR86vjaTcDpYODuz/25njJHFiHg/27vwhvjpfMAcpsSv9Y/3E/xk/G0dnvRIoUnf1OjJ+MHV3dEcIkAIAHNBy+6jm6Np3Oxq+NRhHt2/+4G+32bHyRObAOJbsT7GCAelz3j00uJpEjv+wfW2egdPb+WVx+9jLO3j8TJO0QYRIAwAMqOZ7W70eMx7P+o5Rmj+Px7WLtkjmwqDp2OJTsTrCDAerhBjU2hQJuAIAHpDibTfX6DWsRs91CQh7YXK2nrcjx5t/hU6S4/OzlGlZE0yjgBgDYAI6nsanscIDNM2+3oP4xNoUwCQDgATmexqZywxpslpI+JP1jbAphEgDAEqpqdpSt1Zo9VndUzvT7syNtl5ezR0FSc5V8PWwKOxxgs5TsFtQ/xqZ4tO4FAABsq6qKGAxe3dY2mczejhAY7aJt+3oYHY7u7EyywwHWo3S3YP9xX3jE2tmZBADwjobDV8HBtel0Ns7u2bavBzscYLPYLcg2ESYBALyj83uqZe4bp9m28euh/7gfZ++fxeVnL+Ps/TNBEqxRXX1I80q865zD7hImAQC8o4N7/rH4vnGazdcDsIw6dguWlHjXNYfdJkwCALbOppQcj0YR7dv/iBzt9myc3ePrAVjWsrsFS0q865rDbhMmAQBb5brkeDKJyPlVyfE6AqV+P2I8juh0IlKaPY7Hm1m2zMPz9QCsW0mJd11z2G3CJABgq2xayXG/H3F2FnF5OXsUHOw2Xw/AOpWUeNc1h90mTAIAtso2lhwDwCqUlHjXNYfdJkwCALaKkmMAuFtJiXddc9htKee87jUsrNfr5ZOTk3UvAwBYg+vOpJtH3dpt3TQAAMtKKT3POffmzbMzCQDYKqUlx5ty4xsAD6M6raJ71I3W01Z0j7p3Xltfx5xVvQ5sEzuTAIDGsXsJoNmq0yoGzwa3rq9v77VvHcWqY86qXgc2RenOJGESANA43W7EZPLmeKczu2ELgO3WPerG5OLNb/Sd/U6cvX9W25xVvQ5sCsfcAICdVeeNb47LAWye84u7v6HfHK9jzqpeB7aNMAkAaJy6bny7Pi43mUTkPHscDARKAOt2sH/3N/Sb43XMWdXrwLYRJgEAjTMazTqSbmq3Z+OLGA5v9y5FzN4eDpdbHwDLGR2Oor13+xt9e68do8NRrXNW9TqwbYRJAEDjlN74Nk+dx+UAqE//cT/GT8bR2e9EihSd/c4bZdZ1zFnV68C2UcANAHAPRd4AwC5RwA0AsKS6jssBADSJMAkA4B51HZcDAGiSR+teAADAJuv3hUcAADfZmQQAAABAMWESAAAAAMWESQAAAAAUEyYB0BhVNbvKvdWaPVbVulcEAADNI0wCoBGqKmIwiJhMInKePQ4GAqVVE+gBADSfMAmARhgOI6bT22PT6Wyc1RDowWapTqvoHnWj9bQV3aNuVKd+MwJQD2ESAI1wfr7YOPWrK9CzuwmWV51WMXg2iMnFJHLkmFxMYvBsIFACoBbCJAAa4eBgsXHqV0egZ3cT1GN4PIzpi9vp7vTFNIbHtmsCsDxhEgCNMBpFtNu3x9rt2TirUUegt+rjinZB0VTnF3enuPeNA8AihEkANEK/HzEeR3Q6ESnNHsfj2TirUUegt8rjinZB0WQH+3enuPeNA8AihEkANEa/H3F2FnF5OXsUJNVr3i6eOgK9VR5XVNpOk40OR9Heu53utvfaMTq0XROA5QmTAIC5SnfxLBvorfK4otJ2mqz/uB/jJ+Po7HciRYrOfifGT8bRfyxlB2B5Kee87jUsrNfr5ZOTk3UvAwB2Rrc7C5Be1+nMQqM6VdVsd9D5+WxH0mj0MLvMVvk5AQBsg5TS85xzb948O5MA2CkKl9/NKnfxrOq4otJ2YFtUp1V0j7rRetqK7lE3qlN/eAHrJUwCYGdsWuHyNgVbdXYZbcrnrbQd2AbVaRWDZ4OYXEwiR47JxSQGzwYCJWCtljrmllL6TET8RER8a0R8LOd879mzlNLXRcRJRPxOzvn7r8Z+JiL+xYi4uJr2V3POX5r3uo65AfAuNulY03WwdbMAut3e3DCjrvVu2+cNsG7do25MLt78w6uz34mz989WvyCg0VZ1zO3LEfHpiPhiwdy/HhFfuWP8h3POH736MTdIAoB3VedRrWV312zbTWJ17eLZts8b2Cy7eNzr/OLuP6TuGwdYhaXCpJzzV3LOvzVvXkrpIxHxFyPip5d5PQBYRl1HtUqOy80Lm0qDrZLQalXHxuroMnKDGmyfTQlwdvW418H+3X9I3TcOsAqr6kw6iogfiYjLO943Sin9Rkrpp1JKf+y+J0gpDVJKJymlkw8++ODBFgpAc5UWLs8LZ+btrikJm0qCrdLQapN6oOaps3sJeHibFOAMj4cxfXH7m+/0xTSGx83e2jg6HEV77/YfXu29dowOX/3hVRL41TUHIKKgMyml9PmI+CfveNcw5/zzV3O+EBE/dFdnUkrp+yPi+3LO/1ZK6buu5l13Jn0oIv7viPjHImIcEf9nzvnfn7donUkAvKt5186XdPq0WrPg5nUpzXbtlHQzlbxOyfNsUg9UCZ1JsF02qa+n9bQVOd785psixeVn7/o36+aoTqsYHg/j/OI8DvYPYnQ4iv7j/sv3DZ4NbgVt7b12jJ+Ma58DNF9pZ9JSBdw3XuwLcX+Y9B9FxF+JiD+KiD8eEX8yIv6nnPNffm3ed8WNoOlthEkAPJQ6Apx5YdO1ecFWyfOUvtY889ZSp1W+FrCcVQY4bwtMIjYr2NokJT8vdc0Bmm9VBdxz5Zx/POf8kZxzNyL+UkT83esg6WpnUqSUUkT8QMwKvQFgbUo6feYdlys9yjWvg6jkeUpf621H91Z9VK6O7iVgNVbV11NynK7kuNcuKinormsOwLWlwqSU0qdSSl+NiI9HxOdSSr98Nf7hlNIvFjxFlVI6jYjTiPimiPgPllkPACyrJJyZd7NZaTfTPCXPUzJnXljkhjXgPqsKcEr6kPqP+zF+Mo7OfidSpOjsdxzBirLAr645ANdqOea2ao65AfBQ6ur0qesoV8nzzJtT17E8YDfNO35Wh13uQ1qWziSgTivtTFo1YRIAD6lpnT51FIYDPCR9PcspCfw2bQ6wmYRJAEBEzA+L3LAGrJtdMdvBDidovo0p4AYA1mter9K8DiiAh6YPaTuUdFuVzAG2n51JALADmnZ0D4DVK+m20n8F283OJADgpX5/dqTt8nL2KEgCYFFuhQOuCZMAAAAWVJ1W0T3qRutpK7pH3ahOqwebsylGh6No790+N93ea8focLTQnG36nIG7PVr3AgAAALbJ6yXTk4tJDJ4NIiLuLaJ+1zmb5HpNb7upbd6cbfucgbvpTAIAAFhA96gbk4s3r8ns7Hfi7P2zWuc0zS5+zrBNdCYBAAA8gPOL87njdc1pml38nKGJhEkAAAALqKuIehfLqnfxc4YmEiYBAAAsoK4i6pI5TbOLnzM0kTAJAABgAf3H/Rg/GUdnvxMpUnT2OzF+Mn6jiLqOOU1T+jm78Q02mwJuAAAANsbrN75FzHYvNT1og02ggBsAAICtMzwe3gqSIiKmL6YxPB6uaUXA64RJAAAAbAw3vsHmEyYBAACwMdz4BptPmARboqoiut2IVmv2WOkgBACggdz4BptPmARboKoiBoOIySQi59njYCBQAgCgeXbxljvYNm5zgy3Q7c4CpNd1OhFnZ6teDQAAAE3kNjdokPN7ugbvGwcAAICHIkyCLXBwT9fgfeMAAADwUIRJsAVGo4j27Q7CaLdn4wAAALBKwiTYAv1+xHg860hKafY4Hs/GAQAAYJWESbAl+v1Z2fbl5exRkDRTVbOC8lZr9uiGOwAAgIf1aN0LAHhXVRUxGERMp7O3J5PZ2xHCNgAAgIdiZxKwtYbDV0HStel0Ng4AAMDDECYBW+v8fLFxtpsjjQAAsBmEScDWOjhYbJztdX2kcTKJyPnVkUaBEgAArJ4wCdhao1FEu317rN2ejdMsjjQCAMDmECYBW6vfjxiPIzqdiJRmj+Ox8u0mcqQRgFLVaRXdo260nraie9SN6vTNbax1zQHYVSkTQuhGAAAgAElEQVTnvO41LKzX6+WTk5N1LwOAFel2Z0fbXtfpRJydrXo1AGyq6rSKwbNBTF+82s7a3mvH+Mk4+o/7tc4BaKKU0vOcc2/ePDuTANh4jjQCUGJ4PLwVAEVETF9MY3g8rH0OwC4TJgGwEd52W5sjjQCUOL+4+/zzzfG65gDsMmESAGtXcltbvz870nZ5OXsUJAHwuoP9u690vTle1xyAXSZMggZ5286OTbSq9W7bz8suclsbAHUYHY6ivXf7XHR7rx2jw1HtcwB2mTAJGqJkZ8cmWdV6t+3nZVe5rQ2AOvQf92P8ZByd/U6kSNHZ77xRml3XHIBd5jY3aIhtu+1qVevdtp+XXeXXCQAA1s9tbrBjtm1nx6rWW/o6jsKtl9vaAABgewiToCEO7umDvG983Va13pLXcRRu/dzWBgAA20OYBA2xyp0ddeziWdV6S15H+fNmcFsbAABsB2ESNMSqdnbUtYundL3LBlclr7NtRwQBAADWSQE37Jiqmu24OT+fHfUajRYLnFZZlHwdXN3cNdRu1x+SKX8GAABQwA3coY5dRavcxbOq42fKnwEAAMoJk2CH1BHOrLLoe1XBlfJnAACAcsIk2CF1hDOlu3jqKOleZXCl/Plh1fH1AAAAbAZhEuyQOsKZkl08dZV0O37WDHV9PQAAAJtBATfskG0stF62MJz1U3AOAADbobSAW5gEO2YV4UyrNduB8rqUZsfI2C2+HgAAYDu4zQ240yq6gVbZdcT6zetD8vUA8DCq0yq6R91oPW1F96gb1anzwwCshjAJqJ2uo91R0ofk6wGgftVpFYNng5hcTCJHjsnFJAbPBgIlAFZCmATUrqSkm2YYDm93cEXM3h4OX73t6wGgfsPjYUxf3P4GPH0xjeHx8J6PAID66EwC4J3pQwJYj9bTVuR48xtwihSXn/UNGIB3ozMJgLea13VUQh8SwHoc7N/9jfa+cQCokzAJYAeVdB2V0IcEsB6jw1G0925/A27vtWN06BswAA9PmASwg0q6jiLm717ShwSwHv3H/Rg/GUdnvxMpUnT2OzF+Mo7+Y9+AAXh4OpMAdlBJ19H17qWboVO7LSwCAICm0pkEsAZ19BCtQknXUenuJQAAYLcIkwBqUlcP0SqUdB2dn9/9sfeNAwAAu0GYBFCTbdrJU9J15KY2AADgLsIkgJps206efj/i7GzWkXR29mYPkpvaAACAuwiTgFu2pfNnEzVtJ4+b2gAAgLsIk4CXtqnzZxM1cSfPvN1LAADA7hEmAS9tU+fPJrKTBwAA2AUp5/zuH5zSZyLiJyLiWyPiYznnk3vmnUXE/xsR/ygi/ijn3Lsa/8aI+B8iohsRZxHxr+ac//681+31evnk5M6XApbQas12JL0updnOFAAAAJorpfT8OrN5m2V3Jn05Ij4dEV8smPvdOeePvraoH4uI45zzt0TE8dXbwJo0rfNnE+mkAgAAtt1SYVLO+Ss5599a4ik+GRE/e/XfPxsRP7DMeoDlNLHzZ5PopAIAAJpgVZ1JOSL+dkrpeUppcGP8m3POvxcRcfX4p+97gpTSIKV0klI6+eCDDx54ubCbdP48rDo7qexwAgAA1mVumJRS+nxK6ct3/PjkAq/zHTnnb4+IvxARfy2l9J2LLjTnPM4593LOvffee2/RDwcKub3r4Zyfl43PC4rscILdVZ1W0T3qRutpK7pH3ahO3/yNXzIHAGAZj+ZNyDl/YtkXyTn/7tXj11JKfysiPhaznqXfTyl9KOf8eymlD0XE15Z9LYBNdXAwC37uGr92HRRd72C6DooiXgV7b9vhJPyD5qpOqxg8G8T0xewbwORiEoNns28Q/cf94jkAAMt68GNuKaWvTyn9iev/jog/F7Pi7oiI/zkifvDqv38wIn7+odcD8K6WPVpW0klVchSurh1OwHYZHg9fhkTXpi+mMTweLjQHAGBZS4VJKaVPpZS+GhEfj4jPpZR++Wr8wymlX7ya9s0R8SsppV+PiF+LiM/lnH/p6n0/GRHfm1L67Yj43qu3ATZOHUfLSjqpSoKiklv3HIWD5jm/uPsbxM3xkjkAAMtKOed1r2FhvV4vn5ycrHsZwA7pdu8+otbpzLqlVvk6rx+Fi5jtcLoZTK1qvcDqdI+6Mbl48zd2Z78TZ++fFc8BALhPSul5zrk3b96qbnMD2GqlR8uWVXIUrq4dTsB2GR2Oor13+xtEe68do8PRQnMAAJYlTAIoUHK0rA4lQdH1vLfdureq9QKr03/cj/GTcXT2O5EiRWe/E+Mn41vF2iVzAACW5ZgbQIGSo2WbZNvWCwAArJ9jbgA1Kt0xtCm2bb0AAMD2sDMJAAAAADuTAAAAAKifMAkAAACAYsIkAAAAAIoJkwAAAAAoJkwCAAAAoJgwCQAAAIBiwiQAAAAAigmTAAAAACgmTAIAAACgmDAJAGCHVKdVdI+60Xraiu5RN6rTaqPnbNJaAICZlHNe9xoW1uv18snJybqXAQCwVarTKgbPBjF9MX051t5rx/jJOPqP+xs3Z5PWAgC7IKX0POfcmztPmAQAsBu6R92YXEzeGO/sd+Ls/bONm7NJawGAXVAaJjnmBgCwI84vzueOb9KcTVoLAPCKMAkAYEcc7B/MHd+kOZu0FgDgFWESAMAD25Ty59HhKNp77Vtj7b12jA5HGzlnk9YCALwiTAIAeEDXxc6Ti0nkyDG5mMTg2eBWWFQypw79x/0YPxlHZ78TKVJ09jtvFExv0pxNWkspN8sBsAsUcAMAPCDlz7vDzXIAbDsF3AAAG0D58+4YHg9vBUAREdMX0xgeD2ufAwDrJEwCAHhAyp93h5vlANgVwiQAgLeY110z7/3Kn3eHm+UA2BXCJACAe8wrxi4pzl5l+TPr5WY5AHaFAm4AgHvMK8ZWnM3rqtMqhsfDOL84j4P9gxgdjt4IBeuaAwB1Ky3gFiYBANyj9bQVOd78f6UUKS4/ezn3/QAA28RtbgAAS5rXXaPbBgDYRcIkAIB7zOuu0W0DAOwiYRIAwD3mFWMrzt4t827uA4BdoTMJAADmuL65b/pi+nKsvdcWHgLQKDqTAACgJsPj4a0gKSJi+mIaw+PhmlYEAOsjTAIAgDnOL84XGgeAJhMmAQDAHG7uA4BXhEkAADCHm/sA4BVhEgAAzOHmPgB4xW1uAAAAALjNDQAAAID6CZMAAAAAKCZMAgAAAKCYMAkAAACAYsIkAAAAAIoJkwAAAAAoJkwCAAAAoJgwCQAAAIBiwiQAAAAAigmTAAAAACgmTAIAAACgmDAJAAAAgGLCJAAAAACKCZMAAAAAKCZMAgAAAKCYMAkAAACAYsIkAAAAAIoJkwAAAAAoJkwCAAAAoJgwCQAAAIBiwiQAAAAAigmTAAAAACgmTAIAAACgmDAJAAAAgGLCJAAAAACKLRUmpZQ+k1L6zZTSZUqp95Z5Zyml05TSl1JKJzfGfyKl9DtX419KKX3fMusBAAAA4GE9WvLjvxwRn46I/6pg7nfnnP/gjvGfyjn/Z0uuAwAAAIAVWCpMyjl/JSIipVTPagAAAADYaKvqTMoR8bdTSs9TSoPX3vdvp5R+I6X036aU/tR9T5BSGqSUTlJKJx988MHDrhYAAACAO80Nk1JKn08pffmOH59c4HW+I+f87RHxFyLir6WUvvNq/G9ExD8bER+NiN+LiP/8vifIOY9zzr2cc++9995b4KUBAAAAqMvcY245508s+yI559+9evxaSulvRcTHIuKLOeffv56TUvqvI+IXln0tAAAAAB7Ogx9zSyl9fUrpT1z/d0T8uZgVd0dK6UM3pn7qehwAAACAzbRUmJRS+lRK6asR8fGI+FxK6Zevxj+cUvrFq2nfHBG/klL69Yj4tYj4XM75l67e95+klE5TSr8REd8dEf/uMusBAAAA4GGlnPO617CwXq+XT05O1r0MAGDLVadVDI+HcX5xHgf7BzE6HEX/cX/dywIAWIuU0vOcc2/evLmdSQAATVSdVjF4Nojpi2lEREwuJjF4Nrt0VqAEAHC/B+9MAgDYRMPj4csg6dr0xTSGx8M1rQgAYDsIkwCAnXR+cb7QOAAAM8IkAGAnHewfLDQOAMCMMAkA2Emjw1G099q3xtp77Rgdjta0IgCA7SBMAgB2Uv9xP8ZPxtHZ70SKFJ39ToyfjJVvAwDMkXLO617Dwnq9Xj45OVn3MgAAAAAaI6X0POfcmzfPziQAAAAAigmTAAAAACgmTAIAGqk6raJ71I3W01Z0j7pRnVbrXhIAQCM8WvcCAADqVp1WMXg2iOmLaURETC4mMXg2iIhQsA0AsCQ7kwCAxhkeD18GSdemL6YxPB6uaUUAAM0hTAIAGuf84nyhcQAAygmTAIDalPQUraLL6GD/YKFxAADKCZMAgFpc9xRNLiaRI7/sKboZFpXMqcPocBTtvfatsfZeO0aHo1pfBwBgFwmTAIBalPQUrarLqP+4H+Mn4+jsdyJFis5+J8ZPxsq3AQBq4DY3AKAWJT1Fq+wy6j/uC48AAB6AnUkAQC1Keop0GQEAbD9hEgBQi5KeIl1GAADbT5gEANSipKdIlxEAwPZLOed1r2FhvV4vn5ycrHsZAAAAAI2RUnqec+7Nm2dnEgAAAADFhEkAAAAAFBMmAQAAAFBMmAQAAABAMWESAAAAAMWESQAAAAAUEyYBAAAAUEyYBAAAAEAxYRIAAAAAxYRJAAAAABQTJgEAAABQTJgEAAAAQDFhEgAAAADFhEkAAAAAFBMmAQAAAFBMmAQAAABAMWESAAAAAMWESQAAAAAUEyYBAFunOq2ie9SN1tNWdI+6UZ1W614SAMDOeLTuBQAALKI6rWLwbBDTF9OIiJhcTGLwbBAREf3H/XUuDQBgJ9iZBABsleHx8GWQdG36YhrD4+GaVgQAsFuESQDAVjm/OF9oHACAegmTAICtcrB/sNA4AAD1EiYBAFtldDiK9l771lh7rx2jw9GaVgQAsFuESQDAVuk/7sf4yTg6+51IkaKz34nxk7HybQCAFUk553WvYWG9Xi+fnJysexkAAAAAjZFSep5z7s2bZ2cSALBRqtMqukfdaD1tRfeoG9Vpte4lAQBww6N1LwAA4Fp1WsXg2SCmL6YRETG5mMTg2SAiwjE2AIANYWcSALAxhsfDl0HStemLaQyPh2taEQAArxMmAQAb4/zifKFxAABWT5gEAGyMg/2DhcYBAFg9YRIAsDFGh6No77VvjbX32jE6HK1pRQAAvE6YBABsjP7jfoyfjKOz34kUKTr7nRg/GSvfBgDYICnnvO41LKzX6+WTk5N1LwMAAACgMVJKz3POvXnz7EwCAAAAoJgwCQAAAIBiwiQAGqM6raJ71I3W01Z0j7pRnVbrXhIAADTOo3UvAADqUJ1WMXg2iOmLaURETC4mMXg2iIhQ3gwAADWyMwmARhgeD18GSdemL6YxPB6uaUUAANBMwiQAGuH84nyhcQAA4N0IkwBohIP9g4XGm6SkK6qOPimdVAAARCwZJqWUPpNS+s2U0mVKqfeWed+QUvq5lNLfSyl9JaX08avxb0wp/Z2U0m9fPf6pZdYDwO4aHY6ivde+Ndbea8focLSmFa3GdVfU5GISOfLLrqibQU/JnDpeBwCA3bDszqQvR8SnI+KLc+b9FxHxSznnPxMRfzYivnI1/mMRcZxz/paIOL56GwAW1n/cj/GTcXT2O5EiRWe/E+Mn48aXb5d0RdXRJ6WTCgCAa0vd5pZz/kpERErp3jkppT8ZEd8ZEX/16mP+MCL+8Ordn4yI77r675+NiC9ExI8usyYAdlf/cb/x4dHrSrqi6uiT0kkFAMC1VXQm/TMR8UFE/M2U0v+eUvrplNLXX73vm3POvxcRcfX4p1ewHgBojJKuqDr6pHa5kwoAgNvmhkkppc+nlL58x49PFr7Go4j49oj4Gznnfy4i/r94h+NsKaVBSukkpXTywQcfLPrhANBIJV1RdfRJ7WonFQAAb5obJuWcP5Fz/rY7fvx84Wt8NSK+mnP+1au3fy5m4VJExO+nlD4UEXH1+LW3rGOcc+7lnHvvvfde4UsDQLOVdEXV0Se1q51UAAC8KeWcl3+SlL4QET+Ucz655/3/a0T86znn30op/UREfH3O+YdTSv9pRPw/OeefTCn9WER8Y875R+a9Xq/Xyycnd74UAAAAAO8gpfQ859ybN2+pzqSU0qdSSl+NiI9HxOdSSr98Nf7hlNIv3pj670RElVL6jYj4aET8h1fjPxkR35tS+u2I+N6rtwHYENVpFd2jbrSetqJ71L3zGvhVzSl5DgAA4OHVsjNp1exMAnh41WkVg2eDW9fBt/fat442rWpOyXMAAADLKd2ZJEwC4E7do25MLiZvjHf2O3H2/tlK55Q8BwAAsJyVHHMDoLnOL87njq9qTslzAAAAqyFMAuBOB/sHc8dXNafkOQAAgNUQJgFwp9HhKNp77Vtj7b12jA5HK59T8hwAAMBqCJNgS9R1YxaU6j/ux/jJODr7nUiRorPfeaPwelVzSp6jlNvnAABgOQq4YQvUdWMW7Dq3zwEAwP3c5gYNUteNWbDr3D4HAAD3c5sbNEhdN2bBrnP7HAAALE+YBFugrhuzYNe5fQ4AAJYnTIItUNeNWbDr3D4HAADLEybBFqjrxizYddt4+xwAAGwaBdwAAAAAKOAG2FbVaRXdo260nraie9SN6rR6kDklzwEAAPC6R+teAACvVKdVDJ4NYvpiGhERk4tJDJ4NIiJeHpGqY07JcwAAANzFziSADTI8Hr4MeK5NX0xjeDysdU7Jc7Bb7FQDAKCUnUkAG+T84nzueB1zSp6D3WGnGgAAi7AzCWCDHOwfzB2vY07Jc7A77FQDAGARwiSADTI6HEV7r31rrL3XjtHhqNY5Jc/Bw6urbH1ZdqoBALAIYRLABuk/7sf4yTg6+51IkaKz34nxk/Gto0Z1zCl5Dh7W9dGyycUkcuSXR8tuhkUlc+pgpxoAAItIOed1r2FhvV4vn5ycrHsZAPDOukfdmFxM3hjv7Hfi7P2z4jl1eL0zKWK2U03ACACwW1JKz3POvXnz7EwCgDWoq2y9DnaqAQCwCLe5AQ+iOq1ieDyM84vzONg/iNHhyF9M4YaD/YM7dx29XqQ+b05d+o/7fo8CAFDEziSgdqvqeYFtVlfZOgAArJowCaida8ZhvrrK1gEAYNUUcAO1az1tRY43v7ekSHH52cs1rAgAAIB5FHADa+OacQAAgOYSJgG10/MCAADQXMIkYGHVaRXdo260nraie9R9o1hbzwsAAEBz6UwCFnJ9U9vNgu32XltYBAAAsOV0JgF3mreraB43tQEAAOy2R+teALA6r+8qmlxMYvBsEBFRvKvo/OJ8oXEAAACaxc4k2CElu4rm7VxyUxsAAMBuEybBDpm3q+h659LkYhI58sudSzcDJTe1AQAA7DZhEuyQebuKSnYuuakNAABgt+lMgh0yOhzdeRPb9a6i0j6k/uO+8AgAAGBH2ZkEO2TeriJ9SAAAAMxjZxLsmLftKpq3cwkAAADsTAJe0ocEAADAPCnnvO41LKzX6+WTk5N1LwMAAACgMVJKz3POvXnz7EwCAAAAoJgwCVib6rSK7lE3Wk9b0T3qRnVavdMcAAAAVkcBN7AW1Wl1q+x7cjGJwbNBRMTLjqaSOQAAAKyWnUnAWgyPh7dujYuImL6YxvB4uNAcAAAAVkuYBKzF+cX53PGSOQAAAKyWMAlYi4P9g7njJXMAAABYLWESsBajw1G099q3xtp77RgdjhaaAwAAwGoJk4C16D/ux/jJODr7nUiRorPfifGT8a1i7ZI5AAAArFbKOa97DQvr9Xr55ORk3csAAAAAaIyU0vP/v717j7XsqusA/v2VmWIG6bTQyrtzxRikUqzNSOgfEnBixJKmlrSxZLSmgUzEQIzGpNVJrKhjADVUJVInthTMDY8gYgvlUccHTeThIG2n5RFGmimVKq2GaWRUWln+cdapp2Pn3n0fZ+45t59PsjP7rL32vvtkvmfddX9n73NaazuX6+fKJAAAAAAGU0wCAAAAYDDFJAAAAAAGU0wCAAAAYDDFJAAAAAAGU0wCAAAAYDDFJAAAAAAGU0wCAAAAYDDFJAAAAAAGU0wCAAAAYDDFJAAAAAAGU0wCAAAAYDDFJAAAAAAGU0wCSLJ4aDEL1y7klDedkoVrF7J4aHEqfYYcAwAAYJZt2egTANhoi4cWs+fmPTn28LEkyZGjR7Ln5j1Jkt3n7l63PkOOAQAAMOvWdGVSVV1WVXdX1XeqaucS/U6vqg9U1Zeq6otVdUFv/42q+uequr0vF67lfABWY++BvY8WeMaOPXwsew/sXdc+Q44BAAAw69Z6ZdJdSV6d5E+W6fcHST7WWru0qk5Nsm1i29taa7+3xvMAWLV7j967bPt69BlyDAAAgFm3piuTWmtfbK19eak+VXVakpclub7v8+3W2jfX8nMB1tPZ289etn09+gw5BgAAwKw7GR/A/fwkDyR5Z1V9vqr+tKqeMrH9DVV1Z1XdUFVnnITzAXiMfbv2ZdvWbY9p27Z1W/bt2reufYYcAwAAYNYtW0yqqr+qqrseZ7l44M/YkuT8JO9orf1wkm8lubpve0eS70tyXpL7k/z+Euexp6oOVtXBBx54YOCPBlje7nN3Z/9F+7Nj+45UKju278j+i/Y/5kOx16PPkGMAAADMumqtrf0gVX+b5FdaawcfZ9szk3y6tbbQH/9okqtba686rt9Ckg+31l603M/buXNnO3jw//0oAAAAAFapqj7XWjvhF6yNTf02t9bavyT5WlW9oDftSvKFJKmqZ010vSSjD/QGAAAAYEatqZhUVZdU1X1JLkjykar6eG9/dlXdMtH1jUkWq+rOjG5p+53e/taqOtTbX5Hkl9ZyPgAAAABM17rc5nayuc0NAAAAYH3NzG1uAAAAAGweikkAAAAADKaYBAAAAMBgikkAAAAADKaYBAAAAMBgikkAAAAADKaYBAAAAMBgikkAAAAADKaYBAAAAMBgiknAXFs8tJiFaxdyyptOycK1C1k8tLiqPgAAAAyzZaNPAGC1Fg8tZs/Ne3Ls4WNJkiNHj2TPzXuSJLvP3T24DwAAAMO5MgmYW3sP7H20SDR27OFj2Xtg74r6AAAAMJxiEjC37j1677LtQ/oAAAAwnGISMLfO3n72su1D+gAAADCcYhIwt/bt2pdtW7c9pm3b1m3Zt2vfivoAAAAwnGISMLd2n7s7+y/anx3bd6RS2bF9R/ZftP8xH6w9pA8AAADDVWtto89hxXbu3NkOHjy40acBAAAAsGlU1edaazuX6+fKJAAAAAAGU0wCAAAAYDDFJAAAAAAGU0wCAAAAYDDFJAAAAAAGU0wCAAAAYDDFJAAAAAAGU0wCAAAAYDDFJAAAAAAGU0wCAAAAYDDFJAAAAAAGU0wCAAAAYDDFJAAAAAAGU0wCAAAAYDDFJAAAAAAGU0wCAAAAYDDFJAAAAAAGU0wCAAAAYDDFJAAAAAAGU0wCAAAAYDDFJAAAAAAGU0wCAAAAYLBqrW30OaxYVT2Q5Mg6He7MJA+u07GYf/LAmCwwSR4YkwUmyQNjssAkeWBsHrOwo7V21nKd5rKYtJ6q6mBrbedGnwezQR4YkwUmyQNjssAkeWBMFpgkD4xt5iy4zQ0AAACAwRSTAAAAABhMMSnZv9EnwEyRB8ZkgUnywJgsMEkeGJMFJskDY5s2C0/4z0wCAAAAYDhXJgEAAAAw2NwXk6rqhqr6RlXddYLt26vq5qq6o6rurqorJ7a9paru6stPT7R/b1V9pqq+UlXvq6pTe/uT++PDffvCtJ8fw00pC4tV9eXefkNVbe3tL6+qo1V1e19+ffrPkJWYUh5urKp7Jv7fz+vtVVV/2MeGO6vq/Ok/Q4aaUhZum8jB16vqQ73d2DDjBuThjKr6i/5a/mxVvWhi2yv774TDVXX1RLt5wxyaUhbMG+bUlPJg3jCHppQF84Y5tcY8PO6+VfW0qrq1zxturaozevt8jQ2ttblekrwsyflJ7jrB9l9L8pa+flaSf09yapJXJbk1yZYkT0lyMMlpvd/7k1ze169L8vq+/gtJruvrlyd530Y/f8vUs3BhkurLeyay8PIkH97o52w56Xm4Mcmlj3OsC5N8tOfkpUk+s9HP3zLdLBy3/58nuaKvGxtmfBmQh99Nck1f/4EkB/r6k5L8U5Ln93zckeScvs28YQ6XKWXBvGFOlynlwbxhDpdpZOG4/c0b5mhZbR6W2jfJW5Nc3devzv/NQ+dqbJj7K5Naa5/MaOJ/wi5JnlpVleS7e99HkpyT5O9aa4+01r6V0Yv9lb3fjyX5QN//XUl+qq9f3B+nb9/V+zMD1jsL/Zi3tC7JZ5M8d5rPgfUzjTws4eIk7+5R+XSS06vqWWt+EqyLaWahqp6a0e+MD03j3Fl/A/JwTpIDve+XkixU1TOSvCTJ4dbaV1tr307y3iQXmzfMr/XOQu9n3jCnppGHJZg3zLBpZsG8Yf6sIQ9L7Ts5Pzh+3jA3Y8PcF5MGeHuSFyb5epJDSX6xtfadjP4o+Mmq2lZVZyZ5RZLnJXl6km+21h7p+9+X5Dl9/TlJvpYkffvR3p/5sNIsPKpfpv6zST420XxBjW6L+WhV/eBJeQasp9XmYV+/7PRtVfXk3vbo2NBNjhvMvlWPDUkuyegdqIcm2owN8+2OJK9Okqp6SZIdGRUETvQ6N2/YvFaahUeZN2xKq82DecPms+qxIeYNm9GJ8rCUZ7TW7k+S/u/39Pa5GhueCMWkn0hye5JnJzkvydur6rTW2ieS3JLk7zO6DPlTGb0T/XjvGI6/8m6pbcy+lWZh0h8n+WRr7bb++B+T7Git/VCSP4p3F+bRavLwqxldvvojSZ6W5KrebmyYb2sZG17Tt40ZG+bfm5OcUVW3J3ljks9n6UlxrDsAAALhSURBVPmBecPmtdIsTDJv2HxWkwfzhs1pLWODecPmc6I8rMZcjQ1PhGLSlUk+2C8VO5zknowG9bTW9rXWzmut/XhG/3FfSfJgRpeTben7Pzejd6uTUWXweUnSt2/P0pe8MVtWmoUkSVVdk9HnqPzyuK219lBr7T/6+i1JtvYrF5gfK85Da+3+3v+/k7wzo8uZk4mxoZscN5h9qx0bnp5RBj4ybjM2zL/+f3hla+28JFdkNP7fkxO/zs0bNqlVZCGJecNmtZo8mDdsTmsYG8wbNqEl8rCUfx3fvtb//UZvn6ux4YlQTLo3ya4k6fcuviDJV6vqSf0Fnap6cZIXJ/lEv8f9b5Jc2vf/uSR/2ddv6o/Tt/917898WFEW+uPXZXTVwmv6bS/p7c8cf+5Fv5zxlCT/dhKfC2u3mjyMB/3K6N7m8Tcz3JTkiv4NDC9NcnR86SpzYcVZ6C7L6EMz/2vcYGyYf1V1evVvY0vyuoyuLnkoyT8k+f4afXPbqRl9oPZN5g2b10qz0Pcxb9ikVpkH84ZNaDVZ6MwbNqEl8rCUyfnB8fOGuRkbtizfZbZV1Xsy+hT8M6vqviTXJNmaJK2165L8VpIbq+pQRu8qX9Vae7CqvivJbf21+1CSn5n4vIOrkry3qn47o8vUru/t1yf5s6o6nNE7i5efhKfIQFPKwnVJjiT5VN/+wdbab2b0R8Hrq+qRJP+Z0bf4+ANhhkwpD4tVdVbvf3uSn+/tt2T07QuHkxzL6EoXZsSUspCMfge8+bgfZ2yYcQPy8MIk766q/0nyhSSv7dseqao3JPl4Rt/Yc0Nr7e5+WPOGOTSlLJg3zKkp5cG8YQ5NKQuJecNcWm0eTrRva+36jHLw/qp6bUZval7Wd5mrsaFkFQAAAIChngi3uQEAAACwThSTAAAAABhMMQkAAACAwRSTAAAAABhMMQkAAACAwRSTAAAAABhMMQkAAACAwRSTAAAAABjsfwGGewIfiWxyqAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "# visualizer.plot()\n",
    "for color in ['blue']:\n",
    "    for row in tqdm(visualizer.date_pred_targ_dict.get(color, pd.DataFrame())[1000:1100].itertuples()):\n",
    "        plt.scatter(row[1], row[-2], color=color)\n",
    "        plt.scatter(row[1], row[-1], color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters:\n",
    "\n",
    "- model:\n",
    "    - embedding_dim=features*2  \n",
    "    - hidden_dim=features*16  \n",
    "- learning rate  \n",
    "- batch size (window size?)\n",
    "- DONE - DID IMPROVE criterion = nn.MSELoss()  \n",
    "    - try a loss which scales through time.\n",
    "- optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "- add polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no lr scheduler min val loss:  \n",
    "`Epoch: 2/6... Step: 316... Loss: 0.292909... Val Loss: 0.068783\n",
    "Validation loss decreased (0.072723 --> 0.068783).  Saving model ...`\n",
    "MSE loss: 0.52932\n",
    "MAE loss: 0.64894\n",
    "KLDiv loss: 0.00000\n",
    "\n",
    "with lr scheduler min val loss:  \n",
    "`Epoch: 4/6... Step: 638... Loss: 0.052657... Val Loss: 0.052713\n",
    "Validation loss decreased (0.052825 --> 0.052713).  Saving model ...`\n",
    "MSE loss: 0.14511\n",
    "MAE loss: 0.34314\n",
    "KLDiv loss: 0.00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
