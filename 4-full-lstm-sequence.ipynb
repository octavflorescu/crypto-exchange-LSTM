{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular import *\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoDataset:\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "    TARGET_COLUMN_NAME = 'close'\n",
    "    DATE_COLUMN_NAME = 'date'\n",
    "    ELAPSED = 'Elapsed'\n",
    "    DROP_COLUMNS =['Second', 'Month', 'Year', \\\n",
    "                   'Is_quarter_end', 'Is_quarter_start', \\\n",
    "                   'Is_year_end', 'Is_year_start',\n",
    "                   'Is_month_end', 'Is_month_start',\n",
    "                   'Week', 'Dayofyear', ELAPSED]\n",
    "    True_target_column_name = TARGET_COLUMN_NAME+'_future'\n",
    "\n",
    "    def __init__(self, csv_file=\"BTC-ETH-filtered_with_indicators.csv\", predict_delta=4, sequence_size=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with the crypto stats table.\n",
    "        \"\"\"\n",
    "        from datetime import datetime\n",
    "        self.sequence_size = sequence_size\n",
    "        self.df = pd.read_csv(\"BTC-ETH-filtered_with_indicators.csv\",\n",
    "                              #read dates as dates\n",
    "                              parse_dates=[self.DATE_COLUMN_NAME],\n",
    "                              date_parser=lambda x: datetime.fromtimestamp(int(x)))\n",
    "        add_datepart(self.df, self.DATE_COLUMN_NAME, time=True);\n",
    "        self.df = self.df.astype(float).drop(columns=self.DROP_COLUMNS)\n",
    "        self.df[self.True_target_column_name] = self.df[self.TARGET_COLUMN_NAME].shift(-predict_delta)\n",
    "        trvate_split = tuple(int(x * len(self.df)) for x in (0.75, 0.9, 1.0))\n",
    "        tmp_train_limit = trvate_split[0]- trvate_split[0]%sequence_size\n",
    "        self.train_df = self.df[:tmp_train_limit]\n",
    "        tmp_valid_limit = trvate_split[1] - trvate_split[0]%sequence_size -(trvate_split[1]-trvate_split[0])%sequence_size\n",
    "        self.valid_df = self.df[tmp_train_limit:tmp_valid_limit]\n",
    "        self.test_df = self.df[tmp_valid_limit:(len(self.df)- len(self.df)%sequence_size)]\n",
    "        del tmp_train_limit, tmp_valid_limit\n",
    "        \n",
    "        preprocessing_pipeline = Pipeline([(\"poli-feature\", PolynomialFeatures(degree=2)),\n",
    "                                           (\"normalizer\", StandardScaler())\n",
    "                                          ]).fit(self.train_df.input_data(), self.train_df.target())\n",
    "        temp = preprocessing_pipeline[\"poli-feature\"].get_feature_names(self.train_df.input_data().columns)\n",
    "        self.train_data = self.__transform_df(self.train_df.input_data(), preprocessing_pipeline, columns=temp)\n",
    "        self.valid_data = self.__transform_df(self.valid_df.input_data(), preprocessing_pipeline, columns=temp)\n",
    "        self.test_data = self.__transform_df(self.test_df.input_data(), preprocessing_pipeline, columns=temp)\n",
    "\n",
    "        del temp\n",
    "\n",
    "        self.train_data.drop(columns=['1'], inplace=True)\n",
    "        self.valid_data.drop(columns=['1'], inplace=True)\n",
    "        self.test_data.drop(columns=['1'], inplace=True)\n",
    "        \n",
    "        def create_inout_sequences(input_data, labels, seq_len):\n",
    "            inout_seq = []\n",
    "            L = len(input_data)\n",
    "            for i in range(L-seq_len):\n",
    "                train_seq = input_data[i:i+seq_len]\n",
    "#                 train_label = labels[i+seq_len:i+seq_len+1]\n",
    "                train_label = labels[i+1:i+seq_len+1]\n",
    "                inout_seq.append((torch.from_numpy(train_seq),\n",
    "                                  torch.from_numpy(train_label)))\n",
    "            return inout_seq\n",
    "        \n",
    "#         train_x_y\n",
    "        self.train_loader = create_inout_sequences(input_data=self.train_data.values,\n",
    "                                                   labels=self.train_df.target().values,\n",
    "                                                   seq_len=sequence_size)\n",
    "        self.val_loader = create_inout_sequences(input_data=self.valid_data.values,\n",
    "                                                   labels=self.valid_df.target().values,\n",
    "                                                   seq_len=sequence_size)\n",
    "        self.test_loader = create_inout_sequences(input_data=self.test_data.values,\n",
    "                                                  labels=self.test_df.target().values,\n",
    "                                                  seq_len=sequence_size)\n",
    "\n",
    "    def __transform_df(self, df_to_transform, transformer, columns):\n",
    "        return pd.DataFrame(transformer.transform(df_to_transform), columns=columns)\n",
    "    \n",
    "def input_data(self: pd.DataFrame):\n",
    "    return self.drop(columns=[CryptoDataset.True_target_column_name])\n",
    "def target(self: pd.DataFrame):\n",
    "    return self[[CryptoDataset.True_target_column_name]]\n",
    "\n",
    "pd.DataFrame.input_data = input_data\n",
    "pd.DataFrame.target = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(column, sum(df[column].isna())) for column in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=CryptoDataset(predict_delta=1, sequence_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, features, output_size=1,\n",
    "#                  embedding_dim=10,\n",
    "#                  hidden_dim=50,\n",
    "                 n_layers=1,\n",
    "                 drop_prob=0.5, device=torch.device(\"cpu\")):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        \n",
    "        embedding_dim=features*2\n",
    "        hidden_dim=features\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        \n",
    "        self.liniar = nn.Linear(features, embedding_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=n_layers, dropout=drop_prob, batch_first=True)\n",
    "#         self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_dim, hidden_dim//2),\n",
    "                                nn.BatchNorm1d(num_features=hidden_dim//2),\n",
    "                                nn.Dropout(drop_prob),\n",
    "                                nn.Tanh(),\n",
    "                                nn.Linear(hidden_dim//2, output_size))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.float()\n",
    "        \n",
    "        liniar = F.relu(self.bn1(self.liniar(x)))\n",
    "        lstm_out, hidden = self.lstm(liniar.view(batch_size, -1, liniar.shape[1]),\n",
    "                                     (hidden[0][:,-batch_size:,:],# hidden\n",
    "                                      hidden[1][:,-batch_size:,:]))# cell state\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "#         out = self.dropout(lstm_out)\n",
    "        out = self.fc(lstm_out)\n",
    "        out = out.view(batch_size, -1)\n",
    "#         out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(self.device), # hidden\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(self.device)) # cell state\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_count = dataset.train_data.shape[1]\n",
    "# output_size = 1\n",
    "# embedding_dim = 400\n",
    "# hidden_dim = 512\n",
    "# n_layers = 2\n",
    "\n",
    "model = SentimentNet(features_count, device=DEVICE, n_layers=3)\n",
    "model.to(DEVICE)\n",
    "\n",
    "lr=0.001\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "BATCH_SIZE = dataset.sequence_size\n",
    "epochs = 100\n",
    "counter = 0\n",
    "print_every = 5\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr,\n",
    "                                                   steps_per_epoch=len(dataset.train_loader),\n",
    "                                                   epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100... Step: 5... Loss: 0.188898... Val Loss: 0.024285\n",
      "Epoch: 1/100... Step: 10... Loss: 0.189246... Val Loss: 0.037921\n",
      "Epoch: 1/100... Step: 15... Loss: 0.287417... Val Loss: 0.052623\n",
      "Epoch: 1/100... Step: 20... Loss: 0.299838... Val Loss: 0.032359\n",
      "Epoch: 1/100... Step: 25... Loss: 0.271704... Val Loss: 0.019480\n",
      "Validation loss decreased (0.024285 --> 0.019480).  Saving model ...\n",
      "Epoch: 1/100... Step: 30... Loss: 0.318115... Val Loss: 0.021224\n",
      "Epoch: 1/100... Step: 35... Loss: 0.270838... Val Loss: 0.024802\n",
      "Epoch: 1/100... Step: 40... Loss: 0.251612... Val Loss: 0.034869\n",
      "Epoch: 1/100... Step: 45... Loss: 0.386227... Val Loss: 0.060301\n",
      "Epoch: 1/100... Step: 50... Loss: 0.309185... Val Loss: 0.086509\n",
      "Epoch: 1/100... Step: 55... Loss: 0.389384... Val Loss: 0.136414\n",
      "Epoch: 1/100... Step: 60... Loss: 0.244632... Val Loss: 0.249094\n",
      "Epoch: 1/100... Step: 65... Loss: 0.207292... Val Loss: 0.315712\n",
      "Epoch: 1/100... Step: 70... Loss: 0.491625... Val Loss: 0.322441\n",
      "Epoch: 1/100... Step: 75... Loss: 0.184966... Val Loss: 0.270012\n",
      "Epoch: 1/100... Step: 80... Loss: 0.341778... Val Loss: 0.364696\n",
      "Epoch: 1/100... Step: 85... Loss: 0.273070... Val Loss: 0.285022\n",
      "Epoch: 1/100... Step: 90... Loss: 0.278974... Val Loss: 0.252882\n",
      "Epoch: 1/100... Step: 95... Loss: 0.323963... Val Loss: 0.292591\n",
      "Epoch: 1/100... Step: 100... Loss: 0.282490... Val Loss: 0.318447\n",
      "Epoch: 1/100... Step: 105... Loss: 0.191434... Val Loss: 0.470448\n",
      "Epoch: 1/100... Step: 110... Loss: 0.317599... Val Loss: 0.676179\n",
      "Epoch: 1/100... Step: 115... Loss: 0.217248... Val Loss: 0.600249\n",
      "Epoch: 1/100... Step: 120... Loss: 0.201557... Val Loss: 0.492822\n",
      "Epoch: 1/100... Step: 125... Loss: 0.228418... Val Loss: 0.336028\n",
      "Epoch: 1/100... Step: 130... Loss: 0.316786... Val Loss: 0.503364\n",
      "Epoch: 1/100... Step: 135... Loss: 0.368738... Val Loss: 0.701268\n",
      "Epoch: 1/100... Step: 140... Loss: 0.351944... Val Loss: 0.383720\n",
      "Epoch: 1/100... Step: 145... Loss: 0.209035... Val Loss: 0.442630\n",
      "Epoch: 1/100... Step: 150... Loss: 0.238648... Val Loss: 1.241420\n",
      "Epoch: 1/100... Step: 155... Loss: 0.325290... Val Loss: 1.396865\n",
      "Epoch: 1/100... Step: 160... Loss: 0.159162... Val Loss: 1.300858\n",
      "Epoch: 1/100... Step: 165... Loss: 0.319327... Val Loss: 1.101312\n",
      "Epoch: 1/100... Step: 170... Loss: 0.162060... Val Loss: 0.768980\n",
      "Epoch: 1/100... Step: 175... Loss: 0.274691... Val Loss: 0.586865\n",
      "Epoch: 1/100... Step: 180... Loss: 0.415339... Val Loss: 0.454564\n",
      "Epoch: 1/100... Step: 185... Loss: 0.218165... Val Loss: 0.341476\n",
      "Epoch: 1/100... Step: 190... Loss: 0.119463... Val Loss: 0.572029\n",
      "Epoch: 1/100... Step: 195... Loss: 0.297536... Val Loss: 0.875628\n",
      "Epoch: 1/100... Step: 200... Loss: 0.214945... Val Loss: 1.312197\n",
      "Epoch: 1/100... Step: 205... Loss: 0.150925... Val Loss: 1.524941\n",
      "Epoch: 1/100... Step: 210... Loss: 0.201581... Val Loss: 1.406208\n",
      "Epoch: 1/100... Step: 215... Loss: 0.262740... Val Loss: 0.992196\n",
      "Epoch: 1/100... Step: 220... Loss: 0.223166... Val Loss: 0.886767\n",
      "Epoch: 1/100... Step: 225... Loss: 0.214704... Val Loss: 1.039849\n",
      "Epoch: 1/100... Step: 230... Loss: 0.323391... Val Loss: 1.111289\n",
      "Epoch: 1/100... Step: 235... Loss: 0.210522... Val Loss: 1.027089\n",
      "Epoch: 1/100... Step: 240... Loss: 0.197687... Val Loss: 0.797812\n",
      "Epoch: 1/100... Step: 245... Loss: 0.247984... Val Loss: 0.587871\n",
      "Epoch: 1/100... Step: 250... Loss: 0.251007... Val Loss: 0.272410\n",
      "Epoch: 1/100... Step: 255... Loss: 0.234741... Val Loss: 0.269071\n",
      "Epoch: 1/100... Step: 260... Loss: 0.258501... Val Loss: 0.226465\n",
      "Epoch: 1/100... Step: 265... Loss: 0.308279... Val Loss: 0.267250\n",
      "Epoch: 1/100... Step: 270... Loss: 0.196833... Val Loss: 0.450851\n",
      "Epoch: 1/100... Step: 275... Loss: 0.321770... Val Loss: 0.482059\n",
      "Epoch: 1/100... Step: 280... Loss: 0.212944... Val Loss: 0.386464\n",
      "Epoch: 1/100... Step: 285... Loss: 0.243944... Val Loss: 0.342906\n",
      "Epoch: 1/100... Step: 290... Loss: 0.230999... Val Loss: 0.413973\n",
      "Epoch: 1/100... Step: 295... Loss: 0.293868... Val Loss: 0.365589\n",
      "Epoch: 1/100... Step: 300... Loss: 0.194727... Val Loss: 0.342733\n",
      "Epoch: 1/100... Step: 305... Loss: 0.215163... Val Loss: 0.519350\n",
      "Epoch: 1/100... Step: 310... Loss: 0.217573... Val Loss: 0.405141\n",
      "Epoch: 1/100... Step: 315... Loss: 0.182162... Val Loss: 0.275842\n",
      "Epoch: 1/100... Step: 320... Loss: 0.210554... Val Loss: 0.290402\n",
      "Epoch: 1/100... Step: 325... Loss: 0.218555... Val Loss: 0.278728\n",
      "Epoch: 1/100... Step: 330... Loss: 0.250124... Val Loss: 0.261318\n",
      "Epoch: 1/100... Step: 335... Loss: 0.239508... Val Loss: 0.289947\n",
      "Epoch: 1/100... Step: 340... Loss: 0.224786... Val Loss: 0.411488\n",
      "Epoch: 1/100... Step: 345... Loss: 0.193007... Val Loss: 0.546735\n",
      "Epoch: 1/100... Step: 350... Loss: 0.398531... Val Loss: 0.498074\n",
      "Epoch: 1/100... Step: 355... Loss: 0.275619... Val Loss: 0.402530\n",
      "Epoch: 1/100... Step: 360... Loss: 0.202487... Val Loss: 0.493605\n",
      "Epoch: 1/100... Step: 365... Loss: 0.292092... Val Loss: 0.190727\n",
      "Epoch: 1/100... Step: 370... Loss: 0.289565... Val Loss: 0.231159\n",
      "Epoch: 1/100... Step: 375... Loss: 0.221899... Val Loss: 0.277613\n",
      "Epoch: 1/100... Step: 380... Loss: 0.262096... Val Loss: 0.136875\n",
      "Epoch: 1/100... Step: 385... Loss: 0.228006... Val Loss: 0.176059\n",
      "Epoch: 1/100... Step: 390... Loss: 0.272898... Val Loss: 0.338768\n",
      "Epoch: 1/100... Step: 395... Loss: 0.221549... Val Loss: 0.557261\n",
      "Epoch: 1/100... Step: 400... Loss: 0.195056... Val Loss: 0.674477\n",
      "Epoch: 1/100... Step: 405... Loss: 0.213978... Val Loss: 0.678260\n",
      "Epoch: 1/100... Step: 410... Loss: 0.163301... Val Loss: 0.624642\n",
      "Epoch: 1/100... Step: 415... Loss: 0.162980... Val Loss: 0.395610\n",
      "Epoch: 1/100... Step: 420... Loss: 0.212696... Val Loss: 0.340526\n",
      "Epoch: 1/100... Step: 425... Loss: 0.257640... Val Loss: 0.219353\n",
      "Epoch: 1/100... Step: 430... Loss: 0.164292... Val Loss: 0.229883\n",
      "Epoch: 1/100... Step: 435... Loss: 0.259864... Val Loss: 0.164971\n",
      "Epoch: 1/100... Step: 440... Loss: 0.255108... Val Loss: 0.199863\n",
      "Epoch: 1/100... Step: 445... Loss: 0.189409... Val Loss: 0.381353\n",
      "Epoch: 1/100... Step: 450... Loss: 0.260777... Val Loss: 0.404184\n",
      "Epoch: 1/100... Step: 455... Loss: 0.269539... Val Loss: 0.371396\n",
      "Epoch: 1/100... Step: 460... Loss: 0.380818... Val Loss: 0.332956\n",
      "Epoch: 1/100... Step: 465... Loss: 0.152191... Val Loss: 0.317255\n",
      "Epoch: 1/100... Step: 470... Loss: 0.113966... Val Loss: 0.309914\n",
      "Epoch: 1/100... Step: 475... Loss: 0.182836... Val Loss: 0.489449\n",
      "Epoch: 1/100... Step: 480... Loss: 0.160199... Val Loss: 0.436256\n",
      "Epoch: 1/100... Step: 485... Loss: 0.140880... Val Loss: 0.526110\n",
      "Epoch: 1/100... Step: 490... Loss: 0.271210... Val Loss: 0.616074\n",
      "Epoch: 1/100... Step: 495... Loss: 0.150681... Val Loss: 0.661248\n",
      "Epoch: 1/100... Step: 500... Loss: 0.266223... Val Loss: 0.688580\n",
      "Epoch: 1/100... Step: 505... Loss: 0.141382... Val Loss: 0.666413\n",
      "Epoch: 1/100... Step: 510... Loss: 0.265372... Val Loss: 0.484871\n",
      "Epoch: 1/100... Step: 515... Loss: 0.285079... Val Loss: 0.274294\n",
      "Epoch: 1/100... Step: 520... Loss: 0.175999... Val Loss: 0.211354\n",
      "Epoch: 1/100... Step: 525... Loss: 0.135057... Val Loss: 0.208820\n",
      "Epoch: 1/100... Step: 530... Loss: 0.280276... Val Loss: 0.249600\n",
      "Epoch: 1/100... Step: 535... Loss: 0.147825... Val Loss: 0.413653\n",
      "Epoch: 1/100... Step: 540... Loss: 0.313376... Val Loss: 0.518362\n",
      "Epoch: 1/100... Step: 545... Loss: 0.286118... Val Loss: 0.559983\n",
      "Epoch: 1/100... Step: 550... Loss: 0.265057... Val Loss: 0.656839\n",
      "Epoch: 1/100... Step: 555... Loss: 0.126100... Val Loss: 0.737877\n",
      "Epoch: 1/100... Step: 560... Loss: 0.296521... Val Loss: 0.669093\n",
      "Epoch: 1/100... Step: 565... Loss: 0.231210... Val Loss: 0.625405\n",
      "Epoch: 1/100... Step: 570... Loss: 0.234873... Val Loss: 0.620953\n",
      "Epoch: 1/100... Step: 575... Loss: 0.360507... Val Loss: 0.513619\n",
      "Epoch: 1/100... Step: 580... Loss: 0.162612... Val Loss: 0.334239\n",
      "Epoch: 1/100... Step: 585... Loss: 0.252290... Val Loss: 0.253616\n",
      "Epoch: 1/100... Step: 590... Loss: 0.243531... Val Loss: 0.244687\n",
      "Epoch: 1/100... Step: 595... Loss: 0.179648... Val Loss: 0.212051\n",
      "Epoch: 1/100... Step: 600... Loss: 0.237324... Val Loss: 0.213373\n",
      "Epoch: 1/100... Step: 605... Loss: 0.250652... Val Loss: 0.215847\n",
      "Epoch: 1/100... Step: 610... Loss: 0.237394... Val Loss: 0.232148\n",
      "Epoch: 1/100... Step: 615... Loss: 0.200550... Val Loss: 0.187640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100... Step: 620... Loss: 0.167631... Val Loss: 0.121078\n",
      "Epoch: 1/100... Step: 625... Loss: 0.192212... Val Loss: 0.131322\n",
      "Epoch: 1/100... Step: 630... Loss: 0.114149... Val Loss: 0.121214\n",
      "Epoch: 1/100... Step: 635... Loss: 0.268620... Val Loss: 0.114158\n",
      "Epoch: 1/100... Step: 640... Loss: 0.147691... Val Loss: 0.105112\n",
      "Epoch: 1/100... Step: 645... Loss: 0.162385... Val Loss: 0.126802\n",
      "Epoch: 1/100... Step: 650... Loss: 0.232354... Val Loss: 0.133059\n",
      "Epoch: 1/100... Step: 655... Loss: 0.120945... Val Loss: 0.157584\n",
      "Epoch: 1/100... Step: 660... Loss: 0.146626... Val Loss: 0.148892\n",
      "Epoch: 1/100... Step: 665... Loss: 0.132680... Val Loss: 0.182092\n",
      "Epoch: 1/100... Step: 670... Loss: 0.159796... Val Loss: 0.198605\n",
      "Epoch: 1/100... Step: 675... Loss: 0.146725... Val Loss: 0.144578\n",
      "Epoch: 1/100... Step: 680... Loss: 0.286850... Val Loss: 0.242992\n",
      "Epoch: 1/100... Step: 685... Loss: 0.304779... Val Loss: 0.230835\n",
      "Epoch: 1/100... Step: 690... Loss: 0.163666... Val Loss: 0.194608\n",
      "Epoch: 1/100... Step: 695... Loss: 0.115367... Val Loss: 0.267938\n",
      "Epoch: 1/100... Step: 700... Loss: 0.206210... Val Loss: 0.323658\n",
      "Epoch: 1/100... Step: 705... Loss: 0.212352... Val Loss: 0.321023\n",
      "Epoch: 1/100... Step: 710... Loss: 0.094784... Val Loss: 0.379682\n",
      "Epoch: 1/100... Step: 715... Loss: 0.100842... Val Loss: 0.416908\n",
      "Epoch: 1/100... Step: 720... Loss: 0.236350... Val Loss: 0.347164\n",
      "Epoch: 1/100... Step: 725... Loss: 0.153819... Val Loss: 0.168189\n",
      "Epoch: 1/100... Step: 730... Loss: 0.183943... Val Loss: 0.169708\n",
      "Epoch: 1/100... Step: 735... Loss: 0.176231... Val Loss: 0.310090\n",
      "Epoch: 1/100... Step: 740... Loss: 0.247282... Val Loss: 0.368036\n",
      "Epoch: 1/100... Step: 745... Loss: 0.283524... Val Loss: 0.269393\n",
      "Epoch: 1/100... Step: 750... Loss: 0.197706... Val Loss: 0.238805\n",
      "Epoch: 1/100... Step: 755... Loss: 0.297389... Val Loss: 0.173373\n",
      "Epoch: 1/100... Step: 760... Loss: 0.133362... Val Loss: 0.098713\n",
      "Epoch: 1/100... Step: 765... Loss: 0.227333... Val Loss: 0.096168\n",
      "Epoch: 1/100... Step: 770... Loss: 0.135930... Val Loss: 0.089425\n",
      "Epoch: 1/100... Step: 775... Loss: 0.127668... Val Loss: 0.167672\n",
      "Epoch: 1/100... Step: 780... Loss: 0.181797... Val Loss: 0.231597\n",
      "Epoch: 1/100... Step: 785... Loss: 0.170163... Val Loss: 0.404613\n",
      "Epoch: 1/100... Step: 790... Loss: 0.142316... Val Loss: 0.465305\n",
      "Epoch: 1/100... Step: 795... Loss: 0.148333... Val Loss: 0.412501\n",
      "Epoch: 1/100... Step: 800... Loss: 0.153785... Val Loss: 0.295939\n",
      "Epoch: 1/100... Step: 805... Loss: 0.237511... Val Loss: 0.199914\n",
      "Epoch: 1/100... Step: 810... Loss: 0.197466... Val Loss: 0.174312\n",
      "Epoch: 1/100... Step: 815... Loss: 0.139951... Val Loss: 0.177466\n",
      "Epoch: 1/100... Step: 820... Loss: 0.209171... Val Loss: 0.310194\n",
      "Epoch: 1/100... Step: 825... Loss: 0.200912... Val Loss: 0.612006\n",
      "Epoch: 1/100... Step: 830... Loss: 0.170144... Val Loss: 0.658914\n",
      "Epoch: 1/100... Step: 835... Loss: 0.202623... Val Loss: 0.619492\n",
      "Epoch: 1/100... Step: 840... Loss: 0.181002... Val Loss: 0.539637\n",
      "Epoch: 1/100... Step: 845... Loss: 0.233852... Val Loss: 0.392225\n",
      "Epoch: 1/100... Step: 850... Loss: 0.173232... Val Loss: 0.279068\n",
      "Epoch: 1/100... Step: 855... Loss: 0.103191... Val Loss: 0.262005\n",
      "Epoch: 1/100... Step: 860... Loss: 0.172750... Val Loss: 0.333704\n",
      "Epoch: 1/100... Step: 865... Loss: 0.140325... Val Loss: 0.258676\n",
      "Epoch: 1/100... Step: 870... Loss: 0.143962... Val Loss: 0.103695\n",
      "Epoch: 1/100... Step: 875... Loss: 0.161803... Val Loss: 0.180061\n",
      "Epoch: 1/100... Step: 880... Loss: 0.243978... Val Loss: 0.367006\n",
      "Epoch: 1/100... Step: 885... Loss: 0.081125... Val Loss: 0.379057\n",
      "Epoch: 1/100... Step: 890... Loss: 0.132866... Val Loss: 0.239419\n",
      "Epoch: 1/100... Step: 895... Loss: 0.213491... Val Loss: 0.181654\n",
      "Epoch: 1/100... Step: 900... Loss: 0.071314... Val Loss: 0.231696\n",
      "Epoch: 1/100... Step: 905... Loss: 0.124162... Val Loss: 0.243128\n",
      "Epoch: 1/100... Step: 910... Loss: 0.126775... Val Loss: 0.220140\n",
      "Epoch: 1/100... Step: 915... Loss: 0.221373... Val Loss: 0.197610\n",
      "Epoch: 1/100... Step: 920... Loss: 0.125711... Val Loss: 0.221421\n",
      "Epoch: 1/100... Step: 925... Loss: 0.135596... Val Loss: 0.204423\n",
      "Epoch: 1/100... Step: 930... Loss: 0.177366... Val Loss: 0.097496\n",
      "Epoch: 1/100... Step: 935... Loss: 0.176977... Val Loss: 0.063087\n",
      "Epoch: 1/100... Step: 940... Loss: 0.218882... Val Loss: 0.062324\n",
      "Epoch: 1/100... Step: 945... Loss: 0.182808... Val Loss: 0.104810\n",
      "Epoch: 1/100... Step: 950... Loss: 0.184175... Val Loss: 0.277118\n",
      "Epoch: 1/100... Step: 955... Loss: 0.153226... Val Loss: 0.342225\n",
      "Epoch: 1/100... Step: 960... Loss: 0.218780... Val Loss: 0.265827\n",
      "Epoch: 1/100... Step: 965... Loss: 0.195851... Val Loss: 0.119402\n",
      "Epoch: 1/100... Step: 970... Loss: 0.184152... Val Loss: 0.259459\n",
      "Epoch: 1/100... Step: 975... Loss: 0.155561... Val Loss: 0.385559\n",
      "Epoch: 1/100... Step: 980... Loss: 0.071654... Val Loss: 0.373733\n",
      "Epoch: 1/100... Step: 985... Loss: 0.143447... Val Loss: 0.252919\n",
      "Epoch: 1/100... Step: 990... Loss: 0.219055... Val Loss: 0.198137\n",
      "Epoch: 1/100... Step: 995... Loss: 0.215442... Val Loss: 0.223266\n",
      "Epoch: 1/100... Step: 1000... Loss: 0.126571... Val Loss: 0.187619\n",
      "Epoch: 1/100... Step: 1005... Loss: 0.125746... Val Loss: 0.205477\n",
      "Epoch: 1/100... Step: 1010... Loss: 0.198784... Val Loss: 0.242321\n",
      "Epoch: 1/100... Step: 1015... Loss: 0.234341... Val Loss: 0.349100\n",
      "Epoch: 1/100... Step: 1020... Loss: 0.176594... Val Loss: 0.548430\n",
      "Epoch: 1/100... Step: 1025... Loss: 0.179797... Val Loss: 0.587202\n",
      "Epoch: 1/100... Step: 1030... Loss: 0.173150... Val Loss: 0.591719\n",
      "Epoch: 2/100... Step: 1035... Loss: 0.165707... Val Loss: 0.506237\n",
      "Epoch: 2/100... Step: 1040... Loss: 0.258861... Val Loss: 0.398054\n",
      "Epoch: 2/100... Step: 1045... Loss: 0.117307... Val Loss: 0.160881\n",
      "Epoch: 2/100... Step: 1050... Loss: 0.171411... Val Loss: 0.037152\n",
      "Epoch: 2/100... Step: 1055... Loss: 0.122226... Val Loss: 0.049741\n",
      "Epoch: 2/100... Step: 1060... Loss: 0.110996... Val Loss: 0.050847\n",
      "Epoch: 2/100... Step: 1065... Loss: 0.177020... Val Loss: 0.089536\n",
      "Epoch: 2/100... Step: 1070... Loss: 0.175476... Val Loss: 0.135146\n",
      "Epoch: 2/100... Step: 1075... Loss: 0.121296... Val Loss: 0.151136\n",
      "Epoch: 2/100... Step: 1080... Loss: 0.162703... Val Loss: 0.127323\n",
      "Epoch: 2/100... Step: 1085... Loss: 0.115324... Val Loss: 0.110645\n",
      "Epoch: 2/100... Step: 1090... Loss: 0.207080... Val Loss: 0.108995\n",
      "Epoch: 2/100... Step: 1095... Loss: 0.178358... Val Loss: 0.122407\n",
      "Epoch: 2/100... Step: 1100... Loss: 0.145948... Val Loss: 0.165699\n",
      "Epoch: 2/100... Step: 1105... Loss: 0.213534... Val Loss: 0.264835\n",
      "Epoch: 2/100... Step: 1110... Loss: 0.161780... Val Loss: 0.277864\n",
      "Epoch: 2/100... Step: 1115... Loss: 0.096534... Val Loss: 0.249750\n",
      "Epoch: 2/100... Step: 1120... Loss: 0.088951... Val Loss: 0.194227\n",
      "Epoch: 2/100... Step: 1125... Loss: 0.162464... Val Loss: 0.118712\n",
      "Epoch: 2/100... Step: 1130... Loss: 0.247016... Val Loss: 0.152124\n",
      "Epoch: 2/100... Step: 1135... Loss: 0.171647... Val Loss: 0.150074\n",
      "Epoch: 2/100... Step: 1140... Loss: 0.141680... Val Loss: 0.159258\n",
      "Epoch: 2/100... Step: 1145... Loss: 0.129687... Val Loss: 0.177646\n",
      "Epoch: 2/100... Step: 1150... Loss: 0.147435... Val Loss: 0.217166\n",
      "Epoch: 2/100... Step: 1155... Loss: 0.108157... Val Loss: 0.264383\n",
      "Epoch: 2/100... Step: 1160... Loss: 0.141118... Val Loss: 0.294723\n",
      "Epoch: 2/100... Step: 1165... Loss: 0.104720... Val Loss: 0.269330\n",
      "Epoch: 2/100... Step: 1170... Loss: 0.209837... Val Loss: 0.214805\n",
      "Epoch: 2/100... Step: 1175... Loss: 0.153322... Val Loss: 0.141430\n",
      "Epoch: 2/100... Step: 1180... Loss: 0.159544... Val Loss: 0.026389\n",
      "Epoch: 2/100... Step: 1185... Loss: 0.151293... Val Loss: 0.116978\n",
      "Epoch: 2/100... Step: 1190... Loss: 0.117063... Val Loss: 0.122669\n",
      "Epoch: 2/100... Step: 1195... Loss: 0.106902... Val Loss: 0.158405\n",
      "Epoch: 2/100... Step: 1200... Loss: 0.140140... Val Loss: 0.197274\n",
      "Epoch: 2/100... Step: 1205... Loss: 0.209046... Val Loss: 0.176053\n",
      "Epoch: 2/100... Step: 1210... Loss: 0.120394... Val Loss: 0.191281\n",
      "Epoch: 2/100... Step: 1215... Loss: 0.134598... Val Loss: 0.196339\n",
      "Epoch: 2/100... Step: 1220... Loss: 0.166304... Val Loss: 0.211237\n",
      "Epoch: 2/100... Step: 1225... Loss: 0.147147... Val Loss: 0.196657\n",
      "Epoch: 2/100... Step: 1230... Loss: 0.182991... Val Loss: 0.194682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100... Step: 1235... Loss: 0.152946... Val Loss: 0.196368\n",
      "Epoch: 2/100... Step: 1240... Loss: 0.198750... Val Loss: 0.162756\n",
      "Epoch: 2/100... Step: 1245... Loss: 0.165716... Val Loss: 0.102036\n",
      "Epoch: 2/100... Step: 1250... Loss: 0.098570... Val Loss: 0.108603\n",
      "Epoch: 2/100... Step: 1255... Loss: 0.119322... Val Loss: 0.096325\n",
      "Epoch: 2/100... Step: 1260... Loss: 0.119041... Val Loss: 0.090428\n",
      "Epoch: 2/100... Step: 1265... Loss: 0.149880... Val Loss: 0.119535\n",
      "Epoch: 2/100... Step: 1270... Loss: 0.297403... Val Loss: 0.131211\n",
      "Epoch: 2/100... Step: 1275... Loss: 0.159826... Val Loss: 0.125957\n",
      "Epoch: 2/100... Step: 1280... Loss: 0.214251... Val Loss: 0.126679\n",
      "Epoch: 2/100... Step: 1285... Loss: 0.106991... Val Loss: 0.144895\n",
      "Epoch: 2/100... Step: 1290... Loss: 0.131365... Val Loss: 0.168021\n",
      "Epoch: 2/100... Step: 1295... Loss: 0.187886... Val Loss: 0.141879\n",
      "Epoch: 2/100... Step: 1300... Loss: 0.105340... Val Loss: 0.106455\n",
      "Epoch: 2/100... Step: 1305... Loss: 0.096313... Val Loss: 0.078617\n",
      "Epoch: 2/100... Step: 1310... Loss: 0.127966... Val Loss: 0.069889\n",
      "Epoch: 2/100... Step: 1315... Loss: 0.133272... Val Loss: 0.094287\n",
      "Epoch: 2/100... Step: 1320... Loss: 0.206656... Val Loss: 0.113101\n",
      "Epoch: 2/100... Step: 1325... Loss: 0.218552... Val Loss: 0.158511\n",
      "Epoch: 2/100... Step: 1330... Loss: 0.143366... Val Loss: 0.213889\n",
      "Epoch: 2/100... Step: 1335... Loss: 0.125674... Val Loss: 0.224370\n",
      "Epoch: 2/100... Step: 1340... Loss: 0.141365... Val Loss: 0.228223\n",
      "Epoch: 2/100... Step: 1345... Loss: 0.145524... Val Loss: 0.228818\n",
      "Epoch: 2/100... Step: 1350... Loss: 0.085537... Val Loss: 0.241142\n",
      "Epoch: 2/100... Step: 1355... Loss: 0.116633... Val Loss: 0.240808\n",
      "Epoch: 2/100... Step: 1360... Loss: 0.166664... Val Loss: 0.229145\n",
      "Epoch: 2/100... Step: 1365... Loss: 0.254772... Val Loss: 0.173343\n",
      "Epoch: 2/100... Step: 1370... Loss: 0.113301... Val Loss: 0.162072\n",
      "Epoch: 2/100... Step: 1375... Loss: 0.134975... Val Loss: 0.174764\n",
      "Epoch: 2/100... Step: 1380... Loss: 0.133181... Val Loss: 0.172645\n",
      "Epoch: 2/100... Step: 1385... Loss: 0.197304... Val Loss: 0.182484\n",
      "Epoch: 2/100... Step: 1390... Loss: 0.176382... Val Loss: 0.130715\n",
      "Epoch: 2/100... Step: 1395... Loss: 0.172519... Val Loss: 0.085715\n",
      "Epoch: 2/100... Step: 1400... Loss: 0.176734... Val Loss: 0.025546\n",
      "Epoch: 2/100... Step: 1405... Loss: 0.242332... Val Loss: 0.099870\n",
      "Epoch: 2/100... Step: 1410... Loss: 0.158439... Val Loss: 0.132384\n",
      "Epoch: 2/100... Step: 1415... Loss: 0.198337... Val Loss: 0.082768\n",
      "Epoch: 2/100... Step: 1420... Loss: 0.063339... Val Loss: 0.054135\n",
      "Epoch: 2/100... Step: 1425... Loss: 0.129056... Val Loss: 0.105725\n",
      "Epoch: 2/100... Step: 1430... Loss: 0.133805... Val Loss: 0.153162\n",
      "Epoch: 2/100... Step: 1435... Loss: 0.148920... Val Loss: 0.125907\n",
      "Epoch: 2/100... Step: 1440... Loss: 0.246326... Val Loss: 0.141178\n",
      "Epoch: 2/100... Step: 1445... Loss: 0.183408... Val Loss: 0.096838\n",
      "Epoch: 2/100... Step: 1450... Loss: 0.144410... Val Loss: 0.032547\n",
      "Epoch: 2/100... Step: 1455... Loss: 0.190742... Val Loss: 0.025791\n",
      "Epoch: 2/100... Step: 1460... Loss: 0.253000... Val Loss: 0.047931\n",
      "Epoch: 2/100... Step: 1465... Loss: 0.179427... Val Loss: 0.030631\n",
      "Epoch: 2/100... Step: 1470... Loss: 0.167533... Val Loss: 0.045482\n",
      "Epoch: 2/100... Step: 1475... Loss: 0.225959... Val Loss: 0.084885\n",
      "Epoch: 2/100... Step: 1480... Loss: 0.107259... Val Loss: 0.111743\n",
      "Epoch: 2/100... Step: 1485... Loss: 0.155648... Val Loss: 0.104383\n",
      "Epoch: 2/100... Step: 1490... Loss: 0.081065... Val Loss: 0.102661\n",
      "Epoch: 2/100... Step: 1495... Loss: 0.128327... Val Loss: 0.127427\n",
      "Epoch: 2/100... Step: 1500... Loss: 0.191884... Val Loss: 0.123148\n",
      "Epoch: 2/100... Step: 1505... Loss: 0.109445... Val Loss: 0.168393\n",
      "Epoch: 2/100... Step: 1510... Loss: 0.136279... Val Loss: 0.154868\n",
      "Epoch: 2/100... Step: 1515... Loss: 0.199151... Val Loss: 0.078913\n",
      "Epoch: 2/100... Step: 1520... Loss: 0.092966... Val Loss: 0.068502\n",
      "Epoch: 2/100... Step: 1525... Loss: 0.166006... Val Loss: 0.108839\n",
      "Epoch: 2/100... Step: 1530... Loss: 0.185439... Val Loss: 0.114464\n",
      "Epoch: 2/100... Step: 1535... Loss: 0.150070... Val Loss: 0.095144\n",
      "Epoch: 2/100... Step: 1540... Loss: 0.091942... Val Loss: 0.065054\n",
      "Epoch: 2/100... Step: 1545... Loss: 0.118397... Val Loss: 0.031981\n",
      "Epoch: 2/100... Step: 1550... Loss: 0.166937... Val Loss: 0.023462\n",
      "Epoch: 2/100... Step: 1555... Loss: 0.138726... Val Loss: 0.032160\n",
      "Epoch: 2/100... Step: 1560... Loss: 0.185232... Val Loss: 0.047948\n",
      "Epoch: 2/100... Step: 1565... Loss: 0.184621... Val Loss: 0.094982\n",
      "Epoch: 2/100... Step: 1570... Loss: 0.177451... Val Loss: 0.113499\n",
      "Epoch: 2/100... Step: 1575... Loss: 0.153529... Val Loss: 0.113871\n",
      "Epoch: 2/100... Step: 1580... Loss: 0.087002... Val Loss: 0.124738\n",
      "Epoch: 2/100... Step: 1585... Loss: 0.112310... Val Loss: 0.087205\n",
      "Epoch: 2/100... Step: 1590... Loss: 0.146029... Val Loss: 0.047992\n",
      "Epoch: 2/100... Step: 1595... Loss: 0.137292... Val Loss: 0.042219\n",
      "Epoch: 2/100... Step: 1600... Loss: 0.154340... Val Loss: 0.052440\n",
      "Epoch: 2/100... Step: 1605... Loss: 0.175344... Val Loss: 0.092221\n",
      "Epoch: 2/100... Step: 1610... Loss: 0.187387... Val Loss: 0.107140\n",
      "Epoch: 2/100... Step: 1615... Loss: 0.178479... Val Loss: 0.068713\n",
      "Epoch: 2/100... Step: 1620... Loss: 0.121973... Val Loss: 0.052347\n",
      "Epoch: 2/100... Step: 1625... Loss: 0.122055... Val Loss: 0.101488\n",
      "Epoch: 2/100... Step: 1630... Loss: 0.174967... Val Loss: 0.129461\n",
      "Epoch: 2/100... Step: 1635... Loss: 0.184324... Val Loss: 0.154969\n",
      "Epoch: 2/100... Step: 1640... Loss: 0.126841... Val Loss: 0.164548\n",
      "Epoch: 2/100... Step: 1645... Loss: 0.112528... Val Loss: 0.125596\n",
      "Epoch: 2/100... Step: 1650... Loss: 0.140244... Val Loss: 0.058284\n",
      "Epoch: 2/100... Step: 1655... Loss: 0.123738... Val Loss: 0.038800\n",
      "Epoch: 2/100... Step: 1660... Loss: 0.122593... Val Loss: 0.062852\n",
      "Epoch: 2/100... Step: 1665... Loss: 0.232146... Val Loss: 0.107408\n",
      "Epoch: 2/100... Step: 1670... Loss: 0.129573... Val Loss: 0.125033\n",
      "Epoch: 2/100... Step: 1675... Loss: 0.092750... Val Loss: 0.158307\n",
      "Epoch: 2/100... Step: 1680... Loss: 0.105568... Val Loss: 0.086081\n",
      "Epoch: 2/100... Step: 1685... Loss: 0.117632... Val Loss: 0.138806\n",
      "Epoch: 2/100... Step: 1690... Loss: 0.144066... Val Loss: 0.192123\n",
      "Epoch: 2/100... Step: 1695... Loss: 0.143019... Val Loss: 0.200947\n",
      "Epoch: 2/100... Step: 1700... Loss: 0.105442... Val Loss: 0.236975\n",
      "Epoch: 2/100... Step: 1705... Loss: 0.187624... Val Loss: 0.163914\n",
      "Epoch: 2/100... Step: 1710... Loss: 0.153065... Val Loss: 0.118297\n",
      "Epoch: 2/100... Step: 1715... Loss: 0.142913... Val Loss: 0.091672\n",
      "Epoch: 2/100... Step: 1720... Loss: 0.096426... Val Loss: 0.059175\n",
      "Epoch: 2/100... Step: 1725... Loss: 0.095591... Val Loss: 0.054207\n",
      "Epoch: 2/100... Step: 1730... Loss: 0.090383... Val Loss: 0.089351\n",
      "Epoch: 2/100... Step: 1735... Loss: 0.140697... Val Loss: 0.114972\n",
      "Epoch: 2/100... Step: 1740... Loss: 0.174679... Val Loss: 0.109108\n",
      "Epoch: 2/100... Step: 1745... Loss: 0.143489... Val Loss: 0.101771\n",
      "Epoch: 2/100... Step: 1750... Loss: 0.157037... Val Loss: 0.100975\n",
      "Epoch: 2/100... Step: 1755... Loss: 0.135068... Val Loss: 0.101831\n",
      "Epoch: 2/100... Step: 1760... Loss: 0.168851... Val Loss: 0.121608\n",
      "Epoch: 2/100... Step: 1765... Loss: 0.168577... Val Loss: 0.142034\n",
      "Epoch: 2/100... Step: 1770... Loss: 0.159588... Val Loss: 0.157916\n",
      "Epoch: 2/100... Step: 1775... Loss: 0.122483... Val Loss: 0.140155\n",
      "Epoch: 2/100... Step: 1780... Loss: 0.118312... Val Loss: 0.069492\n",
      "Epoch: 2/100... Step: 1785... Loss: 0.156737... Val Loss: 0.057953\n",
      "Epoch: 2/100... Step: 1790... Loss: 0.168534... Val Loss: 0.057672\n",
      "Epoch: 2/100... Step: 1795... Loss: 0.141810... Val Loss: 0.070789\n",
      "Epoch: 2/100... Step: 1800... Loss: 0.099925... Val Loss: 0.095717\n",
      "Epoch: 2/100... Step: 1805... Loss: 0.116178... Val Loss: 0.082881\n",
      "Epoch: 2/100... Step: 1810... Loss: 0.072767... Val Loss: 0.078383\n",
      "Epoch: 2/100... Step: 1815... Loss: 0.088727... Val Loss: 0.069035\n",
      "Epoch: 2/100... Step: 1820... Loss: 0.148901... Val Loss: 0.055985\n",
      "Epoch: 2/100... Step: 1825... Loss: 0.179815... Val Loss: 0.065370\n",
      "Epoch: 2/100... Step: 1830... Loss: 0.104645... Val Loss: 0.081095\n",
      "Epoch: 2/100... Step: 1835... Loss: 0.115759... Val Loss: 0.122764\n",
      "Epoch: 2/100... Step: 1840... Loss: 0.143994... Val Loss: 0.080412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100... Step: 1845... Loss: 0.114168... Val Loss: 0.024526\n",
      "Epoch: 2/100... Step: 1850... Loss: 0.144591... Val Loss: 0.020178\n",
      "Epoch: 2/100... Step: 1855... Loss: 0.178225... Val Loss: 0.049075\n",
      "Epoch: 2/100... Step: 1860... Loss: 0.114025... Val Loss: 0.068655\n",
      "Epoch: 2/100... Step: 1865... Loss: 0.132236... Val Loss: 0.061460\n",
      "Epoch: 2/100... Step: 1870... Loss: 0.181163... Val Loss: 0.046596\n",
      "Epoch: 2/100... Step: 1875... Loss: 0.099902... Val Loss: 0.038549\n",
      "Epoch: 2/100... Step: 1880... Loss: 0.180273... Val Loss: 0.039476\n",
      "Epoch: 2/100... Step: 1885... Loss: 0.239404... Val Loss: 0.032180\n",
      "Epoch: 2/100... Step: 1890... Loss: 0.094252... Val Loss: 0.024004\n",
      "Epoch: 2/100... Step: 1895... Loss: 0.126030... Val Loss: 0.023949\n",
      "Epoch: 2/100... Step: 1900... Loss: 0.133071... Val Loss: 0.004137\n",
      "Validation loss decreased (0.019480 --> 0.004137).  Saving model ...\n",
      "Epoch: 2/100... Step: 1905... Loss: 0.144408... Val Loss: 0.013898\n",
      "Epoch: 2/100... Step: 1910... Loss: 0.165445... Val Loss: 0.019409\n",
      "Epoch: 2/100... Step: 1915... Loss: 0.140025... Val Loss: 0.024530\n",
      "Epoch: 2/100... Step: 1920... Loss: 0.135149... Val Loss: 0.061325\n",
      "Epoch: 2/100... Step: 1925... Loss: 0.130399... Val Loss: 0.127137\n",
      "Epoch: 2/100... Step: 1930... Loss: 0.130786... Val Loss: 0.135238\n",
      "Epoch: 2/100... Step: 1935... Loss: 0.156819... Val Loss: 0.119352\n",
      "Epoch: 2/100... Step: 1940... Loss: 0.125067... Val Loss: 0.129153\n",
      "Epoch: 2/100... Step: 1945... Loss: 0.157009... Val Loss: 0.131335\n",
      "Epoch: 2/100... Step: 1950... Loss: 0.098205... Val Loss: 0.140326\n",
      "Epoch: 2/100... Step: 1955... Loss: 0.144077... Val Loss: 0.095815\n",
      "Epoch: 2/100... Step: 1960... Loss: 0.166544... Val Loss: 0.041252\n",
      "Epoch: 2/100... Step: 1965... Loss: 0.144706... Val Loss: 0.020908\n",
      "Epoch: 2/100... Step: 1970... Loss: 0.089599... Val Loss: 0.023926\n",
      "Epoch: 2/100... Step: 1975... Loss: 0.152658... Val Loss: 0.070726\n",
      "Epoch: 2/100... Step: 1980... Loss: 0.160922... Val Loss: 0.111205\n",
      "Epoch: 2/100... Step: 1985... Loss: 0.195907... Val Loss: 0.158307\n",
      "Epoch: 2/100... Step: 1990... Loss: 0.091264... Val Loss: 0.114383\n",
      "Epoch: 2/100... Step: 1995... Loss: 0.138971... Val Loss: 0.032132\n",
      "Epoch: 2/100... Step: 2000... Loss: 0.104813... Val Loss: 0.045852\n",
      "Epoch: 2/100... Step: 2005... Loss: 0.185791... Val Loss: 0.060508\n",
      "Epoch: 2/100... Step: 2010... Loss: 0.146994... Val Loss: 0.045826\n",
      "Epoch: 2/100... Step: 2015... Loss: 0.098966... Val Loss: 0.043752\n",
      "Epoch: 2/100... Step: 2020... Loss: 0.077584... Val Loss: 0.051634\n",
      "Epoch: 2/100... Step: 2025... Loss: 0.170516... Val Loss: 0.097159\n",
      "Epoch: 2/100... Step: 2030... Loss: 0.177708... Val Loss: 0.109722\n",
      "Epoch: 2/100... Step: 2035... Loss: 0.121674... Val Loss: 0.074323\n",
      "Epoch: 2/100... Step: 2040... Loss: 0.103911... Val Loss: 0.075058\n",
      "Epoch: 2/100... Step: 2045... Loss: 0.261632... Val Loss: 0.108522\n",
      "Epoch: 2/100... Step: 2050... Loss: 0.165516... Val Loss: 0.147340\n",
      "Epoch: 2/100... Step: 2055... Loss: 0.088936... Val Loss: 0.137571\n",
      "Epoch: 2/100... Step: 2060... Loss: 0.181990... Val Loss: 0.102845\n",
      "Epoch: 3/100... Step: 2065... Loss: 0.143376... Val Loss: 0.073324\n",
      "Epoch: 3/100... Step: 2070... Loss: 0.128524... Val Loss: 0.045661\n",
      "Epoch: 3/100... Step: 2075... Loss: 0.101433... Val Loss: 0.020062\n",
      "Epoch: 3/100... Step: 2080... Loss: 0.208453... Val Loss: 0.068663\n",
      "Epoch: 3/100... Step: 2085... Loss: 0.126513... Val Loss: 0.121936\n",
      "Epoch: 3/100... Step: 2090... Loss: 0.100208... Val Loss: 0.127438\n",
      "Epoch: 3/100... Step: 2095... Loss: 0.082826... Val Loss: 0.070923\n",
      "Epoch: 3/100... Step: 2100... Loss: 0.104471... Val Loss: 0.027920\n",
      "Epoch: 3/100... Step: 2105... Loss: 0.109414... Val Loss: 0.057940\n",
      "Epoch: 3/100... Step: 2110... Loss: 0.117370... Val Loss: 0.111223\n",
      "Epoch: 3/100... Step: 2115... Loss: 0.191098... Val Loss: 0.113783\n",
      "Epoch: 3/100... Step: 2120... Loss: 0.096563... Val Loss: 0.094276\n",
      "Epoch: 3/100... Step: 2125... Loss: 0.108839... Val Loss: 0.062801\n",
      "Epoch: 3/100... Step: 2130... Loss: 0.211070... Val Loss: 0.033030\n",
      "Epoch: 3/100... Step: 2135... Loss: 0.088623... Val Loss: 0.075673\n",
      "Epoch: 3/100... Step: 2140... Loss: 0.129842... Val Loss: 0.102719\n",
      "Epoch: 3/100... Step: 2145... Loss: 0.096781... Val Loss: 0.122016\n",
      "Epoch: 3/100... Step: 2150... Loss: 0.114707... Val Loss: 0.079702\n",
      "Epoch: 3/100... Step: 2155... Loss: 0.090703... Val Loss: 0.028044\n",
      "Epoch: 3/100... Step: 2160... Loss: 0.088735... Val Loss: 0.051794\n",
      "Epoch: 3/100... Step: 2165... Loss: 0.095517... Val Loss: 0.098198\n",
      "Epoch: 3/100... Step: 2170... Loss: 0.085103... Val Loss: 0.111457\n",
      "Epoch: 3/100... Step: 2175... Loss: 0.121513... Val Loss: 0.114101\n",
      "Epoch: 3/100... Step: 2180... Loss: 0.133393... Val Loss: 0.068275\n",
      "Epoch: 3/100... Step: 2185... Loss: 0.108058... Val Loss: 0.032145\n",
      "Epoch: 3/100... Step: 2190... Loss: 0.130762... Val Loss: 0.010195\n",
      "Epoch: 3/100... Step: 2195... Loss: 0.086800... Val Loss: 0.009380\n",
      "Epoch: 3/100... Step: 2200... Loss: 0.140208... Val Loss: 0.029834\n",
      "Epoch: 3/100... Step: 2205... Loss: 0.101969... Val Loss: 0.032340\n",
      "Epoch: 3/100... Step: 2210... Loss: 0.105794... Val Loss: 0.031932\n",
      "Epoch: 3/100... Step: 2215... Loss: 0.108787... Val Loss: 0.028677\n",
      "Epoch: 3/100... Step: 2220... Loss: 0.101861... Val Loss: 0.021250\n",
      "Epoch: 3/100... Step: 2225... Loss: 0.099570... Val Loss: 0.017197\n",
      "Epoch: 3/100... Step: 2230... Loss: 0.139004... Val Loss: 0.049487\n",
      "Epoch: 3/100... Step: 2235... Loss: 0.154720... Val Loss: 0.075622\n",
      "Epoch: 3/100... Step: 2240... Loss: 0.086374... Val Loss: 0.107598\n",
      "Epoch: 3/100... Step: 2245... Loss: 0.127064... Val Loss: 0.138643\n",
      "Epoch: 3/100... Step: 2250... Loss: 0.045930... Val Loss: 0.130908\n",
      "Epoch: 3/100... Step: 2255... Loss: 0.108992... Val Loss: 0.118546\n",
      "Epoch: 3/100... Step: 2260... Loss: 0.098594... Val Loss: 0.122692\n",
      "Epoch: 3/100... Step: 2265... Loss: 0.115431... Val Loss: 0.106710\n",
      "Epoch: 3/100... Step: 2270... Loss: 0.142619... Val Loss: 0.125741\n",
      "Epoch: 3/100... Step: 2275... Loss: 0.145407... Val Loss: 0.113549\n",
      "Epoch: 3/100... Step: 2280... Loss: 0.155278... Val Loss: 0.125218\n",
      "Epoch: 3/100... Step: 2285... Loss: 0.153575... Val Loss: 0.091571\n",
      "Epoch: 3/100... Step: 2290... Loss: 0.075284... Val Loss: 0.039118\n",
      "Epoch: 3/100... Step: 2295... Loss: 0.103431... Val Loss: 0.041088\n",
      "Epoch: 3/100... Step: 2300... Loss: 0.104667... Val Loss: 0.058082\n",
      "Epoch: 3/100... Step: 2305... Loss: 0.077373... Val Loss: 0.070662\n",
      "Epoch: 3/100... Step: 2310... Loss: 0.147046... Val Loss: 0.072969\n",
      "Epoch: 3/100... Step: 2315... Loss: 0.128663... Val Loss: 0.065303\n",
      "Epoch: 3/100... Step: 2320... Loss: 0.125509... Val Loss: 0.049126\n",
      "Epoch: 3/100... Step: 2325... Loss: 0.099105... Val Loss: 0.062806\n",
      "Epoch: 3/100... Step: 2330... Loss: 0.145094... Val Loss: 0.077280\n",
      "Epoch: 3/100... Step: 2335... Loss: 0.129155... Val Loss: 0.076015\n",
      "Epoch: 3/100... Step: 2340... Loss: 0.056641... Val Loss: 0.077148\n",
      "Epoch: 3/100... Step: 2345... Loss: 0.134482... Val Loss: 0.116734\n",
      "Epoch: 3/100... Step: 2350... Loss: 0.075565... Val Loss: 0.148914\n",
      "Epoch: 3/100... Step: 2355... Loss: 0.123658... Val Loss: 0.134749\n",
      "Epoch: 3/100... Step: 2360... Loss: 0.090730... Val Loss: 0.110188\n",
      "Epoch: 3/100... Step: 2365... Loss: 0.088017... Val Loss: 0.098212\n",
      "Epoch: 3/100... Step: 2370... Loss: 0.131229... Val Loss: 0.113871\n",
      "Epoch: 3/100... Step: 2375... Loss: 0.126774... Val Loss: 0.138236\n",
      "Epoch: 3/100... Step: 2380... Loss: 0.109742... Val Loss: 0.163819\n",
      "Epoch: 3/100... Step: 2385... Loss: 0.081505... Val Loss: 0.182319\n",
      "Epoch: 3/100... Step: 2390... Loss: 0.110030... Val Loss: 0.173830\n",
      "Epoch: 3/100... Step: 2395... Loss: 0.070461... Val Loss: 0.150385\n",
      "Epoch: 3/100... Step: 2400... Loss: 0.138950... Val Loss: 0.123365\n",
      "Epoch: 3/100... Step: 2405... Loss: 0.071866... Val Loss: 0.171606\n",
      "Epoch: 3/100... Step: 2410... Loss: 0.126683... Val Loss: 0.148896\n",
      "Epoch: 3/100... Step: 2415... Loss: 0.116868... Val Loss: 0.147371\n",
      "Epoch: 3/100... Step: 2420... Loss: 0.073980... Val Loss: 0.087279\n",
      "Epoch: 3/100... Step: 2425... Loss: 0.070389... Val Loss: 0.027988\n",
      "Epoch: 3/100... Step: 2430... Loss: 0.120842... Val Loss: 0.059802\n",
      "Epoch: 3/100... Step: 2435... Loss: 0.087871... Val Loss: 0.089616\n",
      "Epoch: 3/100... Step: 2440... Loss: 0.079887... Val Loss: 0.074816\n",
      "Epoch: 3/100... Step: 2445... Loss: 0.107771... Val Loss: 0.061124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/100... Step: 2450... Loss: 0.118428... Val Loss: 0.048265\n",
      "Epoch: 3/100... Step: 2455... Loss: 0.117370... Val Loss: 0.020631\n",
      "Epoch: 3/100... Step: 2460... Loss: 0.144241... Val Loss: 0.045068\n",
      "Epoch: 3/100... Step: 2465... Loss: 0.085943... Val Loss: 0.068962\n",
      "Epoch: 3/100... Step: 2470... Loss: 0.107329... Val Loss: 0.118674\n",
      "Epoch: 3/100... Step: 2475... Loss: 0.105505... Val Loss: 0.190749\n",
      "Epoch: 3/100... Step: 2480... Loss: 0.111497... Val Loss: 0.160808\n",
      "Epoch: 3/100... Step: 2485... Loss: 0.097901... Val Loss: 0.046798\n",
      "Epoch: 3/100... Step: 2490... Loss: 0.095641... Val Loss: 0.068253\n",
      "Epoch: 3/100... Step: 2495... Loss: 0.131967... Val Loss: 0.140517\n",
      "Epoch: 3/100... Step: 2500... Loss: 0.101590... Val Loss: 0.132096\n",
      "Epoch: 3/100... Step: 2505... Loss: 0.059195... Val Loss: 0.076327\n",
      "Epoch: 3/100... Step: 2510... Loss: 0.106367... Val Loss: 0.064346\n",
      "Epoch: 3/100... Step: 2515... Loss: 0.113152... Val Loss: 0.120209\n",
      "Epoch: 3/100... Step: 2520... Loss: 0.115656... Val Loss: 0.184509\n",
      "Epoch: 3/100... Step: 2525... Loss: 0.074363... Val Loss: 0.246494\n",
      "Epoch: 3/100... Step: 2530... Loss: 0.100834... Val Loss: 0.222169\n",
      "Epoch: 3/100... Step: 2535... Loss: 0.110722... Val Loss: 0.214155\n",
      "Epoch: 3/100... Step: 2540... Loss: 0.093672... Val Loss: 0.242023\n",
      "Epoch: 3/100... Step: 2545... Loss: 0.169982... Val Loss: 0.193977\n",
      "Epoch: 3/100... Step: 2550... Loss: 0.063254... Val Loss: 0.135214\n",
      "Epoch: 3/100... Step: 2555... Loss: 0.140244... Val Loss: 0.086000\n",
      "Epoch: 3/100... Step: 2560... Loss: 0.103886... Val Loss: 0.109418\n",
      "Epoch: 3/100... Step: 2565... Loss: 0.165072... Val Loss: 0.135395\n",
      "Epoch: 3/100... Step: 2570... Loss: 0.069367... Val Loss: 0.153767\n",
      "Epoch: 3/100... Step: 2575... Loss: 0.121868... Val Loss: 0.129454\n",
      "Epoch: 3/100... Step: 2580... Loss: 0.076556... Val Loss: 0.070348\n",
      "Epoch: 3/100... Step: 2585... Loss: 0.096267... Val Loss: 0.036466\n",
      "Epoch: 3/100... Step: 2590... Loss: 0.140147... Val Loss: 0.068688\n",
      "Epoch: 3/100... Step: 2595... Loss: 0.078813... Val Loss: 0.091535\n",
      "Epoch: 3/100... Step: 2600... Loss: 0.112616... Val Loss: 0.109420\n",
      "Epoch: 3/100... Step: 2605... Loss: 0.176997... Val Loss: 0.106831\n",
      "Epoch: 3/100... Step: 2610... Loss: 0.103826... Val Loss: 0.100112\n",
      "Epoch: 3/100... Step: 2615... Loss: 0.120373... Val Loss: 0.074301\n",
      "Epoch: 3/100... Step: 2620... Loss: 0.120922... Val Loss: 0.068868\n",
      "Epoch: 3/100... Step: 2625... Loss: 0.068824... Val Loss: 0.059657\n",
      "Epoch: 3/100... Step: 2630... Loss: 0.129260... Val Loss: 0.037509\n",
      "Epoch: 3/100... Step: 2635... Loss: 0.034568... Val Loss: 0.038191\n",
      "Epoch: 3/100... Step: 2640... Loss: 0.105710... Val Loss: 0.035366\n",
      "Epoch: 3/100... Step: 2645... Loss: 0.062070... Val Loss: 0.052375\n",
      "Epoch: 3/100... Step: 2650... Loss: 0.094448... Val Loss: 0.072021\n",
      "Epoch: 3/100... Step: 2655... Loss: 0.086648... Val Loss: 0.068705\n",
      "Epoch: 3/100... Step: 2660... Loss: 0.117649... Val Loss: 0.058132\n",
      "Epoch: 3/100... Step: 2665... Loss: 0.077607... Val Loss: 0.058789\n",
      "Epoch: 3/100... Step: 2670... Loss: 0.098264... Val Loss: 0.071351\n",
      "Epoch: 3/100... Step: 2675... Loss: 0.088337... Val Loss: 0.076716\n",
      "Epoch: 3/100... Step: 2680... Loss: 0.084963... Val Loss: 0.101662\n",
      "Epoch: 3/100... Step: 2685... Loss: 0.102809... Val Loss: 0.144117\n",
      "Epoch: 3/100... Step: 2690... Loss: 0.052251... Val Loss: 0.126552\n",
      "Epoch: 3/100... Step: 2695... Loss: 0.064439... Val Loss: 0.096417\n",
      "Epoch: 3/100... Step: 2700... Loss: 0.072898... Val Loss: 0.080462\n",
      "Epoch: 3/100... Step: 2705... Loss: 0.086414... Val Loss: 0.070780\n",
      "Epoch: 3/100... Step: 2710... Loss: 0.090514... Val Loss: 0.065818\n",
      "Epoch: 3/100... Step: 2715... Loss: 0.058820... Val Loss: 0.071497\n",
      "Epoch: 3/100... Step: 2720... Loss: 0.040457... Val Loss: 0.062786\n",
      "Epoch: 3/100... Step: 2725... Loss: 0.084598... Val Loss: 0.045442\n",
      "Epoch: 3/100... Step: 2730... Loss: 0.107772... Val Loss: 0.034868\n",
      "Epoch: 3/100... Step: 2735... Loss: 0.118529... Val Loss: 0.064300\n",
      "Epoch: 3/100... Step: 2740... Loss: 0.047302... Val Loss: 0.091675\n",
      "Epoch: 3/100... Step: 2745... Loss: 0.057659... Val Loss: 0.123413\n",
      "Epoch: 3/100... Step: 2750... Loss: 0.096780... Val Loss: 0.145889\n",
      "Epoch: 3/100... Step: 2755... Loss: 0.069893... Val Loss: 0.160062\n",
      "Epoch: 3/100... Step: 2760... Loss: 0.085149... Val Loss: 0.139624\n",
      "Epoch: 3/100... Step: 2765... Loss: 0.120413... Val Loss: 0.108553\n",
      "Epoch: 3/100... Step: 2770... Loss: 0.123444... Val Loss: 0.056424\n",
      "Epoch: 3/100... Step: 2775... Loss: 0.075306... Val Loss: 0.070837\n",
      "Epoch: 3/100... Step: 2780... Loss: 0.163129... Val Loss: 0.077908\n",
      "Epoch: 3/100... Step: 2785... Loss: 0.097641... Val Loss: 0.055886\n",
      "Epoch: 3/100... Step: 2790... Loss: 0.137457... Val Loss: 0.052244\n",
      "Epoch: 3/100... Step: 2795... Loss: 0.061188... Val Loss: 0.057312\n",
      "Epoch: 3/100... Step: 2800... Loss: 0.103833... Val Loss: 0.085963\n",
      "Epoch: 3/100... Step: 2805... Loss: 0.057155... Val Loss: 0.094179\n",
      "Epoch: 3/100... Step: 2810... Loss: 0.039977... Val Loss: 0.077641\n",
      "Epoch: 3/100... Step: 2815... Loss: 0.111443... Val Loss: 0.059457\n",
      "Epoch: 3/100... Step: 2820... Loss: 0.069408... Val Loss: 0.021067\n",
      "Epoch: 3/100... Step: 2825... Loss: 0.095127... Val Loss: 0.034585\n",
      "Epoch: 3/100... Step: 2830... Loss: 0.059516... Val Loss: 0.080437\n",
      "Epoch: 3/100... Step: 2835... Loss: 0.084754... Val Loss: 0.126210\n",
      "Epoch: 3/100... Step: 2840... Loss: 0.099433... Val Loss: 0.153910\n",
      "Epoch: 3/100... Step: 2845... Loss: 0.116123... Val Loss: 0.141993\n",
      "Epoch: 3/100... Step: 2850... Loss: 0.092116... Val Loss: 0.076461\n",
      "Epoch: 3/100... Step: 2855... Loss: 0.065643... Val Loss: 0.019107\n",
      "Epoch: 3/100... Step: 2860... Loss: 0.072273... Val Loss: 0.031917\n",
      "Epoch: 3/100... Step: 2865... Loss: 0.082806... Val Loss: 0.089219\n",
      "Epoch: 3/100... Step: 2870... Loss: 0.070176... Val Loss: 0.111578\n",
      "Epoch: 3/100... Step: 2875... Loss: 0.088678... Val Loss: 0.069707\n",
      "Epoch: 3/100... Step: 2880... Loss: 0.122703... Val Loss: 0.038303\n",
      "Epoch: 3/100... Step: 2885... Loss: 0.074866... Val Loss: 0.012763\n",
      "Epoch: 3/100... Step: 2890... Loss: 0.090598... Val Loss: 0.055653\n",
      "Epoch: 3/100... Step: 2895... Loss: 0.157719... Val Loss: 0.083608\n",
      "Epoch: 3/100... Step: 2900... Loss: 0.112432... Val Loss: 0.079106\n",
      "Epoch: 3/100... Step: 2905... Loss: 0.095736... Val Loss: 0.051832\n",
      "Epoch: 3/100... Step: 2910... Loss: 0.052256... Val Loss: 0.049557\n",
      "Epoch: 3/100... Step: 2915... Loss: 0.104333... Val Loss: 0.045572\n",
      "Epoch: 3/100... Step: 2920... Loss: 0.074278... Val Loss: 0.041487\n",
      "Epoch: 3/100... Step: 2925... Loss: 0.104041... Val Loss: 0.034369\n",
      "Epoch: 3/100... Step: 2930... Loss: 0.098672... Val Loss: 0.020581\n",
      "Epoch: 3/100... Step: 2935... Loss: 0.063761... Val Loss: 0.012414\n",
      "Epoch: 3/100... Step: 2940... Loss: 0.114247... Val Loss: 0.026836\n",
      "Epoch: 3/100... Step: 2945... Loss: 0.082241... Val Loss: 0.055155\n",
      "Epoch: 3/100... Step: 2950... Loss: 0.109960... Val Loss: 0.054288\n",
      "Epoch: 3/100... Step: 2955... Loss: 0.058244... Val Loss: 0.031380\n",
      "Epoch: 3/100... Step: 2960... Loss: 0.078394... Val Loss: 0.030986\n",
      "Epoch: 3/100... Step: 2965... Loss: 0.055628... Val Loss: 0.034555\n",
      "Epoch: 3/100... Step: 2970... Loss: 0.067950... Val Loss: 0.043225\n",
      "Epoch: 3/100... Step: 2975... Loss: 0.094099... Val Loss: 0.043417\n",
      "Epoch: 3/100... Step: 2980... Loss: 0.076046... Val Loss: 0.057167\n",
      "Epoch: 3/100... Step: 2985... Loss: 0.062345... Val Loss: 0.041773\n",
      "Epoch: 3/100... Step: 2990... Loss: 0.083945... Val Loss: 0.028897\n",
      "Epoch: 3/100... Step: 2995... Loss: 0.062086... Val Loss: 0.018398\n",
      "Epoch: 3/100... Step: 3000... Loss: 0.088880... Val Loss: 0.005462\n",
      "Epoch: 3/100... Step: 3005... Loss: 0.072690... Val Loss: 0.004357\n",
      "Epoch: 3/100... Step: 3010... Loss: 0.090678... Val Loss: 0.002867\n",
      "Validation loss decreased (0.004137 --> 0.002867).  Saving model ...\n",
      "Epoch: 3/100... Step: 3015... Loss: 0.094368... Val Loss: 0.027765\n",
      "Epoch: 3/100... Step: 3020... Loss: 0.104061... Val Loss: 0.055026\n",
      "Epoch: 3/100... Step: 3025... Loss: 0.071375... Val Loss: 0.036822\n",
      "Epoch: 3/100... Step: 3030... Loss: 0.111430... Val Loss: 0.033489\n",
      "Epoch: 3/100... Step: 3035... Loss: 0.038219... Val Loss: 0.041093\n",
      "Epoch: 3/100... Step: 3040... Loss: 0.114488... Val Loss: 0.046434\n",
      "Epoch: 3/100... Step: 3045... Loss: 0.070328... Val Loss: 0.042473\n",
      "Epoch: 3/100... Step: 3050... Loss: 0.088235... Val Loss: 0.078639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/100... Step: 3055... Loss: 0.065022... Val Loss: 0.074065\n",
      "Epoch: 3/100... Step: 3060... Loss: 0.123784... Val Loss: 0.085555\n",
      "Epoch: 3/100... Step: 3065... Loss: 0.047675... Val Loss: 0.074076\n",
      "Epoch: 3/100... Step: 3070... Loss: 0.080182... Val Loss: 0.053520\n",
      "Epoch: 3/100... Step: 3075... Loss: 0.081366... Val Loss: 0.087093\n",
      "Epoch: 3/100... Step: 3080... Loss: 0.041955... Val Loss: 0.090060\n",
      "Epoch: 3/100... Step: 3085... Loss: 0.070560... Val Loss: 0.050845\n",
      "Epoch: 3/100... Step: 3090... Loss: 0.035429... Val Loss: 0.056713\n",
      "Epoch: 3/100... Step: 3095... Loss: 0.051352... Val Loss: 0.066865\n",
      "Epoch: 4/100... Step: 3100... Loss: 0.059806... Val Loss: 0.063477\n",
      "Epoch: 4/100... Step: 3105... Loss: 0.070752... Val Loss: 0.026016\n",
      "Epoch: 4/100... Step: 3110... Loss: 0.107866... Val Loss: 0.012229\n",
      "Epoch: 4/100... Step: 3115... Loss: 0.037178... Val Loss: 0.041523\n",
      "Epoch: 4/100... Step: 3120... Loss: 0.039901... Val Loss: 0.039271\n",
      "Epoch: 4/100... Step: 3125... Loss: 0.062141... Val Loss: 0.013581\n",
      "Epoch: 4/100... Step: 3130... Loss: 0.056959... Val Loss: 0.054952\n",
      "Epoch: 4/100... Step: 3135... Loss: 0.055901... Val Loss: 0.049464\n",
      "Epoch: 4/100... Step: 3140... Loss: 0.081868... Val Loss: 0.030382\n",
      "Epoch: 4/100... Step: 3145... Loss: 0.049071... Val Loss: 0.009811\n",
      "Epoch: 4/100... Step: 3150... Loss: 0.103766... Val Loss: 0.029565\n",
      "Epoch: 4/100... Step: 3155... Loss: 0.062468... Val Loss: 0.015556\n",
      "Epoch: 4/100... Step: 3160... Loss: 0.030162... Val Loss: 0.032569\n",
      "Epoch: 4/100... Step: 3165... Loss: 0.054109... Val Loss: 0.064707\n",
      "Epoch: 4/100... Step: 3170... Loss: 0.098008... Val Loss: 0.072137\n",
      "Epoch: 4/100... Step: 3175... Loss: 0.079625... Val Loss: 0.040531\n",
      "Epoch: 4/100... Step: 3180... Loss: 0.078693... Val Loss: 0.042212\n",
      "Epoch: 4/100... Step: 3185... Loss: 0.103304... Val Loss: 0.047241\n",
      "Epoch: 4/100... Step: 3190... Loss: 0.057616... Val Loss: 0.033266\n",
      "Epoch: 4/100... Step: 3195... Loss: 0.063955... Val Loss: 0.058396\n",
      "Epoch: 4/100... Step: 3200... Loss: 0.105144... Val Loss: 0.101493\n",
      "Epoch: 4/100... Step: 3205... Loss: 0.073569... Val Loss: 0.082844\n",
      "Epoch: 4/100... Step: 3210... Loss: 0.030798... Val Loss: 0.054052\n",
      "Epoch: 4/100... Step: 3215... Loss: 0.077422... Val Loss: 0.046006\n",
      "Epoch: 4/100... Step: 3220... Loss: 0.070904... Val Loss: 0.022867\n",
      "Epoch: 4/100... Step: 3225... Loss: 0.043660... Val Loss: 0.032387\n",
      "Epoch: 4/100... Step: 3230... Loss: 0.068847... Val Loss: 0.061909\n",
      "Epoch: 4/100... Step: 3235... Loss: 0.068151... Val Loss: 0.103363\n",
      "Epoch: 4/100... Step: 3240... Loss: 0.099234... Val Loss: 0.102899\n",
      "Epoch: 4/100... Step: 3245... Loss: 0.050733... Val Loss: 0.050488\n",
      "Epoch: 4/100... Step: 3250... Loss: 0.069243... Val Loss: 0.018084\n",
      "Epoch: 4/100... Step: 3255... Loss: 0.053942... Val Loss: 0.076681\n",
      "Epoch: 4/100... Step: 3260... Loss: 0.049000... Val Loss: 0.086176\n",
      "Epoch: 4/100... Step: 3265... Loss: 0.068310... Val Loss: 0.048964\n",
      "Epoch: 4/100... Step: 3270... Loss: 0.031384... Val Loss: 0.017245\n",
      "Epoch: 4/100... Step: 3275... Loss: 0.049142... Val Loss: 0.029450\n",
      "Epoch: 4/100... Step: 3280... Loss: 0.055547... Val Loss: 0.052790\n",
      "Epoch: 4/100... Step: 3285... Loss: 0.079408... Val Loss: 0.058515\n",
      "Epoch: 4/100... Step: 3290... Loss: 0.065911... Val Loss: 0.060083\n",
      "Epoch: 4/100... Step: 3295... Loss: 0.096750... Val Loss: 0.055626\n",
      "Epoch: 4/100... Step: 3300... Loss: 0.097361... Val Loss: 0.022591\n",
      "Epoch: 4/100... Step: 3305... Loss: 0.085869... Val Loss: 0.064237\n",
      "Epoch: 4/100... Step: 3310... Loss: 0.103097... Val Loss: 0.112797\n",
      "Epoch: 4/100... Step: 3315... Loss: 0.057060... Val Loss: 0.117040\n",
      "Epoch: 4/100... Step: 3320... Loss: 0.059959... Val Loss: 0.071878\n",
      "Epoch: 4/100... Step: 3325... Loss: 0.065072... Val Loss: 0.010621\n",
      "Epoch: 4/100... Step: 3330... Loss: 0.084326... Val Loss: 0.041382\n",
      "Epoch: 4/100... Step: 3335... Loss: 0.092231... Val Loss: 0.052854\n",
      "Epoch: 4/100... Step: 3340... Loss: 0.066523... Val Loss: 0.043874\n",
      "Epoch: 4/100... Step: 3345... Loss: 0.068938... Val Loss: 0.027978\n",
      "Epoch: 4/100... Step: 3350... Loss: 0.044637... Val Loss: 0.032127\n",
      "Epoch: 4/100... Step: 3355... Loss: 0.065240... Val Loss: 0.056147\n",
      "Epoch: 4/100... Step: 3360... Loss: 0.059154... Val Loss: 0.047799\n",
      "Epoch: 4/100... Step: 3365... Loss: 0.071932... Val Loss: 0.019218\n",
      "Epoch: 4/100... Step: 3370... Loss: 0.063070... Val Loss: 0.030824\n",
      "Epoch: 4/100... Step: 3375... Loss: 0.074038... Val Loss: 0.043448\n",
      "Epoch: 4/100... Step: 3380... Loss: 0.048561... Val Loss: 0.036937\n",
      "Epoch: 4/100... Step: 3385... Loss: 0.067178... Val Loss: 0.062250\n",
      "Epoch: 4/100... Step: 3390... Loss: 0.047566... Val Loss: 0.070721\n",
      "Epoch: 4/100... Step: 3395... Loss: 0.096815... Val Loss: 0.069091\n",
      "Epoch: 4/100... Step: 3400... Loss: 0.050246... Val Loss: 0.074182\n",
      "Epoch: 4/100... Step: 3405... Loss: 0.050930... Val Loss: 0.053234\n",
      "Epoch: 4/100... Step: 3410... Loss: 0.044360... Val Loss: 0.042110\n",
      "Epoch: 4/100... Step: 3415... Loss: 0.052315... Val Loss: 0.029646\n",
      "Epoch: 4/100... Step: 3420... Loss: 0.063103... Val Loss: 0.031576\n",
      "Epoch: 4/100... Step: 3425... Loss: 0.096432... Val Loss: 0.052625\n",
      "Epoch: 4/100... Step: 3430... Loss: 0.056565... Val Loss: 0.057762\n",
      "Epoch: 4/100... Step: 3435... Loss: 0.057036... Val Loss: 0.051923\n",
      "Epoch: 4/100... Step: 3440... Loss: 0.029555... Val Loss: 0.050037\n",
      "Epoch: 4/100... Step: 3445... Loss: 0.053760... Val Loss: 0.070026\n",
      "Epoch: 4/100... Step: 3450... Loss: 0.067293... Val Loss: 0.081047\n",
      "Epoch: 4/100... Step: 3455... Loss: 0.026142... Val Loss: 0.036518\n",
      "Epoch: 4/100... Step: 3460... Loss: 0.089413... Val Loss: 0.038273\n",
      "Epoch: 4/100... Step: 3465... Loss: 0.074243... Val Loss: 0.061593\n",
      "Epoch: 4/100... Step: 3470... Loss: 0.070888... Val Loss: 0.046295\n",
      "Epoch: 4/100... Step: 3475... Loss: 0.029983... Val Loss: 0.005636\n",
      "Epoch: 4/100... Step: 3480... Loss: 0.058710... Val Loss: 0.029198\n",
      "Epoch: 4/100... Step: 3485... Loss: 0.078171... Val Loss: 0.029439\n",
      "Epoch: 4/100... Step: 3490... Loss: 0.059737... Val Loss: 0.017512\n",
      "Epoch: 4/100... Step: 3495... Loss: 0.048866... Val Loss: 0.011774\n",
      "Epoch: 4/100... Step: 3500... Loss: 0.060720... Val Loss: 0.027109\n",
      "Epoch: 4/100... Step: 3505... Loss: 0.065971... Val Loss: 0.013411\n",
      "Epoch: 4/100... Step: 3510... Loss: 0.053629... Val Loss: 0.029495\n",
      "Epoch: 4/100... Step: 3515... Loss: 0.046772... Val Loss: 0.061449\n",
      "Epoch: 4/100... Step: 3520... Loss: 0.071527... Val Loss: 0.075348\n",
      "Epoch: 4/100... Step: 3525... Loss: 0.069316... Val Loss: 0.044578\n",
      "Epoch: 4/100... Step: 3530... Loss: 0.069222... Val Loss: 0.076720\n",
      "Epoch: 4/100... Step: 3535... Loss: 0.058184... Val Loss: 0.097735\n",
      "Epoch: 4/100... Step: 3540... Loss: 0.091222... Val Loss: 0.124127\n",
      "Epoch: 4/100... Step: 3545... Loss: 0.042931... Val Loss: 0.119382\n",
      "Epoch: 4/100... Step: 3550... Loss: 0.067281... Val Loss: 0.094059\n",
      "Epoch: 4/100... Step: 3555... Loss: 0.053244... Val Loss: 0.086914\n",
      "Epoch: 4/100... Step: 3560... Loss: 0.031100... Val Loss: 0.076410\n",
      "Epoch: 4/100... Step: 3565... Loss: 0.038402... Val Loss: 0.072400\n",
      "Epoch: 4/100... Step: 3570... Loss: 0.054857... Val Loss: 0.026578\n",
      "Epoch: 4/100... Step: 3575... Loss: 0.024821... Val Loss: 0.070616\n",
      "Epoch: 4/100... Step: 3580... Loss: 0.049282... Val Loss: 0.108171\n",
      "Epoch: 4/100... Step: 3585... Loss: 0.073168... Val Loss: 0.118420\n",
      "Epoch: 4/100... Step: 3590... Loss: 0.029023... Val Loss: 0.116093\n",
      "Epoch: 4/100... Step: 3595... Loss: 0.054298... Val Loss: 0.091406\n",
      "Epoch: 4/100... Step: 3600... Loss: 0.043514... Val Loss: 0.051564\n",
      "Epoch: 4/100... Step: 3605... Loss: 0.045237... Val Loss: 0.046864\n",
      "Epoch: 4/100... Step: 3610... Loss: 0.025484... Val Loss: 0.061709\n",
      "Epoch: 4/100... Step: 3615... Loss: 0.062298... Val Loss: 0.062767\n",
      "Epoch: 4/100... Step: 3620... Loss: 0.082612... Val Loss: 0.032631\n",
      "Epoch: 4/100... Step: 3625... Loss: 0.047383... Val Loss: 0.050507\n",
      "Epoch: 4/100... Step: 3630... Loss: 0.090758... Val Loss: 0.028665\n",
      "Epoch: 4/100... Step: 3635... Loss: 0.049280... Val Loss: 0.017270\n",
      "Epoch: 4/100... Step: 3640... Loss: 0.042643... Val Loss: 0.027507\n",
      "Epoch: 4/100... Step: 3645... Loss: 0.052191... Val Loss: 0.046551\n",
      "Epoch: 4/100... Step: 3650... Loss: 0.014887... Val Loss: 0.046397\n",
      "Epoch: 4/100... Step: 3655... Loss: 0.042398... Val Loss: 0.037052\n",
      "Epoch: 4/100... Step: 3660... Loss: 0.022347... Val Loss: 0.029596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/100... Step: 3665... Loss: 0.054623... Val Loss: 0.043287\n",
      "Epoch: 4/100... Step: 3670... Loss: 0.041344... Val Loss: 0.046937\n",
      "Epoch: 4/100... Step: 3675... Loss: 0.027520... Val Loss: 0.037692\n",
      "Epoch: 4/100... Step: 3680... Loss: 0.042735... Val Loss: 0.035734\n",
      "Epoch: 4/100... Step: 3685... Loss: 0.061915... Val Loss: 0.037566\n",
      "Epoch: 4/100... Step: 3690... Loss: 0.032887... Val Loss: 0.060018\n",
      "Epoch: 4/100... Step: 3695... Loss: 0.060151... Val Loss: 0.074429\n",
      "Epoch: 4/100... Step: 3700... Loss: 0.060721... Val Loss: 0.077058\n",
      "Epoch: 4/100... Step: 3705... Loss: 0.039722... Val Loss: 0.074078\n",
      "Epoch: 4/100... Step: 3710... Loss: 0.021862... Val Loss: 0.062838\n",
      "Epoch: 4/100... Step: 3715... Loss: 0.043740... Val Loss: 0.058732\n",
      "Epoch: 4/100... Step: 3720... Loss: 0.024694... Val Loss: 0.064856\n",
      "Epoch: 4/100... Step: 3725... Loss: 0.043707... Val Loss: 0.071652\n",
      "Epoch: 4/100... Step: 3730... Loss: 0.037589... Val Loss: 0.058650\n",
      "Epoch: 4/100... Step: 3735... Loss: 0.046113... Val Loss: 0.042963\n",
      "Epoch: 4/100... Step: 3740... Loss: 0.058759... Val Loss: 0.048929\n",
      "Epoch: 4/100... Step: 3745... Loss: 0.058277... Val Loss: 0.019873\n",
      "Epoch: 4/100... Step: 3750... Loss: 0.042083... Val Loss: 0.019791\n",
      "Epoch: 4/100... Step: 3755... Loss: 0.058444... Val Loss: 0.073508\n",
      "Epoch: 4/100... Step: 3760... Loss: 0.050622... Val Loss: 0.095683\n",
      "Epoch: 4/100... Step: 3765... Loss: 0.054883... Val Loss: 0.076285\n",
      "Epoch: 4/100... Step: 3770... Loss: 0.014096... Val Loss: 0.045320\n",
      "Epoch: 4/100... Step: 3775... Loss: 0.058795... Val Loss: 0.029460\n",
      "Epoch: 4/100... Step: 3780... Loss: 0.023673... Val Loss: 0.059070\n",
      "Epoch: 4/100... Step: 3785... Loss: 0.046287... Val Loss: 0.105590\n",
      "Epoch: 4/100... Step: 3790... Loss: 0.044014... Val Loss: 0.090658\n",
      "Epoch: 4/100... Step: 3795... Loss: 0.044049... Val Loss: 0.034527\n",
      "Epoch: 4/100... Step: 3800... Loss: 0.060245... Val Loss: 0.044064\n",
      "Epoch: 4/100... Step: 3805... Loss: 0.073581... Val Loss: 0.073977\n",
      "Epoch: 4/100... Step: 3810... Loss: 0.045590... Val Loss: 0.060554\n",
      "Epoch: 4/100... Step: 3815... Loss: 0.046164... Val Loss: 0.012866\n",
      "Epoch: 4/100... Step: 3820... Loss: 0.028496... Val Loss: 0.034994\n",
      "Epoch: 4/100... Step: 3825... Loss: 0.051778... Val Loss: 0.075469\n",
      "Epoch: 4/100... Step: 3830... Loss: 0.031976... Val Loss: 0.098985\n",
      "Epoch: 4/100... Step: 3835... Loss: 0.073027... Val Loss: 0.049753\n",
      "Epoch: 4/100... Step: 3840... Loss: 0.028080... Val Loss: 0.017170\n",
      "Epoch: 4/100... Step: 3845... Loss: 0.057306... Val Loss: 0.004202\n",
      "Epoch: 4/100... Step: 3850... Loss: 0.058359... Val Loss: 0.025069\n",
      "Epoch: 4/100... Step: 3855... Loss: 0.035697... Val Loss: 0.043086\n",
      "Epoch: 4/100... Step: 3860... Loss: 0.044239... Val Loss: 0.075548\n",
      "Epoch: 4/100... Step: 3865... Loss: 0.034111... Val Loss: 0.082920\n",
      "Epoch: 4/100... Step: 3870... Loss: 0.055778... Val Loss: 0.042592\n",
      "Epoch: 4/100... Step: 3875... Loss: 0.025718... Val Loss: 0.039920\n",
      "Epoch: 4/100... Step: 3880... Loss: 0.031786... Val Loss: 0.087740\n",
      "Epoch: 4/100... Step: 3885... Loss: 0.038095... Val Loss: 0.042667\n",
      "Epoch: 4/100... Step: 3890... Loss: 0.056919... Val Loss: 0.001457\n",
      "Validation loss decreased (0.002867 --> 0.001457).  Saving model ...\n",
      "Epoch: 4/100... Step: 3895... Loss: 0.048827... Val Loss: 0.019053\n",
      "Epoch: 4/100... Step: 3900... Loss: 0.035412... Val Loss: 0.027500\n",
      "Epoch: 4/100... Step: 3905... Loss: 0.031904... Val Loss: 0.022951\n",
      "Epoch: 4/100... Step: 3910... Loss: 0.063528... Val Loss: 0.008567\n",
      "Epoch: 4/100... Step: 3915... Loss: 0.041931... Val Loss: 0.000960\n",
      "Validation loss decreased (0.001457 --> 0.000960).  Saving model ...\n",
      "Epoch: 4/100... Step: 3920... Loss: 0.039679... Val Loss: 0.000679\n",
      "Validation loss decreased (0.000960 --> 0.000679).  Saving model ...\n",
      "Epoch: 4/100... Step: 3925... Loss: 0.032202... Val Loss: 0.014531\n",
      "Epoch: 4/100... Step: 3930... Loss: 0.061576... Val Loss: 0.036913\n",
      "Epoch: 4/100... Step: 3935... Loss: 0.057910... Val Loss: 0.021458\n",
      "Epoch: 4/100... Step: 3940... Loss: 0.044078... Val Loss: 0.016147\n",
      "Epoch: 4/100... Step: 3945... Loss: 0.041261... Val Loss: 0.042732\n",
      "Epoch: 4/100... Step: 3950... Loss: 0.038196... Val Loss: 0.024176\n",
      "Epoch: 4/100... Step: 3955... Loss: 0.025490... Val Loss: 0.031691\n",
      "Epoch: 4/100... Step: 3960... Loss: 0.055177... Val Loss: 0.085006\n",
      "Epoch: 4/100... Step: 3965... Loss: 0.060947... Val Loss: 0.105080\n",
      "Epoch: 4/100... Step: 3970... Loss: 0.036018... Val Loss: 0.053963\n",
      "Epoch: 4/100... Step: 3975... Loss: 0.032757... Val Loss: 0.008040\n",
      "Epoch: 4/100... Step: 3980... Loss: 0.056155... Val Loss: 0.063341\n",
      "Epoch: 4/100... Step: 3985... Loss: 0.043783... Val Loss: 0.065672\n",
      "Epoch: 4/100... Step: 3990... Loss: 0.057703... Val Loss: 0.053987\n",
      "Epoch: 4/100... Step: 3995... Loss: 0.037839... Val Loss: 0.023530\n",
      "Epoch: 4/100... Step: 4000... Loss: 0.045930... Val Loss: 0.009171\n",
      "Epoch: 4/100... Step: 4005... Loss: 0.037699... Val Loss: 0.032997\n",
      "Epoch: 4/100... Step: 4010... Loss: 0.027528... Val Loss: 0.060725\n",
      "Epoch: 4/100... Step: 4015... Loss: 0.030257... Val Loss: 0.036675\n",
      "Epoch: 4/100... Step: 4020... Loss: 0.051224... Val Loss: 0.034264\n",
      "Epoch: 4/100... Step: 4025... Loss: 0.053958... Val Loss: 0.060736\n",
      "Epoch: 4/100... Step: 4030... Loss: 0.025107... Val Loss: 0.068081\n",
      "Epoch: 4/100... Step: 4035... Loss: 0.026750... Val Loss: 0.071459\n",
      "Epoch: 4/100... Step: 4040... Loss: 0.040656... Val Loss: 0.032104\n",
      "Epoch: 4/100... Step: 4045... Loss: 0.026194... Val Loss: 0.023429\n",
      "Epoch: 4/100... Step: 4050... Loss: 0.041549... Val Loss: 0.082337\n",
      "Epoch: 4/100... Step: 4055... Loss: 0.028826... Val Loss: 0.078752\n",
      "Epoch: 4/100... Step: 4060... Loss: 0.041365... Val Loss: 0.013331\n",
      "Epoch: 4/100... Step: 4065... Loss: 0.051196... Val Loss: 0.054422\n",
      "Epoch: 4/100... Step: 4070... Loss: 0.069393... Val Loss: 0.063837\n",
      "Epoch: 4/100... Step: 4075... Loss: 0.064590... Val Loss: 0.016102\n",
      "Epoch: 4/100... Step: 4080... Loss: 0.077097... Val Loss: 0.047480\n",
      "Epoch: 4/100... Step: 4085... Loss: 0.034405... Val Loss: 0.050552\n",
      "Epoch: 4/100... Step: 4090... Loss: 0.027665... Val Loss: 0.006344\n",
      "Epoch: 4/100... Step: 4095... Loss: 0.070705... Val Loss: 0.031754\n",
      "Epoch: 4/100... Step: 4100... Loss: 0.046517... Val Loss: 0.047825\n",
      "Epoch: 4/100... Step: 4105... Loss: 0.042088... Val Loss: 0.037992\n",
      "Epoch: 4/100... Step: 4110... Loss: 0.024311... Val Loss: 0.009919\n",
      "Epoch: 4/100... Step: 4115... Loss: 0.045124... Val Loss: 0.039399\n",
      "Epoch: 4/100... Step: 4120... Loss: 0.046776... Val Loss: 0.036147\n",
      "Epoch: 4/100... Step: 4125... Loss: 0.015497... Val Loss: 0.012635\n",
      "Epoch: 5/100... Step: 4130... Loss: 0.033262... Val Loss: 0.042685\n",
      "Epoch: 5/100... Step: 4135... Loss: 0.065549... Val Loss: 0.049856\n",
      "Epoch: 5/100... Step: 4140... Loss: 0.031209... Val Loss: 0.012059\n",
      "Epoch: 5/100... Step: 4145... Loss: 0.027056... Val Loss: 0.022290\n",
      "Epoch: 5/100... Step: 4150... Loss: 0.039799... Val Loss: 0.007295\n",
      "Epoch: 5/100... Step: 4155... Loss: 0.057825... Val Loss: 0.022439\n",
      "Epoch: 5/100... Step: 4160... Loss: 0.031538... Val Loss: 0.040715\n",
      "Epoch: 5/100... Step: 4165... Loss: 0.030746... Val Loss: 0.005060\n",
      "Epoch: 5/100... Step: 4170... Loss: 0.028788... Val Loss: 0.003383\n",
      "Epoch: 5/100... Step: 4175... Loss: 0.036354... Val Loss: 0.002675\n",
      "Epoch: 5/100... Step: 4185... Loss: 0.046994... Val Loss: 0.022359\n",
      "Epoch: 5/100... Step: 4190... Loss: 0.041364... Val Loss: 0.027520\n",
      "Epoch: 5/100... Step: 4195... Loss: 0.021081... Val Loss: 0.030109\n",
      "Epoch: 5/100... Step: 4200... Loss: 0.039936... Val Loss: 0.017073\n",
      "Epoch: 5/100... Step: 4205... Loss: 0.023296... Val Loss: 0.006541\n",
      "Epoch: 5/100... Step: 4210... Loss: 0.024130... Val Loss: 0.012400\n",
      "Epoch: 5/100... Step: 4215... Loss: 0.033822... Val Loss: 0.034194\n",
      "Epoch: 5/100... Step: 4220... Loss: 0.070123... Val Loss: 0.031328\n",
      "Epoch: 5/100... Step: 4225... Loss: 0.031621... Val Loss: 0.032938\n",
      "Epoch: 5/100... Step: 4230... Loss: 0.052586... Val Loss: 0.050743\n",
      "Epoch: 5/100... Step: 4235... Loss: 0.026732... Val Loss: 0.031965\n",
      "Epoch: 5/100... Step: 4240... Loss: 0.031151... Val Loss: 0.029326\n",
      "Epoch: 5/100... Step: 4245... Loss: 0.076077... Val Loss: 0.066249\n",
      "Epoch: 5/100... Step: 4250... Loss: 0.033072... Val Loss: 0.032248\n",
      "Epoch: 5/100... Step: 4255... Loss: 0.029517... Val Loss: 0.007763\n",
      "Epoch: 5/100... Step: 4260... Loss: 0.047657... Val Loss: 0.028522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100... Step: 4265... Loss: 0.069768... Val Loss: 0.032970\n",
      "Epoch: 5/100... Step: 4270... Loss: 0.019228... Val Loss: 0.000334\n",
      "Epoch: 51/100... Step: 52575... Loss: 0.000100... Val Loss: 0.001074\n",
      "Epoch: 51/100... Step: 52580... Loss: 0.000149... Val Loss: 0.000905\n",
      "Epoch: 51/100... Step: 52585... Loss: 0.000213... Val Loss: 0.001091\n",
      "Epoch: 51/100... Step: 52590... Loss: 0.000263... Val Loss: 0.001258\n",
      "Epoch: 51/100... Step: 52595... Loss: 0.000074... Val Loss: 0.001012\n",
      "Epoch: 51/100... Step: 52600... Loss: 0.000287... Val Loss: 0.001146\n",
      "Epoch: 51/100... Step: 52605... Loss: 0.000472... Val Loss: 0.000918\n",
      "Epoch: 51/100... Step: 52610... Loss: 0.000125... Val Loss: 0.000953\n",
      "Epoch: 51/100... Step: 52615... Loss: 0.000210... Val Loss: 0.000705\n",
      "Epoch: 51/100... Step: 52620... Loss: 0.000128... Val Loss: 0.000849\n",
      "Epoch: 51/100... Step: 52625... Loss: 0.000539... Val Loss: 0.000873\n",
      "Epoch: 51/100... Step: 52630... Loss: 0.000187... Val Loss: 0.000654\n",
      "Epoch: 52/100... Step: 52635... Loss: 0.000283... Val Loss: 0.001226\n",
      "Epoch: 52/100... Step: 52640... Loss: 0.000163... Val Loss: 0.001444\n",
      "Epoch: 52/100... Step: 52645... Loss: 0.000191... Val Loss: 0.001293\n",
      "Epoch: 52/100... Step: 52650... Loss: 0.000300... Val Loss: 0.001829\n",
      "Epoch: 52/100... Step: 52655... Loss: 0.000341... Val Loss: 0.001453\n",
      "Epoch: 52/100... Step: 52660... Loss: 0.000296... Val Loss: 0.001653\n",
      "Epoch: 52/100... Step: 52665... Loss: 0.000409... Val Loss: 0.001922\n",
      "Epoch: 52/100... Step: 52670... Loss: 0.000291... Val Loss: 0.001424\n",
      "Epoch: 52/100... Step: 52675... Loss: 0.000180... Val Loss: 0.001430\n",
      "Epoch: 52/100... Step: 52680... Loss: 0.000306... Val Loss: 0.001412\n",
      "Epoch: 52/100... Step: 52685... Loss: 0.000313... Val Loss: 0.001659\n",
      "Epoch: 52/100... Step: 52690... Loss: 0.000256... Val Loss: 0.001736\n",
      "Epoch: 52/100... Step: 52695... Loss: 0.000280... Val Loss: 0.001417\n",
      "Epoch: 52/100... Step: 52700... Loss: 0.000175... Val Loss: 0.001269\n",
      "Epoch: 52/100... Step: 52705... Loss: 0.000238... Val Loss: 0.000792\n",
      "Epoch: 52/100... Step: 52710... Loss: 0.000215... Val Loss: 0.001107\n",
      "Epoch: 52/100... Step: 52715... Loss: 0.000182... Val Loss: 0.001367\n",
      "Epoch: 52/100... Step: 52720... Loss: 0.000319... Val Loss: 0.001465\n",
      "Epoch: 52/100... Step: 52725... Loss: 0.000259... Val Loss: 0.001365\n",
      "Epoch: 52/100... Step: 52730... Loss: 0.000170... Val Loss: 0.001172\n",
      "Epoch: 52/100... Step: 52735... Loss: 0.000499... Val Loss: 0.001449\n",
      "Epoch: 52/100... Step: 52740... Loss: 0.000342... Val Loss: 0.001653\n",
      "Epoch: 52/100... Step: 52745... Loss: 0.000201... Val Loss: 0.001367\n",
      "Epoch: 52/100... Step: 52750... Loss: 0.000607... Val Loss: 0.000822\n",
      "Epoch: 52/100... Step: 52755... Loss: 0.000563... Val Loss: 0.001082\n",
      "Epoch: 52/100... Step: 52760... Loss: 0.000083... Val Loss: 0.001175\n",
      "Epoch: 52/100... Step: 52765... Loss: 0.000294... Val Loss: 0.001225\n",
      "Epoch: 52/100... Step: 52770... Loss: 0.000711... Val Loss: 0.000980\n",
      "Epoch: 52/100... Step: 52775... Loss: 0.000288... Val Loss: 0.002493\n",
      "Epoch: 52/100... Step: 52780... Loss: 0.000236... Val Loss: 0.001728\n",
      "Epoch: 52/100... Step: 52785... Loss: 0.000325... Val Loss: 0.001541\n",
      "Epoch: 52/100... Step: 52790... Loss: 0.000278... Val Loss: 0.001413\n",
      "Epoch: 52/100... Step: 52795... Loss: 0.000137... Val Loss: 0.001332\n",
      "Epoch: 52/100... Step: 52800... Loss: 0.000393... Val Loss: 0.001569\n",
      "Epoch: 52/100... Step: 52805... Loss: 0.000185... Val Loss: 0.001601\n",
      "Epoch: 52/100... Step: 52810... Loss: 0.000310... Val Loss: 0.001412\n",
      "Epoch: 52/100... Step: 52815... Loss: 0.000468... Val Loss: 0.001296\n",
      "Epoch: 52/100... Step: 52820... Loss: 0.000206... Val Loss: 0.001227\n",
      "Epoch: 52/100... Step: 52825... Loss: 0.000358... Val Loss: 0.001203\n",
      "Epoch: 52/100... Step: 52830... Loss: 0.000209... Val Loss: 0.001107\n",
      "Epoch: 52/100... Step: 52835... Loss: 0.000444... Val Loss: 0.000884\n",
      "Epoch: 52/100... Step: 52840... Loss: 0.000349... Val Loss: 0.000710\n",
      "Epoch: 52/100... Step: 52845... Loss: 0.000286... Val Loss: 0.000833\n",
      "Epoch: 52/100... Step: 52850... Loss: 0.000225... Val Loss: 0.001585\n",
      "Epoch: 52/100... Step: 52855... Loss: 0.000505... Val Loss: 0.001177\n",
      "Epoch: 52/100... Step: 52860... Loss: 0.000471... Val Loss: 0.001396\n",
      "Epoch: 52/100... Step: 52865... Loss: 0.000541... Val Loss: 0.000575\n",
      "Epoch: 52/100... Step: 52870... Loss: 0.000261... Val Loss: 0.001389\n",
      "Epoch: 52/100... Step: 52875... Loss: 0.000232... Val Loss: 0.001427\n",
      "Epoch: 52/100... Step: 52880... Loss: 0.000310... Val Loss: 0.001124\n",
      "Epoch: 52/100... Step: 52885... Loss: 0.000508... Val Loss: 0.001255\n",
      "Epoch: 52/100... Step: 52890... Loss: 0.000193... Val Loss: 0.001446\n",
      "Epoch: 52/100... Step: 52895... Loss: 0.000643... Val Loss: 0.000700\n",
      "Epoch: 52/100... Step: 52900... Loss: 0.000400... Val Loss: 0.000843\n",
      "Epoch: 52/100... Step: 52905... Loss: 0.000265... Val Loss: 0.001035\n",
      "Epoch: 52/100... Step: 52910... Loss: 0.000302... Val Loss: 0.001202\n",
      "Epoch: 52/100... Step: 52915... Loss: 0.000843... Val Loss: 0.001030\n",
      "Epoch: 52/100... Step: 52920... Loss: 0.000380... Val Loss: 0.000749\n",
      "Epoch: 52/100... Step: 52925... Loss: 0.000258... Val Loss: 0.000621\n",
      "Epoch: 52/100... Step: 52930... Loss: 0.000307... Val Loss: 0.000438\n",
      "Epoch: 52/100... Step: 52935... Loss: 0.000222... Val Loss: 0.000730\n",
      "Epoch: 52/100... Step: 52940... Loss: 0.000345... Val Loss: 0.000776\n",
      "Epoch: 52/100... Step: 52945... Loss: 0.000151... Val Loss: 0.000493\n",
      "Epoch: 52/100... Step: 52950... Loss: 0.000359... Val Loss: 0.000647\n",
      "Epoch: 52/100... Step: 52955... Loss: 0.000361... Val Loss: 0.000753\n",
      "Epoch: 52/100... Step: 52960... Loss: 0.000265... Val Loss: 0.000529\n",
      "Epoch: 52/100... Step: 52965... Loss: 0.000359... Val Loss: 0.000414\n",
      "Epoch: 52/100... Step: 52970... Loss: 0.000278... Val Loss: 0.000458\n",
      "Epoch: 52/100... Step: 52975... Loss: 0.000308... Val Loss: 0.000554\n",
      "Epoch: 52/100... Step: 52980... Loss: 0.000287... Val Loss: 0.000693\n",
      "Epoch: 52/100... Step: 52985... Loss: 0.000276... Val Loss: 0.000675\n",
      "Epoch: 52/100... Step: 52990... Loss: 0.000297... Val Loss: 0.000782\n",
      "Epoch: 52/100... Step: 52995... Loss: 0.000495... Val Loss: 0.000961\n",
      "Epoch: 52/100... Step: 53000... Loss: 0.000917... Val Loss: 0.000828\n",
      "Epoch: 52/100... Step: 53005... Loss: 0.000506... Val Loss: 0.001082\n",
      "Epoch: 52/100... Step: 53010... Loss: 0.000476... Val Loss: 0.001043\n",
      "Epoch: 52/100... Step: 53015... Loss: 0.000346... Val Loss: 0.001132\n",
      "Epoch: 52/100... Step: 53020... Loss: 0.000561... Val Loss: 0.001190\n",
      "Epoch: 52/100... Step: 53025... Loss: 0.000303... Val Loss: 0.001190\n",
      "Epoch: 52/100... Step: 53030... Loss: 0.000255... Val Loss: 0.001037\n",
      "Epoch: 52/100... Step: 53035... Loss: 0.000267... Val Loss: 0.000954\n",
      "Epoch: 52/100... Step: 53040... Loss: 0.000268... Val Loss: 0.000300\n",
      "Epoch: 52/100... Step: 53045... Loss: 0.000577... Val Loss: 0.000299\n",
      "Epoch: 52/100... Step: 53050... Loss: 0.000401... Val Loss: 0.001331\n",
      "Epoch: 52/100... Step: 53055... Loss: 0.000398... Val Loss: 0.001113\n",
      "Epoch: 52/100... Step: 53060... Loss: 0.000786... Val Loss: 0.001089\n",
      "Epoch: 52/100... Step: 53065... Loss: 0.000360... Val Loss: 0.001275\n",
      "Epoch: 52/100... Step: 53070... Loss: 0.000200... Val Loss: 0.001182\n",
      "Epoch: 52/100... Step: 53075... Loss: 0.000594... Val Loss: 0.000694\n",
      "Epoch: 52/100... Step: 53080... Loss: 0.000804... Val Loss: 0.000888\n",
      "Epoch: 52/100... Step: 53085... Loss: 0.000337... Val Loss: 0.001181\n",
      "Epoch: 52/100... Step: 53090... Loss: 0.000220... Val Loss: 0.001473\n",
      "Epoch: 52/100... Step: 53095... Loss: 0.000163... Val Loss: 0.001560\n",
      "Epoch: 52/100... Step: 53100... Loss: 0.000207... Val Loss: 0.001371\n",
      "Epoch: 52/100... Step: 53105... Loss: 0.000637... Val Loss: 0.001284\n",
      "Epoch: 52/100... Step: 53110... Loss: 0.000416... Val Loss: 0.001397\n",
      "Epoch: 52/100... Step: 53115... Loss: 0.000359... Val Loss: 0.001250\n",
      "Epoch: 52/100... Step: 53120... Loss: 0.000251... Val Loss: 0.000865\n",
      "Epoch: 52/100... Step: 53125... Loss: 0.000378... Val Loss: 0.000998\n",
      "Epoch: 52/100... Step: 53130... Loss: 0.000385... Val Loss: 0.001079\n",
      "Epoch: 52/100... Step: 53135... Loss: 0.000367... Val Loss: 0.000551\n",
      "Epoch: 52/100... Step: 53140... Loss: 0.000205... Val Loss: 0.000428\n",
      "Epoch: 52/100... Step: 53145... Loss: 0.000232... Val Loss: 0.000575\n",
      "Epoch: 52/100... Step: 53150... Loss: 0.000186... Val Loss: 0.000541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52/100... Step: 53155... Loss: 0.000237... Val Loss: 0.000569\n",
      "Epoch: 52/100... Step: 53160... Loss: 0.000507... Val Loss: 0.000495\n",
      "Epoch: 52/100... Step: 53165... Loss: 0.000447... Val Loss: 0.000980\n",
      "Epoch: 52/100... Step: 53170... Loss: 0.000465... Val Loss: 0.000670\n",
      "Epoch: 52/100... Step: 53175... Loss: 0.000329... Val Loss: 0.000696\n",
      "Epoch: 52/100... Step: 53180... Loss: 0.000557... Val Loss: 0.000567\n",
      "Epoch: 52/100... Step: 53185... Loss: 0.000748... Val Loss: 0.000549\n",
      "Epoch: 52/100... Step: 53190... Loss: 0.000671... Val Loss: 0.000671\n",
      "Epoch: 52/100... Step: 53195... Loss: 0.000183... Val Loss: 0.000624\n",
      "Epoch: 52/100... Step: 53200... Loss: 0.000544... Val Loss: 0.000642\n",
      "Epoch: 52/100... Step: 53205... Loss: 0.000744... Val Loss: 0.000635\n",
      "Epoch: 52/100... Step: 53210... Loss: 0.000485... Val Loss: 0.000806\n",
      "Epoch: 52/100... Step: 53215... Loss: 0.001562... Val Loss: 0.000572\n",
      "Epoch: 52/100... Step: 53220... Loss: 0.000459... Val Loss: 0.000980\n",
      "Epoch: 52/100... Step: 53225... Loss: 0.000722... Val Loss: 0.000874\n",
      "Epoch: 52/100... Step: 53230... Loss: 0.000136... Val Loss: 0.001293\n",
      "Epoch: 52/100... Step: 53235... Loss: 0.000515... Val Loss: 0.000955\n",
      "Epoch: 52/100... Step: 53240... Loss: 0.000200... Val Loss: 0.001220\n",
      "Epoch: 52/100... Step: 53245... Loss: 0.000721... Val Loss: 0.000802\n",
      "Epoch: 52/100... Step: 53250... Loss: 0.000562... Val Loss: 0.001168\n",
      "Epoch: 52/100... Step: 53255... Loss: 0.000243... Val Loss: 0.000918\n",
      "Epoch: 52/100... Step: 53260... Loss: 0.000215... Val Loss: 0.000576\n",
      "Epoch: 52/100... Step: 53265... Loss: 0.000607... Val Loss: 0.000924\n",
      "Epoch: 52/100... Step: 53270... Loss: 0.000541... Val Loss: 0.001097\n",
      "Epoch: 52/100... Step: 53275... Loss: 0.000534... Val Loss: 0.000762\n",
      "Epoch: 52/100... Step: 53280... Loss: 0.000427... Val Loss: 0.001236\n",
      "Epoch: 52/100... Step: 53285... Loss: 0.000301... Val Loss: 0.000825\n",
      "Epoch: 52/100... Step: 53290... Loss: 0.000289... Val Loss: 0.000831\n",
      "Epoch: 52/100... Step: 53295... Loss: 0.000252... Val Loss: 0.000841\n",
      "Epoch: 52/100... Step: 53300... Loss: 0.000457... Val Loss: 0.000544\n",
      "Epoch: 52/100... Step: 53305... Loss: 0.000182... Val Loss: 0.001029\n",
      "Epoch: 52/100... Step: 53310... Loss: 0.000679... Val Loss: 0.000726\n",
      "Epoch: 52/100... Step: 53315... Loss: 0.000389... Val Loss: 0.000462\n",
      "Epoch: 52/100... Step: 53320... Loss: 0.000440... Val Loss: 0.000420\n",
      "Epoch: 52/100... Step: 53325... Loss: 0.000358... Val Loss: 0.000269\n",
      "Epoch: 52/100... Step: 53330... Loss: 0.000208... Val Loss: 0.000274\n",
      "Epoch: 52/100... Step: 53335... Loss: 0.000337... Val Loss: 0.000281\n",
      "Epoch: 52/100... Step: 53340... Loss: 0.000094... Val Loss: 0.000360\n",
      "Epoch: 52/100... Step: 53345... Loss: 0.000552... Val Loss: 0.000274\n",
      "Epoch: 52/100... Step: 53350... Loss: 0.000686... Val Loss: 0.000265\n",
      "Epoch: 52/100... Step: 53355... Loss: 0.000644... Val Loss: 0.000496\n",
      "Epoch: 52/100... Step: 53360... Loss: 0.000491... Val Loss: 0.000264\n",
      "Epoch: 52/100... Step: 53365... Loss: 0.000302... Val Loss: 0.000306\n",
      "Epoch: 52/100... Step: 53370... Loss: 0.000434... Val Loss: 0.000303\n",
      "Epoch: 52/100... Step: 53375... Loss: 0.000487... Val Loss: 0.000602\n",
      "Epoch: 52/100... Step: 53380... Loss: 0.000277... Val Loss: 0.000546\n",
      "Epoch: 52/100... Step: 53385... Loss: 0.000327... Val Loss: 0.000340\n",
      "Epoch: 52/100... Step: 53390... Loss: 0.000271... Val Loss: 0.000273\n",
      "Epoch: 52/100... Step: 53395... Loss: 0.000132... Val Loss: 0.000290\n",
      "Epoch: 52/100... Step: 53400... Loss: 0.000102... Val Loss: 0.000332\n",
      "Epoch: 52/100... Step: 53405... Loss: 0.000106... Val Loss: 0.000503\n",
      "Epoch: 52/100... Step: 53410... Loss: 0.000203... Val Loss: 0.000477\n",
      "Epoch: 52/100... Step: 53415... Loss: 0.000445... Val Loss: 0.000799\n",
      "Epoch: 52/100... Step: 53420... Loss: 0.000275... Val Loss: 0.000558\n",
      "Epoch: 52/100... Step: 53425... Loss: 0.000181... Val Loss: 0.000566\n",
      "Epoch: 52/100... Step: 53430... Loss: 0.000186... Val Loss: 0.000415\n",
      "Epoch: 52/100... Step: 53435... Loss: 0.000242... Val Loss: 0.000526\n",
      "Epoch: 52/100... Step: 53440... Loss: 0.000326... Val Loss: 0.000489\n",
      "Epoch: 52/100... Step: 53445... Loss: 0.000175... Val Loss: 0.000553\n",
      "Epoch: 52/100... Step: 53450... Loss: 0.000237... Val Loss: 0.000967\n",
      "Epoch: 52/100... Step: 53455... Loss: 0.000285... Val Loss: 0.000409\n",
      "Epoch: 52/100... Step: 53460... Loss: 0.000361... Val Loss: 0.000670\n",
      "Epoch: 52/100... Step: 53465... Loss: 0.000276... Val Loss: 0.000745\n",
      "Epoch: 52/100... Step: 53470... Loss: 0.000393... Val Loss: 0.000974\n",
      "Epoch: 52/100... Step: 53475... Loss: 0.000493... Val Loss: 0.000618\n",
      "Epoch: 52/100... Step: 53480... Loss: 0.000204... Val Loss: 0.000585\n",
      "Epoch: 52/100... Step: 53485... Loss: 0.000564... Val Loss: 0.000764\n",
      "Epoch: 52/100... Step: 53490... Loss: 0.000205... Val Loss: 0.000659\n",
      "Epoch: 52/100... Step: 53495... Loss: 0.000553... Val Loss: 0.000655\n",
      "Epoch: 52/100... Step: 53500... Loss: 0.000587... Val Loss: 0.000460\n",
      "Epoch: 52/100... Step: 53505... Loss: 0.000319... Val Loss: 0.000958\n",
      "Epoch: 52/100... Step: 53510... Loss: 0.000198... Val Loss: 0.000587\n",
      "Epoch: 52/100... Step: 53515... Loss: 0.000255... Val Loss: 0.000643\n",
      "Epoch: 52/100... Step: 53520... Loss: 0.000454... Val Loss: 0.000746\n",
      "Epoch: 52/100... Step: 53525... Loss: 0.000682... Val Loss: 0.000805\n",
      "Epoch: 52/100... Step: 53530... Loss: 0.000372... Val Loss: 0.000511\n",
      "Epoch: 52/100... Step: 53535... Loss: 0.000437... Val Loss: 0.000576\n",
      "Epoch: 52/100... Step: 53540... Loss: 0.000226... Val Loss: 0.000559\n",
      "Epoch: 52/100... Step: 53545... Loss: 0.000205... Val Loss: 0.000514\n",
      "Epoch: 52/100... Step: 53550... Loss: 0.000469... Val Loss: 0.000457\n",
      "Epoch: 52/100... Step: 53555... Loss: 0.000164... Val Loss: 0.000531\n",
      "Epoch: 52/100... Step: 53560... Loss: 0.000245... Val Loss: 0.000780\n",
      "Epoch: 52/100... Step: 53565... Loss: 0.000127... Val Loss: 0.000766\n",
      "Epoch: 52/100... Step: 53570... Loss: 0.000159... Val Loss: 0.001154\n",
      "Epoch: 52/100... Step: 53575... Loss: 0.000278... Val Loss: 0.000934\n",
      "Epoch: 52/100... Step: 53580... Loss: 0.000328... Val Loss: 0.000708\n",
      "Epoch: 52/100... Step: 53585... Loss: 0.000322... Val Loss: 0.000847\n",
      "Epoch: 52/100... Step: 53590... Loss: 0.000405... Val Loss: 0.000921\n",
      "Epoch: 52/100... Step: 53595... Loss: 0.000401... Val Loss: 0.000634\n",
      "Epoch: 52/100... Step: 53600... Loss: 0.000418... Val Loss: 0.000805\n",
      "Epoch: 52/100... Step: 53605... Loss: 0.000646... Val Loss: 0.000803\n",
      "Epoch: 52/100... Step: 53610... Loss: 0.000761... Val Loss: 0.001550\n",
      "Epoch: 52/100... Step: 53615... Loss: 0.000251... Val Loss: 0.001071\n",
      "Epoch: 52/100... Step: 53620... Loss: 0.000298... Val Loss: 0.000853\n",
      "Epoch: 52/100... Step: 53625... Loss: 0.000276... Val Loss: 0.000960\n",
      "Epoch: 52/100... Step: 53630... Loss: 0.000205... Val Loss: 0.000769\n",
      "Epoch: 52/100... Step: 53635... Loss: 0.000278... Val Loss: 0.000713\n",
      "Epoch: 52/100... Step: 53640... Loss: 0.000290... Val Loss: 0.000832\n",
      "Epoch: 52/100... Step: 53645... Loss: 0.000316... Val Loss: 0.001010\n",
      "Epoch: 52/100... Step: 53650... Loss: 0.000425... Val Loss: 0.001119\n",
      "Epoch: 52/100... Step: 53655... Loss: 0.000443... Val Loss: 0.001276\n",
      "Epoch: 52/100... Step: 53660... Loss: 0.000432... Val Loss: 0.001069\n",
      "Epoch: 53/100... Step: 53665... Loss: 0.000730... Val Loss: 0.000932\n",
      "Epoch: 53/100... Step: 53670... Loss: 0.001184... Val Loss: 0.001106\n",
      "Epoch: 53/100... Step: 53675... Loss: 0.000757... Val Loss: 0.001945\n",
      "Epoch: 53/100... Step: 53680... Loss: 0.000524... Val Loss: 0.001930\n",
      "Epoch: 53/100... Step: 53685... Loss: 0.000473... Val Loss: 0.001748\n",
      "Epoch: 53/100... Step: 53690... Loss: 0.000401... Val Loss: 0.001929\n",
      "Epoch: 53/100... Step: 53695... Loss: 0.000332... Val Loss: 0.002330\n",
      "Epoch: 53/100... Step: 53700... Loss: 0.000411... Val Loss: 0.001863\n",
      "Epoch: 53/100... Step: 53705... Loss: 0.000258... Val Loss: 0.001752\n",
      "Epoch: 53/100... Step: 53710... Loss: 0.000341... Val Loss: 0.002164\n",
      "Epoch: 53/100... Step: 53715... Loss: 0.000629... Val Loss: 0.001683\n",
      "Epoch: 53/100... Step: 53720... Loss: 0.000564... Val Loss: 0.001034\n",
      "Epoch: 53/100... Step: 53725... Loss: 0.000961... Val Loss: 0.001568\n",
      "Epoch: 53/100... Step: 53730... Loss: 0.000499... Val Loss: 0.001479\n",
      "Epoch: 53/100... Step: 53735... Loss: 0.001052... Val Loss: 0.001390\n",
      "Epoch: 53/100... Step: 53740... Loss: 0.000373... Val Loss: 0.001210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53/100... Step: 53745... Loss: 0.000457... Val Loss: 0.001567\n",
      "Epoch: 53/100... Step: 53750... Loss: 0.000849... Val Loss: 0.001790\n",
      "Epoch: 53/100... Step: 53755... Loss: 0.000241... Val Loss: 0.001173\n",
      "Epoch: 53/100... Step: 53760... Loss: 0.000197... Val Loss: 0.000913\n",
      "Epoch: 53/100... Step: 53765... Loss: 0.000565... Val Loss: 0.000824\n",
      "Epoch: 53/100... Step: 53770... Loss: 0.000365... Val Loss: 0.001039\n",
      "Epoch: 53/100... Step: 53775... Loss: 0.000518... Val Loss: 0.001255\n",
      "Epoch: 53/100... Step: 53780... Loss: 0.000569... Val Loss: 0.001005\n",
      "Epoch: 53/100... Step: 53785... Loss: 0.000479... Val Loss: 0.001261\n",
      "Epoch: 53/100... Step: 53790... Loss: 0.000412... Val Loss: 0.001469\n",
      "Epoch: 53/100... Step: 53795... Loss: 0.000396... Val Loss: 0.001190\n",
      "Epoch: 53/100... Step: 53800... Loss: 0.000618... Val Loss: 0.001948\n",
      "Epoch: 53/100... Step: 53805... Loss: 0.000474... Val Loss: 0.001069\n",
      "Epoch: 53/100... Step: 53810... Loss: 0.000253... Val Loss: 0.001996\n",
      "Epoch: 53/100... Step: 53815... Loss: 0.000597... Val Loss: 0.001495\n",
      "Epoch: 53/100... Step: 53820... Loss: 0.000496... Val Loss: 0.001387\n",
      "Epoch: 53/100... Step: 53825... Loss: 0.000192... Val Loss: 0.001531\n",
      "Epoch: 53/100... Step: 53830... Loss: 0.000855... Val Loss: 0.001541\n",
      "Epoch: 53/100... Step: 53835... Loss: 0.000783... Val Loss: 0.001560\n",
      "Epoch: 53/100... Step: 53840... Loss: 0.000272... Val Loss: 0.001207\n",
      "Epoch: 53/100... Step: 53845... Loss: 0.001006... Val Loss: 0.001294\n",
      "Epoch: 53/100... Step: 53850... Loss: 0.001080... Val Loss: 0.001387\n",
      "Epoch: 53/100... Step: 53855... Loss: 0.000806... Val Loss: 0.001395\n",
      "Epoch: 53/100... Step: 53860... Loss: 0.000226... Val Loss: 0.001494\n",
      "Epoch: 53/100... Step: 53865... Loss: 0.000801... Val Loss: 0.001169\n",
      "Epoch: 53/100... Step: 53870... Loss: 0.000354... Val Loss: 0.001716\n",
      "Epoch: 53/100... Step: 53875... Loss: 0.000687... Val Loss: 0.001185\n",
      "Epoch: 53/100... Step: 53880... Loss: 0.000445... Val Loss: 0.000940\n",
      "Epoch: 53/100... Step: 53885... Loss: 0.000795... Val Loss: 0.001382\n",
      "Epoch: 53/100... Step: 53890... Loss: 0.000440... Val Loss: 0.000849\n",
      "Epoch: 53/100... Step: 53895... Loss: 0.000563... Val Loss: 0.000642\n",
      "Epoch: 53/100... Step: 53900... Loss: 0.000426... Val Loss: 0.000651\n",
      "Epoch: 53/100... Step: 53905... Loss: 0.000717... Val Loss: 0.001442\n",
      "Epoch: 53/100... Step: 53910... Loss: 0.000362... Val Loss: 0.001403\n",
      "Epoch: 53/100... Step: 53915... Loss: 0.000839... Val Loss: 0.001151\n",
      "Epoch: 53/100... Step: 53920... Loss: 0.000235... Val Loss: 0.001329\n",
      "Epoch: 53/100... Step: 53925... Loss: 0.000257... Val Loss: 0.000803\n",
      "Epoch: 53/100... Step: 53930... Loss: 0.000251... Val Loss: 0.000776\n",
      "Epoch: 53/100... Step: 53935... Loss: 0.000927... Val Loss: 0.000936\n",
      "Epoch: 53/100... Step: 53940... Loss: 0.000318... Val Loss: 0.000837\n",
      "Epoch: 53/100... Step: 53945... Loss: 0.000354... Val Loss: 0.000933\n",
      "Epoch: 53/100... Step: 53950... Loss: 0.000210... Val Loss: 0.000882\n",
      "Epoch: 53/100... Step: 53955... Loss: 0.000493... Val Loss: 0.000774\n",
      "Epoch: 53/100... Step: 53960... Loss: 0.000485... Val Loss: 0.000636\n",
      "Epoch: 53/100... Step: 53965... Loss: 0.000331... Val Loss: 0.000607\n",
      "Epoch: 53/100... Step: 53970... Loss: 0.000334... Val Loss: 0.001151\n",
      "Epoch: 53/100... Step: 53975... Loss: 0.000153... Val Loss: 0.000949\n",
      "Epoch: 53/100... Step: 53980... Loss: 0.000177... Val Loss: 0.000479\n",
      "Epoch: 53/100... Step: 53985... Loss: 0.000663... Val Loss: 0.000795\n",
      "Epoch: 53/100... Step: 53990... Loss: 0.000229... Val Loss: 0.000380\n",
      "Epoch: 53/100... Step: 53995... Loss: 0.000425... Val Loss: 0.000487\n",
      "Epoch: 53/100... Step: 54000... Loss: 0.000492... Val Loss: 0.000909\n",
      "Epoch: 53/100... Step: 54005... Loss: 0.000258... Val Loss: 0.000682\n",
      "Epoch: 53/100... Step: 54010... Loss: 0.000229... Val Loss: 0.000468\n",
      "Epoch: 53/100... Step: 54015... Loss: 0.000358... Val Loss: 0.000441\n",
      "Epoch: 53/100... Step: 54020... Loss: 0.000395... Val Loss: 0.000829\n",
      "Epoch: 53/100... Step: 54025... Loss: 0.000210... Val Loss: 0.000511\n",
      "Epoch: 53/100... Step: 54030... Loss: 0.000567... Val Loss: 0.000690\n",
      "Epoch: 53/100... Step: 54035... Loss: 0.000292... Val Loss: 0.001019\n",
      "Epoch: 53/100... Step: 54040... Loss: 0.000346... Val Loss: 0.001259\n",
      "Epoch: 53/100... Step: 54045... Loss: 0.000247... Val Loss: 0.001250\n",
      "Epoch: 53/100... Step: 54050... Loss: 0.000374... Val Loss: 0.000999\n",
      "Epoch: 53/100... Step: 54055... Loss: 0.000496... Val Loss: 0.001103\n",
      "Epoch: 53/100... Step: 54060... Loss: 0.000866... Val Loss: 0.001615\n",
      "Epoch: 53/100... Step: 54065... Loss: 0.000676... Val Loss: 0.000882\n",
      "Epoch: 53/100... Step: 54070... Loss: 0.000567... Val Loss: 0.000392\n",
      "Epoch: 53/100... Step: 54075... Loss: 0.000247... Val Loss: 0.000286\n",
      "Epoch: 53/100... Step: 54080... Loss: 0.000843... Val Loss: 0.000300\n",
      "Epoch: 53/100... Step: 54085... Loss: 0.000321... Val Loss: 0.000760\n",
      "Epoch: 53/100... Step: 54090... Loss: 0.000537... Val Loss: 0.001090\n",
      "Epoch: 53/100... Step: 54095... Loss: 0.000318... Val Loss: 0.001037\n",
      "Epoch: 53/100... Step: 54100... Loss: 0.000178... Val Loss: 0.001025\n",
      "Epoch: 53/100... Step: 54105... Loss: 0.000346... Val Loss: 0.001276\n",
      "Epoch: 53/100... Step: 54110... Loss: 0.000679... Val Loss: 0.000944\n",
      "Epoch: 53/100... Step: 54115... Loss: 0.000315... Val Loss: 0.001066\n",
      "Epoch: 53/100... Step: 54120... Loss: 0.000500... Val Loss: 0.001060\n",
      "Epoch: 53/100... Step: 54125... Loss: 0.000346... Val Loss: 0.001526\n",
      "Epoch: 53/100... Step: 54130... Loss: 0.000685... Val Loss: 0.001324\n",
      "Epoch: 53/100... Step: 54135... Loss: 0.000403... Val Loss: 0.000953\n",
      "Epoch: 53/100... Step: 54140... Loss: 0.000358... Val Loss: 0.001187\n",
      "Epoch: 53/100... Step: 54145... Loss: 0.000386... Val Loss: 0.001259\n",
      "Epoch: 53/100... Step: 54150... Loss: 0.000289... Val Loss: 0.001065\n",
      "Epoch: 53/100... Step: 54155... Loss: 0.000265... Val Loss: 0.001153\n",
      "Epoch: 53/100... Step: 54160... Loss: 0.000347... Val Loss: 0.000695\n",
      "Epoch: 53/100... Step: 54165... Loss: 0.000258... Val Loss: 0.001312\n",
      "Epoch: 53/100... Step: 54170... Loss: 0.000192... Val Loss: 0.000827\n",
      "Epoch: 53/100... Step: 54175... Loss: 0.000114... Val Loss: 0.000434\n",
      "Epoch: 53/100... Step: 54180... Loss: 0.000117... Val Loss: 0.000389\n",
      "Epoch: 53/100... Step: 54185... Loss: 0.000329... Val Loss: 0.000513\n",
      "Epoch: 53/100... Step: 54190... Loss: 0.000263... Val Loss: 0.000869\n",
      "Epoch: 53/100... Step: 54195... Loss: 0.000222... Val Loss: 0.000616\n",
      "Epoch: 53/100... Step: 54200... Loss: 0.000400... Val Loss: 0.000329\n",
      "Epoch: 53/100... Step: 54205... Loss: 0.000464... Val Loss: 0.000724\n",
      "Epoch: 53/100... Step: 54210... Loss: 0.000077... Val Loss: 0.001140\n",
      "Epoch: 53/100... Step: 54215... Loss: 0.000103... Val Loss: 0.000974\n",
      "Epoch: 53/100... Step: 54220... Loss: 0.000338... Val Loss: 0.000730\n",
      "Epoch: 53/100... Step: 54225... Loss: 0.000405... Val Loss: 0.000809\n",
      "Epoch: 53/100... Step: 54230... Loss: 0.000144... Val Loss: 0.000454\n",
      "Epoch: 53/100... Step: 54235... Loss: 0.000416... Val Loss: 0.000927\n",
      "Epoch: 53/100... Step: 54240... Loss: 0.000296... Val Loss: 0.001332\n",
      "Epoch: 53/100... Step: 54245... Loss: 0.000205... Val Loss: 0.001052\n",
      "Epoch: 53/100... Step: 54250... Loss: 0.000256... Val Loss: 0.000965\n",
      "Epoch: 53/100... Step: 54255... Loss: 0.000308... Val Loss: 0.000828\n",
      "Epoch: 53/100... Step: 54260... Loss: 0.000446... Val Loss: 0.000931\n",
      "Epoch: 53/100... Step: 54265... Loss: 0.000267... Val Loss: 0.000860\n",
      "Epoch: 53/100... Step: 54270... Loss: 0.000109... Val Loss: 0.000813\n",
      "Epoch: 53/100... Step: 54275... Loss: 0.000259... Val Loss: 0.000763\n",
      "Epoch: 53/100... Step: 54280... Loss: 0.000369... Val Loss: 0.000899\n",
      "Epoch: 53/100... Step: 54285... Loss: 0.000118... Val Loss: 0.000842\n",
      "Epoch: 53/100... Step: 54290... Loss: 0.000319... Val Loss: 0.001032\n",
      "Epoch: 53/100... Step: 54295... Loss: 0.000189... Val Loss: 0.000648\n",
      "Epoch: 53/100... Step: 54300... Loss: 0.000243... Val Loss: 0.000764\n",
      "Epoch: 53/100... Step: 54305... Loss: 0.000213... Val Loss: 0.000739\n",
      "Epoch: 53/100... Step: 54310... Loss: 0.000150... Val Loss: 0.000470\n",
      "Epoch: 53/100... Step: 54315... Loss: 0.000204... Val Loss: 0.000642\n",
      "Epoch: 53/100... Step: 54320... Loss: 0.000317... Val Loss: 0.000943\n",
      "Epoch: 53/100... Step: 54325... Loss: 0.000301... Val Loss: 0.000779\n",
      "Epoch: 53/100... Step: 54330... Loss: 0.000491... Val Loss: 0.000671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53/100... Step: 54335... Loss: 0.000188... Val Loss: 0.000364\n",
      "Epoch: 53/100... Step: 54340... Loss: 0.000126... Val Loss: 0.000436\n",
      "Epoch: 53/100... Step: 54345... Loss: 0.000240... Val Loss: 0.000327\n",
      "Epoch: 53/100... Step: 54350... Loss: 0.000300... Val Loss: 0.000330\n",
      "Epoch: 53/100... Step: 54355... Loss: 0.000268... Val Loss: 0.000552\n",
      "Epoch: 53/100... Step: 54360... Loss: 0.000182... Val Loss: 0.000256\n",
      "Validation loss decreased (0.000258 --> 0.000256).  Saving model ...\n",
      "Epoch: 53/100... Step: 54365... Loss: 0.000417... Val Loss: 0.000274\n",
      "Epoch: 53/100... Step: 54370... Loss: 0.000393... Val Loss: 0.000258\n",
      "Epoch: 53/100... Step: 54375... Loss: 0.000188... Val Loss: 0.000269\n",
      "Epoch: 53/100... Step: 54380... Loss: 0.000373... Val Loss: 0.000416\n",
      "Epoch: 53/100... Step: 54385... Loss: 0.000149... Val Loss: 0.000315\n",
      "Epoch: 53/100... Step: 54390... Loss: 0.000178... Val Loss: 0.000393\n",
      "Epoch: 53/100... Step: 54395... Loss: 0.000156... Val Loss: 0.000407\n",
      "Epoch: 53/100... Step: 54400... Loss: 0.000252... Val Loss: 0.000340\n",
      "Epoch: 53/100... Step: 54405... Loss: 0.000174... Val Loss: 0.000347\n",
      "Epoch: 53/100... Step: 54410... Loss: 0.000193... Val Loss: 0.000422\n",
      "Epoch: 53/100... Step: 54415... Loss: 0.000422... Val Loss: 0.000436\n",
      "Epoch: 53/100... Step: 54420... Loss: 0.000173... Val Loss: 0.000826\n",
      "Epoch: 53/100... Step: 54425... Loss: 0.000228... Val Loss: 0.000497\n",
      "Epoch: 53/100... Step: 54430... Loss: 0.000258... Val Loss: 0.000440\n",
      "Epoch: 53/100... Step: 54435... Loss: 0.000148... Val Loss: 0.000340\n",
      "Epoch: 53/100... Step: 54440... Loss: 0.000333... Val Loss: 0.000399\n",
      "Epoch: 53/100... Step: 54445... Loss: 0.000311... Val Loss: 0.000423\n",
      "Epoch: 53/100... Step: 54450... Loss: 0.000159... Val Loss: 0.000650\n",
      "Epoch: 53/100... Step: 54455... Loss: 0.000100... Val Loss: 0.000563\n",
      "Epoch: 53/100... Step: 54460... Loss: 0.000326... Val Loss: 0.000442\n",
      "Epoch: 53/100... Step: 54465... Loss: 0.000138... Val Loss: 0.000503\n",
      "Epoch: 53/100... Step: 54470... Loss: 0.000272... Val Loss: 0.000696\n",
      "Epoch: 53/100... Step: 54475... Loss: 0.000053... Val Loss: 0.000710\n",
      "Epoch: 53/100... Step: 54480... Loss: 0.000411... Val Loss: 0.000878\n",
      "Epoch: 53/100... Step: 54485... Loss: 0.000302... Val Loss: 0.000865\n",
      "Epoch: 53/100... Step: 54490... Loss: 0.000327... Val Loss: 0.000819\n",
      "Epoch: 53/100... Step: 54495... Loss: 0.000130... Val Loss: 0.000683\n",
      "Epoch: 53/100... Step: 54500... Loss: 0.000094... Val Loss: 0.000875\n",
      "Epoch: 53/100... Step: 54505... Loss: 0.000190... Val Loss: 0.000712\n",
      "Epoch: 53/100... Step: 54510... Loss: 0.000111... Val Loss: 0.000804\n",
      "Epoch: 53/100... Step: 54515... Loss: 0.000429... Val Loss: 0.000466\n",
      "Epoch: 53/100... Step: 54520... Loss: 0.000145... Val Loss: 0.000455\n",
      "Epoch: 53/100... Step: 54525... Loss: 0.000235... Val Loss: 0.000646\n",
      "Epoch: 53/100... Step: 54530... Loss: 0.000247... Val Loss: 0.000714\n",
      "Epoch: 53/100... Step: 54535... Loss: 0.000214... Val Loss: 0.000775\n",
      "Epoch: 53/100... Step: 54540... Loss: 0.000338... Val Loss: 0.000829\n",
      "Epoch: 53/100... Step: 54545... Loss: 0.000126... Val Loss: 0.000785\n",
      "Epoch: 53/100... Step: 54550... Loss: 0.000411... Val Loss: 0.000888\n",
      "Epoch: 53/100... Step: 54555... Loss: 0.000121... Val Loss: 0.000755\n",
      "Epoch: 53/100... Step: 54560... Loss: 0.000066... Val Loss: 0.000546\n",
      "Epoch: 53/100... Step: 54565... Loss: 0.000120... Val Loss: 0.000527\n",
      "Epoch: 53/100... Step: 54570... Loss: 0.000120... Val Loss: 0.000636\n",
      "Epoch: 53/100... Step: 54575... Loss: 0.000179... Val Loss: 0.000534\n",
      "Epoch: 53/100... Step: 54580... Loss: 0.000162... Val Loss: 0.000447\n",
      "Epoch: 53/100... Step: 54585... Loss: 0.000152... Val Loss: 0.000641\n",
      "Epoch: 53/100... Step: 54590... Loss: 0.000261... Val Loss: 0.000825\n",
      "Epoch: 53/100... Step: 54595... Loss: 0.000258... Val Loss: 0.000687\n",
      "Epoch: 53/100... Step: 54600... Loss: 0.000377... Val Loss: 0.000592\n",
      "Epoch: 53/100... Step: 54605... Loss: 0.000223... Val Loss: 0.000656\n",
      "Epoch: 53/100... Step: 54610... Loss: 0.000345... Val Loss: 0.001007\n",
      "Epoch: 53/100... Step: 54615... Loss: 0.000279... Val Loss: 0.001063\n",
      "Epoch: 53/100... Step: 54620... Loss: 0.000232... Val Loss: 0.001280\n",
      "Epoch: 53/100... Step: 54625... Loss: 0.000264... Val Loss: 0.001005\n",
      "Epoch: 53/100... Step: 54630... Loss: 0.000126... Val Loss: 0.001140\n",
      "Epoch: 53/100... Step: 54635... Loss: 0.000064... Val Loss: 0.001190\n",
      "Epoch: 53/100... Step: 54640... Loss: 0.000363... Val Loss: 0.001125\n",
      "Epoch: 53/100... Step: 54645... Loss: 0.000306... Val Loss: 0.001078\n",
      "Epoch: 53/100... Step: 54650... Loss: 0.000250... Val Loss: 0.000941\n",
      "Epoch: 53/100... Step: 54655... Loss: 0.000150... Val Loss: 0.000778\n",
      "Epoch: 53/100... Step: 54660... Loss: 0.000099... Val Loss: 0.000918\n",
      "Epoch: 53/100... Step: 54665... Loss: 0.000144... Val Loss: 0.000805\n",
      "Epoch: 53/100... Step: 54670... Loss: 0.000143... Val Loss: 0.000892\n",
      "Epoch: 53/100... Step: 54675... Loss: 0.000091... Val Loss: 0.000801\n",
      "Epoch: 53/100... Step: 54680... Loss: 0.000115... Val Loss: 0.000855\n",
      "Epoch: 53/100... Step: 54685... Loss: 0.000123... Val Loss: 0.001041\n",
      "Epoch: 53/100... Step: 54690... Loss: 0.000055... Val Loss: 0.000958\n",
      "Epoch: 53/100... Step: 54695... Loss: 0.000072... Val Loss: 0.001070\n",
      "Epoch: 54/100... Step: 54700... Loss: 0.000214... Val Loss: 0.001668\n",
      "Epoch: 54/100... Step: 54705... Loss: 0.000079... Val Loss: 0.001735\n",
      "Epoch: 54/100... Step: 54710... Loss: 0.000332... Val Loss: 0.001431\n",
      "Epoch: 54/100... Step: 54715... Loss: 0.000254... Val Loss: 0.001722\n",
      "Epoch: 54/100... Step: 54720... Loss: 0.000240... Val Loss: 0.001720\n",
      "Epoch: 54/100... Step: 54725... Loss: 0.000120... Val Loss: 0.001900\n",
      "Epoch: 54/100... Step: 54730... Loss: 0.000146... Val Loss: 0.001853\n",
      "Epoch: 54/100... Step: 54735... Loss: 0.000118... Val Loss: 0.001620\n",
      "Epoch: 54/100... Step: 54740... Loss: 0.000185... Val Loss: 0.001847\n",
      "Epoch: 54/100... Step: 54745... Loss: 0.000048... Val Loss: 0.001877\n",
      "Epoch: 54/100... Step: 54750... Loss: 0.000064... Val Loss: 0.001871\n",
      "Epoch: 54/100... Step: 54755... Loss: 0.000100... Val Loss: 0.001792\n",
      "Epoch: 54/100... Step: 54760... Loss: 0.000157... Val Loss: 0.001646\n",
      "Epoch: 54/100... Step: 54765... Loss: 0.000167... Val Loss: 0.001505\n",
      "Epoch: 54/100... Step: 54770... Loss: 0.000103... Val Loss: 0.001231\n",
      "Epoch: 54/100... Step: 54775... Loss: 0.000208... Val Loss: 0.001395\n",
      "Epoch: 54/100... Step: 54780... Loss: 0.000131... Val Loss: 0.001421\n",
      "Epoch: 54/100... Step: 54785... Loss: 0.000143... Val Loss: 0.001268\n",
      "Epoch: 54/100... Step: 54790... Loss: 0.000175... Val Loss: 0.001126\n",
      "Epoch: 54/100... Step: 54795... Loss: 0.000154... Val Loss: 0.001070\n",
      "Epoch: 54/100... Step: 54800... Loss: 0.000152... Val Loss: 0.001146\n",
      "Epoch: 54/100... Step: 54805... Loss: 0.000145... Val Loss: 0.001137\n",
      "Epoch: 54/100... Step: 54810... Loss: 0.000042... Val Loss: 0.001166\n",
      "Epoch: 54/100... Step: 54815... Loss: 0.000093... Val Loss: 0.001144\n",
      "Epoch: 54/100... Step: 54820... Loss: 0.000139... Val Loss: 0.001215\n",
      "Epoch: 54/100... Step: 54825... Loss: 0.000086... Val Loss: 0.001384\n",
      "Epoch: 54/100... Step: 54830... Loss: 0.000062... Val Loss: 0.001213\n",
      "Epoch: 54/100... Step: 54835... Loss: 0.000163... Val Loss: 0.001449\n",
      "Epoch: 54/100... Step: 54840... Loss: 0.000136... Val Loss: 0.001617\n",
      "Epoch: 54/100... Step: 54845... Loss: 0.000087... Val Loss: 0.001727\n",
      "Epoch: 54/100... Step: 54850... Loss: 0.000096... Val Loss: 0.001452\n",
      "Epoch: 54/100... Step: 54855... Loss: 0.000216... Val Loss: 0.001506\n",
      "Epoch: 54/100... Step: 54860... Loss: 0.000190... Val Loss: 0.001451\n",
      "Epoch: 54/100... Step: 54865... Loss: 0.000155... Val Loss: 0.001260\n",
      "Epoch: 54/100... Step: 54870... Loss: 0.000214... Val Loss: 0.001297\n",
      "Epoch: 54/100... Step: 54875... Loss: 0.000156... Val Loss: 0.001351\n",
      "Epoch: 54/100... Step: 54880... Loss: 0.000227... Val Loss: 0.001281\n",
      "Epoch: 54/100... Step: 54885... Loss: 0.000156... Val Loss: 0.001094\n",
      "Epoch: 54/100... Step: 54890... Loss: 0.000142... Val Loss: 0.001036\n",
      "Epoch: 54/100... Step: 54895... Loss: 0.000216... Val Loss: 0.001058\n",
      "Epoch: 54/100... Step: 54900... Loss: 0.000102... Val Loss: 0.001353\n",
      "Epoch: 54/100... Step: 54905... Loss: 0.000242... Val Loss: 0.001415\n",
      "Epoch: 54/100... Step: 54910... Loss: 0.000137... Val Loss: 0.001145\n",
      "Epoch: 54/100... Step: 54915... Loss: 0.000134... Val Loss: 0.001287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54/100... Step: 54920... Loss: 0.000188... Val Loss: 0.001162\n",
      "Epoch: 54/100... Step: 54925... Loss: 0.000189... Val Loss: 0.001169\n",
      "Epoch: 54/100... Step: 54930... Loss: 0.000189... Val Loss: 0.000877\n",
      "Epoch: 54/100... Step: 54935... Loss: 0.000225... Val Loss: 0.001060\n",
      "Epoch: 54/100... Step: 54940... Loss: 0.000152... Val Loss: 0.001027\n",
      "Epoch: 54/100... Step: 54945... Loss: 0.000193... Val Loss: 0.001206\n",
      "Epoch: 54/100... Step: 54950... Loss: 0.000151... Val Loss: 0.001049\n",
      "Epoch: 54/100... Step: 54955... Loss: 0.000180... Val Loss: 0.001141\n",
      "Epoch: 54/100... Step: 54960... Loss: 0.000235... Val Loss: 0.000957\n",
      "Epoch: 54/100... Step: 54965... Loss: 0.000113... Val Loss: 0.000752\n",
      "Epoch: 54/100... Step: 54970... Loss: 0.000147... Val Loss: 0.000747\n",
      "Epoch: 54/100... Step: 54975... Loss: 0.000143... Val Loss: 0.000856\n",
      "Epoch: 54/100... Step: 54980... Loss: 0.000099... Val Loss: 0.000972\n",
      "Epoch: 54/100... Step: 54985... Loss: 0.000131... Val Loss: 0.000814\n",
      "Epoch: 54/100... Step: 54990... Loss: 0.000078... Val Loss: 0.000560\n",
      "Epoch: 54/100... Step: 54995... Loss: 0.000068... Val Loss: 0.000553\n",
      "Epoch: 54/100... Step: 55000... Loss: 0.000199... Val Loss: 0.000652\n",
      "Epoch: 54/100... Step: 55005... Loss: 0.000126... Val Loss: 0.000555\n",
      "Epoch: 54/100... Step: 55010... Loss: 0.000165... Val Loss: 0.000521\n",
      "Epoch: 54/100... Step: 55015... Loss: 0.000214... Val Loss: 0.000560\n",
      "Epoch: 54/100... Step: 55020... Loss: 0.000186... Val Loss: 0.000576\n",
      "Epoch: 54/100... Step: 55025... Loss: 0.000171... Val Loss: 0.000475\n",
      "Epoch: 54/100... Step: 55030... Loss: 0.000151... Val Loss: 0.000485\n",
      "Epoch: 54/100... Step: 55035... Loss: 0.000097... Val Loss: 0.000691\n",
      "Epoch: 54/100... Step: 55040... Loss: 0.000221... Val Loss: 0.000563\n",
      "Epoch: 54/100... Step: 55045... Loss: 0.000141... Val Loss: 0.000430\n",
      "Epoch: 54/100... Step: 55050... Loss: 0.000260... Val Loss: 0.000739\n",
      "Epoch: 54/100... Step: 55055... Loss: 0.000139... Val Loss: 0.000864\n",
      "Epoch: 54/100... Step: 55060... Loss: 0.000110... Val Loss: 0.000984\n",
      "Epoch: 54/100... Step: 55065... Loss: 0.000072... Val Loss: 0.001069\n",
      "Epoch: 54/100... Step: 55070... Loss: 0.000208... Val Loss: 0.000882\n",
      "Epoch: 54/100... Step: 55075... Loss: 0.000132... Val Loss: 0.000969\n",
      "Epoch: 54/100... Step: 55080... Loss: 0.000180... Val Loss: 0.000994\n",
      "Epoch: 54/100... Step: 55085... Loss: 0.000105... Val Loss: 0.001022\n",
      "Epoch: 54/100... Step: 55090... Loss: 0.000054... Val Loss: 0.001124\n",
      "Epoch: 54/100... Step: 55095... Loss: 0.000115... Val Loss: 0.001050\n",
      "Epoch: 54/100... Step: 55100... Loss: 0.000240... Val Loss: 0.000961\n",
      "Epoch: 54/100... Step: 55105... Loss: 0.000139... Val Loss: 0.000275\n",
      "Epoch: 54/100... Step: 55110... Loss: 0.000706... Val Loss: 0.000283\n",
      "Epoch: 54/100... Step: 55115... Loss: 0.000322... Val Loss: 0.001745\n",
      "Epoch: 54/100... Step: 55120... Loss: 0.000380... Val Loss: 0.001005\n",
      "Epoch: 54/100... Step: 55125... Loss: 0.000307... Val Loss: 0.000687\n",
      "Epoch: 54/100... Step: 55130... Loss: 0.000215... Val Loss: 0.001101\n",
      "Epoch: 54/100... Step: 55135... Loss: 0.000110... Val Loss: 0.001243\n",
      "Epoch: 54/100... Step: 55140... Loss: 0.000273... Val Loss: 0.001203\n",
      "Epoch: 54/100... Step: 55145... Loss: 0.000117... Val Loss: 0.001245\n",
      "Epoch: 54/100... Step: 55150... Loss: 0.000120... Val Loss: 0.001437\n",
      "Epoch: 54/100... Step: 55155... Loss: 0.000150... Val Loss: 0.001481\n",
      "Epoch: 54/100... Step: 55160... Loss: 0.000091... Val Loss: 0.001582\n",
      "Epoch: 54/100... Step: 55165... Loss: 0.000182... Val Loss: 0.001356\n",
      "Epoch: 54/100... Step: 55170... Loss: 0.000076... Val Loss: 0.001190\n",
      "Epoch: 54/100... Step: 55175... Loss: 0.000161... Val Loss: 0.001266\n",
      "Epoch: 54/100... Step: 55180... Loss: 0.000197... Val Loss: 0.001029\n",
      "Epoch: 54/100... Step: 55185... Loss: 0.000102... Val Loss: 0.001012\n",
      "Epoch: 54/100... Step: 55190... Loss: 0.000104... Val Loss: 0.000790\n",
      "Epoch: 54/100... Step: 55195... Loss: 0.000111... Val Loss: 0.000813\n",
      "Epoch: 54/100... Step: 55200... Loss: 0.000178... Val Loss: 0.000538\n",
      "Epoch: 54/100... Step: 55205... Loss: 0.000084... Val Loss: 0.000354\n",
      "Epoch: 54/100... Step: 55210... Loss: 0.000206... Val Loss: 0.000481\n",
      "Epoch: 54/100... Step: 55215... Loss: 0.000116... Val Loss: 0.000423\n",
      "Epoch: 54/100... Step: 55220... Loss: 0.000194... Val Loss: 0.000675\n",
      "Epoch: 54/100... Step: 55225... Loss: 0.000177... Val Loss: 0.000658\n",
      "Epoch: 54/100... Step: 55230... Loss: 0.000079... Val Loss: 0.000510\n",
      "Epoch: 54/100... Step: 55235... Loss: 0.000164... Val Loss: 0.000731\n",
      "Epoch: 54/100... Step: 55240... Loss: 0.000110... Val Loss: 0.000992\n",
      "Epoch: 54/100... Step: 55245... Loss: 0.000198... Val Loss: 0.000978\n",
      "Epoch: 54/100... Step: 55250... Loss: 0.000171... Val Loss: 0.000741\n",
      "Epoch: 54/100... Step: 55255... Loss: 0.000106... Val Loss: 0.000528\n",
      "Epoch: 54/100... Step: 55260... Loss: 0.000076... Val Loss: 0.000751\n",
      "Epoch: 54/100... Step: 55265... Loss: 0.000137... Val Loss: 0.000626\n",
      "Epoch: 54/100... Step: 55270... Loss: 0.000400... Val Loss: 0.000990\n",
      "Epoch: 54/100... Step: 55275... Loss: 0.000145... Val Loss: 0.001116\n",
      "Epoch: 54/100... Step: 55280... Loss: 0.000126... Val Loss: 0.000941\n",
      "Epoch: 54/100... Step: 55285... Loss: 0.000285... Val Loss: 0.000800\n",
      "Epoch: 54/100... Step: 55290... Loss: 0.000280... Val Loss: 0.000647\n",
      "Epoch: 54/100... Step: 55295... Loss: 0.000157... Val Loss: 0.000871\n",
      "Epoch: 54/100... Step: 55300... Loss: 0.000088... Val Loss: 0.001029\n",
      "Epoch: 54/100... Step: 55305... Loss: 0.000092... Val Loss: 0.000873\n",
      "Epoch: 54/100... Step: 55310... Loss: 0.000078... Val Loss: 0.001015\n",
      "Epoch: 54/100... Step: 55315... Loss: 0.000137... Val Loss: 0.001064\n",
      "Epoch: 54/100... Step: 55320... Loss: 0.000274... Val Loss: 0.000838\n",
      "Epoch: 54/100... Step: 55325... Loss: 0.000180... Val Loss: 0.000957\n",
      "Epoch: 54/100... Step: 55330... Loss: 0.000367... Val Loss: 0.000929\n",
      "Epoch: 54/100... Step: 55335... Loss: 0.000328... Val Loss: 0.000930\n",
      "Epoch: 54/100... Step: 55340... Loss: 0.000256... Val Loss: 0.000903\n",
      "Epoch: 54/100... Step: 55345... Loss: 0.000190... Val Loss: 0.000827\n",
      "Epoch: 54/100... Step: 55350... Loss: 0.000034... Val Loss: 0.000918\n",
      "Epoch: 54/100... Step: 55355... Loss: 0.000130... Val Loss: 0.000735\n",
      "Epoch: 54/100... Step: 55360... Loss: 0.000182... Val Loss: 0.000523\n",
      "Epoch: 54/100... Step: 55365... Loss: 0.000292... Val Loss: 0.000413\n",
      "Epoch: 54/100... Step: 55370... Loss: 0.000218... Val Loss: 0.000297\n",
      "Epoch: 54/100... Step: 55375... Loss: 0.000233... Val Loss: 0.000436\n",
      "Epoch: 54/100... Step: 55380... Loss: 0.000212... Val Loss: 0.000274\n",
      "Epoch: 54/100... Step: 55385... Loss: 0.000067... Val Loss: 0.000321\n",
      "Epoch: 54/100... Step: 55390... Loss: 0.000074... Val Loss: 0.000322\n",
      "Epoch: 54/100... Step: 55395... Loss: 0.000087... Val Loss: 0.000265\n",
      "Epoch: 54/100... Step: 55400... Loss: 0.000249... Val Loss: 0.000269\n",
      "Epoch: 54/100... Step: 55405... Loss: 0.000099... Val Loss: 0.000350\n",
      "Epoch: 54/100... Step: 55410... Loss: 0.000285... Val Loss: 0.000439\n",
      "Epoch: 54/100... Step: 55415... Loss: 0.000088... Val Loss: 0.000443\n",
      "Epoch: 54/100... Step: 55420... Loss: 0.000067... Val Loss: 0.000333\n",
      "Epoch: 54/100... Step: 55425... Loss: 0.000095... Val Loss: 0.000349\n",
      "Epoch: 54/100... Step: 55430... Loss: 0.000329... Val Loss: 0.000372\n",
      "Epoch: 54/100... Step: 55435... Loss: 0.000268... Val Loss: 0.000317\n",
      "Epoch: 54/100... Step: 55440... Loss: 0.000165... Val Loss: 0.000655\n",
      "Epoch: 54/100... Step: 55445... Loss: 0.000334... Val Loss: 0.000490\n",
      "Epoch: 54/100... Step: 55450... Loss: 0.000171... Val Loss: 0.000318\n",
      "Epoch: 54/100... Step: 55455... Loss: 0.000172... Val Loss: 0.000348\n",
      "Epoch: 54/100... Step: 55460... Loss: 0.000164... Val Loss: 0.000367\n",
      "Epoch: 54/100... Step: 55465... Loss: 0.000161... Val Loss: 0.000344\n",
      "Epoch: 54/100... Step: 55470... Loss: 0.000325... Val Loss: 0.000550\n",
      "Epoch: 54/100... Step: 55475... Loss: 0.000244... Val Loss: 0.000425\n",
      "Epoch: 54/100... Step: 55480... Loss: 0.000250... Val Loss: 0.000372\n",
      "Epoch: 54/100... Step: 55485... Loss: 0.000454... Val Loss: 0.000422\n",
      "Epoch: 54/100... Step: 55490... Loss: 0.000327... Val Loss: 0.000319\n",
      "Epoch: 54/100... Step: 55495... Loss: 0.000398... Val Loss: 0.000267\n",
      "Epoch: 54/100... Step: 55500... Loss: 0.000267... Val Loss: 0.000408\n",
      "Epoch: 54/100... Step: 55505... Loss: 0.000243... Val Loss: 0.000820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54/100... Step: 55510... Loss: 0.000218... Val Loss: 0.000507\n",
      "Epoch: 54/100... Step: 55515... Loss: 0.000107... Val Loss: 0.000456\n",
      "Epoch: 54/100... Step: 55520... Loss: 0.000231... Val Loss: 0.000742\n",
      "Epoch: 54/100... Step: 55525... Loss: 0.000260... Val Loss: 0.000780\n",
      "Epoch: 54/100... Step: 55530... Loss: 0.000854... Val Loss: 0.001004\n",
      "Epoch: 54/100... Step: 55535... Loss: 0.000319... Val Loss: 0.000530\n",
      "Epoch: 54/100... Step: 55540... Loss: 0.000383... Val Loss: 0.000728\n",
      "Epoch: 54/100... Step: 55545... Loss: 0.001150... Val Loss: 0.000853\n",
      "Epoch: 54/100... Step: 55550... Loss: 0.000522... Val Loss: 0.000661\n",
      "Epoch: 54/100... Step: 55555... Loss: 0.000494... Val Loss: 0.000844\n",
      "Epoch: 54/100... Step: 55560... Loss: 0.000294... Val Loss: 0.001195\n",
      "Epoch: 54/100... Step: 55565... Loss: 0.000426... Val Loss: 0.000699\n",
      "Epoch: 54/100... Step: 55570... Loss: 0.000519... Val Loss: 0.000825\n",
      "Epoch: 54/100... Step: 55575... Loss: 0.000196... Val Loss: 0.000428\n",
      "Epoch: 54/100... Step: 55580... Loss: 0.000966... Val Loss: 0.000470\n",
      "Epoch: 54/100... Step: 55585... Loss: 0.000608... Val Loss: 0.000606\n",
      "Epoch: 54/100... Step: 55590... Loss: 0.000942... Val Loss: 0.000263\n",
      "Epoch: 54/100... Step: 55595... Loss: 0.000175... Val Loss: 0.000845\n",
      "Epoch: 54/100... Step: 55600... Loss: 0.000867... Val Loss: 0.000535\n",
      "Epoch: 54/100... Step: 55605... Loss: 0.000527... Val Loss: 0.000841\n",
      "Epoch: 54/100... Step: 55610... Loss: 0.001038... Val Loss: 0.001217\n",
      "Epoch: 54/100... Step: 55615... Loss: 0.000288... Val Loss: 0.000540\n",
      "Epoch: 54/100... Step: 55620... Loss: 0.001110... Val Loss: 0.000824\n",
      "Epoch: 54/100... Step: 55625... Loss: 0.000446... Val Loss: 0.001447\n",
      "Epoch: 54/100... Step: 55630... Loss: 0.000555... Val Loss: 0.000970\n",
      "Epoch: 54/100... Step: 55635... Loss: 0.000377... Val Loss: 0.001303\n",
      "Epoch: 54/100... Step: 55640... Loss: 0.000949... Val Loss: 0.000740\n",
      "Epoch: 54/100... Step: 55645... Loss: 0.000524... Val Loss: 0.000792\n",
      "Epoch: 54/100... Step: 55650... Loss: 0.000513... Val Loss: 0.001254\n",
      "Epoch: 54/100... Step: 55655... Loss: 0.001098... Val Loss: 0.001486\n",
      "Epoch: 54/100... Step: 55660... Loss: 0.000361... Val Loss: 0.000985\n",
      "Epoch: 54/100... Step: 55665... Loss: 0.000450... Val Loss: 0.001289\n",
      "Epoch: 54/100... Step: 55670... Loss: 0.000244... Val Loss: 0.000929\n",
      "Epoch: 54/100... Step: 55675... Loss: 0.000351... Val Loss: 0.000699\n",
      "Epoch: 54/100... Step: 55680... Loss: 0.000534... Val Loss: 0.001100\n",
      "Epoch: 54/100... Step: 55685... Loss: 0.000477... Val Loss: 0.001323\n",
      "Epoch: 54/100... Step: 55690... Loss: 0.000519... Val Loss: 0.001189\n",
      "Epoch: 54/100... Step: 55695... Loss: 0.000515... Val Loss: 0.000750\n",
      "Epoch: 54/100... Step: 55700... Loss: 0.000278... Val Loss: 0.000732\n",
      "Epoch: 54/100... Step: 55705... Loss: 0.000257... Val Loss: 0.000855\n",
      "Epoch: 54/100... Step: 55710... Loss: 0.000741... Val Loss: 0.000610\n",
      "Epoch: 54/100... Step: 55715... Loss: 0.000915... Val Loss: 0.000989\n",
      "Epoch: 54/100... Step: 55720... Loss: 0.000746... Val Loss: 0.001162\n",
      "Epoch: 54/100... Step: 55725... Loss: 0.000288... Val Loss: 0.001695\n",
      "Epoch: 55/100... Step: 55730... Loss: 0.000781... Val Loss: 0.001803\n",
      "Epoch: 55/100... Step: 55735... Loss: 0.000353... Val Loss: 0.001667\n",
      "Epoch: 55/100... Step: 55740... Loss: 0.000649... Val Loss: 0.001360\n",
      "Epoch: 55/100... Step: 55745... Loss: 0.000781... Val Loss: 0.001091\n",
      "Epoch: 55/100... Step: 55750... Loss: 0.000765... Val Loss: 0.001617\n",
      "Epoch: 55/100... Step: 55755... Loss: 0.000684... Val Loss: 0.001662\n",
      "Epoch: 55/100... Step: 55760... Loss: 0.000207... Val Loss: 0.002051\n",
      "Epoch: 55/100... Step: 55765... Loss: 0.000704... Val Loss: 0.001924\n",
      "Epoch: 55/100... Step: 55770... Loss: 0.000514... Val Loss: 0.001776\n",
      "Epoch: 55/100... Step: 55775... Loss: 0.000438... Val Loss: 0.001755\n",
      "Epoch: 55/100... Step: 55780... Loss: 0.000375... Val Loss: 0.001610\n",
      "Epoch: 55/100... Step: 55785... Loss: 0.000240... Val Loss: 0.001810\n",
      "Epoch: 55/100... Step: 55790... Loss: 0.000187... Val Loss: 0.001365\n",
      "Epoch: 55/100... Step: 55795... Loss: 0.000769... Val Loss: 0.001268\n",
      "Epoch: 55/100... Step: 55800... Loss: 0.000565... Val Loss: 0.000678\n",
      "Epoch: 55/100... Step: 55805... Loss: 0.000255... Val Loss: 0.001218\n",
      "Epoch: 55/100... Step: 55810... Loss: 0.000879... Val Loss: 0.001910\n",
      "Epoch: 55/100... Step: 55815... Loss: 0.000576... Val Loss: 0.001409\n",
      "Epoch: 55/100... Step: 55820... Loss: 0.000631... Val Loss: 0.001494\n",
      "Epoch: 55/100... Step: 55825... Loss: 0.000927... Val Loss: 0.001036\n",
      "Epoch: 55/100... Step: 55830... Loss: 0.000341... Val Loss: 0.001096\n",
      "Epoch: 55/100... Step: 55835... Loss: 0.000851... Val Loss: 0.000906\n",
      "Epoch: 55/100... Step: 55840... Loss: 0.000466... Val Loss: 0.001161\n",
      "Epoch: 55/100... Step: 55845... Loss: 0.000326... Val Loss: 0.001257\n",
      "Epoch: 55/100... Step: 55850... Loss: 0.000564... Val Loss: 0.001352\n",
      "Epoch: 55/100... Step: 55855... Loss: 0.000907... Val Loss: 0.001973\n",
      "Epoch: 55/100... Step: 55860... Loss: 0.000453... Val Loss: 0.001534\n",
      "Epoch: 55/100... Step: 55865... Loss: 0.000221... Val Loss: 0.002073\n",
      "Epoch: 55/100... Step: 55870... Loss: 0.000191... Val Loss: 0.001767\n",
      "Epoch: 55/100... Step: 55875... Loss: 0.000320... Val Loss: 0.001833\n",
      "Epoch: 55/100... Step: 55880... Loss: 0.000274... Val Loss: 0.001466\n",
      "Epoch: 55/100... Step: 55885... Loss: 0.000107... Val Loss: 0.001339\n",
      "Epoch: 55/100... Step: 55890... Loss: 0.000295... Val Loss: 0.001217\n",
      "Epoch: 55/100... Step: 55895... Loss: 0.000155... Val Loss: 0.001442\n",
      "Epoch: 55/100... Step: 55900... Loss: 0.000155... Val Loss: 0.001648\n",
      "Epoch: 55/100... Step: 55905... Loss: 0.000093... Val Loss: 0.001622\n",
      "Epoch: 55/100... Step: 55910... Loss: 0.000088... Val Loss: 0.001560\n",
      "Epoch: 55/100... Step: 55915... Loss: 0.000248... Val Loss: 0.001258\n",
      "Epoch: 55/100... Step: 55920... Loss: 0.000226... Val Loss: 0.001338\n",
      "Epoch: 55/100... Step: 55925... Loss: 0.000474... Val Loss: 0.001630\n",
      "Epoch: 55/100... Step: 55930... Loss: 0.000469... Val Loss: 0.001326\n",
      "Epoch: 55/100... Step: 55935... Loss: 0.000442... Val Loss: 0.001356\n",
      "Epoch: 55/100... Step: 55940... Loss: 0.000165... Val Loss: 0.001165\n",
      "Epoch: 55/100... Step: 55945... Loss: 0.000101... Val Loss: 0.001743\n",
      "Epoch: 55/100... Step: 55950... Loss: 0.000264... Val Loss: 0.001339\n",
      "Epoch: 55/100... Step: 55955... Loss: 0.000284... Val Loss: 0.001463\n",
      "Epoch: 55/100... Step: 55960... Loss: 0.000127... Val Loss: 0.001056\n",
      "Epoch: 55/100... Step: 55965... Loss: 0.000254... Val Loss: 0.001187\n",
      "Epoch: 55/100... Step: 55970... Loss: 0.000169... Val Loss: 0.001389\n",
      "Epoch: 55/100... Step: 55975... Loss: 0.000180... Val Loss: 0.001260\n",
      "Epoch: 55/100... Step: 55980... Loss: 0.000223... Val Loss: 0.001328\n",
      "Epoch: 55/100... Step: 55985... Loss: 0.000144... Val Loss: 0.001307\n",
      "Epoch: 55/100... Step: 55990... Loss: 0.000204... Val Loss: 0.001125\n",
      "Epoch: 55/100... Step: 55995... Loss: 0.000348... Val Loss: 0.001022\n",
      "Epoch: 55/100... Step: 56000... Loss: 0.000191... Val Loss: 0.000926\n",
      "Epoch: 55/100... Step: 56005... Loss: 0.000201... Val Loss: 0.000875\n",
      "Epoch: 55/100... Step: 56010... Loss: 0.000197... Val Loss: 0.001097\n",
      "Epoch: 55/100... Step: 56015... Loss: 0.000242... Val Loss: 0.000831\n",
      "Epoch: 55/100... Step: 56020... Loss: 0.000105... Val Loss: 0.000723\n",
      "Epoch: 55/100... Step: 56025... Loss: 0.000246... Val Loss: 0.000643\n",
      "Epoch: 55/100... Step: 56030... Loss: 0.000270... Val Loss: 0.000637\n",
      "Epoch: 55/100... Step: 56035... Loss: 0.000286... Val Loss: 0.000702\n",
      "Epoch: 55/100... Step: 56040... Loss: 0.000203... Val Loss: 0.000709\n",
      "Epoch: 55/100... Step: 56045... Loss: 0.000111... Val Loss: 0.000492\n",
      "Epoch: 55/100... Step: 56050... Loss: 0.000166... Val Loss: 0.000599\n",
      "Epoch: 55/100... Step: 56055... Loss: 0.000194... Val Loss: 0.000431\n",
      "Epoch: 55/100... Step: 56060... Loss: 0.000111... Val Loss: 0.000450\n",
      "Epoch: 55/100... Step: 56065... Loss: 0.000294... Val Loss: 0.000430\n",
      "Epoch: 55/100... Step: 56070... Loss: 0.000122... Val Loss: 0.000678\n",
      "Epoch: 55/100... Step: 56075... Loss: 0.000217... Val Loss: 0.000307\n",
      "Epoch: 55/100... Step: 56080... Loss: 0.000228... Val Loss: 0.000308\n",
      "Epoch: 55/100... Step: 56085... Loss: 0.000145... Val Loss: 0.000811\n",
      "Epoch: 55/100... Step: 56090... Loss: 0.000326... Val Loss: 0.000827\n",
      "Epoch: 55/100... Step: 56095... Loss: 0.000429... Val Loss: 0.000831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55/100... Step: 56100... Loss: 0.000163... Val Loss: 0.001044\n",
      "Epoch: 55/100... Step: 56105... Loss: 0.000120... Val Loss: 0.001040\n",
      "Epoch: 55/100... Step: 56110... Loss: 0.000066... Val Loss: 0.001042\n",
      "Epoch: 55/100... Step: 56115... Loss: 0.000123... Val Loss: 0.000853\n",
      "Epoch: 55/100... Step: 56120... Loss: 0.000334... Val Loss: 0.000823\n",
      "Epoch: 55/100... Step: 56125... Loss: 0.000151... Val Loss: 0.001038\n",
      "Epoch: 55/100... Step: 56130... Loss: 0.000188... Val Loss: 0.001063\n",
      "Epoch: 55/100... Step: 56135... Loss: 0.000277... Val Loss: 0.000442\n",
      "Epoch: 55/100... Step: 56140... Loss: 0.000630... Val Loss: 0.000290\n",
      "Epoch: 55/100... Step: 56145... Loss: 0.000615... Val Loss: 0.001184\n",
      "Epoch: 55/100... Step: 56150... Loss: 0.000412... Val Loss: 0.000768\n",
      "Epoch: 55/100... Step: 56155... Loss: 0.000278... Val Loss: 0.000824\n",
      "Epoch: 55/100... Step: 56160... Loss: 0.000191... Val Loss: 0.001225\n",
      "Epoch: 55/100... Step: 56165... Loss: 0.000158... Val Loss: 0.001253\n",
      "Epoch: 55/100... Step: 56170... Loss: 0.000603... Val Loss: 0.000997\n",
      "Epoch: 55/100... Step: 56175... Loss: 0.000332... Val Loss: 0.001156\n",
      "Epoch: 55/100... Step: 56180... Loss: 0.000320... Val Loss: 0.001344\n",
      "Epoch: 55/100... Step: 56185... Loss: 0.000251... Val Loss: 0.001292\n",
      "Epoch: 55/100... Step: 56190... Loss: 0.000234... Val Loss: 0.001502\n",
      "Epoch: 55/100... Step: 56195... Loss: 0.000154... Val Loss: 0.001344\n",
      "Epoch: 55/100... Step: 56200... Loss: 0.000279... Val Loss: 0.001215\n",
      "Epoch: 55/100... Step: 56205... Loss: 0.000233... Val Loss: 0.001136\n",
      "Epoch: 55/100... Step: 56210... Loss: 0.000237... Val Loss: 0.001421\n",
      "Epoch: 55/100... Step: 56215... Loss: 0.000334... Val Loss: 0.000734\n",
      "Epoch: 55/100... Step: 56220... Loss: 0.000113... Val Loss: 0.001922\n",
      "Epoch: 55/100... Step: 56225... Loss: 0.000638... Val Loss: 0.001452\n",
      "Epoch: 55/100... Step: 56230... Loss: 0.000123... Val Loss: 0.000398\n",
      "Epoch: 55/100... Step: 56235... Loss: 0.000493... Val Loss: 0.000941\n",
      "Epoch: 55/100... Step: 56240... Loss: 0.000298... Val Loss: 0.000299\n",
      "Epoch: 55/100... Step: 56245... Loss: 0.000250... Val Loss: 0.000371\n",
      "Epoch: 55/100... Step: 56250... Loss: 0.000183... Val Loss: 0.000674\n",
      "Epoch: 55/100... Step: 56255... Loss: 0.000426... Val Loss: 0.001105\n",
      "Epoch: 55/100... Step: 56260... Loss: 0.000085... Val Loss: 0.001000\n",
      "Epoch: 55/100... Step: 56265... Loss: 0.000624... Val Loss: 0.000717\n",
      "Epoch: 55/100... Step: 56270... Loss: 0.000487... Val Loss: 0.000934\n",
      "Epoch: 55/100... Step: 56275... Loss: 0.000110... Val Loss: 0.000670\n",
      "Epoch: 55/100... Step: 56280... Loss: 0.000214... Val Loss: 0.000912\n",
      "Epoch: 55/100... Step: 56285... Loss: 0.000161... Val Loss: 0.000862\n",
      "Epoch: 55/100... Step: 56290... Loss: 0.000305... Val Loss: 0.000639\n",
      "Epoch: 55/100... Step: 56295... Loss: 0.000519... Val Loss: 0.000594\n",
      "Epoch: 55/100... Step: 56300... Loss: 0.000239... Val Loss: 0.000604\n",
      "Epoch: 55/100... Step: 56305... Loss: 0.000393... Val Loss: 0.000897\n",
      "Epoch: 55/100... Step: 56310... Loss: 0.000425... Val Loss: 0.000858\n",
      "Epoch: 55/100... Step: 56315... Loss: 0.000089... Val Loss: 0.001400\n",
      "Epoch: 55/100... Step: 56320... Loss: 0.000244... Val Loss: 0.000940\n",
      "Epoch: 55/100... Step: 56325... Loss: 0.000377... Val Loss: 0.000953\n",
      "Epoch: 55/100... Step: 56330... Loss: 0.000476... Val Loss: 0.001038\n",
      "Epoch: 55/100... Step: 56335... Loss: 0.000254... Val Loss: 0.000937\n",
      "Epoch: 55/100... Step: 56340... Loss: 0.000394... Val Loss: 0.001139\n",
      "Epoch: 55/100... Step: 56345... Loss: 0.000417... Val Loss: 0.000511\n",
      "Epoch: 55/100... Step: 56350... Loss: 0.000256... Val Loss: 0.000963\n",
      "Epoch: 55/100... Step: 56355... Loss: 0.000344... Val Loss: 0.000843\n",
      "Epoch: 55/100... Step: 56360... Loss: 0.000056... Val Loss: 0.001092\n",
      "Epoch: 55/100... Step: 56365... Loss: 0.000100... Val Loss: 0.000662\n",
      "Epoch: 55/100... Step: 56370... Loss: 0.000164... Val Loss: 0.000476\n",
      "Epoch: 55/100... Step: 56375... Loss: 0.000210... Val Loss: 0.000669\n",
      "Epoch: 55/100... Step: 56380... Loss: 0.000154... Val Loss: 0.000725\n",
      "Epoch: 55/100... Step: 56385... Loss: 0.000127... Val Loss: 0.000856\n",
      "Epoch: 55/100... Step: 56390... Loss: 0.000081... Val Loss: 0.000866\n",
      "Epoch: 55/100... Step: 56395... Loss: 0.000179... Val Loss: 0.000482\n",
      "Epoch: 55/100... Step: 56400... Loss: 0.000236... Val Loss: 0.000348\n",
      "Epoch: 55/100... Step: 56405... Loss: 0.000223... Val Loss: 0.000590\n",
      "Epoch: 55/100... Step: 56410... Loss: 0.000143... Val Loss: 0.000294\n",
      "Epoch: 55/100... Step: 56415... Loss: 0.000343... Val Loss: 0.000350\n",
      "Epoch: 55/100... Step: 56420... Loss: 0.000153... Val Loss: 0.000393\n",
      "Epoch: 55/100... Step: 56425... Loss: 0.000525... Val Loss: 0.000262\n",
      "Epoch: 55/100... Step: 56430... Loss: 0.000430... Val Loss: 0.000445\n",
      "Epoch: 55/100... Step: 56435... Loss: 0.000282... Val Loss: 0.000836\n",
      "Epoch: 55/100... Step: 56440... Loss: 0.000264... Val Loss: 0.000415\n",
      "Epoch: 55/100... Step: 56445... Loss: 0.000073... Val Loss: 0.000733\n",
      "Epoch: 55/100... Step: 56450... Loss: 0.000240... Val Loss: 0.000537\n",
      "Epoch: 55/100... Step: 56455... Loss: 0.000143... Val Loss: 0.000615\n",
      "Epoch: 55/100... Step: 56460... Loss: 0.000503... Val Loss: 0.000571\n",
      "Epoch: 55/100... Step: 56465... Loss: 0.000225... Val Loss: 0.000281\n",
      "Epoch: 55/100... Step: 56470... Loss: 0.000249... Val Loss: 0.000275\n",
      "Epoch: 55/100... Step: 56475... Loss: 0.000532... Val Loss: 0.000759\n",
      "Epoch: 55/100... Step: 56480... Loss: 0.000736... Val Loss: 0.001396\n",
      "Epoch: 55/100... Step: 56485... Loss: 0.000385... Val Loss: 0.000444\n",
      "Epoch: 55/100... Step: 56490... Loss: 0.000488... Val Loss: 0.000275\n",
      "Epoch: 55/100... Step: 56495... Loss: 0.000281... Val Loss: 0.000396\n",
      "Epoch: 55/100... Step: 56500... Loss: 0.000289... Val Loss: 0.000651\n",
      "Epoch: 55/100... Step: 56505... Loss: 0.000238... Val Loss: 0.000649\n",
      "Epoch: 55/100... Step: 56510... Loss: 0.000264... Val Loss: 0.000683\n",
      "Epoch: 55/100... Step: 56515... Loss: 0.000441... Val Loss: 0.000463\n",
      "Epoch: 55/100... Step: 56520... Loss: 0.000275... Val Loss: 0.000459\n",
      "Epoch: 55/100... Step: 56525... Loss: 0.000192... Val Loss: 0.000345\n",
      "Epoch: 55/100... Step: 56530... Loss: 0.000829... Val Loss: 0.000929\n",
      "Epoch: 55/100... Step: 56535... Loss: 0.000359... Val Loss: 0.000829\n",
      "Epoch: 55/100... Step: 56540... Loss: 0.000369... Val Loss: 0.000966\n",
      "Epoch: 55/100... Step: 56545... Loss: 0.000372... Val Loss: 0.000938\n",
      "Epoch: 55/100... Step: 56550... Loss: 0.000346... Val Loss: 0.000820\n",
      "Epoch: 55/100... Step: 56555... Loss: 0.000344... Val Loss: 0.001079\n",
      "Epoch: 55/100... Step: 56560... Loss: 0.000649... Val Loss: 0.000894\n",
      "Epoch: 55/100... Step: 56565... Loss: 0.000290... Val Loss: 0.000600\n",
      "Epoch: 55/100... Step: 56570... Loss: 0.000119... Val Loss: 0.000781\n",
      "Epoch: 55/100... Step: 56575... Loss: 0.000247... Val Loss: 0.000622\n",
      "Epoch: 55/100... Step: 56580... Loss: 0.000206... Val Loss: 0.000427\n",
      "Epoch: 55/100... Step: 56585... Loss: 0.000280... Val Loss: 0.000538\n",
      "Epoch: 55/100... Step: 56590... Loss: 0.000204... Val Loss: 0.000943\n",
      "Epoch: 55/100... Step: 56595... Loss: 0.000357... Val Loss: 0.000782\n",
      "Epoch: 55/100... Step: 56600... Loss: 0.000127... Val Loss: 0.000794\n",
      "Epoch: 55/100... Step: 56605... Loss: 0.000117... Val Loss: 0.000641\n",
      "Epoch: 55/100... Step: 56610... Loss: 0.000353... Val Loss: 0.000864\n",
      "Epoch: 55/100... Step: 56615... Loss: 0.000508... Val Loss: 0.000662\n",
      "Epoch: 55/100... Step: 56620... Loss: 0.000300... Val Loss: 0.000849\n",
      "Epoch: 55/100... Step: 56625... Loss: 0.000121... Val Loss: 0.000664\n",
      "Epoch: 55/100... Step: 56630... Loss: 0.000096... Val Loss: 0.000547\n",
      "Epoch: 55/100... Step: 56635... Loss: 0.000184... Val Loss: 0.000649\n",
      "Epoch: 55/100... Step: 56640... Loss: 0.000381... Val Loss: 0.000514\n",
      "Epoch: 55/100... Step: 56645... Loss: 0.000292... Val Loss: 0.000611\n",
      "Epoch: 55/100... Step: 56650... Loss: 0.000341... Val Loss: 0.000514\n",
      "Epoch: 55/100... Step: 56655... Loss: 0.000269... Val Loss: 0.000716\n",
      "Epoch: 55/100... Step: 56660... Loss: 0.000304... Val Loss: 0.001048\n",
      "Epoch: 55/100... Step: 56665... Loss: 0.000353... Val Loss: 0.000733\n",
      "Epoch: 55/100... Step: 56670... Loss: 0.000374... Val Loss: 0.001792\n",
      "Epoch: 55/100... Step: 56675... Loss: 0.000347... Val Loss: 0.000826\n",
      "Epoch: 55/100... Step: 56680... Loss: 0.000357... Val Loss: 0.001161\n",
      "Epoch: 55/100... Step: 56685... Loss: 0.000350... Val Loss: 0.001296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55/100... Step: 56690... Loss: 0.000117... Val Loss: 0.000894\n",
      "Epoch: 55/100... Step: 56695... Loss: 0.000197... Val Loss: 0.000973\n",
      "Epoch: 55/100... Step: 56700... Loss: 0.000311... Val Loss: 0.000888\n",
      "Epoch: 55/100... Step: 56705... Loss: 0.000256... Val Loss: 0.000984\n",
      "Epoch: 55/100... Step: 56710... Loss: 0.000184... Val Loss: 0.001069\n",
      "Epoch: 55/100... Step: 56715... Loss: 0.000240... Val Loss: 0.001177\n",
      "Epoch: 55/100... Step: 56720... Loss: 0.000160... Val Loss: 0.000880\n",
      "Epoch: 55/100... Step: 56725... Loss: 0.000090... Val Loss: 0.000729\n",
      "Epoch: 55/100... Step: 56730... Loss: 0.000363... Val Loss: 0.001024\n",
      "Epoch: 55/100... Step: 56735... Loss: 0.000605... Val Loss: 0.000591\n",
      "Epoch: 55/100... Step: 56740... Loss: 0.000237... Val Loss: 0.000917\n",
      "Epoch: 55/100... Step: 56745... Loss: 0.000349... Val Loss: 0.001072\n",
      "Epoch: 55/100... Step: 56750... Loss: 0.000458... Val Loss: 0.001178\n",
      "Epoch: 55/100... Step: 56755... Loss: 0.000357... Val Loss: 0.000892\n",
      "Epoch: 55/100... Step: 56760... Loss: 0.000224... Val Loss: 0.000960\n",
      "Epoch: 56/100... Step: 56765... Loss: 0.000266... Val Loss: 0.001303\n",
      "Epoch: 56/100... Step: 56770... Loss: 0.000153... Val Loss: 0.001487\n",
      "Epoch: 56/100... Step: 56775... Loss: 0.000387... Val Loss: 0.001700\n",
      "Epoch: 56/100... Step: 56780... Loss: 0.000341... Val Loss: 0.001907\n",
      "Epoch: 56/100... Step: 56785... Loss: 0.000350... Val Loss: 0.001642\n",
      "Epoch: 56/100... Step: 56790... Loss: 0.000268... Val Loss: 0.001922\n",
      "Epoch: 56/100... Step: 56795... Loss: 0.000573... Val Loss: 0.001844\n",
      "Epoch: 56/100... Step: 56800... Loss: 0.000326... Val Loss: 0.001986\n",
      "Epoch: 56/100... Step: 56805... Loss: 0.000290... Val Loss: 0.001991\n",
      "Epoch: 56/100... Step: 56810... Loss: 0.000360... Val Loss: 0.001703\n",
      "Epoch: 56/100... Step: 56815... Loss: 0.000370... Val Loss: 0.001579\n",
      "Epoch: 56/100... Step: 56820... Loss: 0.000375... Val Loss: 0.001640\n",
      "Epoch: 56/100... Step: 56825... Loss: 0.000496... Val Loss: 0.001703\n",
      "Epoch: 56/100... Step: 56830... Loss: 0.000419... Val Loss: 0.001244\n",
      "Epoch: 56/100... Step: 56835... Loss: 0.000401... Val Loss: 0.001142\n",
      "Epoch: 56/100... Step: 56840... Loss: 0.000167... Val Loss: 0.001390\n",
      "Epoch: 56/100... Step: 56845... Loss: 0.000289... Val Loss: 0.001644\n",
      "Epoch: 56/100... Step: 56850... Loss: 0.000258... Val Loss: 0.001468\n",
      "Epoch: 56/100... Step: 56855... Loss: 0.000249... Val Loss: 0.001079\n",
      "Epoch: 56/100... Step: 56860... Loss: 0.000151... Val Loss: 0.000997\n",
      "Epoch: 56/100... Step: 56865... Loss: 0.000112... Val Loss: 0.001215\n",
      "Epoch: 56/100... Step: 56870... Loss: 0.000089... Val Loss: 0.001091\n",
      "Epoch: 56/100... Step: 56875... Loss: 0.000162... Val Loss: 0.001111\n",
      "Epoch: 56/100... Step: 56880... Loss: 0.000193... Val Loss: 0.001037\n",
      "Epoch: 56/100... Step: 56885... Loss: 0.000434... Val Loss: 0.001112\n",
      "Epoch: 56/100... Step: 56890... Loss: 0.000094... Val Loss: 0.001222\n",
      "Epoch: 56/100... Step: 56895... Loss: 0.000295... Val Loss: 0.001444\n",
      "Epoch: 56/100... Step: 56900... Loss: 0.000307... Val Loss: 0.001273\n",
      "Epoch: 56/100... Step: 56905... Loss: 0.000140... Val Loss: 0.001591\n",
      "Epoch: 56/100... Step: 56910... Loss: 0.000229... Val Loss: 0.001920\n",
      "Epoch: 56/100... Step: 56915... Loss: 0.000250... Val Loss: 0.001638\n",
      "Epoch: 56/100... Step: 56920... Loss: 0.000088... Val Loss: 0.001422\n",
      "Epoch: 56/100... Step: 56925... Loss: 0.000101... Val Loss: 0.001353\n",
      "Epoch: 56/100... Step: 56930... Loss: 0.000288... Val Loss: 0.001387\n",
      "Epoch: 56/100... Step: 56935... Loss: 0.000351... Val Loss: 0.001182\n",
      "Epoch: 56/100... Step: 56940... Loss: 0.000133... Val Loss: 0.001032\n",
      "Epoch: 56/100... Step: 56945... Loss: 0.000407... Val Loss: 0.001099\n",
      "Epoch: 56/100... Step: 56950... Loss: 0.000155... Val Loss: 0.001090\n",
      "Epoch: 56/100... Step: 56955... Loss: 0.000254... Val Loss: 0.001012\n",
      "Epoch: 56/100... Step: 56960... Loss: 0.000230... Val Loss: 0.001111\n",
      "Epoch: 56/100... Step: 56965... Loss: 0.000188... Val Loss: 0.001284\n",
      "Epoch: 56/100... Step: 56970... Loss: 0.000202... Val Loss: 0.001222\n",
      "Epoch: 56/100... Step: 56975... Loss: 0.000047... Val Loss: 0.001209\n",
      "Epoch: 56/100... Step: 56980... Loss: 0.000221... Val Loss: 0.001240\n",
      "Epoch: 56/100... Step: 56985... Loss: 0.000190... Val Loss: 0.001028\n",
      "Epoch: 56/100... Step: 56990... Loss: 0.000200... Val Loss: 0.001008\n",
      "Epoch: 56/100... Step: 56995... Loss: 0.000252... Val Loss: 0.001051\n",
      "Epoch: 56/100... Step: 57000... Loss: 0.000095... Val Loss: 0.001261\n",
      "Epoch: 56/100... Step: 57005... Loss: 0.000130... Val Loss: 0.001111\n",
      "Epoch: 56/100... Step: 57010... Loss: 0.000157... Val Loss: 0.001107\n",
      "Epoch: 56/100... Step: 57015... Loss: 0.000144... Val Loss: 0.001222\n",
      "Epoch: 56/100... Step: 57020... Loss: 0.000090... Val Loss: 0.001037\n",
      "Epoch: 56/100... Step: 57025... Loss: 0.000097... Val Loss: 0.000762\n",
      "Epoch: 56/100... Step: 57030... Loss: 0.000058... Val Loss: 0.000786\n",
      "Epoch: 56/100... Step: 57035... Loss: 0.000244... Val Loss: 0.000708\n",
      "Epoch: 56/100... Step: 57040... Loss: 0.000226... Val Loss: 0.000845\n",
      "Epoch: 56/100... Step: 57045... Loss: 0.000107... Val Loss: 0.000968\n",
      "Epoch: 56/100... Step: 57050... Loss: 0.000203... Val Loss: 0.000736\n",
      "Epoch: 56/100... Step: 57055... Loss: 0.000117... Val Loss: 0.000567\n",
      "Epoch: 56/100... Step: 57060... Loss: 0.000046... Val Loss: 0.000701\n",
      "Epoch: 56/100... Step: 57065... Loss: 0.000128... Val Loss: 0.000852\n",
      "Epoch: 56/100... Step: 57070... Loss: 0.000182... Val Loss: 0.000694\n",
      "Epoch: 56/100... Step: 57075... Loss: 0.000217... Val Loss: 0.000582\n",
      "Epoch: 56/100... Step: 57080... Loss: 0.000206... Val Loss: 0.000677\n",
      "Epoch: 56/100... Step: 57085... Loss: 0.000307... Val Loss: 0.000470\n",
      "Epoch: 56/100... Step: 57090... Loss: 0.000263... Val Loss: 0.000453\n",
      "Epoch: 56/100... Step: 57095... Loss: 0.000166... Val Loss: 0.000568\n",
      "Epoch: 56/100... Step: 57100... Loss: 0.000156... Val Loss: 0.000494\n",
      "Epoch: 56/100... Step: 57105... Loss: 0.000188... Val Loss: 0.000381\n",
      "Epoch: 56/100... Step: 57110... Loss: 0.000264... Val Loss: 0.000270\n",
      "Epoch: 56/100... Step: 57115... Loss: 0.000211... Val Loss: 0.000625\n",
      "Epoch: 56/100... Step: 57120... Loss: 0.000205... Val Loss: 0.000587\n",
      "Epoch: 56/100... Step: 57125... Loss: 0.000124... Val Loss: 0.000913\n",
      "Epoch: 56/100... Step: 57130... Loss: 0.000193... Val Loss: 0.000812\n",
      "Epoch: 56/100... Step: 57135... Loss: 0.000222... Val Loss: 0.001122\n",
      "Epoch: 56/100... Step: 57140... Loss: 0.000143... Val Loss: 0.001044\n",
      "Epoch: 56/100... Step: 57145... Loss: 0.000091... Val Loss: 0.000922\n",
      "Epoch: 56/100... Step: 57150... Loss: 0.000222... Val Loss: 0.000863\n",
      "Epoch: 56/100... Step: 57155... Loss: 0.000184... Val Loss: 0.000996\n",
      "Epoch: 56/100... Step: 57160... Loss: 0.000242... Val Loss: 0.001256\n",
      "Epoch: 56/100... Step: 57165... Loss: 0.000334... Val Loss: 0.000953\n",
      "Epoch: 56/100... Step: 57170... Loss: 0.000308... Val Loss: 0.000390\n",
      "Epoch: 56/100... Step: 57175... Loss: 0.000840... Val Loss: 0.000419\n",
      "Epoch: 56/100... Step: 57180... Loss: 0.000393... Val Loss: 0.000783\n",
      "Epoch: 56/100... Step: 57185... Loss: 0.000531... Val Loss: 0.001458\n",
      "Epoch: 56/100... Step: 57190... Loss: 0.000287... Val Loss: 0.001338\n",
      "Epoch: 56/100... Step: 57195... Loss: 0.000318... Val Loss: 0.001117\n",
      "Epoch: 56/100... Step: 57200... Loss: 0.000220... Val Loss: 0.001287\n",
      "Epoch: 56/100... Step: 57205... Loss: 0.000524... Val Loss: 0.001542\n",
      "Epoch: 56/100... Step: 57210... Loss: 0.000272... Val Loss: 0.001570\n",
      "Epoch: 56/100... Step: 57215... Loss: 0.000197... Val Loss: 0.001411\n",
      "Epoch: 56/100... Step: 57220... Loss: 0.000216... Val Loss: 0.001558\n",
      "Epoch: 56/100... Step: 57225... Loss: 0.000142... Val Loss: 0.001397\n",
      "Epoch: 56/100... Step: 57230... Loss: 0.000286... Val Loss: 0.001094\n",
      "Epoch: 56/100... Step: 57235... Loss: 0.000226... Val Loss: 0.000857\n",
      "Epoch: 56/100... Step: 57240... Loss: 0.000256... Val Loss: 0.001121\n",
      "Epoch: 56/100... Step: 57245... Loss: 0.000290... Val Loss: 0.001044\n",
      "Epoch: 56/100... Step: 57250... Loss: 0.000244... Val Loss: 0.000856\n",
      "Epoch: 56/100... Step: 57255... Loss: 0.000117... Val Loss: 0.001098\n",
      "Epoch: 56/100... Step: 57260... Loss: 0.000377... Val Loss: 0.000530\n",
      "Epoch: 56/100... Step: 57265... Loss: 0.000276... Val Loss: 0.000457\n",
      "Epoch: 56/100... Step: 57270... Loss: 0.000420... Val Loss: 0.000538\n",
      "Epoch: 56/100... Step: 57275... Loss: 0.000145... Val Loss: 0.000517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56/100... Step: 57280... Loss: 0.000122... Val Loss: 0.000613\n",
      "Epoch: 56/100... Step: 57285... Loss: 0.000209... Val Loss: 0.000693\n",
      "Epoch: 56/100... Step: 57290... Loss: 0.000229... Val Loss: 0.000599\n",
      "Epoch: 56/100... Step: 57295... Loss: 0.000227... Val Loss: 0.000496\n",
      "Epoch: 56/100... Step: 57300... Loss: 0.000175... Val Loss: 0.000606\n",
      "Epoch: 56/100... Step: 57305... Loss: 0.000221... Val Loss: 0.000872\n",
      "Epoch: 56/100... Step: 57310... Loss: 0.000249... Val Loss: 0.001079\n",
      "Epoch: 56/100... Step: 57315... Loss: 0.000128... Val Loss: 0.000977\n",
      "Epoch: 56/100... Step: 57320... Loss: 0.000156... Val Loss: 0.000850\n",
      "Epoch: 56/100... Step: 57325... Loss: 0.000219... Val Loss: 0.000945\n",
      "Epoch: 56/100... Step: 57330... Loss: 0.000477... Val Loss: 0.001155\n",
      "Epoch: 56/100... Step: 57335... Loss: 0.000161... Val Loss: 0.001310\n",
      "Epoch: 56/100... Step: 57340... Loss: 0.000457... Val Loss: 0.000736\n",
      "Epoch: 56/100... Step: 57345... Loss: 0.000369... Val Loss: 0.001013\n",
      "Epoch: 56/100... Step: 57350... Loss: 0.000121... Val Loss: 0.000947\n",
      "Epoch: 56/100... Step: 57355... Loss: 0.000292... Val Loss: 0.000983\n",
      "Epoch: 56/100... Step: 57360... Loss: 0.000244... Val Loss: 0.001068\n",
      "Epoch: 56/100... Step: 57365... Loss: 0.000293... Val Loss: 0.000984\n",
      "Epoch: 56/100... Step: 57370... Loss: 0.000122... Val Loss: 0.000884\n",
      "Epoch: 56/100... Step: 57375... Loss: 0.000129... Val Loss: 0.000783\n",
      "Epoch: 56/100... Step: 57380... Loss: 0.000136... Val Loss: 0.000803\n",
      "Epoch: 56/100... Step: 57385... Loss: 0.000132... Val Loss: 0.000841\n",
      "Epoch: 56/100... Step: 57390... Loss: 0.000137... Val Loss: 0.000769\n",
      "Epoch: 56/100... Step: 57395... Loss: 0.000209... Val Loss: 0.001239\n",
      "Epoch: 56/100... Step: 57400... Loss: 0.000122... Val Loss: 0.000790\n",
      "Epoch: 56/100... Step: 57405... Loss: 0.000205... Val Loss: 0.000578\n",
      "Epoch: 56/100... Step: 57410... Loss: 0.000162... Val Loss: 0.000672\n",
      "Epoch: 56/100... Step: 57415... Loss: 0.000093... Val Loss: 0.000865\n",
      "Epoch: 56/100... Step: 57420... Loss: 0.000097... Val Loss: 0.000823\n",
      "Epoch: 56/100... Step: 57425... Loss: 0.000113... Val Loss: 0.000544\n",
      "Epoch: 56/100... Step: 57430... Loss: 0.000148... Val Loss: 0.000499\n",
      "Epoch: 56/100... Step: 57435... Loss: 0.000074... Val Loss: 0.000400\n",
      "Epoch: 56/100... Step: 57440... Loss: 0.000127... Val Loss: 0.000277\n",
      "Epoch: 56/100... Step: 57445... Loss: 0.000141... Val Loss: 0.000265\n",
      "Epoch: 56/100... Step: 57450... Loss: 0.000070... Val Loss: 0.000420\n",
      "Epoch: 56/100... Step: 57455... Loss: 0.000122... Val Loss: 0.000298\n",
      "Epoch: 56/100... Step: 57460... Loss: 0.000097... Val Loss: 0.000291\n",
      "Epoch: 56/100... Step: 57465... Loss: 0.000107... Val Loss: 0.000293\n",
      "Epoch: 56/100... Step: 57470... Loss: 0.000164... Val Loss: 0.000356\n",
      "Epoch: 56/100... Step: 57475... Loss: 0.000098... Val Loss: 0.000369\n",
      "Epoch: 56/100... Step: 57480... Loss: 0.000104... Val Loss: 0.000501\n",
      "Epoch: 56/100... Step: 57485... Loss: 0.000068... Val Loss: 0.000362\n",
      "Epoch: 56/100... Step: 57490... Loss: 0.000067... Val Loss: 0.000349\n",
      "Epoch: 56/100... Step: 57495... Loss: 0.000105... Val Loss: 0.000402\n",
      "Epoch: 56/100... Step: 57500... Loss: 0.000232... Val Loss: 0.000339\n",
      "Epoch: 56/100... Step: 57505... Loss: 0.000192... Val Loss: 0.000409\n",
      "Epoch: 56/100... Step: 57510... Loss: 0.000145... Val Loss: 0.000336\n",
      "Epoch: 56/100... Step: 57515... Loss: 0.000189... Val Loss: 0.000604\n",
      "Epoch: 56/100... Step: 57520... Loss: 0.000117... Val Loss: 0.000304\n",
      "Epoch: 56/100... Step: 57525... Loss: 0.000173... Val Loss: 0.000336\n",
      "Epoch: 56/100... Step: 57530... Loss: 0.000097... Val Loss: 0.000460\n",
      "Epoch: 56/100... Step: 57535... Loss: 0.000122... Val Loss: 0.000317\n",
      "Epoch: 56/100... Step: 57540... Loss: 0.000274... Val Loss: 0.000587\n",
      "Epoch: 56/100... Step: 57545... Loss: 0.000199... Val Loss: 0.000626\n",
      "Epoch: 56/100... Step: 57550... Loss: 0.000169... Val Loss: 0.000526\n",
      "Epoch: 56/100... Step: 57555... Loss: 0.000188... Val Loss: 0.000380\n",
      "Epoch: 56/100... Step: 57560... Loss: 0.000130... Val Loss: 0.000349\n",
      "Epoch: 56/100... Step: 57565... Loss: 0.000203... Val Loss: 0.000707\n",
      "Epoch: 56/100... Step: 57570... Loss: 0.000084... Val Loss: 0.000741\n",
      "Epoch: 56/100... Step: 57575... Loss: 0.000150... Val Loss: 0.000648\n",
      "Epoch: 56/100... Step: 57580... Loss: 0.000126... Val Loss: 0.000667\n",
      "Epoch: 56/100... Step: 57585... Loss: 0.000167... Val Loss: 0.000554\n",
      "Epoch: 56/100... Step: 57590... Loss: 0.000206... Val Loss: 0.000597\n",
      "Epoch: 56/100... Step: 57595... Loss: 0.000146... Val Loss: 0.000568\n",
      "Epoch: 56/100... Step: 57600... Loss: 0.000270... Val Loss: 0.000624\n",
      "Epoch: 56/100... Step: 57605... Loss: 0.000203... Val Loss: 0.000584\n",
      "Epoch: 56/100... Step: 57610... Loss: 0.000121... Val Loss: 0.000643\n",
      "Epoch: 56/100... Step: 57615... Loss: 0.000181... Val Loss: 0.000828\n",
      "Epoch: 56/100... Step: 57620... Loss: 0.000272... Val Loss: 0.000772\n",
      "Epoch: 56/100... Step: 57625... Loss: 0.000135... Val Loss: 0.001025\n",
      "Epoch: 56/100... Step: 57630... Loss: 0.000101... Val Loss: 0.000726\n",
      "Epoch: 56/100... Step: 57635... Loss: 0.000313... Val Loss: 0.000676\n",
      "Epoch: 56/100... Step: 57640... Loss: 0.000269... Val Loss: 0.000811\n",
      "Epoch: 56/100... Step: 57645... Loss: 0.000267... Val Loss: 0.000799\n",
      "Epoch: 56/100... Step: 57650... Loss: 0.000266... Val Loss: 0.000571\n",
      "Epoch: 56/100... Step: 57655... Loss: 0.000150... Val Loss: 0.000698\n",
      "Epoch: 56/100... Step: 57660... Loss: 0.000134... Val Loss: 0.000725\n",
      "Epoch: 56/100... Step: 57665... Loss: 0.000250... Val Loss: 0.000685\n",
      "Epoch: 56/100... Step: 57670... Loss: 0.000236... Val Loss: 0.000496\n",
      "Epoch: 56/100... Step: 57675... Loss: 0.000229... Val Loss: 0.000533\n",
      "Epoch: 56/100... Step: 57680... Loss: 0.000343... Val Loss: 0.000742\n",
      "Epoch: 56/100... Step: 57685... Loss: 0.000125... Val Loss: 0.000702\n",
      "Epoch: 56/100... Step: 57690... Loss: 0.000314... Val Loss: 0.000852\n",
      "Epoch: 56/100... Step: 57695... Loss: 0.000366... Val Loss: 0.000970\n",
      "Epoch: 56/100... Step: 57700... Loss: 0.000680... Val Loss: 0.000780\n",
      "Epoch: 56/100... Step: 57705... Loss: 0.000353... Val Loss: 0.001326\n",
      "Epoch: 56/100... Step: 57710... Loss: 0.000309... Val Loss: 0.000669\n",
      "Epoch: 56/100... Step: 57715... Loss: 0.000345... Val Loss: 0.000971\n",
      "Epoch: 56/100... Step: 57720... Loss: 0.000204... Val Loss: 0.001304\n",
      "Epoch: 56/100... Step: 57725... Loss: 0.000124... Val Loss: 0.000855\n",
      "Epoch: 56/100... Step: 57730... Loss: 0.000221... Val Loss: 0.001186\n",
      "Epoch: 56/100... Step: 57735... Loss: 0.000177... Val Loss: 0.000949\n",
      "Epoch: 56/100... Step: 57740... Loss: 0.000079... Val Loss: 0.000776\n",
      "Epoch: 56/100... Step: 57745... Loss: 0.000230... Val Loss: 0.001014\n",
      "Epoch: 56/100... Step: 57750... Loss: 0.000087... Val Loss: 0.001178\n",
      "Epoch: 56/100... Step: 57755... Loss: 0.000263... Val Loss: 0.001286\n",
      "Epoch: 56/100... Step: 57760... Loss: 0.000381... Val Loss: 0.000875\n",
      "Epoch: 56/100... Step: 57765... Loss: 0.000233... Val Loss: 0.000886\n",
      "Epoch: 56/100... Step: 57770... Loss: 0.000119... Val Loss: 0.000982\n",
      "Epoch: 56/100... Step: 57775... Loss: 0.000093... Val Loss: 0.001184\n",
      "Epoch: 56/100... Step: 57780... Loss: 0.000206... Val Loss: 0.001213\n",
      "Epoch: 56/100... Step: 57785... Loss: 0.000115... Val Loss: 0.001191\n",
      "Epoch: 56/100... Step: 57790... Loss: 0.000244... Val Loss: 0.000911\n",
      "Epoch: 57/100... Step: 57795... Loss: 0.000413... Val Loss: 0.001522\n",
      "Epoch: 57/100... Step: 57800... Loss: 0.000174... Val Loss: 0.001694\n",
      "Epoch: 57/100... Step: 57805... Loss: 0.000255... Val Loss: 0.001609\n",
      "Epoch: 57/100... Step: 57810... Loss: 0.000198... Val Loss: 0.001880\n",
      "Epoch: 57/100... Step: 57815... Loss: 0.000163... Val Loss: 0.001625\n",
      "Epoch: 57/100... Step: 57820... Loss: 0.000123... Val Loss: 0.001853\n",
      "Epoch: 57/100... Step: 57825... Loss: 0.000154... Val Loss: 0.001951\n",
      "Epoch: 57/100... Step: 57830... Loss: 0.000198... Val Loss: 0.001768\n",
      "Epoch: 57/100... Step: 57835... Loss: 0.000209... Val Loss: 0.001939\n",
      "Epoch: 57/100... Step: 57840... Loss: 0.000206... Val Loss: 0.001994\n",
      "Epoch: 57/100... Step: 57845... Loss: 0.000176... Val Loss: 0.001777\n",
      "Epoch: 57/100... Step: 57850... Loss: 0.000090... Val Loss: 0.001629\n",
      "Epoch: 57/100... Step: 57855... Loss: 0.000218... Val Loss: 0.001570\n",
      "Epoch: 57/100... Step: 57860... Loss: 0.000240... Val Loss: 0.001444\n",
      "Epoch: 57/100... Step: 57865... Loss: 0.000200... Val Loss: 0.000957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57/100... Step: 57870... Loss: 0.000267... Val Loss: 0.001145\n",
      "Epoch: 57/100... Step: 57875... Loss: 0.000238... Val Loss: 0.001239\n",
      "Epoch: 57/100... Step: 57880... Loss: 0.000135... Val Loss: 0.001472\n",
      "Epoch: 57/100... Step: 57885... Loss: 0.000229... Val Loss: 0.001453\n",
      "Epoch: 57/100... Step: 57890... Loss: 0.000242... Val Loss: 0.000970\n",
      "Epoch: 57/100... Step: 57895... Loss: 0.000151... Val Loss: 0.000992\n",
      "Epoch: 57/100... Step: 57900... Loss: 0.000138... Val Loss: 0.001024\n",
      "Epoch: 57/100... Step: 57905... Loss: 0.000069... Val Loss: 0.001175\n",
      "Epoch: 57/100... Step: 57910... Loss: 0.000098... Val Loss: 0.001139\n",
      "Epoch: 57/100... Step: 57915... Loss: 0.000115... Val Loss: 0.001130\n",
      "Epoch: 57/100... Step: 57920... Loss: 0.000165... Val Loss: 0.001399\n",
      "Epoch: 57/100... Step: 57925... Loss: 0.000085... Val Loss: 0.001240\n",
      "Epoch: 57/100... Step: 57930... Loss: 0.000116... Val Loss: 0.001219\n",
      "Epoch: 57/100... Step: 57935... Loss: 0.000234... Val Loss: 0.001409\n",
      "Epoch: 57/100... Step: 57940... Loss: 0.000075... Val Loss: 0.001831\n",
      "Epoch: 57/100... Step: 57945... Loss: 0.000354... Val Loss: 0.001664\n",
      "Epoch: 57/100... Step: 57950... Loss: 0.000094... Val Loss: 0.001691\n",
      "Epoch: 57/100... Step: 57955... Loss: 0.000165... Val Loss: 0.001472\n",
      "Epoch: 57/100... Step: 57960... Loss: 0.000233... Val Loss: 0.001497\n",
      "Epoch: 57/100... Step: 57965... Loss: 0.000264... Val Loss: 0.001606\n",
      "Epoch: 57/100... Step: 57970... Loss: 0.000126... Val Loss: 0.001222\n",
      "Epoch: 57/100... Step: 57975... Loss: 0.000352... Val Loss: 0.001201\n",
      "Epoch: 57/100... Step: 57980... Loss: 0.000165... Val Loss: 0.001125\n",
      "Epoch: 57/100... Step: 57985... Loss: 0.000152... Val Loss: 0.001085\n",
      "Epoch: 57/100... Step: 57990... Loss: 0.000123... Val Loss: 0.001092\n",
      "Epoch: 57/100... Step: 57995... Loss: 0.000251... Val Loss: 0.001228\n",
      "Epoch: 57/100... Step: 58000... Loss: 0.000266... Val Loss: 0.001453\n",
      "Epoch: 57/100... Step: 58005... Loss: 0.000242... Val Loss: 0.000983\n",
      "Epoch: 57/100... Step: 58010... Loss: 0.000203... Val Loss: 0.001673\n",
      "Epoch: 57/100... Step: 58015... Loss: 0.000092... Val Loss: 0.001025\n",
      "Epoch: 57/100... Step: 58020... Loss: 0.000277... Val Loss: 0.001035\n",
      "Epoch: 57/100... Step: 58025... Loss: 0.000091... Val Loss: 0.000966\n",
      "Epoch: 57/100... Step: 58030... Loss: 0.000331... Val Loss: 0.001084\n",
      "Epoch: 57/100... Step: 58035... Loss: 0.000298... Val Loss: 0.001115\n",
      "Epoch: 57/100... Step: 58040... Loss: 0.000136... Val Loss: 0.001231\n",
      "Epoch: 57/100... Step: 58045... Loss: 0.000534... Val Loss: 0.001426\n",
      "Epoch: 57/100... Step: 58050... Loss: 0.000232... Val Loss: 0.001069\n",
      "Epoch: 57/100... Step: 58055... Loss: 0.000208... Val Loss: 0.001186\n",
      "Epoch: 57/100... Step: 58060... Loss: 0.000107... Val Loss: 0.001316\n",
      "Epoch: 57/100... Step: 58065... Loss: 0.000837... Val Loss: 0.000641\n",
      "Epoch: 57/100... Step: 58070... Loss: 0.000182... Val Loss: 0.000712\n",
      "Epoch: 57/100... Step: 58075... Loss: 0.000214... Val Loss: 0.000960\n",
      "Epoch: 57/100... Step: 58080... Loss: 0.000234... Val Loss: 0.000786\n",
      "Epoch: 57/100... Step: 58085... Loss: 0.000390... Val Loss: 0.000527\n",
      "Epoch: 57/100... Step: 58090... Loss: 0.000156... Val Loss: 0.000509\n",
      "Epoch: 57/100... Step: 58095... Loss: 0.000156... Val Loss: 0.000669\n",
      "Epoch: 57/100... Step: 58100... Loss: 0.000191... Val Loss: 0.000608\n",
      "Epoch: 57/100... Step: 58105... Loss: 0.000364... Val Loss: 0.000842\n",
      "Epoch: 57/100... Step: 58110... Loss: 0.000224... Val Loss: 0.000946\n",
      "Epoch: 57/100... Step: 58115... Loss: 0.000363... Val Loss: 0.000386\n",
      "Epoch: 57/100... Step: 58120... Loss: 0.000196... Val Loss: 0.000327\n",
      "Epoch: 57/100... Step: 58125... Loss: 0.000265... Val Loss: 0.000395\n",
      "Epoch: 57/100... Step: 58130... Loss: 0.000159... Val Loss: 0.000472\n",
      "Epoch: 57/100... Step: 58135... Loss: 0.000167... Val Loss: 0.000827\n",
      "Epoch: 57/100... Step: 58140... Loss: 0.000123... Val Loss: 0.000345\n",
      "Epoch: 57/100... Step: 58145... Loss: 0.000194... Val Loss: 0.000456\n",
      "Epoch: 57/100... Step: 58150... Loss: 0.000242... Val Loss: 0.000685\n",
      "Epoch: 57/100... Step: 58155... Loss: 0.000449... Val Loss: 0.000753\n",
      "Epoch: 57/100... Step: 58160... Loss: 0.000289... Val Loss: 0.001089\n",
      "Epoch: 57/100... Step: 58165... Loss: 0.000265... Val Loss: 0.000953\n",
      "Epoch: 57/100... Step: 58170... Loss: 0.000201... Val Loss: 0.001010\n",
      "Epoch: 57/100... Step: 58175... Loss: 0.000228... Val Loss: 0.001226\n",
      "Epoch: 57/100... Step: 58180... Loss: 0.000087... Val Loss: 0.000803\n",
      "Epoch: 57/100... Step: 58185... Loss: 0.000310... Val Loss: 0.000858\n",
      "Epoch: 57/100... Step: 58190... Loss: 0.000319... Val Loss: 0.000845\n",
      "Epoch: 57/100... Step: 58195... Loss: 0.000348... Val Loss: 0.000914\n",
      "Epoch: 57/100... Step: 58200... Loss: 0.000406... Val Loss: 0.000533\n",
      "Epoch: 57/100... Step: 58205... Loss: 0.000578... Val Loss: 0.000276\n",
      "Epoch: 57/100... Step: 58210... Loss: 0.000554... Val Loss: 0.001372\n",
      "Epoch: 57/100... Step: 58215... Loss: 0.000303... Val Loss: 0.000865\n",
      "Epoch: 57/100... Step: 58220... Loss: 0.000434... Val Loss: 0.000862\n",
      "Epoch: 57/100... Step: 58225... Loss: 0.000302... Val Loss: 0.001162\n",
      "Epoch: 57/100... Step: 58230... Loss: 0.000323... Val Loss: 0.001119\n",
      "Epoch: 57/100... Step: 58235... Loss: 0.000223... Val Loss: 0.001289\n",
      "Epoch: 57/100... Step: 58240... Loss: 0.000169... Val Loss: 0.001180\n",
      "Epoch: 57/100... Step: 58245... Loss: 0.000215... Val Loss: 0.001441\n",
      "Epoch: 57/100... Step: 58250... Loss: 0.000462... Val Loss: 0.001506\n",
      "Epoch: 57/100... Step: 58255... Loss: 0.000469... Val Loss: 0.001555\n",
      "Epoch: 57/100... Step: 58260... Loss: 0.000150... Val Loss: 0.001371\n",
      "Epoch: 57/100... Step: 58265... Loss: 0.000092... Val Loss: 0.001100\n",
      "Epoch: 57/100... Step: 58270... Loss: 0.000161... Val Loss: 0.001269\n",
      "Epoch: 57/100... Step: 58275... Loss: 0.000429... Val Loss: 0.001109\n",
      "Epoch: 57/100... Step: 58280... Loss: 0.000711... Val Loss: 0.001290\n",
      "Epoch: 57/100... Step: 58285... Loss: 0.000333... Val Loss: 0.000941\n",
      "Epoch: 57/100... Step: 58290... Loss: 0.000596... Val Loss: 0.000815\n",
      "Epoch: 57/100... Step: 58295... Loss: 0.000374... Val Loss: 0.001275\n",
      "Epoch: 57/100... Step: 58300... Loss: 0.000485... Val Loss: 0.000279\n",
      "Epoch: 57/100... Step: 58305... Loss: 0.000263... Val Loss: 0.000437\n",
      "Epoch: 57/100... Step: 58310... Loss: 0.000332... Val Loss: 0.000399\n",
      "Epoch: 57/100... Step: 58315... Loss: 0.000419... Val Loss: 0.000577\n",
      "Epoch: 57/100... Step: 58320... Loss: 0.000205... Val Loss: 0.000758\n",
      "Epoch: 57/100... Step: 58325... Loss: 0.000479... Val Loss: 0.000475\n",
      "Epoch: 57/100... Step: 58330... Loss: 0.000492... Val Loss: 0.000421\n",
      "Epoch: 57/100... Step: 58335... Loss: 0.000343... Val Loss: 0.000817\n",
      "Epoch: 57/100... Step: 58340... Loss: 0.000275... Val Loss: 0.000926\n",
      "Epoch: 57/100... Step: 58345... Loss: 0.000150... Val Loss: 0.001084\n",
      "Epoch: 57/100... Step: 58350... Loss: 0.000173... Val Loss: 0.000905\n",
      "Epoch: 57/100... Step: 58355... Loss: 0.000193... Val Loss: 0.000875\n",
      "Epoch: 57/100... Step: 58360... Loss: 0.000236... Val Loss: 0.000810\n",
      "Epoch: 57/100... Step: 58365... Loss: 0.000335... Val Loss: 0.001198\n",
      "Epoch: 57/100... Step: 58370... Loss: 0.000270... Val Loss: 0.000899\n",
      "Epoch: 57/100... Step: 58375... Loss: 0.000209... Val Loss: 0.000781\n",
      "Epoch: 57/100... Step: 58380... Loss: 0.000157... Val Loss: 0.001118\n",
      "Epoch: 57/100... Step: 58385... Loss: 0.000246... Val Loss: 0.000759\n",
      "Epoch: 57/100... Step: 58390... Loss: 0.000148... Val Loss: 0.000795\n",
      "Epoch: 57/100... Step: 58395... Loss: 0.000312... Val Loss: 0.001087\n",
      "Epoch: 57/100... Step: 58400... Loss: 0.000142... Val Loss: 0.001033\n",
      "Epoch: 57/100... Step: 58405... Loss: 0.000150... Val Loss: 0.000947\n",
      "Epoch: 57/100... Step: 58410... Loss: 0.000207... Val Loss: 0.000936\n",
      "Epoch: 57/100... Step: 58415... Loss: 0.000085... Val Loss: 0.000811\n",
      "Epoch: 57/100... Step: 58420... Loss: 0.000150... Val Loss: 0.000980\n",
      "Epoch: 57/100... Step: 58425... Loss: 0.000112... Val Loss: 0.000817\n",
      "Epoch: 57/100... Step: 58430... Loss: 0.000263... Val Loss: 0.000869\n",
      "Epoch: 57/100... Step: 58435... Loss: 0.000108... Val Loss: 0.000762\n",
      "Epoch: 57/100... Step: 58440... Loss: 0.000147... Val Loss: 0.000618\n",
      "Epoch: 57/100... Step: 58445... Loss: 0.000251... Val Loss: 0.001076\n",
      "Epoch: 57/100... Step: 58450... Loss: 0.000211... Val Loss: 0.000906\n",
      "Epoch: 57/100... Step: 58455... Loss: 0.000103... Val Loss: 0.000731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57/100... Step: 58460... Loss: 0.000181... Val Loss: 0.000520\n",
      "Epoch: 57/100... Step: 58465... Loss: 0.000154... Val Loss: 0.000340\n",
      "Epoch: 57/100... Step: 58470... Loss: 0.000259... Val Loss: 0.000441\n",
      "Epoch: 57/100... Step: 58475... Loss: 0.000321... Val Loss: 0.000458\n",
      "Epoch: 57/100... Step: 58480... Loss: 0.000134... Val Loss: 0.000325\n",
      "Epoch: 57/100... Step: 58485... Loss: 0.000205... Val Loss: 0.000295\n",
      "Epoch: 57/100... Step: 58490... Loss: 0.000114... Val Loss: 0.000258\n",
      "Epoch: 57/100... Step: 58495... Loss: 0.000204... Val Loss: 0.000294\n",
      "Epoch: 57/100... Step: 58500... Loss: 0.000637... Val Loss: 0.000321\n",
      "Epoch: 57/100... Step: 58505... Loss: 0.000184... Val Loss: 0.000280\n",
      "Epoch: 57/100... Step: 58510... Loss: 0.000120... Val Loss: 0.000309\n",
      "Epoch: 57/100... Step: 58515... Loss: 0.000099... Val Loss: 0.000446\n",
      "Epoch: 57/100... Step: 58520... Loss: 0.000094... Val Loss: 0.000444\n",
      "Epoch: 57/100... Step: 58525... Loss: 0.000119... Val Loss: 0.000380\n",
      "Epoch: 57/100... Step: 58530... Loss: 0.000197... Val Loss: 0.000624\n",
      "Epoch: 57/100... Step: 58535... Loss: 0.000266... Val Loss: 0.000259\n",
      "Epoch: 57/100... Step: 58540... Loss: 0.000583... Val Loss: 0.001150\n",
      "Epoch: 57/100... Step: 58545... Loss: 0.000297... Val Loss: 0.000467\n",
      "Epoch: 57/100... Step: 58550... Loss: 0.000375... Val Loss: 0.001000\n",
      "Epoch: 57/100... Step: 58555... Loss: 0.000378... Val Loss: 0.000416\n",
      "Epoch: 57/100... Step: 58560... Loss: 0.000369... Val Loss: 0.000555\n",
      "Epoch: 57/100... Step: 58565... Loss: 0.000152... Val Loss: 0.000406\n",
      "Epoch: 57/100... Step: 58570... Loss: 0.000250... Val Loss: 0.000562\n",
      "Epoch: 57/100... Step: 58575... Loss: 0.000490... Val Loss: 0.000787\n",
      "Epoch: 57/100... Step: 58580... Loss: 0.000177... Val Loss: 0.000407\n",
      "Epoch: 57/100... Step: 58585... Loss: 0.000146... Val Loss: 0.000423\n",
      "Epoch: 57/100... Step: 58590... Loss: 0.000354... Val Loss: 0.000345\n",
      "Epoch: 57/100... Step: 58595... Loss: 0.000344... Val Loss: 0.000492\n",
      "Epoch: 57/100... Step: 58600... Loss: 0.000267... Val Loss: 0.000873\n",
      "Epoch: 57/100... Step: 58605... Loss: 0.000155... Val Loss: 0.000526\n",
      "Epoch: 57/100... Step: 58610... Loss: 0.000200... Val Loss: 0.000652\n",
      "Epoch: 57/100... Step: 58615... Loss: 0.000197... Val Loss: 0.000650\n",
      "Epoch: 57/100... Step: 58620... Loss: 0.000180... Val Loss: 0.000597\n",
      "Epoch: 57/100... Step: 58625... Loss: 0.000141... Val Loss: 0.000674\n",
      "Epoch: 57/100... Step: 58630... Loss: 0.000142... Val Loss: 0.000676\n",
      "Epoch: 57/100... Step: 58635... Loss: 0.000148... Val Loss: 0.000567\n",
      "Epoch: 57/100... Step: 58640... Loss: 0.000288... Val Loss: 0.000510\n",
      "Epoch: 57/100... Step: 58645... Loss: 0.000155... Val Loss: 0.000822\n",
      "Epoch: 57/100... Step: 58650... Loss: 0.000108... Val Loss: 0.000733\n",
      "Epoch: 57/100... Step: 58655... Loss: 0.000131... Val Loss: 0.000885\n",
      "Epoch: 57/100... Step: 58660... Loss: 0.000277... Val Loss: 0.001024\n",
      "Epoch: 57/100... Step: 58665... Loss: 0.000171... Val Loss: 0.001015\n",
      "Epoch: 57/100... Step: 58670... Loss: 0.000480... Val Loss: 0.001379\n",
      "Epoch: 57/100... Step: 58675... Loss: 0.000172... Val Loss: 0.000620\n",
      "Epoch: 57/100... Step: 58680... Loss: 0.000319... Val Loss: 0.000553\n",
      "Epoch: 57/100... Step: 58685... Loss: 0.000280... Val Loss: 0.000845\n",
      "Epoch: 57/100... Step: 58690... Loss: 0.000184... Val Loss: 0.000829\n",
      "Epoch: 57/100... Step: 58695... Loss: 0.000419... Val Loss: 0.000501\n",
      "Epoch: 57/100... Step: 58700... Loss: 0.000141... Val Loss: 0.000382\n",
      "Epoch: 57/100... Step: 58705... Loss: 0.000412... Val Loss: 0.000357\n",
      "Epoch: 57/100... Step: 58710... Loss: 0.000170... Val Loss: 0.000498\n",
      "Epoch: 57/100... Step: 58715... Loss: 0.000115... Val Loss: 0.000588\n",
      "Epoch: 57/100... Step: 58720... Loss: 0.000264... Val Loss: 0.000721\n",
      "Epoch: 57/100... Step: 58725... Loss: 0.000267... Val Loss: 0.000956\n",
      "Epoch: 57/100... Step: 58730... Loss: 0.000120... Val Loss: 0.000871\n",
      "Epoch: 57/100... Step: 58735... Loss: 0.000190... Val Loss: 0.001181\n",
      "Epoch: 57/100... Step: 58740... Loss: 0.000237... Val Loss: 0.000586\n",
      "Epoch: 57/100... Step: 58745... Loss: 0.000188... Val Loss: 0.000955\n",
      "Epoch: 57/100... Step: 58750... Loss: 0.000306... Val Loss: 0.000727\n",
      "Epoch: 57/100... Step: 58755... Loss: 0.000096... Val Loss: 0.000798\n",
      "Epoch: 57/100... Step: 58760... Loss: 0.000085... Val Loss: 0.000885\n",
      "Epoch: 57/100... Step: 58765... Loss: 0.000234... Val Loss: 0.001008\n",
      "Epoch: 57/100... Step: 58770... Loss: 0.000175... Val Loss: 0.000907\n",
      "Epoch: 57/100... Step: 58775... Loss: 0.000177... Val Loss: 0.000915\n",
      "Epoch: 57/100... Step: 58780... Loss: 0.000085... Val Loss: 0.000838\n",
      "Epoch: 57/100... Step: 58785... Loss: 0.000152... Val Loss: 0.000885\n",
      "Epoch: 57/100... Step: 58790... Loss: 0.000384... Val Loss: 0.001099\n",
      "Epoch: 57/100... Step: 58795... Loss: 0.000339... Val Loss: 0.000705\n",
      "Epoch: 57/100... Step: 58800... Loss: 0.000138... Val Loss: 0.000688\n",
      "Epoch: 57/100... Step: 58805... Loss: 0.000302... Val Loss: 0.001018\n",
      "Epoch: 57/100... Step: 58810... Loss: 0.000291... Val Loss: 0.001304\n",
      "Epoch: 57/100... Step: 58815... Loss: 0.000187... Val Loss: 0.001228\n",
      "Epoch: 57/100... Step: 58820... Loss: 0.000317... Val Loss: 0.001025\n",
      "Epoch: 58/100... Step: 58825... Loss: 0.000217... Val Loss: 0.001484\n",
      "Epoch: 58/100... Step: 58830... Loss: 0.000287... Val Loss: 0.001332\n",
      "Epoch: 58/100... Step: 58835... Loss: 0.000217... Val Loss: 0.001530\n",
      "Epoch: 58/100... Step: 58840... Loss: 0.000302... Val Loss: 0.001655\n",
      "Epoch: 58/100... Step: 58845... Loss: 0.000388... Val Loss: 0.001627\n",
      "Epoch: 58/100... Step: 58850... Loss: 0.000141... Val Loss: 0.001804\n",
      "Epoch: 58/100... Step: 58855... Loss: 0.000273... Val Loss: 0.001953\n",
      "Epoch: 58/100... Step: 58860... Loss: 0.000142... Val Loss: 0.001798\n",
      "Epoch: 58/100... Step: 58865... Loss: 0.000139... Val Loss: 0.001663\n",
      "Epoch: 58/100... Step: 58870... Loss: 0.000172... Val Loss: 0.001878\n",
      "Epoch: 58/100... Step: 58875... Loss: 0.000133... Val Loss: 0.001714\n",
      "Epoch: 58/100... Step: 58880... Loss: 0.000145... Val Loss: 0.001699\n",
      "Epoch: 58/100... Step: 58885... Loss: 0.000124... Val Loss: 0.001651\n",
      "Epoch: 58/100... Step: 58890... Loss: 0.000117... Val Loss: 0.001594\n",
      "Epoch: 58/100... Step: 58895... Loss: 0.000217... Val Loss: 0.001198\n",
      "Epoch: 58/100... Step: 58900... Loss: 0.000355... Val Loss: 0.000956\n",
      "Epoch: 58/100... Step: 58905... Loss: 0.000189... Val Loss: 0.001255\n",
      "Epoch: 58/100... Step: 58910... Loss: 0.000324... Val Loss: 0.001241\n",
      "Epoch: 58/100... Step: 58915... Loss: 0.000166... Val Loss: 0.001460\n",
      "Epoch: 58/100... Step: 58920... Loss: 0.000267... Val Loss: 0.001114\n",
      "Epoch: 58/100... Step: 58925... Loss: 0.000172... Val Loss: 0.001214\n",
      "Epoch: 58/100... Step: 58930... Loss: 0.000120... Val Loss: 0.001175\n",
      "Epoch: 58/100... Step: 58935... Loss: 0.000171... Val Loss: 0.001117\n",
      "Epoch: 58/100... Step: 58940... Loss: 0.000120... Val Loss: 0.001279\n",
      "Epoch: 58/100... Step: 58945... Loss: 0.000200... Val Loss: 0.001107\n",
      "Epoch: 58/100... Step: 58950... Loss: 0.000381... Val Loss: 0.001158\n",
      "Epoch: 58/100... Step: 58955... Loss: 0.000315... Val Loss: 0.001130\n",
      "Epoch: 58/100... Step: 58960... Loss: 0.000244... Val Loss: 0.001548\n",
      "Epoch: 58/100... Step: 58965... Loss: 0.000282... Val Loss: 0.001614\n",
      "Epoch: 58/100... Step: 58970... Loss: 0.000124... Val Loss: 0.001783\n",
      "Epoch: 58/100... Step: 58975... Loss: 0.000119... Val Loss: 0.001681\n",
      "Epoch: 58/100... Step: 58980... Loss: 0.000091... Val Loss: 0.001638\n",
      "Epoch: 58/100... Step: 58985... Loss: 0.000233... Val Loss: 0.001496\n",
      "Epoch: 58/100... Step: 58990... Loss: 0.000285... Val Loss: 0.001376\n",
      "Epoch: 58/100... Step: 58995... Loss: 0.000428... Val Loss: 0.001381\n",
      "Epoch: 58/100... Step: 59000... Loss: 0.000122... Val Loss: 0.001266\n",
      "Epoch: 58/100... Step: 59005... Loss: 0.000222... Val Loss: 0.001190\n",
      "Epoch: 58/100... Step: 59010... Loss: 0.000133... Val Loss: 0.001075\n",
      "Epoch: 58/100... Step: 59015... Loss: 0.000174... Val Loss: 0.001226\n",
      "Epoch: 58/100... Step: 59020... Loss: 0.000134... Val Loss: 0.001247\n",
      "Epoch: 58/100... Step: 59025... Loss: 0.000336... Val Loss: 0.001077\n",
      "Epoch: 58/100... Step: 59030... Loss: 0.000145... Val Loss: 0.001214\n",
      "Epoch: 58/100... Step: 59035... Loss: 0.000127... Val Loss: 0.001585\n",
      "Epoch: 58/100... Step: 59040... Loss: 0.000077... Val Loss: 0.001367\n",
      "Epoch: 58/100... Step: 59045... Loss: 0.000562... Val Loss: 0.000942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58/100... Step: 59050... Loss: 0.000406... Val Loss: 0.001192\n",
      "Epoch: 58/100... Step: 59055... Loss: 0.000226... Val Loss: 0.000953\n",
      "Epoch: 58/100... Step: 59060... Loss: 0.000158... Val Loss: 0.001079\n",
      "Epoch: 58/100... Step: 59065... Loss: 0.000081... Val Loss: 0.001267\n",
      "Epoch: 58/100... Step: 59070... Loss: 0.000377... Val Loss: 0.001218\n",
      "Epoch: 58/100... Step: 59075... Loss: 0.000234... Val Loss: 0.001248\n",
      "Epoch: 58/100... Step: 59080... Loss: 0.000342... Val Loss: 0.001125\n",
      "Epoch: 58/100... Step: 59085... Loss: 0.000418... Val Loss: 0.000948\n",
      "Epoch: 58/100... Step: 59090... Loss: 0.000203... Val Loss: 0.000885\n",
      "Epoch: 58/100... Step: 59095... Loss: 0.000204... Val Loss: 0.000948\n",
      "Epoch: 58/100... Step: 59100... Loss: 0.000348... Val Loss: 0.000908\n",
      "Epoch: 58/100... Step: 59105... Loss: 0.000236... Val Loss: 0.000882\n",
      "Epoch: 58/100... Step: 59110... Loss: 0.000259... Val Loss: 0.000938\n",
      "Epoch: 58/100... Step: 59115... Loss: 0.000175... Val Loss: 0.000561\n",
      "Epoch: 58/100... Step: 59120... Loss: 0.000183... Val Loss: 0.000515\n",
      "Epoch: 58/100... Step: 59125... Loss: 0.000194... Val Loss: 0.000509\n",
      "Epoch: 58/100... Step: 59130... Loss: 0.000256... Val Loss: 0.000659\n",
      "Epoch: 58/100... Step: 59135... Loss: 0.000129... Val Loss: 0.000490\n",
      "Epoch: 58/100... Step: 59140... Loss: 0.000372... Val Loss: 0.000545\n",
      "Epoch: 58/100... Step: 59145... Loss: 0.000321... Val Loss: 0.000615\n",
      "Epoch: 58/100... Step: 59150... Loss: 0.000176... Val Loss: 0.000513\n",
      "Epoch: 58/100... Step: 59155... Loss: 0.000336... Val Loss: 0.000428\n",
      "Epoch: 58/100... Step: 59160... Loss: 0.000308... Val Loss: 0.000691\n",
      "Epoch: 58/100... Step: 59165... Loss: 0.000331... Val Loss: 0.000749\n",
      "Epoch: 58/100... Step: 59170... Loss: 0.000306... Val Loss: 0.000483\n",
      "Epoch: 58/100... Step: 59175... Loss: 0.000216... Val Loss: 0.000470\n",
      "Epoch: 58/100... Step: 59180... Loss: 0.000203... Val Loss: 0.000728\n",
      "Epoch: 58/100... Step: 59185... Loss: 0.000090... Val Loss: 0.000781\n",
      "Epoch: 58/100... Step: 59190... Loss: 0.000311... Val Loss: 0.000931\n",
      "Epoch: 58/100... Step: 59195... Loss: 0.000241... Val Loss: 0.000772\n",
      "Epoch: 58/100... Step: 59200... Loss: 0.000106... Val Loss: 0.000968\n",
      "Epoch: 58/100... Step: 59205... Loss: 0.000209... Val Loss: 0.000945\n",
      "Epoch: 58/100... Step: 59210... Loss: 0.000163... Val Loss: 0.000901\n",
      "Epoch: 58/100... Step: 59215... Loss: 0.000103... Val Loss: 0.000965\n",
      "Epoch: 58/100... Step: 59220... Loss: 0.000181... Val Loss: 0.000891\n",
      "Epoch: 58/100... Step: 59225... Loss: 0.000085... Val Loss: 0.001272\n",
      "Epoch: 58/100... Step: 59230... Loss: 0.000304... Val Loss: 0.000583\n",
      "Epoch: 58/100... Step: 59235... Loss: 0.000151... Val Loss: 0.000496\n",
      "Epoch: 58/100... Step: 59240... Loss: 0.000695... Val Loss: 0.000674\n",
      "Epoch: 58/100... Step: 59245... Loss: 0.000161... Val Loss: 0.000861\n",
      "Epoch: 58/100... Step: 59250... Loss: 0.000222... Val Loss: 0.000990\n",
      "Epoch: 58/100... Step: 59255... Loss: 0.000311... Val Loss: 0.001063\n",
      "Epoch: 58/100... Step: 59260... Loss: 0.000254... Val Loss: 0.001246\n",
      "Epoch: 58/100... Step: 59265... Loss: 0.000391... Val Loss: 0.000832\n",
      "Epoch: 58/100... Step: 59270... Loss: 0.000332... Val Loss: 0.001101\n",
      "Epoch: 58/100... Step: 59275... Loss: 0.000230... Val Loss: 0.001096\n",
      "Epoch: 58/100... Step: 59280... Loss: 0.000173... Val Loss: 0.001270\n",
      "Epoch: 58/100... Step: 59285... Loss: 0.000361... Val Loss: 0.001273\n",
      "Epoch: 58/100... Step: 59290... Loss: 0.000602... Val Loss: 0.000987\n",
      "Epoch: 58/100... Step: 59295... Loss: 0.000163... Val Loss: 0.001033\n",
      "Epoch: 58/100... Step: 59300... Loss: 0.000646... Val Loss: 0.000915\n",
      "Epoch: 58/100... Step: 59305... Loss: 0.000562... Val Loss: 0.001078\n",
      "Epoch: 58/100... Step: 59310... Loss: 0.000440... Val Loss: 0.000957\n",
      "Epoch: 58/100... Step: 59315... Loss: 0.000247... Val Loss: 0.001019\n",
      "Epoch: 58/100... Step: 59320... Loss: 0.000196... Val Loss: 0.000560\n",
      "Epoch: 58/100... Step: 59325... Loss: 0.000273... Val Loss: 0.000680\n",
      "Epoch: 58/100... Step: 59330... Loss: 0.000218... Val Loss: 0.000748\n",
      "Epoch: 58/100... Step: 59335... Loss: 0.000303... Val Loss: 0.000413\n",
      "Epoch: 58/100... Step: 59340... Loss: 0.000158... Val Loss: 0.000422\n",
      "Epoch: 58/100... Step: 59345... Loss: 0.000222... Val Loss: 0.000614\n",
      "Epoch: 58/100... Step: 59350... Loss: 0.000155... Val Loss: 0.000677\n",
      "Epoch: 58/100... Step: 59355... Loss: 0.000229... Val Loss: 0.000598\n",
      "Epoch: 58/100... Step: 59360... Loss: 0.000108... Val Loss: 0.000428\n",
      "Epoch: 58/100... Step: 59365... Loss: 0.000205... Val Loss: 0.000604\n",
      "Epoch: 58/100... Step: 59370... Loss: 0.000111... Val Loss: 0.000509\n",
      "Epoch: 58/100... Step: 59375... Loss: 0.000301... Val Loss: 0.001247\n",
      "Epoch: 58/100... Step: 59380... Loss: 0.000326... Val Loss: 0.001017\n",
      "Epoch: 58/100... Step: 59385... Loss: 0.000125... Val Loss: 0.000950\n",
      "Epoch: 58/100... Step: 59390... Loss: 0.000235... Val Loss: 0.000982\n",
      "Epoch: 58/100... Step: 59395... Loss: 0.000259... Val Loss: 0.000775\n",
      "Epoch: 58/100... Step: 59400... Loss: 0.000172... Val Loss: 0.000791\n",
      "Epoch: 58/100... Step: 59405... Loss: 0.000199... Val Loss: 0.000706\n",
      "Epoch: 58/100... Step: 59410... Loss: 0.000088... Val Loss: 0.000837\n",
      "Epoch: 58/100... Step: 59415... Loss: 0.000137... Val Loss: 0.000829\n",
      "Epoch: 58/100... Step: 59420... Loss: 0.000201... Val Loss: 0.001119\n",
      "Epoch: 58/100... Step: 59425... Loss: 0.000668... Val Loss: 0.001160\n",
      "Epoch: 58/100... Step: 59430... Loss: 0.000332... Val Loss: 0.001421\n",
      "Epoch: 58/100... Step: 59435... Loss: 0.000264... Val Loss: 0.000764\n",
      "Epoch: 58/100... Step: 59440... Loss: 0.000441... Val Loss: 0.000533\n",
      "Epoch: 58/100... Step: 59445... Loss: 0.000225... Val Loss: 0.000980\n",
      "Epoch: 58/100... Step: 59450... Loss: 0.000416... Val Loss: 0.000822\n",
      "Epoch: 58/100... Step: 59455... Loss: 0.000078... Val Loss: 0.001071\n",
      "Epoch: 58/100... Step: 59460... Loss: 0.000303... Val Loss: 0.000740\n",
      "Epoch: 58/100... Step: 59465... Loss: 0.000120... Val Loss: 0.000598\n",
      "Epoch: 58/100... Step: 59470... Loss: 0.000106... Val Loss: 0.000950\n",
      "Epoch: 58/100... Step: 59475... Loss: 0.000245... Val Loss: 0.001056\n",
      "Epoch: 58/100... Step: 59480... Loss: 0.000143... Val Loss: 0.000950\n",
      "Epoch: 58/100... Step: 59485... Loss: 0.000393... Val Loss: 0.000997\n",
      "Epoch: 58/100... Step: 59490... Loss: 0.000181... Val Loss: 0.000627\n",
      "Epoch: 58/100... Step: 59495... Loss: 0.000219... Val Loss: 0.000482\n",
      "Epoch: 58/100... Step: 59500... Loss: 0.000090... Val Loss: 0.000369\n",
      "Epoch: 58/100... Step: 59505... Loss: 0.000236... Val Loss: 0.000441\n",
      "Epoch: 58/100... Step: 59510... Loss: 0.000195... Val Loss: 0.000326\n",
      "Epoch: 58/100... Step: 59515... Loss: 0.000158... Val Loss: 0.000347\n",
      "Epoch: 58/100... Step: 59520... Loss: 0.000351... Val Loss: 0.000345\n",
      "Epoch: 58/100... Step: 59525... Loss: 0.000073... Val Loss: 0.000265\n",
      "Epoch: 58/100... Step: 59530... Loss: 0.000341... Val Loss: 0.000276\n",
      "Epoch: 58/100... Step: 59535... Loss: 0.000217... Val Loss: 0.000442\n",
      "Epoch: 58/100... Step: 59540... Loss: 0.000209... Val Loss: 0.000380\n",
      "Epoch: 58/100... Step: 59545... Loss: 0.000213... Val Loss: 0.000421\n",
      "Epoch: 58/100... Step: 59550... Loss: 0.000131... Val Loss: 0.000412\n",
      "Epoch: 58/100... Step: 59555... Loss: 0.000186... Val Loss: 0.000351\n",
      "Epoch: 58/100... Step: 59560... Loss: 0.000392... Val Loss: 0.000544\n",
      "Epoch: 58/100... Step: 59565... Loss: 0.000500... Val Loss: 0.000720\n",
      "Epoch: 58/100... Step: 59570... Loss: 0.000764... Val Loss: 0.001323\n",
      "Epoch: 58/100... Step: 59575... Loss: 0.000409... Val Loss: 0.002022\n",
      "Epoch: 58/100... Step: 59580... Loss: 0.000073... Val Loss: 0.000390\n",
      "Epoch: 58/100... Step: 59585... Loss: 0.000231... Val Loss: 0.000603\n",
      "Epoch: 58/100... Step: 59590... Loss: 0.000360... Val Loss: 0.000500\n",
      "Epoch: 58/100... Step: 59595... Loss: 0.000225... Val Loss: 0.000599\n",
      "Epoch: 58/100... Step: 59600... Loss: 0.000266... Val Loss: 0.000503\n",
      "Epoch: 58/100... Step: 59605... Loss: 0.000131... Val Loss: 0.000745\n",
      "Epoch: 58/100... Step: 59610... Loss: 0.000306... Val Loss: 0.000558\n",
      "Epoch: 58/100... Step: 59615... Loss: 0.000464... Val Loss: 0.000618\n",
      "Epoch: 58/100... Step: 59620... Loss: 0.000350... Val Loss: 0.000662\n",
      "Epoch: 58/100... Step: 59625... Loss: 0.000256... Val Loss: 0.000664\n",
      "Epoch: 58/100... Step: 59630... Loss: 0.000339... Val Loss: 0.000560\n",
      "Epoch: 58/100... Step: 59635... Loss: 0.000345... Val Loss: 0.000827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58/100... Step: 59640... Loss: 0.000210... Val Loss: 0.000603\n",
      "Epoch: 58/100... Step: 59645... Loss: 0.000312... Val Loss: 0.000656\n",
      "Epoch: 58/100... Step: 59650... Loss: 0.000275... Val Loss: 0.000743\n",
      "Epoch: 58/100... Step: 59655... Loss: 0.000072... Val Loss: 0.000611\n",
      "Epoch: 58/100... Step: 59660... Loss: 0.000278... Val Loss: 0.000748\n",
      "Epoch: 58/100... Step: 59665... Loss: 0.000062... Val Loss: 0.000622\n",
      "Epoch: 58/100... Step: 59670... Loss: 0.000200... Val Loss: 0.000669\n",
      "Epoch: 58/100... Step: 59675... Loss: 0.000330... Val Loss: 0.000746\n",
      "Epoch: 58/100... Step: 59680... Loss: 0.000145... Val Loss: 0.000452\n",
      "Epoch: 58/100... Step: 59685... Loss: 0.000204... Val Loss: 0.000912\n",
      "Epoch: 58/100... Step: 59690... Loss: 0.000173... Val Loss: 0.000876\n",
      "Epoch: 58/100... Step: 59695... Loss: 0.000215... Val Loss: 0.000637\n",
      "Epoch: 58/100... Step: 59700... Loss: 0.000255... Val Loss: 0.001241\n",
      "Epoch: 58/100... Step: 59705... Loss: 0.000064... Val Loss: 0.000951\n",
      "Epoch: 58/100... Step: 59710... Loss: 0.000362... Val Loss: 0.000755\n",
      "Epoch: 58/100... Step: 59715... Loss: 0.000350... Val Loss: 0.000283\n",
      "Epoch: 58/100... Step: 59720... Loss: 0.000438... Val Loss: 0.000614\n",
      "Epoch: 58/100... Step: 59725... Loss: 0.000824... Val Loss: 0.000914\n",
      "Epoch: 58/100... Step: 59730... Loss: 0.000143... Val Loss: 0.000471\n",
      "Epoch: 58/100... Step: 59735... Loss: 0.000299... Val Loss: 0.000486\n",
      "Epoch: 58/100... Step: 59740... Loss: 0.000170... Val Loss: 0.000412\n",
      "Epoch: 58/100... Step: 59745... Loss: 0.000075... Val Loss: 0.000515\n",
      "Epoch: 58/100... Step: 59750... Loss: 0.000384... Val Loss: 0.000773\n",
      "Epoch: 58/100... Step: 59755... Loss: 0.000437... Val Loss: 0.000773\n",
      "Epoch: 58/100... Step: 59760... Loss: 0.000289... Val Loss: 0.000914\n",
      "Epoch: 58/100... Step: 59765... Loss: 0.000123... Val Loss: 0.001198\n",
      "Epoch: 58/100... Step: 59770... Loss: 0.000202... Val Loss: 0.000644\n",
      "Epoch: 58/100... Step: 59775... Loss: 0.000254... Val Loss: 0.000825\n",
      "Epoch: 58/100... Step: 59780... Loss: 0.000233... Val Loss: 0.001089\n",
      "Epoch: 58/100... Step: 59785... Loss: 0.000360... Val Loss: 0.000722\n",
      "Epoch: 58/100... Step: 59790... Loss: 0.000234... Val Loss: 0.000853\n",
      "Epoch: 58/100... Step: 59795... Loss: 0.000209... Val Loss: 0.000895\n",
      "Epoch: 58/100... Step: 59800... Loss: 0.000189... Val Loss: 0.000978\n",
      "Epoch: 58/100... Step: 59805... Loss: 0.000285... Val Loss: 0.000912\n",
      "Epoch: 58/100... Step: 59810... Loss: 0.000093... Val Loss: 0.000914\n",
      "Epoch: 58/100... Step: 59815... Loss: 0.000147... Val Loss: 0.001121\n",
      "Epoch: 58/100... Step: 59820... Loss: 0.000104... Val Loss: 0.001126\n",
      "Epoch: 58/100... Step: 59825... Loss: 0.000147... Val Loss: 0.000971\n",
      "Epoch: 58/100... Step: 59830... Loss: 0.000196... Val Loss: 0.000698\n",
      "Epoch: 58/100... Step: 59835... Loss: 0.000174... Val Loss: 0.000691\n",
      "Epoch: 58/100... Step: 59840... Loss: 0.000183... Val Loss: 0.000996\n",
      "Epoch: 58/100... Step: 59845... Loss: 0.000167... Val Loss: 0.001004\n",
      "Epoch: 58/100... Step: 59850... Loss: 0.000074... Val Loss: 0.000972\n",
      "Epoch: 58/100... Step: 59855... Loss: 0.000130... Val Loss: 0.000819\n",
      "Epoch: 59/100... Step: 59860... Loss: 0.000480... Val Loss: 0.001777\n",
      "Epoch: 59/100... Step: 59865... Loss: 0.000337... Val Loss: 0.001643\n",
      "Epoch: 59/100... Step: 59870... Loss: 0.000239... Val Loss: 0.001487\n",
      "Epoch: 59/100... Step: 59875... Loss: 0.000337... Val Loss: 0.001468\n",
      "Epoch: 59/100... Step: 59880... Loss: 0.000158... Val Loss: 0.001728\n",
      "Epoch: 59/100... Step: 59885... Loss: 0.000122... Val Loss: 0.001836\n",
      "Epoch: 59/100... Step: 59890... Loss: 0.000257... Val Loss: 0.001725\n",
      "Epoch: 59/100... Step: 59895... Loss: 0.000285... Val Loss: 0.001612\n",
      "Epoch: 59/100... Step: 59900... Loss: 0.000333... Val Loss: 0.001872\n",
      "Epoch: 59/100... Step: 59905... Loss: 0.000159... Val Loss: 0.001869\n",
      "Epoch: 59/100... Step: 59910... Loss: 0.000131... Val Loss: 0.001790\n",
      "Epoch: 59/100... Step: 59915... Loss: 0.000187... Val Loss: 0.001778\n",
      "Epoch: 59/100... Step: 59920... Loss: 0.000140... Val Loss: 0.001593\n",
      "Epoch: 59/100... Step: 59925... Loss: 0.000220... Val Loss: 0.001410\n",
      "Epoch: 59/100... Step: 59930... Loss: 0.000101... Val Loss: 0.001197\n",
      "Epoch: 59/100... Step: 59935... Loss: 0.000203... Val Loss: 0.001226\n",
      "Epoch: 59/100... Step: 59940... Loss: 0.000198... Val Loss: 0.001326\n",
      "Epoch: 59/100... Step: 59945... Loss: 0.000093... Val Loss: 0.001186\n",
      "Epoch: 59/100... Step: 59950... Loss: 0.000179... Val Loss: 0.001047\n",
      "Epoch: 59/100... Step: 59955... Loss: 0.000301... Val Loss: 0.001163\n",
      "Epoch: 59/100... Step: 59960... Loss: 0.000207... Val Loss: 0.001287\n",
      "Epoch: 59/100... Step: 59965... Loss: 0.000076... Val Loss: 0.001310\n",
      "Epoch: 59/100... Step: 59970... Loss: 0.000189... Val Loss: 0.001151\n",
      "Epoch: 59/100... Step: 59975... Loss: 0.000080... Val Loss: 0.001127\n",
      "Epoch: 59/100... Step: 59980... Loss: 0.000124... Val Loss: 0.001210\n",
      "Epoch: 59/100... Step: 59985... Loss: 0.000114... Val Loss: 0.001298\n",
      "Epoch: 59/100... Step: 59990... Loss: 0.000140... Val Loss: 0.001298\n",
      "Epoch: 59/100... Step: 59995... Loss: 0.000161... Val Loss: 0.001524\n",
      "Epoch: 59/100... Step: 60000... Loss: 0.000202... Val Loss: 0.001436\n",
      "Epoch: 59/100... Step: 60005... Loss: 0.000111... Val Loss: 0.001898\n",
      "Epoch: 59/100... Step: 60010... Loss: 0.000083... Val Loss: 0.001717\n",
      "Epoch: 59/100... Step: 60015... Loss: 0.000243... Val Loss: 0.001563\n",
      "Epoch: 59/100... Step: 60020... Loss: 0.000088... Val Loss: 0.001535\n",
      "Epoch: 59/100... Step: 60025... Loss: 0.000163... Val Loss: 0.001487\n",
      "Epoch: 59/100... Step: 60030... Loss: 0.000102... Val Loss: 0.001363\n",
      "Epoch: 59/100... Step: 60035... Loss: 0.000061... Val Loss: 0.001285\n",
      "Epoch: 59/100... Step: 60040... Loss: 0.000109... Val Loss: 0.001255\n",
      "Epoch: 59/100... Step: 60045... Loss: 0.000104... Val Loss: 0.001250\n",
      "Epoch: 59/100... Step: 60050... Loss: 0.000215... Val Loss: 0.001373\n",
      "Epoch: 59/100... Step: 60055... Loss: 0.000054... Val Loss: 0.001364\n",
      "Epoch: 59/100... Step: 60060... Loss: 0.000290... Val Loss: 0.001048\n",
      "Epoch: 59/100... Step: 60065... Loss: 0.000147... Val Loss: 0.001099\n",
      "Epoch: 59/100... Step: 60070... Loss: 0.000082... Val Loss: 0.001137\n",
      "Epoch: 59/100... Step: 60075... Loss: 0.000234... Val Loss: 0.001250\n",
      "Epoch: 59/100... Step: 60080... Loss: 0.000132... Val Loss: 0.001229\n",
      "Epoch: 59/100... Step: 60085... Loss: 0.000248... Val Loss: 0.001213\n",
      "Epoch: 59/100... Step: 60090... Loss: 0.000063... Val Loss: 0.001058\n",
      "Epoch: 59/100... Step: 60095... Loss: 0.000084... Val Loss: 0.001195\n",
      "Epoch: 59/100... Step: 60100... Loss: 0.000080... Val Loss: 0.001183\n",
      "Epoch: 59/100... Step: 60105... Loss: 0.000083... Val Loss: 0.001067\n",
      "Epoch: 59/100... Step: 60110... Loss: 0.000056... Val Loss: 0.001123\n",
      "Epoch: 59/100... Step: 60115... Loss: 0.000137... Val Loss: 0.000971\n",
      "Epoch: 59/100... Step: 60120... Loss: 0.000156... Val Loss: 0.000948\n",
      "Epoch: 59/100... Step: 60125... Loss: 0.000130... Val Loss: 0.000865\n",
      "Epoch: 59/100... Step: 60130... Loss: 0.000105... Val Loss: 0.000914\n",
      "Epoch: 59/100... Step: 60135... Loss: 0.000184... Val Loss: 0.000945\n",
      "Epoch: 59/100... Step: 60140... Loss: 0.000179... Val Loss: 0.001164\n",
      "Epoch: 59/100... Step: 60145... Loss: 0.000197... Val Loss: 0.000810\n",
      "Epoch: 59/100... Step: 60150... Loss: 0.000237... Val Loss: 0.000461\n",
      "Epoch: 59/100... Step: 60155... Loss: 0.000094... Val Loss: 0.000426\n",
      "Epoch: 59/100... Step: 60160... Loss: 0.000134... Val Loss: 0.000615\n",
      "Epoch: 59/100... Step: 60165... Loss: 0.000136... Val Loss: 0.000558\n",
      "Epoch: 59/100... Step: 60170... Loss: 0.000060... Val Loss: 0.000584\n",
      "Epoch: 59/100... Step: 60175... Loss: 0.000044... Val Loss: 0.000623\n",
      "Epoch: 59/100... Step: 60180... Loss: 0.000108... Val Loss: 0.000564\n",
      "Epoch: 59/100... Step: 60185... Loss: 0.000067... Val Loss: 0.000417\n",
      "Epoch: 59/100... Step: 60190... Loss: 0.000084... Val Loss: 0.000485\n",
      "Epoch: 59/100... Step: 60195... Loss: 0.000107... Val Loss: 0.000742\n",
      "Epoch: 59/100... Step: 60200... Loss: 0.000232... Val Loss: 0.000651\n",
      "Epoch: 59/100... Step: 60205... Loss: 0.000286... Val Loss: 0.000267\n",
      "Epoch: 59/100... Step: 60210... Loss: 0.000203... Val Loss: 0.000525\n",
      "Epoch: 59/100... Step: 60215... Loss: 0.000066... Val Loss: 0.000698\n",
      "Epoch: 59/100... Step: 60220... Loss: 0.000163... Val Loss: 0.000843\n",
      "Epoch: 59/100... Step: 60225... Loss: 0.000096... Val Loss: 0.000993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59/100... Step: 60230... Loss: 0.000047... Val Loss: 0.000975\n",
      "Epoch: 59/100... Step: 60235... Loss: 0.000066... Val Loss: 0.001110\n",
      "Epoch: 59/100... Step: 60240... Loss: 0.000128... Val Loss: 0.001066\n",
      "Epoch: 59/100... Step: 60245... Loss: 0.000120... Val Loss: 0.000978\n",
      "Epoch: 59/100... Step: 60250... Loss: 0.000184... Val Loss: 0.000988\n",
      "Epoch: 59/100... Step: 60255... Loss: 0.000082... Val Loss: 0.001014\n",
      "Epoch: 59/100... Step: 60260... Loss: 0.000248... Val Loss: 0.001063\n",
      "Epoch: 59/100... Step: 60265... Loss: 0.000159... Val Loss: 0.000281\n",
      "Epoch: 59/100... Step: 60270... Loss: 0.000750... Val Loss: 0.000422\n",
      "Epoch: 59/100... Step: 60275... Loss: 0.000269... Val Loss: 0.001803\n",
      "Epoch: 59/100... Step: 60280... Loss: 0.000369... Val Loss: 0.000580\n",
      "Epoch: 59/100... Step: 60285... Loss: 0.000447... Val Loss: 0.001436\n",
      "Epoch: 59/100... Step: 60290... Loss: 0.000318... Val Loss: 0.001242\n",
      "Epoch: 59/100... Step: 60295... Loss: 0.000224... Val Loss: 0.001028\n",
      "Epoch: 59/100... Step: 60300... Loss: 0.000169... Val Loss: 0.001151\n",
      "Epoch: 59/100... Step: 60305... Loss: 0.000110... Val Loss: 0.001312\n",
      "Epoch: 59/100... Step: 60310... Loss: 0.000147... Val Loss: 0.001414\n",
      "Epoch: 59/100... Step: 60315... Loss: 0.000103... Val Loss: 0.001343\n",
      "Epoch: 59/100... Step: 60320... Loss: 0.000160... Val Loss: 0.001390\n",
      "Epoch: 59/100... Step: 60325... Loss: 0.000233... Val Loss: 0.001222\n",
      "Epoch: 59/100... Step: 60330... Loss: 0.000189... Val Loss: 0.001160\n",
      "Epoch: 59/100... Step: 60335... Loss: 0.000127... Val Loss: 0.001205\n",
      "Epoch: 59/100... Step: 60340... Loss: 0.000203... Val Loss: 0.001153\n",
      "Epoch: 59/100... Step: 60345... Loss: 0.000178... Val Loss: 0.000869\n",
      "Epoch: 59/100... Step: 60350... Loss: 0.000127... Val Loss: 0.001203\n",
      "Epoch: 59/100... Step: 60355... Loss: 0.000324... Val Loss: 0.001037\n",
      "Epoch: 59/100... Step: 60360... Loss: 0.000208... Val Loss: 0.000739\n",
      "Epoch: 59/100... Step: 60365... Loss: 0.000208... Val Loss: 0.000341\n",
      "Epoch: 59/100... Step: 60370... Loss: 0.000148... Val Loss: 0.000444\n",
      "Epoch: 59/100... Step: 60375... Loss: 0.000200... Val Loss: 0.000801\n",
      "Epoch: 59/100... Step: 60380... Loss: 0.000352... Val Loss: 0.000370\n",
      "Epoch: 59/100... Step: 60385... Loss: 0.000074... Val Loss: 0.000516\n",
      "Epoch: 59/100... Step: 60390... Loss: 0.000210... Val Loss: 0.000563\n",
      "Epoch: 59/100... Step: 60395... Loss: 0.000140... Val Loss: 0.000513\n",
      "Epoch: 59/100... Step: 60400... Loss: 0.000125... Val Loss: 0.000788\n",
      "Epoch: 59/100... Step: 60405... Loss: 0.000091... Val Loss: 0.000925\n",
      "Epoch: 59/100... Step: 60410... Loss: 0.000166... Val Loss: 0.000875\n",
      "Epoch: 59/100... Step: 60415... Loss: 0.000077... Val Loss: 0.000757\n",
      "Epoch: 59/100... Step: 60420... Loss: 0.000159... Val Loss: 0.000813\n",
      "Epoch: 59/100... Step: 60425... Loss: 0.000084... Val Loss: 0.000735\n",
      "Epoch: 59/100... Step: 60430... Loss: 0.000169... Val Loss: 0.001081\n",
      "Epoch: 59/100... Step: 60435... Loss: 0.000158... Val Loss: 0.000925\n",
      "Epoch: 59/100... Step: 60440... Loss: 0.000308... Val Loss: 0.000971\n",
      "Epoch: 59/100... Step: 60445... Loss: 0.000244... Val Loss: 0.001041\n",
      "Epoch: 59/100... Step: 60450... Loss: 0.000513... Val Loss: 0.000742\n",
      "Epoch: 59/100... Step: 60455... Loss: 0.000189... Val Loss: 0.000729\n",
      "Epoch: 59/100... Step: 60460... Loss: 0.000249... Val Loss: 0.001091\n",
      "Epoch: 59/100... Step: 60465... Loss: 0.000220... Val Loss: 0.000932\n",
      "Epoch: 59/100... Step: 60470... Loss: 0.000100... Val Loss: 0.000750\n",
      "Epoch: 59/100... Step: 60475... Loss: 0.000154... Val Loss: 0.000942\n",
      "Epoch: 59/100... Step: 60480... Loss: 0.000058... Val Loss: 0.000686\n",
      "Epoch: 59/100... Step: 60485... Loss: 0.000213... Val Loss: 0.000970\n",
      "Epoch: 59/100... Step: 60490... Loss: 0.000288... Val Loss: 0.000917\n",
      "Epoch: 59/100... Step: 60495... Loss: 0.000152... Val Loss: 0.000791\n",
      "Epoch: 59/100... Step: 60500... Loss: 0.000131... Val Loss: 0.000859\n",
      "Epoch: 59/100... Step: 60505... Loss: 0.000279... Val Loss: 0.000529\n",
      "Epoch: 59/100... Step: 60510... Loss: 0.000160... Val Loss: 0.000852\n",
      "Epoch: 59/100... Step: 60515... Loss: 0.000097... Val Loss: 0.000917\n",
      "Epoch: 59/100... Step: 60520... Loss: 0.000129... Val Loss: 0.000617\n",
      "Epoch: 59/100... Step: 60525... Loss: 0.000100... Val Loss: 0.000440\n",
      "Epoch: 59/100... Step: 60530... Loss: 0.000163... Val Loss: 0.000357\n",
      "Epoch: 59/100... Step: 60535... Loss: 0.000135... Val Loss: 0.000491\n",
      "Epoch: 59/100... Step: 60540... Loss: 0.000110... Val Loss: 0.000268\n",
      "Epoch: 59/100... Step: 60545... Loss: 0.000077... Val Loss: 0.000420\n",
      "Epoch: 59/100... Step: 60550... Loss: 0.000189... Val Loss: 0.000482\n",
      "Epoch: 59/100... Step: 60555... Loss: 0.000207... Val Loss: 0.000416\n",
      "Epoch: 59/100... Step: 60560... Loss: 0.000187... Val Loss: 0.000333\n",
      "Epoch: 59/100... Step: 60565... Loss: 0.000056... Val Loss: 0.000290\n",
      "Epoch: 59/100... Step: 60570... Loss: 0.000070... Val Loss: 0.000449\n",
      "Epoch: 59/100... Step: 60575... Loss: 0.000058... Val Loss: 0.000468\n",
      "Epoch: 59/100... Step: 60580... Loss: 0.000079... Val Loss: 0.000334\n",
      "Epoch: 59/100... Step: 60585... Loss: 0.000080... Val Loss: 0.000318\n",
      "Epoch: 59/100... Step: 60590... Loss: 0.000187... Val Loss: 0.000338\n",
      "Epoch: 59/100... Step: 60595... Loss: 0.000261... Val Loss: 0.000269\n",
      "Epoch: 59/100... Step: 60600... Loss: 0.000225... Val Loss: 0.000264\n",
      "Epoch: 59/100... Step: 60605... Loss: 0.000136... Val Loss: 0.000430\n",
      "Epoch: 59/100... Step: 60610... Loss: 0.000033... Val Loss: 0.000339\n",
      "Epoch: 59/100... Step: 60615... Loss: 0.000253... Val Loss: 0.000762\n",
      "Epoch: 59/100... Step: 60620... Loss: 0.000100... Val Loss: 0.000341\n",
      "Epoch: 59/100... Step: 60625... Loss: 0.000142... Val Loss: 0.000432\n",
      "Epoch: 59/100... Step: 60630... Loss: 0.000182... Val Loss: 0.000450\n",
      "Epoch: 59/100... Step: 60635... Loss: 0.000237... Val Loss: 0.000499\n",
      "Epoch: 59/100... Step: 60640... Loss: 0.000119... Val Loss: 0.000701\n",
      "Epoch: 59/100... Step: 60645... Loss: 0.000135... Val Loss: 0.000471\n",
      "Epoch: 59/100... Step: 60650... Loss: 0.000178... Val Loss: 0.000395\n",
      "Epoch: 59/100... Step: 60655... Loss: 0.000074... Val Loss: 0.000340\n",
      "Epoch: 59/100... Step: 60660... Loss: 0.000173... Val Loss: 0.000552\n",
      "Epoch: 59/100... Step: 60665... Loss: 0.000109... Val Loss: 0.000721\n",
      "Epoch: 59/100... Step: 60670... Loss: 0.000204... Val Loss: 0.000728\n",
      "Epoch: 59/100... Step: 60675... Loss: 0.000151... Val Loss: 0.000694\n",
      "Epoch: 59/100... Step: 60680... Loss: 0.000228... Val Loss: 0.000552\n",
      "Epoch: 59/100... Step: 60685... Loss: 0.000274... Val Loss: 0.000593\n",
      "Epoch: 59/100... Step: 60690... Loss: 0.000114... Val Loss: 0.000751\n",
      "Epoch: 59/100... Step: 60695... Loss: 0.000173... Val Loss: 0.000700\n",
      "Epoch: 59/100... Step: 60700... Loss: 0.000172... Val Loss: 0.000659\n",
      "Epoch: 59/100... Step: 60705... Loss: 0.000177... Val Loss: 0.000556\n",
      "Epoch: 59/100... Step: 60710... Loss: 0.000210... Val Loss: 0.000609\n",
      "Epoch: 59/100... Step: 60715... Loss: 0.000193... Val Loss: 0.000624\n",
      "Epoch: 59/100... Step: 60720... Loss: 0.000259... Val Loss: 0.000653\n",
      "Epoch: 59/100... Step: 60725... Loss: 0.000250... Val Loss: 0.000952\n",
      "Epoch: 59/100... Step: 60730... Loss: 0.000093... Val Loss: 0.000942\n",
      "Epoch: 59/100... Step: 60735... Loss: 0.000127... Val Loss: 0.000853\n",
      "Epoch: 59/100... Step: 60740... Loss: 0.000243... Val Loss: 0.000967\n",
      "Epoch: 59/100... Step: 60745... Loss: 0.000297... Val Loss: 0.000702\n",
      "Epoch: 59/100... Step: 60750... Loss: 0.000249... Val Loss: 0.000815\n",
      "Epoch: 59/100... Step: 60755... Loss: 0.000266... Val Loss: 0.000650\n",
      "Epoch: 59/100... Step: 60760... Loss: 0.000351... Val Loss: 0.000566\n",
      "Epoch: 59/100... Step: 60765... Loss: 0.000351... Val Loss: 0.000589\n",
      "Epoch: 59/100... Step: 60770... Loss: 0.000378... Val Loss: 0.000483\n",
      "Epoch: 59/100... Step: 60775... Loss: 0.000238... Val Loss: 0.000627\n",
      "Epoch: 59/100... Step: 60780... Loss: 0.000226... Val Loss: 0.000576\n",
      "Epoch: 59/100... Step: 60785... Loss: 0.000604... Val Loss: 0.000910\n",
      "Epoch: 59/100... Step: 60790... Loss: 0.000304... Val Loss: 0.001358\n",
      "Epoch: 59/100... Step: 60795... Loss: 0.000243... Val Loss: 0.000814\n",
      "Epoch: 59/100... Step: 60800... Loss: 0.000466... Val Loss: 0.000748\n",
      "Epoch: 59/100... Step: 60805... Loss: 0.000341... Val Loss: 0.000770\n",
      "Epoch: 59/100... Step: 60810... Loss: 0.000292... Val Loss: 0.000744\n",
      "Epoch: 59/100... Step: 60815... Loss: 0.000232... Val Loss: 0.000865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59/100... Step: 60820... Loss: 0.000124... Val Loss: 0.000933\n",
      "Epoch: 59/100... Step: 60825... Loss: 0.000093... Val Loss: 0.000978\n",
      "Epoch: 59/100... Step: 60830... Loss: 0.000471... Val Loss: 0.001052\n",
      "Epoch: 59/100... Step: 60835... Loss: 0.000437... Val Loss: 0.001415\n",
      "Epoch: 59/100... Step: 60840... Loss: 0.000159... Val Loss: 0.001019\n",
      "Epoch: 59/100... Step: 60845... Loss: 0.000056... Val Loss: 0.000886\n",
      "Epoch: 59/100... Step: 60850... Loss: 0.000262... Val Loss: 0.000948\n",
      "Epoch: 59/100... Step: 60855... Loss: 0.000071... Val Loss: 0.000999\n",
      "Epoch: 59/100... Step: 60860... Loss: 0.000154... Val Loss: 0.000905\n",
      "Epoch: 59/100... Step: 60865... Loss: 0.000151... Val Loss: 0.000821\n",
      "Epoch: 59/100... Step: 60870... Loss: 0.000080... Val Loss: 0.000970\n",
      "Epoch: 59/100... Step: 60875... Loss: 0.000249... Val Loss: 0.000974\n",
      "Epoch: 59/100... Step: 60880... Loss: 0.000132... Val Loss: 0.001184\n",
      "Epoch: 59/100... Step: 60885... Loss: 0.000354... Val Loss: 0.001181\n",
      "Epoch: 60/100... Step: 60890... Loss: 0.000101... Val Loss: 0.001532\n",
      "Epoch: 60/100... Step: 60895... Loss: 0.000219... Val Loss: 0.001683\n",
      "Epoch: 60/100... Step: 60900... Loss: 0.000170... Val Loss: 0.001623\n",
      "Epoch: 60/100... Step: 60905... Loss: 0.000367... Val Loss: 0.002192\n",
      "Epoch: 60/100... Step: 60910... Loss: 0.000341... Val Loss: 0.001551\n",
      "Epoch: 60/100... Step: 60915... Loss: 0.000372... Val Loss: 0.001619\n",
      "Epoch: 60/100... Step: 60920... Loss: 0.000115... Val Loss: 0.001980\n",
      "Epoch: 60/100... Step: 60925... Loss: 0.000160... Val Loss: 0.001774\n",
      "Epoch: 60/100... Step: 60930... Loss: 0.000243... Val Loss: 0.001859\n",
      "Epoch: 60/100... Step: 60935... Loss: 0.000104... Val Loss: 0.001931\n",
      "Epoch: 60/100... Step: 60940... Loss: 0.000117... Val Loss: 0.001807\n",
      "Epoch: 60/100... Step: 60945... Loss: 0.000152... Val Loss: 0.001717\n",
      "Epoch: 60/100... Step: 60950... Loss: 0.000230... Val Loss: 0.001453\n",
      "Epoch: 60/100... Step: 60955... Loss: 0.000083... Val Loss: 0.001549\n",
      "Epoch: 60/100... Step: 60960... Loss: 0.000188... Val Loss: 0.001129\n",
      "Epoch: 60/100... Step: 60965... Loss: 0.000301... Val Loss: 0.001353\n",
      "Epoch: 60/100... Step: 60970... Loss: 0.000241... Val Loss: 0.001504\n",
      "Epoch: 60/100... Step: 60975... Loss: 0.000113... Val Loss: 0.001441\n",
      "Epoch: 60/100... Step: 60980... Loss: 0.000134... Val Loss: 0.001196\n",
      "Epoch: 60/100... Step: 60985... Loss: 0.000352... Val Loss: 0.000999\n",
      "Epoch: 60/100... Step: 60990... Loss: 0.000168... Val Loss: 0.001121\n",
      "Epoch: 60/100... Step: 60995... Loss: 0.000293... Val Loss: 0.001222\n",
      "Epoch: 60/100... Step: 61000... Loss: 0.000357... Val Loss: 0.001309\n",
      "Epoch: 60/100... Step: 61005... Loss: 0.000146... Val Loss: 0.001142\n",
      "Epoch: 60/100... Step: 61010... Loss: 0.000172... Val Loss: 0.001355\n",
      "Epoch: 60/100... Step: 61015... Loss: 0.000270... Val Loss: 0.001171\n",
      "Epoch: 60/100... Step: 61020... Loss: 0.000213... Val Loss: 0.001674\n",
      "Epoch: 60/100... Step: 61025... Loss: 0.000162... Val Loss: 0.001240\n",
      "Epoch: 60/100... Step: 61030... Loss: 0.000139... Val Loss: 0.001548\n",
      "Epoch: 60/100... Step: 61035... Loss: 0.000632... Val Loss: 0.002394\n",
      "Epoch: 60/100... Step: 61040... Loss: 0.000127... Val Loss: 0.001394\n",
      "Epoch: 60/100... Step: 61045... Loss: 0.000194... Val Loss: 0.001692\n",
      "Epoch: 60/100... Step: 61050... Loss: 0.000348... Val Loss: 0.001252\n",
      "Epoch: 60/100... Step: 61055... Loss: 0.000195... Val Loss: 0.001369\n",
      "Epoch: 60/100... Step: 61060... Loss: 0.000302... Val Loss: 0.001296\n",
      "Epoch: 60/100... Step: 61065... Loss: 0.000212... Val Loss: 0.001386\n",
      "Epoch: 60/100... Step: 61070... Loss: 0.000194... Val Loss: 0.001502\n",
      "Epoch: 60/100... Step: 61075... Loss: 0.000355... Val Loss: 0.001217\n",
      "Epoch: 60/100... Step: 61080... Loss: 0.000345... Val Loss: 0.001278\n",
      "Epoch: 60/100... Step: 61085... Loss: 0.000201... Val Loss: 0.001140\n",
      "Epoch: 60/100... Step: 61090... Loss: 0.000258... Val Loss: 0.001285\n",
      "Epoch: 60/100... Step: 61095... Loss: 0.000186... Val Loss: 0.001242\n",
      "Epoch: 60/100... Step: 61100... Loss: 0.000460... Val Loss: 0.001129\n",
      "Epoch: 60/100... Step: 61105... Loss: 0.000298... Val Loss: 0.001496\n",
      "Epoch: 60/100... Step: 61110... Loss: 0.000176... Val Loss: 0.001230\n",
      "Epoch: 60/100... Step: 61115... Loss: 0.000186... Val Loss: 0.001100\n",
      "Epoch: 60/100... Step: 61120... Loss: 0.000252... Val Loss: 0.001025\n",
      "Epoch: 60/100... Step: 61125... Loss: 0.000196... Val Loss: 0.001064\n",
      "Epoch: 60/100... Step: 61130... Loss: 0.000092... Val Loss: 0.001097\n",
      "Epoch: 60/100... Step: 61135... Loss: 0.000117... Val Loss: 0.000980\n",
      "Epoch: 60/100... Step: 61140... Loss: 0.000077... Val Loss: 0.001176\n",
      "Epoch: 60/100... Step: 61145... Loss: 0.000176... Val Loss: 0.000961\n",
      "Epoch: 60/100... Step: 61150... Loss: 0.000092... Val Loss: 0.000847\n",
      "Epoch: 60/100... Step: 61155... Loss: 0.000182... Val Loss: 0.000895\n",
      "Epoch: 60/100... Step: 61160... Loss: 0.000085... Val Loss: 0.000909\n",
      "Epoch: 60/100... Step: 61165... Loss: 0.000145... Val Loss: 0.000926\n",
      "Epoch: 60/100... Step: 61170... Loss: 0.000171... Val Loss: 0.001042\n",
      "Epoch: 60/100... Step: 61175... Loss: 0.000122... Val Loss: 0.000776\n",
      "Epoch: 60/100... Step: 61180... Loss: 0.000193... Val Loss: 0.000618\n",
      "Epoch: 60/100... Step: 61185... Loss: 0.000234... Val Loss: 0.000536\n",
      "Epoch: 60/100... Step: 61190... Loss: 0.000220... Val Loss: 0.000659\n",
      "Epoch: 60/100... Step: 61195... Loss: 0.000367... Val Loss: 0.000596\n",
      "Epoch: 60/100... Step: 61200... Loss: 0.000202... Val Loss: 0.000499\n",
      "Epoch: 60/100... Step: 61205... Loss: 0.000120... Val Loss: 0.000581\n",
      "Epoch: 60/100... Step: 61210... Loss: 0.000061... Val Loss: 0.000688\n",
      "Epoch: 60/100... Step: 61215... Loss: 0.000234... Val Loss: 0.000473\n",
      "Epoch: 60/100... Step: 61220... Loss: 0.000116... Val Loss: 0.000330\n",
      "Epoch: 60/100... Step: 61225... Loss: 0.000239... Val Loss: 0.000642\n",
      "Epoch: 60/100... Step: 61230... Loss: 0.000279... Val Loss: 0.000827\n",
      "Epoch: 60/100... Step: 61235... Loss: 0.000226... Val Loss: 0.000398\n",
      "Epoch: 60/100... Step: 61240... Loss: 0.000193... Val Loss: 0.000499\n",
      "Epoch: 60/100... Step: 61245... Loss: 0.000223... Val Loss: 0.000719\n",
      "Epoch: 60/100... Step: 61250... Loss: 0.000181... Val Loss: 0.000841\n",
      "Epoch: 60/100... Step: 61255... Loss: 0.000245... Val Loss: 0.000914\n",
      "Epoch: 60/100... Step: 61260... Loss: 0.000127... Val Loss: 0.000978\n",
      "Epoch: 60/100... Step: 61265... Loss: 0.000105... Val Loss: 0.001041\n",
      "Epoch: 60/100... Step: 61270... Loss: 0.000129... Val Loss: 0.000903\n",
      "Epoch: 60/100... Step: 61275... Loss: 0.000164... Val Loss: 0.000839\n",
      "Epoch: 60/100... Step: 61280... Loss: 0.000152... Val Loss: 0.000848\n",
      "Epoch: 60/100... Step: 61285... Loss: 0.000186... Val Loss: 0.001016\n",
      "Epoch: 60/100... Step: 61290... Loss: 0.000155... Val Loss: 0.001030\n",
      "Epoch: 60/100... Step: 61295... Loss: 0.000354... Val Loss: 0.000674\n",
      "Epoch: 60/100... Step: 61300... Loss: 0.000449... Val Loss: 0.000561\n",
      "Epoch: 60/100... Step: 61305... Loss: 0.000565... Val Loss: 0.001227\n",
      "Epoch: 60/100... Step: 61310... Loss: 0.000386... Val Loss: 0.000587\n",
      "Epoch: 60/100... Step: 61315... Loss: 0.000235... Val Loss: 0.000950\n",
      "Epoch: 60/100... Step: 61320... Loss: 0.000318... Val Loss: 0.001221\n",
      "Epoch: 60/100... Step: 61325... Loss: 0.000238... Val Loss: 0.001204\n",
      "Epoch: 60/100... Step: 61330... Loss: 0.000138... Val Loss: 0.001014\n",
      "Epoch: 60/100... Step: 61335... Loss: 0.000100... Val Loss: 0.001191\n",
      "Epoch: 60/100... Step: 61340... Loss: 0.000126... Val Loss: 0.001369\n",
      "Epoch: 60/100... Step: 61345... Loss: 0.000239... Val Loss: 0.001411\n",
      "Epoch: 60/100... Step: 61350... Loss: 0.000205... Val Loss: 0.001504\n",
      "Epoch: 60/100... Step: 61355... Loss: 0.000305... Val Loss: 0.001494\n",
      "Epoch: 60/100... Step: 61360... Loss: 0.000104... Val Loss: 0.001230\n",
      "Epoch: 60/100... Step: 61365... Loss: 0.000178... Val Loss: 0.001129\n",
      "Epoch: 60/100... Step: 61370... Loss: 0.000293... Val Loss: 0.001223\n",
      "Epoch: 60/100... Step: 61375... Loss: 0.000511... Val Loss: 0.000639\n",
      "Epoch: 60/100... Step: 61380... Loss: 0.000282... Val Loss: 0.001123\n",
      "Epoch: 60/100... Step: 61385... Loss: 0.000199... Val Loss: 0.000590\n",
      "Epoch: 60/100... Step: 61390... Loss: 0.000122... Val Loss: 0.000908\n",
      "Epoch: 60/100... Step: 61395... Loss: 0.000146... Val Loss: 0.000355\n",
      "Epoch: 60/100... Step: 61400... Loss: 0.000095... Val Loss: 0.000490\n",
      "Epoch: 60/100... Step: 61405... Loss: 0.000184... Val Loss: 0.000469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60/100... Step: 61410... Loss: 0.000124... Val Loss: 0.000594\n",
      "Epoch: 60/100... Step: 61415... Loss: 0.000297... Val Loss: 0.000716\n",
      "Epoch: 60/100... Step: 61420... Loss: 0.000161... Val Loss: 0.000508\n",
      "Epoch: 60/100... Step: 61425... Loss: 0.000275... Val Loss: 0.000732\n",
      "Epoch: 60/100... Step: 61430... Loss: 0.000124... Val Loss: 0.000751\n",
      "Epoch: 60/100... Step: 61435... Loss: 0.000156... Val Loss: 0.000896\n",
      "Epoch: 60/100... Step: 61440... Loss: 0.000107... Val Loss: 0.000745\n",
      "Epoch: 60/100... Step: 61445... Loss: 0.000192... Val Loss: 0.000651\n",
      "Epoch: 60/100... Step: 61450... Loss: 0.000261... Val Loss: 0.000938\n",
      "Epoch: 60/100... Step: 61455... Loss: 0.000297... Val Loss: 0.001063\n",
      "Epoch: 60/100... Step: 61460... Loss: 0.000270... Val Loss: 0.000975\n",
      "Epoch: 60/100... Step: 61465... Loss: 0.000398... Val Loss: 0.000834\n",
      "Epoch: 60/100... Step: 61470... Loss: 0.000088... Val Loss: 0.000955\n",
      "Epoch: 60/100... Step: 61475... Loss: 0.000253... Val Loss: 0.000871\n",
      "Epoch: 60/100... Step: 61480... Loss: 0.000169... Val Loss: 0.000982\n",
      "Epoch: 60/100... Step: 61485... Loss: 0.000251... Val Loss: 0.001109\n",
      "Epoch: 60/100... Step: 61490... Loss: 0.000288... Val Loss: 0.000941\n",
      "Epoch: 60/100... Step: 61495... Loss: 0.000193... Val Loss: 0.001104\n",
      "Epoch: 60/100... Step: 61500... Loss: 0.000161... Val Loss: 0.000700\n",
      "Epoch: 60/100... Step: 61505... Loss: 0.000272... Val Loss: 0.000693\n",
      "Epoch: 60/100... Step: 61510... Loss: 0.000282... Val Loss: 0.001005\n",
      "Epoch: 60/100... Step: 61515... Loss: 0.000226... Val Loss: 0.000821\n",
      "Epoch: 60/100... Step: 61520... Loss: 0.000261... Val Loss: 0.000743\n",
      "Epoch: 60/100... Step: 61525... Loss: 0.000223... Val Loss: 0.000652\n",
      "Epoch: 60/100... Step: 61530... Loss: 0.000152... Val Loss: 0.000637\n",
      "Epoch: 60/100... Step: 61535... Loss: 0.000293... Val Loss: 0.000891\n",
      "Epoch: 60/100... Step: 61540... Loss: 0.000366... Val Loss: 0.000907\n",
      "Epoch: 60/100... Step: 61545... Loss: 0.000100... Val Loss: 0.001015\n",
      "Epoch: 60/100... Step: 61550... Loss: 0.000085... Val Loss: 0.000843\n",
      "Epoch: 60/100... Step: 61555... Loss: 0.000161... Val Loss: 0.000409\n",
      "Epoch: 60/100... Step: 61560... Loss: 0.000082... Val Loss: 0.000798\n",
      "Epoch: 60/100... Step: 61565... Loss: 0.000316... Val Loss: 0.000482\n",
      "Epoch: 60/100... Step: 61570... Loss: 0.000189... Val Loss: 0.000270\n",
      "Epoch: 60/100... Step: 61575... Loss: 0.000297... Val Loss: 0.000266\n",
      "Epoch: 60/100... Step: 61580... Loss: 0.000198... Val Loss: 0.000302\n",
      "Epoch: 60/100... Step: 61585... Loss: 0.000245... Val Loss: 0.000259\n",
      "Epoch: 60/100... Step: 61590... Loss: 0.000124... Val Loss: 0.000345\n",
      "Epoch: 60/100... Step: 61595... Loss: 0.000169... Val Loss: 0.000287\n",
      "Epoch: 60/100... Step: 61600... Loss: 0.000200... Val Loss: 0.000328\n",
      "Epoch: 60/100... Step: 61605... Loss: 0.000056... Val Loss: 0.000358\n",
      "Epoch: 60/100... Step: 61610... Loss: 0.000176... Val Loss: 0.000518\n",
      "Epoch: 60/100... Step: 61615... Loss: 0.000412... Val Loss: 0.000329\n",
      "Epoch: 60/100... Step: 61620... Loss: 0.000389... Val Loss: 0.000660\n",
      "Epoch: 60/100... Step: 61625... Loss: 0.000569... Val Loss: 0.000389\n",
      "Epoch: 60/100... Step: 61630... Loss: 0.000226... Val Loss: 0.000541\n",
      "Epoch: 60/100... Step: 61635... Loss: 0.000199... Val Loss: 0.000643\n",
      "Epoch: 60/100... Step: 61640... Loss: 0.000358... Val Loss: 0.002746\n",
      "Epoch: 60/100... Step: 61645... Loss: 0.000198... Val Loss: 0.000673\n",
      "Epoch: 60/100... Step: 61650... Loss: 0.000440... Val Loss: 0.000513\n",
      "Epoch: 60/100... Step: 61655... Loss: 0.000306... Val Loss: 0.000470\n",
      "Epoch: 60/100... Step: 61660... Loss: 0.000213... Val Loss: 0.000620\n",
      "Epoch: 60/100... Step: 61665... Loss: 0.000193... Val Loss: 0.000582\n",
      "Epoch: 60/100... Step: 61670... Loss: 0.000469... Val Loss: 0.000470\n",
      "Epoch: 60/100... Step: 61675... Loss: 0.000327... Val Loss: 0.001124\n",
      "Epoch: 60/100... Step: 61680... Loss: 0.000150... Val Loss: 0.000485\n",
      "Epoch: 60/100... Step: 61685... Loss: 0.000109... Val Loss: 0.000642\n",
      "Epoch: 60/100... Step: 61690... Loss: 0.000145... Val Loss: 0.000477\n",
      "Epoch: 60/100... Step: 61695... Loss: 0.000397... Val Loss: 0.000741\n",
      "Epoch: 60/100... Step: 61700... Loss: 0.000267... Val Loss: 0.000733\n",
      "Epoch: 60/100... Step: 61705... Loss: 0.000396... Val Loss: 0.000504\n",
      "Epoch: 60/100... Step: 61710... Loss: 0.000126... Val Loss: 0.000775\n",
      "Epoch: 60/100... Step: 61715... Loss: 0.000171... Val Loss: 0.000778\n",
      "Epoch: 60/100... Step: 61720... Loss: 0.000399... Val Loss: 0.000937\n",
      "Epoch: 60/100... Step: 61725... Loss: 0.000230... Val Loss: 0.000606\n",
      "Epoch: 60/100... Step: 61730... Loss: 0.000148... Val Loss: 0.000495\n",
      "Epoch: 60/100... Step: 61735... Loss: 0.000352... Val Loss: 0.000403\n",
      "Epoch: 60/100... Step: 61740... Loss: 0.000516... Val Loss: 0.000585\n",
      "Epoch: 60/100... Step: 61745... Loss: 0.000663... Val Loss: 0.000541\n",
      "Epoch: 60/100... Step: 61750... Loss: 0.000342... Val Loss: 0.000740\n",
      "Epoch: 60/100... Step: 61755... Loss: 0.000311... Val Loss: 0.000439\n",
      "Epoch: 60/100... Step: 61760... Loss: 0.000343... Val Loss: 0.000812\n",
      "Epoch: 60/100... Step: 61765... Loss: 0.000358... Val Loss: 0.000869\n",
      "Epoch: 60/100... Step: 61770... Loss: 0.000317... Val Loss: 0.000867\n",
      "Epoch: 60/100... Step: 61775... Loss: 0.000167... Val Loss: 0.000710\n",
      "Epoch: 60/100... Step: 61780... Loss: 0.000397... Val Loss: 0.000996\n",
      "Epoch: 60/100... Step: 61785... Loss: 0.000408... Val Loss: 0.000822\n",
      "Epoch: 60/100... Step: 61790... Loss: 0.000497... Val Loss: 0.000589\n",
      "Epoch: 60/100... Step: 61795... Loss: 0.000075... Val Loss: 0.000484\n",
      "Epoch: 60/100... Step: 61800... Loss: 0.000538... Val Loss: 0.000467\n",
      "Epoch: 60/100... Step: 61805... Loss: 0.000380... Val Loss: 0.000871\n",
      "Epoch: 60/100... Step: 61810... Loss: 0.000211... Val Loss: 0.000863\n",
      "Epoch: 60/100... Step: 61815... Loss: 0.000216... Val Loss: 0.000727\n",
      "Epoch: 60/100... Step: 61820... Loss: 0.000170... Val Loss: 0.000965\n",
      "Epoch: 60/100... Step: 61825... Loss: 0.000148... Val Loss: 0.000720\n",
      "Epoch: 60/100... Step: 61830... Loss: 0.000297... Val Loss: 0.001257\n",
      "Epoch: 60/100... Step: 61835... Loss: 0.000136... Val Loss: 0.000648\n",
      "Epoch: 60/100... Step: 61840... Loss: 0.000248... Val Loss: 0.000752\n",
      "Epoch: 60/100... Step: 61845... Loss: 0.000582... Val Loss: 0.001019\n",
      "Epoch: 60/100... Step: 61850... Loss: 0.000556... Val Loss: 0.001301\n",
      "Epoch: 60/100... Step: 61855... Loss: 0.000529... Val Loss: 0.001435\n",
      "Epoch: 60/100... Step: 61860... Loss: 0.000109... Val Loss: 0.000927\n",
      "Epoch: 60/100... Step: 61865... Loss: 0.000322... Val Loss: 0.000831\n",
      "Epoch: 60/100... Step: 61870... Loss: 0.000271... Val Loss: 0.001249\n",
      "Epoch: 60/100... Step: 61875... Loss: 0.000191... Val Loss: 0.000981\n",
      "Epoch: 60/100... Step: 61880... Loss: 0.000159... Val Loss: 0.001023\n",
      "Epoch: 60/100... Step: 61885... Loss: 0.000476... Val Loss: 0.000720\n",
      "Epoch: 60/100... Step: 61890... Loss: 0.000202... Val Loss: 0.001272\n",
      "Epoch: 60/100... Step: 61895... Loss: 0.000193... Val Loss: 0.000988\n",
      "Epoch: 60/100... Step: 61900... Loss: 0.000270... Val Loss: 0.001151\n",
      "Epoch: 60/100... Step: 61905... Loss: 0.000289... Val Loss: 0.000960\n",
      "Epoch: 60/100... Step: 61910... Loss: 0.000201... Val Loss: 0.000876\n",
      "Epoch: 60/100... Step: 61915... Loss: 0.000383... Val Loss: 0.001170\n",
      "Epoch: 60/100... Step: 61920... Loss: 0.000353... Val Loss: 0.000845\n",
      "Epoch: 61/100... Step: 61925... Loss: 0.000286... Val Loss: 0.001413\n",
      "Epoch: 61/100... Step: 61930... Loss: 0.000523... Val Loss: 0.001707\n",
      "Epoch: 61/100... Step: 61935... Loss: 0.000392... Val Loss: 0.001739\n",
      "Epoch: 61/100... Step: 61940... Loss: 0.000287... Val Loss: 0.002052\n",
      "Epoch: 61/100... Step: 61945... Loss: 0.000253... Val Loss: 0.001448\n",
      "Epoch: 61/100... Step: 61950... Loss: 0.000216... Val Loss: 0.001710\n",
      "Epoch: 61/100... Step: 61955... Loss: 0.000240... Val Loss: 0.001817\n",
      "Epoch: 61/100... Step: 61960... Loss: 0.000368... Val Loss: 0.001418\n",
      "Epoch: 61/100... Step: 61965... Loss: 0.000294... Val Loss: 0.001777\n",
      "Epoch: 61/100... Step: 61970... Loss: 0.000327... Val Loss: 0.001833\n",
      "Epoch: 61/100... Step: 61975... Loss: 0.000327... Val Loss: 0.001903\n",
      "Epoch: 61/100... Step: 61980... Loss: 0.000353... Val Loss: 0.001628\n",
      "Epoch: 61/100... Step: 61985... Loss: 0.000247... Val Loss: 0.001164\n",
      "Epoch: 61/100... Step: 61990... Loss: 0.000292... Val Loss: 0.001333\n",
      "Epoch: 61/100... Step: 61995... Loss: 0.000198... Val Loss: 0.001186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61/100... Step: 62000... Loss: 0.000250... Val Loss: 0.001229\n",
      "Epoch: 61/100... Step: 62005... Loss: 0.000275... Val Loss: 0.001452\n",
      "Epoch: 61/100... Step: 62010... Loss: 0.000347... Val Loss: 0.001473\n",
      "Epoch: 61/100... Step: 62015... Loss: 0.000374... Val Loss: 0.001137\n",
      "Epoch: 61/100... Step: 62020... Loss: 0.000140... Val Loss: 0.001067\n",
      "Epoch: 61/100... Step: 62025... Loss: 0.000239... Val Loss: 0.000998\n",
      "Epoch: 61/100... Step: 62030... Loss: 0.000256... Val Loss: 0.000955\n",
      "Epoch: 61/100... Step: 62035... Loss: 0.000345... Val Loss: 0.000946\n",
      "Epoch: 61/100... Step: 62040... Loss: 0.000185... Val Loss: 0.001124\n",
      "Epoch: 61/100... Step: 62045... Loss: 0.000254... Val Loss: 0.001593\n",
      "Epoch: 61/100... Step: 62050... Loss: 0.000070... Val Loss: 0.001521\n",
      "Epoch: 61/100... Step: 62055... Loss: 0.000324... Val Loss: 0.001357\n",
      "Epoch: 61/100... Step: 62060... Loss: 0.000206... Val Loss: 0.001152\n",
      "Epoch: 61/100... Step: 62065... Loss: 0.000217... Val Loss: 0.001749\n",
      "Epoch: 61/100... Step: 62070... Loss: 0.000154... Val Loss: 0.001491\n",
      "Epoch: 61/100... Step: 62075... Loss: 0.000096... Val Loss: 0.001666\n",
      "Epoch: 61/100... Step: 62080... Loss: 0.000104... Val Loss: 0.001467\n",
      "Epoch: 61/100... Step: 62085... Loss: 0.000138... Val Loss: 0.001362\n",
      "Epoch: 61/100... Step: 62090... Loss: 0.000160... Val Loss: 0.001473\n",
      "Epoch: 61/100... Step: 62095... Loss: 0.000103... Val Loss: 0.001320\n",
      "Epoch: 61/100... Step: 62100... Loss: 0.000134... Val Loss: 0.001432\n",
      "Epoch: 61/100... Step: 62105... Loss: 0.000250... Val Loss: 0.001439\n",
      "Epoch: 61/100... Step: 62110... Loss: 0.000181... Val Loss: 0.001349\n",
      "Epoch: 61/100... Step: 62115... Loss: 0.000260... Val Loss: 0.001217\n",
      "Epoch: 61/100... Step: 62120... Loss: 0.000134... Val Loss: 0.001076\n",
      "Epoch: 61/100... Step: 62125... Loss: 0.000152... Val Loss: 0.001085\n",
      "Epoch: 61/100... Step: 62130... Loss: 0.000144... Val Loss: 0.001258\n",
      "Epoch: 61/100... Step: 62135... Loss: 0.000207... Val Loss: 0.001351\n",
      "Epoch: 61/100... Step: 62140... Loss: 0.000148... Val Loss: 0.001170\n",
      "Epoch: 61/100... Step: 62145... Loss: 0.000177... Val Loss: 0.001249\n",
      "Epoch: 61/100... Step: 62150... Loss: 0.000292... Val Loss: 0.001079\n",
      "Epoch: 61/100... Step: 62155... Loss: 0.000145... Val Loss: 0.001106\n",
      "Epoch: 61/100... Step: 62160... Loss: 0.000262... Val Loss: 0.001221\n",
      "Epoch: 61/100... Step: 62165... Loss: 0.000199... Val Loss: 0.001092\n",
      "Epoch: 61/100... Step: 62170... Loss: 0.000059... Val Loss: 0.001031\n",
      "Epoch: 61/100... Step: 62175... Loss: 0.000184... Val Loss: 0.001074\n",
      "Epoch: 61/100... Step: 62180... Loss: 0.000156... Val Loss: 0.000885\n",
      "Epoch: 61/100... Step: 62185... Loss: 0.000060... Val Loss: 0.000855\n",
      "Epoch: 61/100... Step: 62190... Loss: 0.000109... Val Loss: 0.000880\n",
      "Epoch: 61/100... Step: 62195... Loss: 0.000295... Val Loss: 0.000774\n",
      "Epoch: 61/100... Step: 62200... Loss: 0.000182... Val Loss: 0.000740\n",
      "Epoch: 61/100... Step: 62205... Loss: 0.000157... Val Loss: 0.000806\n",
      "Epoch: 61/100... Step: 62210... Loss: 0.000184... Val Loss: 0.000843\n",
      "Epoch: 61/100... Step: 62215... Loss: 0.000145... Val Loss: 0.000711\n",
      "Epoch: 61/100... Step: 62220... Loss: 0.000051... Val Loss: 0.000625\n",
      "Epoch: 61/100... Step: 62225... Loss: 0.000118... Val Loss: 0.000836\n",
      "Epoch: 61/100... Step: 62230... Loss: 0.000181... Val Loss: 0.000553\n",
      "Epoch: 61/100... Step: 62235... Loss: 0.000369... Val Loss: 0.000559\n",
      "Epoch: 61/100... Step: 62240... Loss: 0.000140... Val Loss: 0.000709\n",
      "Epoch: 61/100... Step: 62245... Loss: 0.000137... Val Loss: 0.000545\n",
      "Epoch: 61/100... Step: 62250... Loss: 0.000124... Val Loss: 0.000572\n",
      "Epoch: 61/100... Step: 62255... Loss: 0.000243... Val Loss: 0.000451\n",
      "Epoch: 61/100... Step: 62260... Loss: 0.000131... Val Loss: 0.000812\n",
      "Epoch: 61/100... Step: 62265... Loss: 0.000245... Val Loss: 0.000527\n",
      "Epoch: 61/100... Step: 62270... Loss: 0.000166... Val Loss: 0.000418\n",
      "Epoch: 61/100... Step: 62275... Loss: 0.000206... Val Loss: 0.000647\n",
      "Epoch: 61/100... Step: 62280... Loss: 0.000126... Val Loss: 0.000780\n",
      "Epoch: 61/100... Step: 62285... Loss: 0.000168... Val Loss: 0.000954\n",
      "Epoch: 61/100... Step: 62290... Loss: 0.000088... Val Loss: 0.000979\n",
      "Epoch: 61/100... Step: 62295... Loss: 0.000090... Val Loss: 0.000995\n",
      "Epoch: 61/100... Step: 62300... Loss: 0.000094... Val Loss: 0.001051\n",
      "Epoch: 61/100... Step: 62305... Loss: 0.000083... Val Loss: 0.001109\n",
      "Epoch: 61/100... Step: 62310... Loss: 0.000135... Val Loss: 0.001042\n",
      "Epoch: 61/100... Step: 62315... Loss: 0.000177... Val Loss: 0.001071\n",
      "Epoch: 61/100... Step: 62320... Loss: 0.000152... Val Loss: 0.001091\n",
      "Epoch: 61/100... Step: 62325... Loss: 0.000350... Val Loss: 0.000890\n",
      "Epoch: 61/100... Step: 62330... Loss: 0.000220... Val Loss: 0.000339\n",
      "Epoch: 61/100... Step: 62335... Loss: 0.000757... Val Loss: 0.000262\n",
      "Epoch: 61/100... Step: 62340... Loss: 0.000450... Val Loss: 0.001528\n",
      "Epoch: 61/100... Step: 62345... Loss: 0.000172... Val Loss: 0.001171\n",
      "Epoch: 61/100... Step: 62350... Loss: 0.000297... Val Loss: 0.000906\n",
      "Epoch: 61/100... Step: 62355... Loss: 0.000254... Val Loss: 0.001038\n",
      "Epoch: 61/100... Step: 62360... Loss: 0.000163... Val Loss: 0.001065\n",
      "Epoch: 61/100... Step: 62365... Loss: 0.000151... Val Loss: 0.001178\n",
      "Epoch: 61/100... Step: 62370... Loss: 0.000128... Val Loss: 0.001266\n",
      "Epoch: 61/100... Step: 62375... Loss: 0.000114... Val Loss: 0.001327\n",
      "Epoch: 61/100... Step: 62380... Loss: 0.000200... Val Loss: 0.001453\n",
      "Epoch: 61/100... Step: 62385... Loss: 0.000119... Val Loss: 0.001573\n",
      "Epoch: 61/100... Step: 62390... Loss: 0.000130... Val Loss: 0.001144\n",
      "Epoch: 61/100... Step: 62395... Loss: 0.000144... Val Loss: 0.001035\n",
      "Epoch: 61/100... Step: 62400... Loss: 0.000101... Val Loss: 0.001123\n",
      "Epoch: 61/100... Step: 62405... Loss: 0.000165... Val Loss: 0.000969\n",
      "Epoch: 61/100... Step: 62410... Loss: 0.000091... Val Loss: 0.000658\n",
      "Epoch: 61/100... Step: 62415... Loss: 0.000196... Val Loss: 0.000931\n",
      "Epoch: 61/100... Step: 62420... Loss: 0.000158... Val Loss: 0.000956\n",
      "Epoch: 61/100... Step: 62425... Loss: 0.000178... Val Loss: 0.000370\n",
      "Epoch: 61/100... Step: 62430... Loss: 0.000237... Val Loss: 0.000503\n",
      "Epoch: 61/100... Step: 62435... Loss: 0.000137... Val Loss: 0.000362\n",
      "Epoch: 61/100... Step: 62440... Loss: 0.000222... Val Loss: 0.000480\n",
      "Epoch: 61/100... Step: 62445... Loss: 0.000145... Val Loss: 0.000725\n",
      "Epoch: 61/100... Step: 62450... Loss: 0.000116... Val Loss: 0.000774\n",
      "Epoch: 61/100... Step: 62455... Loss: 0.000090... Val Loss: 0.000601\n",
      "Epoch: 61/100... Step: 62460... Loss: 0.000088... Val Loss: 0.000774\n",
      "Epoch: 61/100... Step: 62465... Loss: 0.000104... Val Loss: 0.000832\n",
      "Epoch: 61/100... Step: 62470... Loss: 0.000186... Val Loss: 0.000908\n",
      "Epoch: 61/100... Step: 62475... Loss: 0.000130... Val Loss: 0.000821\n",
      "Epoch: 61/100... Step: 62480... Loss: 0.000118... Val Loss: 0.000633\n",
      "Epoch: 61/100... Step: 62485... Loss: 0.000080... Val Loss: 0.000895\n",
      "Epoch: 61/100... Step: 62490... Loss: 0.000231... Val Loss: 0.000784\n",
      "Epoch: 61/100... Step: 62495... Loss: 0.000246... Val Loss: 0.000944\n",
      "Epoch: 61/100... Step: 62500... Loss: 0.000157... Val Loss: 0.001238\n",
      "Epoch: 61/100... Step: 62505... Loss: 0.000230... Val Loss: 0.000751\n",
      "Epoch: 61/100... Step: 62510... Loss: 0.000178... Val Loss: 0.000714\n",
      "Epoch: 61/100... Step: 62515... Loss: 0.000288... Val Loss: 0.001248\n",
      "Epoch: 61/100... Step: 62520... Loss: 0.000143... Val Loss: 0.001019\n",
      "Epoch: 61/100... Step: 62525... Loss: 0.000076... Val Loss: 0.000871\n",
      "Epoch: 61/100... Step: 62530... Loss: 0.000220... Val Loss: 0.000908\n",
      "Epoch: 61/100... Step: 62535... Loss: 0.000242... Val Loss: 0.000757\n",
      "Epoch: 61/100... Step: 62540... Loss: 0.000204... Val Loss: 0.001061\n",
      "Epoch: 61/100... Step: 62545... Loss: 0.000183... Val Loss: 0.000722\n",
      "Epoch: 61/100... Step: 62550... Loss: 0.000287... Val Loss: 0.000689\n",
      "Epoch: 61/100... Step: 62555... Loss: 0.000075... Val Loss: 0.000608\n",
      "Epoch: 61/100... Step: 62560... Loss: 0.000272... Val Loss: 0.001011\n",
      "Epoch: 61/100... Step: 62565... Loss: 0.000098... Val Loss: 0.000600\n",
      "Epoch: 61/100... Step: 62570... Loss: 0.000196... Val Loss: 0.000569\n",
      "Epoch: 61/100... Step: 62575... Loss: 0.000152... Val Loss: 0.000882\n",
      "Epoch: 61/100... Step: 62580... Loss: 0.000192... Val Loss: 0.000776\n",
      "Epoch: 61/100... Step: 62585... Loss: 0.000214... Val Loss: 0.000790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61/100... Step: 62590... Loss: 0.000114... Val Loss: 0.000385\n",
      "Epoch: 61/100... Step: 62595... Loss: 0.000154... Val Loss: 0.000447\n",
      "Epoch: 61/100... Step: 62600... Loss: 0.000157... Val Loss: 0.000348\n",
      "Epoch: 61/100... Step: 62605... Loss: 0.000115... Val Loss: 0.000340\n",
      "Epoch: 61/100... Step: 62610... Loss: 0.000087... Val Loss: 0.000300\n",
      "Epoch: 61/100... Step: 62615... Loss: 0.000169... Val Loss: 0.000285\n",
      "Epoch: 61/100... Step: 62620... Loss: 0.000127... Val Loss: 0.000287\n",
      "Epoch: 61/100... Step: 62625... Loss: 0.000206... Val Loss: 0.000263\n",
      "Epoch: 61/100... Step: 62630... Loss: 0.000194... Val Loss: 0.000339\n",
      "Epoch: 61/100... Step: 62635... Loss: 0.000065... Val Loss: 0.000314\n",
      "Epoch: 61/100... Step: 62640... Loss: 0.000072... Val Loss: 0.000388\n",
      "Epoch: 61/100... Step: 62645... Loss: 0.000085... Val Loss: 0.000420\n",
      "Epoch: 61/100... Step: 62650... Loss: 0.000066... Val Loss: 0.000367\n",
      "Epoch: 61/100... Step: 62655... Loss: 0.000103... Val Loss: 0.000397\n",
      "Epoch: 61/100... Step: 62660... Loss: 0.000222... Val Loss: 0.000305\n",
      "Epoch: 61/100... Step: 62665... Loss: 0.000157... Val Loss: 0.000504\n",
      "Epoch: 61/100... Step: 62670... Loss: 0.000166... Val Loss: 0.000281\n",
      "Epoch: 61/100... Step: 62675... Loss: 0.000154... Val Loss: 0.000668\n",
      "Epoch: 61/100... Step: 62680... Loss: 0.000134... Val Loss: 0.000321\n",
      "Epoch: 61/100... Step: 62685... Loss: 0.000171... Val Loss: 0.000414\n",
      "Epoch: 61/100... Step: 62690... Loss: 0.000087... Val Loss: 0.000350\n",
      "Epoch: 61/100... Step: 62695... Loss: 0.000056... Val Loss: 0.000440\n",
      "Epoch: 61/100... Step: 62700... Loss: 0.000149... Val Loss: 0.000554\n",
      "Epoch: 61/100... Step: 62705... Loss: 0.000026... Val Loss: 0.000554\n",
      "Epoch: 61/100... Step: 62710... Loss: 0.000261... Val Loss: 0.000463\n",
      "Epoch: 61/100... Step: 62715... Loss: 0.000168... Val Loss: 0.000305\n",
      "Epoch: 61/100... Step: 62720... Loss: 0.000165... Val Loss: 0.000357\n",
      "Epoch: 61/100... Step: 62725... Loss: 0.000130... Val Loss: 0.000566\n",
      "Epoch: 61/100... Step: 62730... Loss: 0.000061... Val Loss: 0.000657\n",
      "Epoch: 61/100... Step: 62735... Loss: 0.000113... Val Loss: 0.000660\n",
      "Epoch: 61/100... Step: 62740... Loss: 0.000130... Val Loss: 0.000551\n",
      "Epoch: 61/100... Step: 62745... Loss: 0.000107... Val Loss: 0.000599\n",
      "Epoch: 61/100... Step: 62750... Loss: 0.000211... Val Loss: 0.000349\n",
      "Epoch: 61/100... Step: 62755... Loss: 0.000341... Val Loss: 0.000771\n",
      "Epoch: 61/100... Step: 62760... Loss: 0.000128... Val Loss: 0.000774\n",
      "Epoch: 61/100... Step: 62765... Loss: 0.000230... Val Loss: 0.000493\n",
      "Epoch: 61/100... Step: 62770... Loss: 0.000118... Val Loss: 0.000491\n",
      "Epoch: 61/100... Step: 62775... Loss: 0.000102... Val Loss: 0.000685\n",
      "Epoch: 61/100... Step: 62780... Loss: 0.000184... Val Loss: 0.000921\n",
      "Epoch: 61/100... Step: 62785... Loss: 0.000108... Val Loss: 0.000674\n",
      "Epoch: 61/100... Step: 62790... Loss: 0.000149... Val Loss: 0.000871\n",
      "Epoch: 61/100... Step: 62795... Loss: 0.000056... Val Loss: 0.000718\n",
      "Epoch: 61/100... Step: 62800... Loss: 0.000077... Val Loss: 0.000791\n",
      "Epoch: 61/100... Step: 62805... Loss: 0.000224... Val Loss: 0.000689\n",
      "Epoch: 61/100... Step: 62810... Loss: 0.000105... Val Loss: 0.000618\n",
      "Epoch: 61/100... Step: 62815... Loss: 0.000091... Val Loss: 0.000804\n",
      "Epoch: 61/100... Step: 62820... Loss: 0.000067... Val Loss: 0.000716\n",
      "Epoch: 61/100... Step: 62825... Loss: 0.000078... Val Loss: 0.000627\n",
      "Epoch: 61/100... Step: 62830... Loss: 0.000157... Val Loss: 0.000504\n",
      "Epoch: 61/100... Step: 62835... Loss: 0.000251... Val Loss: 0.000555\n",
      "Epoch: 61/100... Step: 62840... Loss: 0.000150... Val Loss: 0.000587\n",
      "Epoch: 61/100... Step: 62845... Loss: 0.000147... Val Loss: 0.000806\n",
      "Epoch: 61/100... Step: 62850... Loss: 0.000104... Val Loss: 0.000688\n",
      "Epoch: 61/100... Step: 62855... Loss: 0.000114... Val Loss: 0.001114\n",
      "Epoch: 61/100... Step: 62860... Loss: 0.000137... Val Loss: 0.000815\n",
      "Epoch: 61/100... Step: 62865... Loss: 0.000076... Val Loss: 0.000562\n",
      "Epoch: 61/100... Step: 62870... Loss: 0.000096... Val Loss: 0.000626\n",
      "Epoch: 61/100... Step: 62875... Loss: 0.000123... Val Loss: 0.000952\n",
      "Epoch: 61/100... Step: 62880... Loss: 0.000233... Val Loss: 0.001072\n",
      "Epoch: 61/100... Step: 62885... Loss: 0.000446... Val Loss: 0.000698\n",
      "Epoch: 61/100... Step: 62890... Loss: 0.000087... Val Loss: 0.001023\n",
      "Epoch: 61/100... Step: 62895... Loss: 0.000131... Val Loss: 0.000989\n",
      "Epoch: 61/100... Step: 62900... Loss: 0.000253... Val Loss: 0.001018\n",
      "Epoch: 61/100... Step: 62905... Loss: 0.000263... Val Loss: 0.001045\n",
      "Epoch: 61/100... Step: 62910... Loss: 0.000093... Val Loss: 0.001298\n",
      "Epoch: 61/100... Step: 62915... Loss: 0.000204... Val Loss: 0.001024\n",
      "Epoch: 61/100... Step: 62920... Loss: 0.000225... Val Loss: 0.000910\n",
      "Epoch: 61/100... Step: 62925... Loss: 0.000144... Val Loss: 0.000890\n",
      "Epoch: 61/100... Step: 62930... Loss: 0.000137... Val Loss: 0.000814\n",
      "Epoch: 61/100... Step: 62935... Loss: 0.000234... Val Loss: 0.000895\n",
      "Epoch: 61/100... Step: 62940... Loss: 0.000096... Val Loss: 0.001106\n",
      "Epoch: 61/100... Step: 62945... Loss: 0.000286... Val Loss: 0.001012\n",
      "Epoch: 61/100... Step: 62950... Loss: 0.000178... Val Loss: 0.000914\n",
      "Epoch: 62/100... Step: 62955... Loss: 0.000255... Val Loss: 0.001475\n",
      "Epoch: 62/100... Step: 62960... Loss: 0.000172... Val Loss: 0.001557\n",
      "Epoch: 62/100... Step: 62965... Loss: 0.000214... Val Loss: 0.001648\n",
      "Epoch: 62/100... Step: 62970... Loss: 0.000252... Val Loss: 0.001596\n",
      "Epoch: 62/100... Step: 62975... Loss: 0.000193... Val Loss: 0.001721\n",
      "Epoch: 62/100... Step: 62980... Loss: 0.000192... Val Loss: 0.001875\n",
      "Epoch: 62/100... Step: 62985... Loss: 0.000200... Val Loss: 0.001922\n",
      "Epoch: 62/100... Step: 62990... Loss: 0.000266... Val Loss: 0.001786\n",
      "Epoch: 62/100... Step: 62995... Loss: 0.000296... Val Loss: 0.001799\n",
      "Epoch: 62/100... Step: 63000... Loss: 0.000229... Val Loss: 0.001847\n",
      "Epoch: 62/100... Step: 63005... Loss: 0.000169... Val Loss: 0.001766\n",
      "Epoch: 62/100... Step: 63010... Loss: 0.000145... Val Loss: 0.001701\n",
      "Epoch: 62/100... Step: 63015... Loss: 0.000278... Val Loss: 0.001634\n",
      "Epoch: 62/100... Step: 63020... Loss: 0.000229... Val Loss: 0.001408\n",
      "Epoch: 62/100... Step: 63025... Loss: 0.000156... Val Loss: 0.001048\n",
      "Epoch: 62/100... Step: 63030... Loss: 0.000157... Val Loss: 0.001197\n",
      "Epoch: 62/100... Step: 63035... Loss: 0.000117... Val Loss: 0.001485\n",
      "Epoch: 62/100... Step: 63040... Loss: 0.000124... Val Loss: 0.001316\n",
      "Epoch: 62/100... Step: 63045... Loss: 0.000286... Val Loss: 0.001173\n",
      "Epoch: 62/100... Step: 63050... Loss: 0.000150... Val Loss: 0.001054\n",
      "Epoch: 62/100... Step: 63055... Loss: 0.000124... Val Loss: 0.001063\n",
      "Epoch: 62/100... Step: 63060... Loss: 0.000141... Val Loss: 0.001166\n",
      "Epoch: 62/100... Step: 63065... Loss: 0.000059... Val Loss: 0.001311\n",
      "Epoch: 62/100... Step: 63070... Loss: 0.000108... Val Loss: 0.001268\n",
      "Epoch: 62/100... Step: 63075... Loss: 0.000174... Val Loss: 0.001101\n",
      "Epoch: 62/100... Step: 63080... Loss: 0.000196... Val Loss: 0.001290\n",
      "Epoch: 62/100... Step: 63085... Loss: 0.000081... Val Loss: 0.001438\n",
      "Epoch: 62/100... Step: 63090... Loss: 0.000036... Val Loss: 0.001426\n",
      "Epoch: 62/100... Step: 63095... Loss: 0.000124... Val Loss: 0.001961\n",
      "Epoch: 62/100... Step: 63100... Loss: 0.000216... Val Loss: 0.001867\n",
      "Epoch: 62/100... Step: 63105... Loss: 0.000111... Val Loss: 0.001403\n",
      "Epoch: 62/100... Step: 63110... Loss: 0.000204... Val Loss: 0.001364\n",
      "Epoch: 62/100... Step: 63115... Loss: 0.000199... Val Loss: 0.001281\n",
      "Epoch: 62/100... Step: 63120... Loss: 0.000188... Val Loss: 0.001188\n",
      "Epoch: 62/100... Step: 63125... Loss: 0.000088... Val Loss: 0.001254\n",
      "Epoch: 62/100... Step: 63130... Loss: 0.000070... Val Loss: 0.001247\n",
      "Epoch: 62/100... Step: 63135... Loss: 0.000132... Val Loss: 0.001388\n",
      "Epoch: 62/100... Step: 63140... Loss: 0.000136... Val Loss: 0.001342\n",
      "Epoch: 62/100... Step: 63145... Loss: 0.000273... Val Loss: 0.001135\n",
      "Epoch: 62/100... Step: 63150... Loss: 0.000117... Val Loss: 0.001057\n",
      "Epoch: 62/100... Step: 63155... Loss: 0.000184... Val Loss: 0.001231\n",
      "Epoch: 62/100... Step: 63160... Loss: 0.000055... Val Loss: 0.001225\n",
      "Epoch: 62/100... Step: 63165... Loss: 0.000102... Val Loss: 0.001381\n",
      "Epoch: 62/100... Step: 63170... Loss: 0.000269... Val Loss: 0.001361\n",
      "Epoch: 62/100... Step: 63175... Loss: 0.000262... Val Loss: 0.001225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62/100... Step: 63180... Loss: 0.000124... Val Loss: 0.001176\n",
      "Epoch: 62/100... Step: 63185... Loss: 0.000075... Val Loss: 0.000990\n",
      "Epoch: 62/100... Step: 63190... Loss: 0.000129... Val Loss: 0.001093\n",
      "Epoch: 62/100... Step: 63195... Loss: 0.000083... Val Loss: 0.001202\n",
      "Epoch: 62/100... Step: 63200... Loss: 0.000120... Val Loss: 0.001200\n",
      "Epoch: 62/100... Step: 63205... Loss: 0.000046... Val Loss: 0.001394\n",
      "Epoch: 62/100... Step: 63210... Loss: 0.000115... Val Loss: 0.001054\n",
      "Epoch: 62/100... Step: 63215... Loss: 0.000202... Val Loss: 0.000799\n",
      "Epoch: 62/100... Step: 63220... Loss: 0.000155... Val Loss: 0.000737\n",
      "Epoch: 62/100... Step: 63225... Loss: 0.000078... Val Loss: 0.000779\n",
      "Epoch: 62/100... Step: 63230... Loss: 0.000131... Val Loss: 0.000784\n",
      "Epoch: 62/100... Step: 63235... Loss: 0.000322... Val Loss: 0.000918\n",
      "Epoch: 62/100... Step: 63240... Loss: 0.000131... Val Loss: 0.000899\n",
      "Epoch: 62/100... Step: 63245... Loss: 0.000186... Val Loss: 0.000710\n",
      "Epoch: 62/100... Step: 63250... Loss: 0.000223... Val Loss: 0.000530\n",
      "Epoch: 62/100... Step: 63255... Loss: 0.000144... Val Loss: 0.000552\n",
      "Epoch: 62/100... Step: 63260... Loss: 0.000222... Val Loss: 0.000538\n",
      "Epoch: 62/100... Step: 63265... Loss: 0.000161... Val Loss: 0.000503\n",
      "Epoch: 62/100... Step: 63270... Loss: 0.000101... Val Loss: 0.000664\n",
      "Epoch: 62/100... Step: 63275... Loss: 0.000192... Val Loss: 0.000836\n",
      "Epoch: 62/100... Step: 63280... Loss: 0.000187... Val Loss: 0.000555\n",
      "Epoch: 62/100... Step: 63285... Loss: 0.000220... Val Loss: 0.000418\n",
      "Epoch: 62/100... Step: 63290... Loss: 0.000132... Val Loss: 0.000388\n",
      "Epoch: 62/100... Step: 63295... Loss: 0.000398... Val Loss: 0.000772\n",
      "Epoch: 62/100... Step: 63300... Loss: 0.000206... Val Loss: 0.000377\n",
      "Epoch: 62/100... Step: 63305... Loss: 0.000255... Val Loss: 0.000396\n",
      "Epoch: 62/100... Step: 63310... Loss: 0.000116... Val Loss: 0.000701\n",
      "Epoch: 62/100... Step: 63315... Loss: 0.000198... Val Loss: 0.000979\n",
      "Epoch: 62/100... Step: 63320... Loss: 0.000153... Val Loss: 0.000946\n",
      "Epoch: 62/100... Step: 63325... Loss: 0.000181... Val Loss: 0.001149\n",
      "Epoch: 62/100... Step: 63330... Loss: 0.000117... Val Loss: 0.001043\n",
      "Epoch: 62/100... Step: 63335... Loss: 0.000168... Val Loss: 0.000951\n",
      "Epoch: 62/100... Step: 63340... Loss: 0.000051... Val Loss: 0.000933\n",
      "Epoch: 62/100... Step: 63345... Loss: 0.000097... Val Loss: 0.001004\n",
      "Epoch: 62/100... Step: 63350... Loss: 0.000201... Val Loss: 0.000965\n",
      "Epoch: 62/100... Step: 63355... Loss: 0.000178... Val Loss: 0.001044\n",
      "Epoch: 62/100... Step: 63360... Loss: 0.000296... Val Loss: 0.000487\n",
      "Epoch: 62/100... Step: 63365... Loss: 0.000591... Val Loss: 0.000464\n",
      "Epoch: 62/100... Step: 63370... Loss: 0.000331... Val Loss: 0.001505\n",
      "Epoch: 62/100... Step: 63375... Loss: 0.000345... Val Loss: 0.000608\n",
      "Epoch: 62/100... Step: 63380... Loss: 0.000239... Val Loss: 0.001337\n",
      "Epoch: 62/100... Step: 63385... Loss: 0.000269... Val Loss: 0.001190\n",
      "Epoch: 62/100... Step: 63390... Loss: 0.000208... Val Loss: 0.001025\n",
      "Epoch: 62/100... Step: 63395... Loss: 0.000118... Val Loss: 0.000910\n",
      "Epoch: 62/100... Step: 63400... Loss: 0.000249... Val Loss: 0.001394\n",
      "Epoch: 62/100... Step: 63405... Loss: 0.000085... Val Loss: 0.001460\n",
      "Epoch: 62/100... Step: 63410... Loss: 0.000229... Val Loss: 0.001298\n",
      "Epoch: 62/100... Step: 63415... Loss: 0.000141... Val Loss: 0.001350\n",
      "Epoch: 62/100... Step: 63420... Loss: 0.000214... Val Loss: 0.001248\n",
      "Epoch: 62/100... Step: 63425... Loss: 0.000092... Val Loss: 0.001115\n",
      "Epoch: 62/100... Step: 63430... Loss: 0.000119... Val Loss: 0.001251\n",
      "Epoch: 62/100... Step: 63435... Loss: 0.000124... Val Loss: 0.001204\n",
      "Epoch: 62/100... Step: 63440... Loss: 0.000161... Val Loss: 0.000894\n",
      "Epoch: 62/100... Step: 63445... Loss: 0.000108... Val Loss: 0.000937\n",
      "Epoch: 62/100... Step: 63450... Loss: 0.000109... Val Loss: 0.000708\n",
      "Epoch: 62/100... Step: 63455... Loss: 0.000173... Val Loss: 0.000602\n",
      "Epoch: 62/100... Step: 63460... Loss: 0.000114... Val Loss: 0.000273\n",
      "Epoch: 62/100... Step: 63465... Loss: 0.000142... Val Loss: 0.000406\n",
      "Epoch: 62/100... Step: 63470... Loss: 0.000062... Val Loss: 0.000426\n",
      "Epoch: 62/100... Step: 63475... Loss: 0.000088... Val Loss: 0.000583\n",
      "Epoch: 62/100... Step: 63480... Loss: 0.000076... Val Loss: 0.000774\n",
      "Epoch: 62/100... Step: 63485... Loss: 0.000158... Val Loss: 0.000726\n",
      "Epoch: 62/100... Step: 63490... Loss: 0.000117... Val Loss: 0.000698\n",
      "Epoch: 62/100... Step: 63495... Loss: 0.000111... Val Loss: 0.000853\n",
      "Epoch: 62/100... Step: 63500... Loss: 0.000162... Val Loss: 0.000974\n",
      "Epoch: 62/100... Step: 63505... Loss: 0.000205... Val Loss: 0.000873\n",
      "Epoch: 62/100... Step: 63510... Loss: 0.000089... Val Loss: 0.000777\n",
      "Epoch: 62/100... Step: 63515... Loss: 0.000085... Val Loss: 0.000605\n",
      "Epoch: 62/100... Step: 63520... Loss: 0.000122... Val Loss: 0.000862\n",
      "Epoch: 62/100... Step: 63525... Loss: 0.000137... Val Loss: 0.000968\n",
      "Epoch: 62/100... Step: 63530... Loss: 0.000150... Val Loss: 0.000871\n",
      "Epoch: 62/100... Step: 63535... Loss: 0.000336... Val Loss: 0.000987\n",
      "Epoch: 62/100... Step: 63540... Loss: 0.000141... Val Loss: 0.000863\n",
      "Epoch: 62/100... Step: 63545... Loss: 0.000187... Val Loss: 0.001137\n",
      "Epoch: 62/100... Step: 63550... Loss: 0.000089... Val Loss: 0.000828\n",
      "Epoch: 62/100... Step: 63555... Loss: 0.000072... Val Loss: 0.000977\n",
      "Epoch: 62/100... Step: 63560... Loss: 0.000056... Val Loss: 0.000863\n",
      "Epoch: 62/100... Step: 63565... Loss: 0.000206... Val Loss: 0.000969\n",
      "Epoch: 62/100... Step: 63570... Loss: 0.000147... Val Loss: 0.000885\n",
      "Epoch: 62/100... Step: 63575... Loss: 0.000152... Val Loss: 0.001147\n",
      "Epoch: 62/100... Step: 63580... Loss: 0.000181... Val Loss: 0.000794\n",
      "Epoch: 62/100... Step: 63585... Loss: 0.000202... Val Loss: 0.001050\n",
      "Epoch: 62/100... Step: 63590... Loss: 0.000134... Val Loss: 0.000957\n",
      "Epoch: 62/100... Step: 63595... Loss: 0.000212... Val Loss: 0.000795\n",
      "Epoch: 62/100... Step: 63600... Loss: 0.000284... Val Loss: 0.000522\n",
      "Epoch: 62/100... Step: 63605... Loss: 0.000191... Val Loss: 0.000974\n",
      "Epoch: 62/100... Step: 63610... Loss: 0.000085... Val Loss: 0.000846\n",
      "Epoch: 62/100... Step: 63615... Loss: 0.000133... Val Loss: 0.000717\n",
      "Epoch: 62/100... Step: 63620... Loss: 0.000252... Val Loss: 0.000437\n",
      "Epoch: 62/100... Step: 63625... Loss: 0.000143... Val Loss: 0.000520\n",
      "Epoch: 62/100... Step: 63630... Loss: 0.000058... Val Loss: 0.000429\n",
      "Epoch: 62/100... Step: 63635... Loss: 0.000124... Val Loss: 0.000313\n",
      "Epoch: 62/100... Step: 63640... Loss: 0.000261... Val Loss: 0.000536\n",
      "Epoch: 62/100... Step: 63645... Loss: 0.000051... Val Loss: 0.000303\n",
      "Epoch: 62/100... Step: 63650... Loss: 0.000113... Val Loss: 0.000268\n",
      "Epoch: 62/100... Step: 63655... Loss: 0.000084... Val Loss: 0.000291\n",
      "Epoch: 62/100... Step: 63660... Loss: 0.000242... Val Loss: 0.000349\n",
      "Epoch: 62/100... Step: 63665... Loss: 0.000148... Val Loss: 0.000299\n",
      "Epoch: 62/100... Step: 63670... Loss: 0.000261... Val Loss: 0.000372\n",
      "Epoch: 62/100... Step: 63675... Loss: 0.000136... Val Loss: 0.000377\n",
      "Epoch: 62/100... Step: 63680... Loss: 0.000264... Val Loss: 0.000385\n",
      "Epoch: 62/100... Step: 63685... Loss: 0.000178... Val Loss: 0.000256\n",
      "Epoch: 62/100... Step: 63690... Loss: 0.000225... Val Loss: 0.000381\n",
      "Epoch: 62/100... Step: 63695... Loss: 0.000108... Val Loss: 0.000350\n",
      "Epoch: 62/100... Step: 63700... Loss: 0.000268... Val Loss: 0.000971\n",
      "Epoch: 62/100... Step: 63705... Loss: 0.000238... Val Loss: 0.000298\n",
      "Epoch: 62/100... Step: 63710... Loss: 0.000080... Val Loss: 0.000639\n",
      "Epoch: 62/100... Step: 63715... Loss: 0.000197... Val Loss: 0.000435\n",
      "Epoch: 62/100... Step: 63720... Loss: 0.000215... Val Loss: 0.000365\n",
      "Epoch: 62/100... Step: 63725... Loss: 0.000034... Val Loss: 0.000306\n",
      "Epoch: 62/100... Step: 63730... Loss: 0.000225... Val Loss: 0.000524\n",
      "Epoch: 62/100... Step: 63735... Loss: 0.000171... Val Loss: 0.000753\n",
      "Epoch: 62/100... Step: 63740... Loss: 0.000112... Val Loss: 0.000545\n",
      "Epoch: 62/100... Step: 63745... Loss: 0.000165... Val Loss: 0.000486\n",
      "Epoch: 62/100... Step: 63750... Loss: 0.000113... Val Loss: 0.000305\n",
      "Epoch: 62/100... Step: 63755... Loss: 0.000172... Val Loss: 0.000552\n",
      "Epoch: 62/100... Step: 63760... Loss: 0.000213... Val Loss: 0.000833\n",
      "Epoch: 62/100... Step: 63765... Loss: 0.000128... Val Loss: 0.000557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62/100... Step: 63770... Loss: 0.000086... Val Loss: 0.000667\n",
      "Epoch: 62/100... Step: 63775... Loss: 0.000087... Val Loss: 0.000577\n",
      "Epoch: 62/100... Step: 63780... Loss: 0.000302... Val Loss: 0.000628\n",
      "Epoch: 62/100... Step: 63785... Loss: 0.000158... Val Loss: 0.000406\n",
      "Epoch: 62/100... Step: 63790... Loss: 0.000292... Val Loss: 0.000654\n",
      "Epoch: 62/100... Step: 63795... Loss: 0.000121... Val Loss: 0.000548\n",
      "Epoch: 62/100... Step: 63800... Loss: 0.000226... Val Loss: 0.000826\n",
      "Epoch: 62/100... Step: 63805... Loss: 0.000224... Val Loss: 0.000677\n",
      "Epoch: 62/100... Step: 63810... Loss: 0.000197... Val Loss: 0.000669\n",
      "Epoch: 62/100... Step: 63815... Loss: 0.000199... Val Loss: 0.001051\n",
      "Epoch: 62/100... Step: 63820... Loss: 0.000096... Val Loss: 0.000811\n",
      "Epoch: 62/100... Step: 63825... Loss: 0.000096... Val Loss: 0.000918\n",
      "Epoch: 62/100... Step: 63830... Loss: 0.000151... Val Loss: 0.000809\n",
      "Epoch: 62/100... Step: 63835... Loss: 0.000235... Val Loss: 0.001126\n",
      "Epoch: 62/100... Step: 63840... Loss: 0.000315... Val Loss: 0.000733\n",
      "Epoch: 62/100... Step: 63845... Loss: 0.000268... Val Loss: 0.000503\n",
      "Epoch: 62/100... Step: 63850... Loss: 0.000177... Val Loss: 0.000735\n",
      "Epoch: 62/100... Step: 63855... Loss: 0.000189... Val Loss: 0.000832\n",
      "Epoch: 62/100... Step: 63860... Loss: 0.000084... Val Loss: 0.000653\n",
      "Epoch: 62/100... Step: 63865... Loss: 0.000093... Val Loss: 0.000644\n",
      "Epoch: 62/100... Step: 63870... Loss: 0.000113... Val Loss: 0.000636\n",
      "Epoch: 62/100... Step: 63875... Loss: 0.000047... Val Loss: 0.000677\n",
      "Epoch: 62/100... Step: 63880... Loss: 0.000083... Val Loss: 0.000830\n",
      "Epoch: 62/100... Step: 63885... Loss: 0.000092... Val Loss: 0.000790\n",
      "Epoch: 62/100... Step: 63890... Loss: 0.000207... Val Loss: 0.001143\n",
      "Epoch: 62/100... Step: 63895... Loss: 0.000111... Val Loss: 0.000635\n",
      "Epoch: 62/100... Step: 63900... Loss: 0.000154... Val Loss: 0.000853\n",
      "Epoch: 62/100... Step: 63905... Loss: 0.000197... Val Loss: 0.001124\n",
      "Epoch: 62/100... Step: 63910... Loss: 0.000098... Val Loss: 0.001004\n",
      "Epoch: 62/100... Step: 63915... Loss: 0.000426... Val Loss: 0.000726\n",
      "Epoch: 62/100... Step: 63920... Loss: 0.000447... Val Loss: 0.001145\n",
      "Epoch: 62/100... Step: 63925... Loss: 0.000103... Val Loss: 0.001192\n",
      "Epoch: 62/100... Step: 63930... Loss: 0.000137... Val Loss: 0.000917\n",
      "Epoch: 62/100... Step: 63935... Loss: 0.000145... Val Loss: 0.000896\n",
      "Epoch: 62/100... Step: 63940... Loss: 0.000117... Val Loss: 0.001198\n",
      "Epoch: 62/100... Step: 63945... Loss: 0.000102... Val Loss: 0.001079\n",
      "Epoch: 62/100... Step: 63950... Loss: 0.000076... Val Loss: 0.001061\n",
      "Epoch: 62/100... Step: 63955... Loss: 0.000125... Val Loss: 0.000939\n",
      "Epoch: 62/100... Step: 63960... Loss: 0.000127... Val Loss: 0.000903\n",
      "Epoch: 62/100... Step: 63965... Loss: 0.000099... Val Loss: 0.000995\n",
      "Epoch: 62/100... Step: 63970... Loss: 0.000159... Val Loss: 0.001038\n",
      "Epoch: 62/100... Step: 63975... Loss: 0.000173... Val Loss: 0.000993\n",
      "Epoch: 62/100... Step: 63980... Loss: 0.000131... Val Loss: 0.001042\n",
      "Epoch: 63/100... Step: 63985... Loss: 0.000304... Val Loss: 0.001340\n",
      "Epoch: 63/100... Step: 63990... Loss: 0.000296... Val Loss: 0.001312\n",
      "Epoch: 63/100... Step: 63995... Loss: 0.000090... Val Loss: 0.001402\n",
      "Epoch: 63/100... Step: 64000... Loss: 0.000191... Val Loss: 0.001670\n",
      "Epoch: 63/100... Step: 64005... Loss: 0.000181... Val Loss: 0.001754\n",
      "Epoch: 63/100... Step: 64010... Loss: 0.000145... Val Loss: 0.001749\n",
      "Epoch: 63/100... Step: 64015... Loss: 0.000154... Val Loss: 0.001805\n",
      "Epoch: 63/100... Step: 64020... Loss: 0.000148... Val Loss: 0.001608\n",
      "Epoch: 63/100... Step: 64025... Loss: 0.000140... Val Loss: 0.001590\n",
      "Epoch: 63/100... Step: 64030... Loss: 0.000187... Val Loss: 0.001810\n",
      "Epoch: 63/100... Step: 64035... Loss: 0.000065... Val Loss: 0.001649\n",
      "Epoch: 63/100... Step: 64040... Loss: 0.000109... Val Loss: 0.001645\n",
      "Epoch: 63/100... Step: 64045... Loss: 0.000162... Val Loss: 0.001721\n",
      "Epoch: 63/100... Step: 64050... Loss: 0.000110... Val Loss: 0.001571\n",
      "Epoch: 63/100... Step: 64055... Loss: 0.000247... Val Loss: 0.001389\n",
      "Epoch: 63/100... Step: 64060... Loss: 0.000116... Val Loss: 0.001317\n",
      "Epoch: 63/100... Step: 64065... Loss: 0.000231... Val Loss: 0.001324\n",
      "Epoch: 63/100... Step: 64070... Loss: 0.000180... Val Loss: 0.001094\n",
      "Epoch: 63/100... Step: 64075... Loss: 0.000152... Val Loss: 0.001249\n",
      "Epoch: 63/100... Step: 64080... Loss: 0.000186... Val Loss: 0.001295\n",
      "Epoch: 63/100... Step: 64085... Loss: 0.000238... Val Loss: 0.001068\n",
      "Epoch: 63/100... Step: 64090... Loss: 0.000182... Val Loss: 0.001077\n",
      "Epoch: 63/100... Step: 64095... Loss: 0.000090... Val Loss: 0.001113\n",
      "Epoch: 63/100... Step: 64100... Loss: 0.000230... Val Loss: 0.001208\n",
      "Epoch: 63/100... Step: 64105... Loss: 0.000098... Val Loss: 0.001124\n",
      "Epoch: 63/100... Step: 64110... Loss: 0.000091... Val Loss: 0.001342\n",
      "Epoch: 63/100... Step: 64115... Loss: 0.000072... Val Loss: 0.001402\n",
      "Epoch: 63/100... Step: 64120... Loss: 0.000137... Val Loss: 0.001697\n",
      "Epoch: 63/100... Step: 64125... Loss: 0.000204... Val Loss: 0.001555\n",
      "Epoch: 63/100... Step: 64130... Loss: 0.000092... Val Loss: 0.001920\n",
      "Epoch: 63/100... Step: 64135... Loss: 0.000059... Val Loss: 0.001529\n",
      "Epoch: 63/100... Step: 64140... Loss: 0.000128... Val Loss: 0.001591\n",
      "Epoch: 63/100... Step: 64145... Loss: 0.000088... Val Loss: 0.001493\n",
      "Epoch: 63/100... Step: 64150... Loss: 0.000113... Val Loss: 0.001426\n",
      "Epoch: 63/100... Step: 64155... Loss: 0.000057... Val Loss: 0.001518\n",
      "Epoch: 63/100... Step: 64160... Loss: 0.000225... Val Loss: 0.001392\n",
      "Epoch: 63/100... Step: 64165... Loss: 0.000175... Val Loss: 0.001399\n",
      "Epoch: 63/100... Step: 64170... Loss: 0.000185... Val Loss: 0.001218\n",
      "Epoch: 63/100... Step: 64175... Loss: 0.000078... Val Loss: 0.001135\n",
      "Epoch: 63/100... Step: 64180... Loss: 0.000100... Val Loss: 0.001095\n",
      "Epoch: 63/100... Step: 64185... Loss: 0.000100... Val Loss: 0.001264\n",
      "Epoch: 63/100... Step: 64190... Loss: 0.000087... Val Loss: 0.001184\n",
      "Epoch: 63/100... Step: 64195... Loss: 0.000099... Val Loss: 0.001234\n",
      "Epoch: 63/100... Step: 64200... Loss: 0.000138... Val Loss: 0.001561\n",
      "Epoch: 63/100... Step: 64205... Loss: 0.000156... Val Loss: 0.001228\n",
      "Epoch: 63/100... Step: 64210... Loss: 0.000171... Val Loss: 0.001489\n",
      "Epoch: 63/100... Step: 64215... Loss: 0.000120... Val Loss: 0.001132\n",
      "Epoch: 63/100... Step: 64220... Loss: 0.000076... Val Loss: 0.001171\n",
      "Epoch: 63/100... Step: 64225... Loss: 0.000124... Val Loss: 0.001238\n",
      "Epoch: 63/100... Step: 64230... Loss: 0.000140... Val Loss: 0.001130\n",
      "Epoch: 63/100... Step: 64235... Loss: 0.000268... Val Loss: 0.001160\n",
      "Epoch: 63/100... Step: 64240... Loss: 0.000271... Val Loss: 0.000953\n",
      "Epoch: 63/100... Step: 64245... Loss: 0.000077... Val Loss: 0.000912\n",
      "Epoch: 63/100... Step: 64250... Loss: 0.000192... Val Loss: 0.001005\n",
      "Epoch: 63/100... Step: 64255... Loss: 0.000068... Val Loss: 0.000858\n",
      "Epoch: 63/100... Step: 64260... Loss: 0.000069... Val Loss: 0.000877\n",
      "Epoch: 63/100... Step: 64265... Loss: 0.000065... Val Loss: 0.000992\n",
      "Epoch: 63/100... Step: 64270... Loss: 0.000219... Val Loss: 0.000918\n",
      "Epoch: 63/100... Step: 64275... Loss: 0.000122... Val Loss: 0.000653\n",
      "Epoch: 63/100... Step: 64280... Loss: 0.000184... Val Loss: 0.000543\n",
      "Epoch: 63/100... Step: 64285... Loss: 0.000100... Val Loss: 0.000655\n",
      "Epoch: 63/100... Step: 64290... Loss: 0.000199... Val Loss: 0.000741\n",
      "Epoch: 63/100... Step: 64295... Loss: 0.000274... Val Loss: 0.000538\n",
      "Epoch: 63/100... Step: 64300... Loss: 0.000055... Val Loss: 0.000584\n",
      "Epoch: 63/100... Step: 64305... Loss: 0.000078... Val Loss: 0.000687\n",
      "Epoch: 63/100... Step: 64310... Loss: 0.000181... Val Loss: 0.000473\n",
      "Epoch: 63/100... Step: 64315... Loss: 0.000191... Val Loss: 0.000459\n",
      "Epoch: 63/100... Step: 64320... Loss: 0.000236... Val Loss: 0.000460\n",
      "Epoch: 63/100... Step: 64325... Loss: 0.000103... Val Loss: 0.000566\n",
      "Epoch: 63/100... Step: 64330... Loss: 0.000248... Val Loss: 0.000569\n",
      "Epoch: 63/100... Step: 64335... Loss: 0.000158... Val Loss: 0.000511\n",
      "Epoch: 63/100... Step: 64340... Loss: 0.000148... Val Loss: 0.000616\n",
      "Epoch: 63/100... Step: 64345... Loss: 0.000124... Val Loss: 0.000745\n",
      "Epoch: 63/100... Step: 64350... Loss: 0.000095... Val Loss: 0.000817\n",
      "Epoch: 63/100... Step: 64355... Loss: 0.000077... Val Loss: 0.001064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63/100... Step: 64360... Loss: 0.000229... Val Loss: 0.001081\n",
      "Epoch: 63/100... Step: 64365... Loss: 0.000158... Val Loss: 0.001201\n",
      "Epoch: 63/100... Step: 64370... Loss: 0.000178... Val Loss: 0.000934\n",
      "Epoch: 63/100... Step: 64375... Loss: 0.000103... Val Loss: 0.001064\n",
      "Epoch: 63/100... Step: 64380... Loss: 0.000138... Val Loss: 0.001079\n",
      "Epoch: 63/100... Step: 64385... Loss: 0.000087... Val Loss: 0.001002\n",
      "Epoch: 63/100... Step: 64390... Loss: 0.000401... Val Loss: 0.000861\n",
      "Epoch: 63/100... Step: 64395... Loss: 0.000253... Val Loss: 0.000430\n",
      "Epoch: 63/100... Step: 64400... Loss: 0.000751... Val Loss: 0.000520\n",
      "Epoch: 63/100... Step: 64405... Loss: 0.000552... Val Loss: 0.001184\n",
      "Epoch: 63/100... Step: 64410... Loss: 0.000344... Val Loss: 0.001226\n",
      "Epoch: 63/100... Step: 64415... Loss: 0.000220... Val Loss: 0.001096\n",
      "Epoch: 63/100... Step: 64420... Loss: 0.000186... Val Loss: 0.001194\n",
      "Epoch: 63/100... Step: 64425... Loss: 0.000203... Val Loss: 0.000937\n",
      "Epoch: 63/100... Step: 64430... Loss: 0.000189... Val Loss: 0.001077\n",
      "Epoch: 63/100... Step: 64435... Loss: 0.000205... Val Loss: 0.001222\n",
      "Epoch: 63/100... Step: 64440... Loss: 0.000267... Val Loss: 0.001194\n",
      "Epoch: 63/100... Step: 64445... Loss: 0.000290... Val Loss: 0.001345\n",
      "Epoch: 63/100... Step: 64450... Loss: 0.000151... Val Loss: 0.001464\n",
      "Epoch: 63/100... Step: 64455... Loss: 0.000275... Val Loss: 0.001019\n",
      "Epoch: 63/100... Step: 64460... Loss: 0.000159... Val Loss: 0.000998\n",
      "Epoch: 63/100... Step: 64465... Loss: 0.000072... Val Loss: 0.001019\n",
      "Epoch: 63/100... Step: 64470... Loss: 0.000300... Val Loss: 0.001053\n",
      "Epoch: 63/100... Step: 64475... Loss: 0.000173... Val Loss: 0.000905\n",
      "Epoch: 63/100... Step: 64480... Loss: 0.000203... Val Loss: 0.000955\n",
      "Epoch: 63/100... Step: 64485... Loss: 0.000228... Val Loss: 0.000715\n",
      "Epoch: 63/100... Step: 64490... Loss: 0.000173... Val Loss: 0.000743\n",
      "Epoch: 63/100... Step: 64495... Loss: 0.000173... Val Loss: 0.000371\n",
      "Epoch: 63/100... Step: 64500... Loss: 0.000171... Val Loss: 0.000589\n",
      "Epoch: 63/100... Step: 64505... Loss: 0.000175... Val Loss: 0.000587\n",
      "Epoch: 63/100... Step: 64510... Loss: 0.000216... Val Loss: 0.000716\n",
      "Epoch: 63/100... Step: 64515... Loss: 0.000161... Val Loss: 0.000682\n",
      "Epoch: 63/100... Step: 64520... Loss: 0.000129... Val Loss: 0.000489\n",
      "Epoch: 63/100... Step: 64525... Loss: 0.000114... Val Loss: 0.000637\n",
      "Epoch: 63/100... Step: 64530... Loss: 0.000136... Val Loss: 0.000927\n",
      "Epoch: 63/100... Step: 64535... Loss: 0.000133... Val Loss: 0.000728\n",
      "Epoch: 63/100... Step: 64540... Loss: 0.000220... Val Loss: 0.000844\n",
      "Epoch: 63/100... Step: 64545... Loss: 0.000108... Val Loss: 0.000631\n",
      "Epoch: 63/100... Step: 64550... Loss: 0.000189... Val Loss: 0.000717\n",
      "Epoch: 63/100... Step: 64555... Loss: 0.000379... Val Loss: 0.001235\n",
      "Epoch: 63/100... Step: 64560... Loss: 0.000287... Val Loss: 0.000645\n",
      "Epoch: 63/100... Step: 64565... Loss: 0.000233... Val Loss: 0.000804\n",
      "Epoch: 63/100... Step: 64570... Loss: 0.000610... Val Loss: 0.001231\n",
      "Epoch: 63/100... Step: 64575... Loss: 0.000263... Val Loss: 0.000863\n",
      "Epoch: 63/100... Step: 64580... Loss: 0.000262... Val Loss: 0.001107\n",
      "Epoch: 63/100... Step: 64585... Loss: 0.000138... Val Loss: 0.000815\n",
      "Epoch: 63/100... Step: 64590... Loss: 0.000199... Val Loss: 0.001113\n",
      "Epoch: 63/100... Step: 64595... Loss: 0.000573... Val Loss: 0.000849\n",
      "Epoch: 63/100... Step: 64600... Loss: 0.000383... Val Loss: 0.000732\n",
      "Epoch: 63/100... Step: 64605... Loss: 0.000256... Val Loss: 0.000668\n",
      "Epoch: 63/100... Step: 64610... Loss: 0.000196... Val Loss: 0.001090\n",
      "Epoch: 63/100... Step: 64615... Loss: 0.000273... Val Loss: 0.000760\n",
      "Epoch: 63/100... Step: 64620... Loss: 0.000484... Val Loss: 0.000906\n",
      "Epoch: 63/100... Step: 64625... Loss: 0.000375... Val Loss: 0.000515\n",
      "Epoch: 63/100... Step: 64630... Loss: 0.000194... Val Loss: 0.000572\n",
      "Epoch: 63/100... Step: 64635... Loss: 0.000392... Val Loss: 0.000861\n",
      "Epoch: 63/100... Step: 64640... Loss: 0.000281... Val Loss: 0.000748\n",
      "Epoch: 63/100... Step: 64645... Loss: 0.000395... Val Loss: 0.000747\n",
      "Epoch: 63/100... Step: 64650... Loss: 0.000200... Val Loss: 0.000295\n",
      "Epoch: 63/100... Step: 64655... Loss: 0.000204... Val Loss: 0.000308\n",
      "Epoch: 63/100... Step: 64660... Loss: 0.000180... Val Loss: 0.000860\n",
      "Epoch: 63/100... Step: 64665... Loss: 0.000259... Val Loss: 0.000337\n",
      "Epoch: 63/100... Step: 64670... Loss: 0.000218... Val Loss: 0.000306\n",
      "Epoch: 63/100... Step: 64675... Loss: 0.000302... Val Loss: 0.000499\n",
      "Epoch: 63/100... Step: 64680... Loss: 0.000192... Val Loss: 0.000439\n",
      "Epoch: 63/100... Step: 64685... Loss: 0.000317... Val Loss: 0.000262\n",
      "Epoch: 63/100... Step: 64690... Loss: 0.000292... Val Loss: 0.000279\n",
      "Epoch: 63/100... Step: 64695... Loss: 0.000397... Val Loss: 0.000265\n",
      "Epoch: 63/100... Step: 64700... Loss: 0.000205... Val Loss: 0.000514\n",
      "Epoch: 63/100... Step: 64705... Loss: 0.000310... Val Loss: 0.000376\n",
      "Epoch: 63/100... Step: 64710... Loss: 0.000165... Val Loss: 0.000295\n",
      "Epoch: 63/100... Step: 64715... Loss: 0.000122... Val Loss: 0.000322\n",
      "Epoch: 63/100... Step: 64720... Loss: 0.000309... Val Loss: 0.000374\n",
      "Epoch: 63/100... Step: 64725... Loss: 0.000179... Val Loss: 0.000274\n",
      "Epoch: 63/100... Step: 64730... Loss: 0.000221... Val Loss: 0.001326\n",
      "Epoch: 63/100... Step: 64735... Loss: 0.000181... Val Loss: 0.000439\n",
      "Epoch: 63/100... Step: 64740... Loss: 0.000103... Val Loss: 0.000609\n",
      "Epoch: 63/100... Step: 64745... Loss: 0.000176... Val Loss: 0.000374\n",
      "Epoch: 63/100... Step: 64750... Loss: 0.000167... Val Loss: 0.000560\n",
      "Epoch: 63/100... Step: 64755... Loss: 0.000076... Val Loss: 0.000507\n",
      "Epoch: 63/100... Step: 64760... Loss: 0.000265... Val Loss: 0.000633\n",
      "Epoch: 63/100... Step: 64765... Loss: 0.000183... Val Loss: 0.000514\n",
      "Epoch: 63/100... Step: 64770... Loss: 0.000158... Val Loss: 0.000552\n",
      "Epoch: 63/100... Step: 64775... Loss: 0.000258... Val Loss: 0.000448\n",
      "Epoch: 63/100... Step: 64780... Loss: 0.000143... Val Loss: 0.000444\n",
      "Epoch: 63/100... Step: 64785... Loss: 0.000263... Val Loss: 0.000458\n",
      "Epoch: 63/100... Step: 64790... Loss: 0.000229... Val Loss: 0.000630\n",
      "Epoch: 63/100... Step: 64795... Loss: 0.000245... Val Loss: 0.001007\n",
      "Epoch: 63/100... Step: 64800... Loss: 0.000280... Val Loss: 0.000687\n",
      "Epoch: 63/100... Step: 64805... Loss: 0.000072... Val Loss: 0.000759\n",
      "Epoch: 63/100... Step: 64810... Loss: 0.000205... Val Loss: 0.000548\n",
      "Epoch: 63/100... Step: 64815... Loss: 0.000722... Val Loss: 0.000712\n",
      "Epoch: 63/100... Step: 64820... Loss: 0.000198... Val Loss: 0.000734\n",
      "Epoch: 63/100... Step: 64825... Loss: 0.000344... Val Loss: 0.000801\n",
      "Epoch: 63/100... Step: 64830... Loss: 0.000408... Val Loss: 0.000578\n",
      "Epoch: 63/100... Step: 64835... Loss: 0.000335... Val Loss: 0.000517\n",
      "Epoch: 63/100... Step: 64840... Loss: 0.000269... Val Loss: 0.000762\n",
      "Epoch: 63/100... Step: 64845... Loss: 0.000190... Val Loss: 0.000594\n",
      "Epoch: 63/100... Step: 64850... Loss: 0.000101... Val Loss: 0.000984\n",
      "Epoch: 63/100... Step: 64855... Loss: 0.000063... Val Loss: 0.000845\n",
      "Epoch: 63/100... Step: 64860... Loss: 0.000093... Val Loss: 0.000813\n",
      "Epoch: 63/100... Step: 64865... Loss: 0.000147... Val Loss: 0.001251\n",
      "Epoch: 63/100... Step: 64870... Loss: 0.000196... Val Loss: 0.000758\n",
      "Epoch: 63/100... Step: 64875... Loss: 0.000397... Val Loss: 0.000890\n",
      "Epoch: 63/100... Step: 64880... Loss: 0.000285... Val Loss: 0.000686\n",
      "Epoch: 63/100... Step: 64885... Loss: 0.000296... Val Loss: 0.000567\n",
      "Epoch: 63/100... Step: 64890... Loss: 0.000164... Val Loss: 0.000599\n",
      "Epoch: 63/100... Step: 64895... Loss: 0.000128... Val Loss: 0.000651\n",
      "Epoch: 63/100... Step: 64900... Loss: 0.000092... Val Loss: 0.000547\n",
      "Epoch: 63/100... Step: 64905... Loss: 0.000221... Val Loss: 0.000801\n",
      "Epoch: 63/100... Step: 64910... Loss: 0.000311... Val Loss: 0.000857\n",
      "Epoch: 63/100... Step: 64915... Loss: 0.000130... Val Loss: 0.000619\n",
      "Epoch: 63/100... Step: 64920... Loss: 0.000186... Val Loss: 0.001263\n",
      "Epoch: 63/100... Step: 64925... Loss: 0.000251... Val Loss: 0.000576\n",
      "Epoch: 63/100... Step: 64930... Loss: 0.000086... Val Loss: 0.000904\n",
      "Epoch: 63/100... Step: 64935... Loss: 0.000096... Val Loss: 0.000760\n",
      "Epoch: 63/100... Step: 64940... Loss: 0.000110... Val Loss: 0.000867\n",
      "Epoch: 63/100... Step: 64945... Loss: 0.000196... Val Loss: 0.000832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63/100... Step: 64950... Loss: 0.000172... Val Loss: 0.000771\n",
      "Epoch: 63/100... Step: 64955... Loss: 0.000240... Val Loss: 0.001161\n",
      "Epoch: 63/100... Step: 64960... Loss: 0.000276... Val Loss: 0.000845\n",
      "Epoch: 63/100... Step: 64965... Loss: 0.000290... Val Loss: 0.000879\n",
      "Epoch: 63/100... Step: 64970... Loss: 0.000362... Val Loss: 0.001030\n",
      "Epoch: 63/100... Step: 64975... Loss: 0.000257... Val Loss: 0.000830\n",
      "Epoch: 63/100... Step: 64980... Loss: 0.000304... Val Loss: 0.001163\n",
      "Epoch: 63/100... Step: 64985... Loss: 0.000258... Val Loss: 0.000939\n",
      "Epoch: 63/100... Step: 64990... Loss: 0.000184... Val Loss: 0.000833\n",
      "Epoch: 63/100... Step: 64995... Loss: 0.000103... Val Loss: 0.000943\n",
      "Epoch: 63/100... Step: 65000... Loss: 0.000137... Val Loss: 0.001025\n",
      "Epoch: 63/100... Step: 65005... Loss: 0.000157... Val Loss: 0.001038\n",
      "Epoch: 63/100... Step: 65010... Loss: 0.000213... Val Loss: 0.001086\n",
      "Epoch: 63/100... Step: 65015... Loss: 0.000202... Val Loss: 0.000843\n",
      "Epoch: 64/100... Step: 65020... Loss: 0.000216... Val Loss: 0.001285\n",
      "Epoch: 64/100... Step: 65025... Loss: 0.000268... Val Loss: 0.001666\n",
      "Epoch: 64/100... Step: 65030... Loss: 0.000199... Val Loss: 0.001776\n",
      "Epoch: 64/100... Step: 65035... Loss: 0.000201... Val Loss: 0.001776\n",
      "Epoch: 64/100... Step: 65040... Loss: 0.000345... Val Loss: 0.001602\n",
      "Epoch: 64/100... Step: 65045... Loss: 0.000270... Val Loss: 0.001775\n",
      "Epoch: 64/100... Step: 65050... Loss: 0.000104... Val Loss: 0.001857\n",
      "Epoch: 64/100... Step: 65055... Loss: 0.000163... Val Loss: 0.001451\n",
      "Epoch: 64/100... Step: 65060... Loss: 0.000164... Val Loss: 0.001914\n",
      "Epoch: 64/100... Step: 65065... Loss: 0.000101... Val Loss: 0.001872\n",
      "Epoch: 64/100... Step: 65070... Loss: 0.000173... Val Loss: 0.001673\n",
      "Epoch: 64/100... Step: 65075... Loss: 0.000203... Val Loss: 0.001731\n",
      "Epoch: 64/100... Step: 65080... Loss: 0.000097... Val Loss: 0.001608\n",
      "Epoch: 64/100... Step: 65085... Loss: 0.000168... Val Loss: 0.001541\n",
      "Epoch: 64/100... Step: 65090... Loss: 0.000203... Val Loss: 0.001015\n",
      "Epoch: 64/100... Step: 65095... Loss: 0.000283... Val Loss: 0.001118\n",
      "Epoch: 64/100... Step: 65100... Loss: 0.000125... Val Loss: 0.001316\n",
      "Epoch: 64/100... Step: 65105... Loss: 0.000103... Val Loss: 0.001356\n",
      "Epoch: 64/100... Step: 65110... Loss: 0.000155... Val Loss: 0.001193\n",
      "Epoch: 64/100... Step: 65115... Loss: 0.000086... Val Loss: 0.000999\n",
      "Epoch: 64/100... Step: 65120... Loss: 0.000073... Val Loss: 0.001110\n",
      "Epoch: 64/100... Step: 65125... Loss: 0.000036... Val Loss: 0.001200\n",
      "Epoch: 64/100... Step: 65130... Loss: 0.000094... Val Loss: 0.001190\n",
      "Epoch: 64/100... Step: 65135... Loss: 0.000210... Val Loss: 0.001028\n",
      "Epoch: 64/100... Step: 65140... Loss: 0.000155... Val Loss: 0.001273\n",
      "Epoch: 64/100... Step: 65145... Loss: 0.000080... Val Loss: 0.001267\n",
      "Epoch: 64/100... Step: 65150... Loss: 0.000106... Val Loss: 0.001493\n",
      "Epoch: 64/100... Step: 65155... Loss: 0.000099... Val Loss: 0.001677\n",
      "Epoch: 64/100... Step: 65160... Loss: 0.000202... Val Loss: 0.001513\n",
      "Epoch: 64/100... Step: 65165... Loss: 0.000191... Val Loss: 0.001892\n",
      "Epoch: 64/100... Step: 65170... Loss: 0.000062... Val Loss: 0.001586\n",
      "Epoch: 64/100... Step: 65175... Loss: 0.000112... Val Loss: 0.001553\n",
      "Epoch: 64/100... Step: 65180... Loss: 0.000088... Val Loss: 0.001515\n",
      "Epoch: 64/100... Step: 65185... Loss: 0.000098... Val Loss: 0.001341\n",
      "Epoch: 64/100... Step: 65190... Loss: 0.000094... Val Loss: 0.001317\n",
      "Epoch: 64/100... Step: 65195... Loss: 0.000096... Val Loss: 0.001201\n",
      "Epoch: 64/100... Step: 65200... Loss: 0.000157... Val Loss: 0.001119\n",
      "Epoch: 64/100... Step: 65205... Loss: 0.000149... Val Loss: 0.001190\n",
      "Epoch: 64/100... Step: 65210... Loss: 0.000078... Val Loss: 0.001155\n",
      "Epoch: 64/100... Step: 65215... Loss: 0.000085... Val Loss: 0.001141\n",
      "Epoch: 64/100... Step: 65220... Loss: 0.000158... Val Loss: 0.001167\n",
      "Epoch: 64/100... Step: 65225... Loss: 0.000102... Val Loss: 0.001303\n",
      "Epoch: 64/100... Step: 65230... Loss: 0.000090... Val Loss: 0.001188\n",
      "Epoch: 64/100... Step: 65235... Loss: 0.000145... Val Loss: 0.001124\n",
      "Epoch: 64/100... Step: 65240... Loss: 0.000114... Val Loss: 0.001238\n",
      "Epoch: 64/100... Step: 65245... Loss: 0.000139... Val Loss: 0.001189\n",
      "Epoch: 64/100... Step: 65250... Loss: 0.000070... Val Loss: 0.001077\n",
      "Epoch: 64/100... Step: 65255... Loss: 0.000135... Val Loss: 0.001183\n",
      "Epoch: 64/100... Step: 65260... Loss: 0.000103... Val Loss: 0.001146\n",
      "Epoch: 64/100... Step: 65265... Loss: 0.000060... Val Loss: 0.001044\n",
      "Epoch: 64/100... Step: 65270... Loss: 0.000061... Val Loss: 0.001150\n",
      "Epoch: 64/100... Step: 65275... Loss: 0.000097... Val Loss: 0.000991\n",
      "Epoch: 64/100... Step: 65280... Loss: 0.000065... Val Loss: 0.000843\n",
      "Epoch: 64/100... Step: 65285... Loss: 0.000123... Val Loss: 0.000832\n",
      "Epoch: 64/100... Step: 65290... Loss: 0.000099... Val Loss: 0.000920\n",
      "Epoch: 64/100... Step: 65295... Loss: 0.000128... Val Loss: 0.000920\n",
      "Epoch: 64/100... Step: 65300... Loss: 0.000125... Val Loss: 0.000871\n",
      "Epoch: 64/100... Step: 65305... Loss: 0.000252... Val Loss: 0.000774\n",
      "Epoch: 64/100... Step: 65310... Loss: 0.000111... Val Loss: 0.000599\n",
      "Epoch: 64/100... Step: 65315... Loss: 0.000104... Val Loss: 0.000590\n",
      "Epoch: 64/100... Step: 65320... Loss: 0.000122... Val Loss: 0.000685\n",
      "Epoch: 64/100... Step: 65325... Loss: 0.000130... Val Loss: 0.000670\n",
      "Epoch: 64/100... Step: 65330... Loss: 0.000076... Val Loss: 0.000653\n",
      "Epoch: 64/100... Step: 65335... Loss: 0.000059... Val Loss: 0.000606\n",
      "Epoch: 64/100... Step: 65340... Loss: 0.000092... Val Loss: 0.000537\n",
      "Epoch: 64/100... Step: 65345... Loss: 0.000075... Val Loss: 0.000441\n",
      "Epoch: 64/100... Step: 65350... Loss: 0.000050... Val Loss: 0.000419\n",
      "Epoch: 64/100... Step: 65355... Loss: 0.000097... Val Loss: 0.000600\n",
      "Epoch: 64/100... Step: 65360... Loss: 0.000153... Val Loss: 0.000612\n",
      "Epoch: 64/100... Step: 65365... Loss: 0.000080... Val Loss: 0.000289\n",
      "Epoch: 64/100... Step: 65370... Loss: 0.000198... Val Loss: 0.000562\n",
      "Epoch: 64/100... Step: 65375... Loss: 0.000069... Val Loss: 0.000743\n",
      "Epoch: 64/100... Step: 65380... Loss: 0.000124... Val Loss: 0.000983\n",
      "Epoch: 64/100... Step: 65385... Loss: 0.000122... Val Loss: 0.001078\n",
      "Epoch: 64/100... Step: 65390... Loss: 0.000130... Val Loss: 0.001040\n",
      "Epoch: 64/100... Step: 65395... Loss: 0.000153... Val Loss: 0.001209\n",
      "Epoch: 64/100... Step: 65400... Loss: 0.000112... Val Loss: 0.001082\n",
      "Epoch: 64/100... Step: 65405... Loss: 0.000107... Val Loss: 0.001079\n",
      "Epoch: 64/100... Step: 65410... Loss: 0.000120... Val Loss: 0.001034\n",
      "Epoch: 64/100... Step: 65415... Loss: 0.000147... Val Loss: 0.001164\n",
      "Epoch: 64/100... Step: 65420... Loss: 0.000227... Val Loss: 0.001198\n",
      "Epoch: 64/100... Step: 65425... Loss: 0.000136... Val Loss: 0.000286\n",
      "Epoch: 64/100... Step: 65430... Loss: 0.000701... Val Loss: 0.000426\n",
      "Epoch: 64/100... Step: 65435... Loss: 0.000436... Val Loss: 0.001347\n",
      "Epoch: 64/100... Step: 65440... Loss: 0.000089... Val Loss: 0.000804\n",
      "Epoch: 64/100... Step: 65445... Loss: 0.000276... Val Loss: 0.001176\n",
      "Epoch: 64/100... Step: 65450... Loss: 0.000116... Val Loss: 0.001328\n",
      "Epoch: 64/100... Step: 65455... Loss: 0.000243... Val Loss: 0.001168\n",
      "Epoch: 64/100... Step: 65460... Loss: 0.000305... Val Loss: 0.000899\n",
      "Epoch: 64/100... Step: 65465... Loss: 0.000300... Val Loss: 0.001280\n",
      "Epoch: 64/100... Step: 65470... Loss: 0.000154... Val Loss: 0.001546\n",
      "Epoch: 64/100... Step: 65475... Loss: 0.000146... Val Loss: 0.001289\n",
      "Epoch: 64/100... Step: 65480... Loss: 0.000070... Val Loss: 0.001398\n",
      "Epoch: 64/100... Step: 65485... Loss: 0.000210... Val Loss: 0.001275\n",
      "Epoch: 64/100... Step: 65490... Loss: 0.000141... Val Loss: 0.001170\n",
      "Epoch: 64/100... Step: 65495... Loss: 0.000083... Val Loss: 0.001353\n",
      "Epoch: 64/100... Step: 65500... Loss: 0.000151... Val Loss: 0.001109\n",
      "Epoch: 64/100... Step: 65505... Loss: 0.000147... Val Loss: 0.000816\n",
      "Epoch: 64/100... Step: 65510... Loss: 0.000142... Val Loss: 0.000844\n",
      "Epoch: 64/100... Step: 65515... Loss: 0.000080... Val Loss: 0.000713\n",
      "Epoch: 64/100... Step: 65520... Loss: 0.000149... Val Loss: 0.000478\n",
      "Epoch: 64/100... Step: 65525... Loss: 0.000140... Val Loss: 0.000363\n",
      "Epoch: 64/100... Step: 65530... Loss: 0.000131... Val Loss: 0.000286\n",
      "Epoch: 64/100... Step: 65535... Loss: 0.000106... Val Loss: 0.000474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64/100... Step: 65540... Loss: 0.000144... Val Loss: 0.000732\n",
      "Epoch: 64/100... Step: 65545... Loss: 0.000110... Val Loss: 0.000867\n",
      "Epoch: 64/100... Step: 65550... Loss: 0.000252... Val Loss: 0.000736\n",
      "Epoch: 64/100... Step: 65555... Loss: 0.000112... Val Loss: 0.000655\n",
      "Epoch: 64/100... Step: 65560... Loss: 0.000127... Val Loss: 0.000767\n",
      "Epoch: 64/100... Step: 65565... Loss: 0.000104... Val Loss: 0.000827\n",
      "Epoch: 64/100... Step: 65570... Loss: 0.000110... Val Loss: 0.000696\n",
      "Epoch: 64/100... Step: 65575... Loss: 0.000135... Val Loss: 0.000638\n",
      "Epoch: 64/100... Step: 65580... Loss: 0.000119... Val Loss: 0.000698\n",
      "Epoch: 64/100... Step: 65585... Loss: 0.000128... Val Loss: 0.000818\n",
      "Epoch: 64/100... Step: 65590... Loss: 0.000117... Val Loss: 0.000940\n",
      "Epoch: 64/100... Step: 65595... Loss: 0.000115... Val Loss: 0.000768\n",
      "Epoch: 64/100... Step: 65600... Loss: 0.000122... Val Loss: 0.000859\n",
      "Epoch: 64/100... Step: 65605... Loss: 0.000054... Val Loss: 0.000808\n",
      "Epoch: 64/100... Step: 65610... Loss: 0.000078... Val Loss: 0.000794\n",
      "Epoch: 64/100... Step: 65615... Loss: 0.000055... Val Loss: 0.000902\n",
      "Epoch: 64/100... Step: 65620... Loss: 0.000063... Val Loss: 0.000964\n",
      "Epoch: 64/100... Step: 65625... Loss: 0.000085... Val Loss: 0.000931\n",
      "Epoch: 64/100... Step: 65630... Loss: 0.000146... Val Loss: 0.000826\n",
      "Epoch: 64/100... Step: 65635... Loss: 0.000140... Val Loss: 0.000971\n",
      "Epoch: 64/100... Step: 65640... Loss: 0.000123... Val Loss: 0.000946\n",
      "Epoch: 64/100... Step: 65645... Loss: 0.000102... Val Loss: 0.000865\n",
      "Epoch: 64/100... Step: 65650... Loss: 0.000059... Val Loss: 0.000916\n",
      "Epoch: 64/100... Step: 65655... Loss: 0.000072... Val Loss: 0.000904\n",
      "Epoch: 64/100... Step: 65660... Loss: 0.000104... Val Loss: 0.000587\n",
      "Epoch: 64/100... Step: 65665... Loss: 0.000220... Val Loss: 0.000710\n",
      "Epoch: 64/100... Step: 65670... Loss: 0.000089... Val Loss: 0.000817\n",
      "Epoch: 64/100... Step: 65675... Loss: 0.000085... Val Loss: 0.000806\n",
      "Epoch: 64/100... Step: 65680... Loss: 0.000138... Val Loss: 0.000575\n",
      "Epoch: 64/100... Step: 65685... Loss: 0.000245... Val Loss: 0.000341\n",
      "Epoch: 64/100... Step: 65690... Loss: 0.000060... Val Loss: 0.000579\n",
      "Epoch: 64/100... Step: 65695... Loss: 0.000196... Val Loss: 0.000485\n",
      "Epoch: 64/100... Step: 65700... Loss: 0.000116... Val Loss: 0.000260\n",
      "Epoch: 64/100... Step: 65705... Loss: 0.000131... Val Loss: 0.000353\n",
      "Epoch: 64/100... Step: 65710... Loss: 0.000115... Val Loss: 0.000265\n",
      "Epoch: 64/100... Step: 65715... Loss: 0.000209... Val Loss: 0.000255\n",
      "Validation loss decreased (0.000256 --> 0.000255).  Saving model ...\n",
      "Epoch: 64/100... Step: 65720... Loss: 0.000078... Val Loss: 0.000278\n",
      "Epoch: 64/100... Step: 65725... Loss: 0.000086... Val Loss: 0.000282\n",
      "Epoch: 64/100... Step: 65730... Loss: 0.000057... Val Loss: 0.000463\n",
      "Epoch: 64/100... Step: 65735... Loss: 0.000112... Val Loss: 0.000515\n",
      "Epoch: 64/100... Step: 65740... Loss: 0.000096... Val Loss: 0.000364\n",
      "Epoch: 64/100... Step: 65745... Loss: 0.000224... Val Loss: 0.000587\n",
      "Epoch: 64/100... Step: 65750... Loss: 0.000166... Val Loss: 0.000389\n",
      "Epoch: 64/100... Step: 65755... Loss: 0.000285... Val Loss: 0.000264\n",
      "Epoch: 64/100... Step: 65760... Loss: 0.000319... Val Loss: 0.000545\n",
      "Epoch: 64/100... Step: 65765... Loss: 0.000125... Val Loss: 0.000382\n",
      "Epoch: 64/100... Step: 65770... Loss: 0.000059... Val Loss: 0.000759\n",
      "Epoch: 64/100... Step: 65775... Loss: 0.000125... Val Loss: 0.000358\n",
      "Epoch: 64/100... Step: 65780... Loss: 0.000156... Val Loss: 0.000346\n",
      "Epoch: 64/100... Step: 65785... Loss: 0.000171... Val Loss: 0.000433\n",
      "Epoch: 64/100... Step: 65790... Loss: 0.000187... Val Loss: 0.000358\n",
      "Epoch: 64/100... Step: 65795... Loss: 0.000124... Val Loss: 0.000399\n",
      "Epoch: 64/100... Step: 65800... Loss: 0.000259... Val Loss: 0.000631\n",
      "Epoch: 64/100... Step: 65805... Loss: 0.000180... Val Loss: 0.000742\n",
      "Epoch: 64/100... Step: 65810... Loss: 0.000174... Val Loss: 0.000376\n",
      "Epoch: 64/100... Step: 65815... Loss: 0.000149... Val Loss: 0.000288\n",
      "Epoch: 64/100... Step: 65820... Loss: 0.000148... Val Loss: 0.000611\n",
      "Epoch: 64/100... Step: 65825... Loss: 0.000130... Val Loss: 0.000818\n",
      "Epoch: 64/100... Step: 65830... Loss: 0.000095... Val Loss: 0.000697\n",
      "Epoch: 64/100... Step: 65835... Loss: 0.000161... Val Loss: 0.000607\n",
      "Epoch: 64/100... Step: 65840... Loss: 0.000042... Val Loss: 0.000686\n",
      "Epoch: 64/100... Step: 65845... Loss: 0.000057... Val Loss: 0.000598\n",
      "Epoch: 64/100... Step: 65850... Loss: 0.000094... Val Loss: 0.000620\n",
      "Epoch: 64/100... Step: 65855... Loss: 0.000051... Val Loss: 0.000607\n",
      "Epoch: 64/100... Step: 65860... Loss: 0.000122... Val Loss: 0.000648\n",
      "Epoch: 64/100... Step: 65865... Loss: 0.000089... Val Loss: 0.000608\n",
      "Epoch: 64/100... Step: 65870... Loss: 0.000064... Val Loss: 0.000583\n",
      "Epoch: 64/100... Step: 65875... Loss: 0.000122... Val Loss: 0.000737\n",
      "Epoch: 64/100... Step: 65880... Loss: 0.000066... Val Loss: 0.000770\n",
      "Epoch: 64/100... Step: 65885... Loss: 0.000102... Val Loss: 0.000820\n",
      "Epoch: 64/100... Step: 65890... Loss: 0.000092... Val Loss: 0.000661\n",
      "Epoch: 64/100... Step: 65895... Loss: 0.000113... Val Loss: 0.000713\n",
      "Epoch: 64/100... Step: 65900... Loss: 0.000154... Val Loss: 0.000794\n",
      "Epoch: 64/100... Step: 65905... Loss: 0.000059... Val Loss: 0.000687\n",
      "Epoch: 64/100... Step: 65910... Loss: 0.000169... Val Loss: 0.000676\n",
      "Epoch: 64/100... Step: 65915... Loss: 0.000113... Val Loss: 0.000625\n",
      "Epoch: 64/100... Step: 65920... Loss: 0.000119... Val Loss: 0.000572\n",
      "Epoch: 64/100... Step: 65925... Loss: 0.000215... Val Loss: 0.000410\n",
      "Epoch: 64/100... Step: 65930... Loss: 0.000082... Val Loss: 0.000452\n",
      "Epoch: 64/100... Step: 65935... Loss: 0.000167... Val Loss: 0.000809\n",
      "Epoch: 64/100... Step: 65940... Loss: 0.000197... Val Loss: 0.000850\n",
      "Epoch: 64/100... Step: 65945... Loss: 0.000167... Val Loss: 0.000777\n",
      "Epoch: 64/100... Step: 65950... Loss: 0.000207... Val Loss: 0.000590\n",
      "Epoch: 64/100... Step: 65955... Loss: 0.000120... Val Loss: 0.000934\n",
      "Epoch: 64/100... Step: 65960... Loss: 0.000107... Val Loss: 0.000713\n",
      "Epoch: 64/100... Step: 65965... Loss: 0.000172... Val Loss: 0.000685\n",
      "Epoch: 64/100... Step: 65970... Loss: 0.000174... Val Loss: 0.001053\n",
      "Epoch: 64/100... Step: 65975... Loss: 0.000105... Val Loss: 0.000756\n",
      "Epoch: 64/100... Step: 65980... Loss: 0.000156... Val Loss: 0.000901\n",
      "Epoch: 64/100... Step: 65985... Loss: 0.000132... Val Loss: 0.001000\n",
      "Epoch: 64/100... Step: 65990... Loss: 0.000093... Val Loss: 0.000961\n",
      "Epoch: 64/100... Step: 65995... Loss: 0.000174... Val Loss: 0.001032\n",
      "Epoch: 64/100... Step: 66000... Loss: 0.000150... Val Loss: 0.000926\n",
      "Epoch: 64/100... Step: 66005... Loss: 0.000097... Val Loss: 0.001083\n",
      "Epoch: 64/100... Step: 66010... Loss: 0.000219... Val Loss: 0.001068\n",
      "Epoch: 64/100... Step: 66015... Loss: 0.000136... Val Loss: 0.001224\n",
      "Epoch: 64/100... Step: 66020... Loss: 0.000205... Val Loss: 0.001022\n",
      "Epoch: 64/100... Step: 66025... Loss: 0.000096... Val Loss: 0.000903\n",
      "Epoch: 64/100... Step: 66030... Loss: 0.000156... Val Loss: 0.000938\n",
      "Epoch: 64/100... Step: 66035... Loss: 0.000162... Val Loss: 0.001162\n",
      "Epoch: 64/100... Step: 66040... Loss: 0.000096... Val Loss: 0.001106\n",
      "Epoch: 64/100... Step: 66045... Loss: 0.000164... Val Loss: 0.001007\n",
      "Epoch: 65/100... Step: 66050... Loss: 0.000185... Val Loss: 0.001465\n",
      "Epoch: 65/100... Step: 66055... Loss: 0.000212... Val Loss: 0.001321\n",
      "Epoch: 65/100... Step: 66060... Loss: 0.000116... Val Loss: 0.001548\n",
      "Epoch: 65/100... Step: 66065... Loss: 0.000179... Val Loss: 0.001685\n",
      "Epoch: 65/100... Step: 66070... Loss: 0.000156... Val Loss: 0.001534\n",
      "Epoch: 65/100... Step: 66075... Loss: 0.000128... Val Loss: 0.001671\n",
      "Epoch: 65/100... Step: 66080... Loss: 0.000151... Val Loss: 0.001794\n",
      "Epoch: 65/100... Step: 66085... Loss: 0.000184... Val Loss: 0.001756\n",
      "Epoch: 65/100... Step: 66090... Loss: 0.000147... Val Loss: 0.001772\n",
      "Epoch: 65/100... Step: 66095... Loss: 0.000107... Val Loss: 0.001790\n",
      "Epoch: 65/100... Step: 66100... Loss: 0.000085... Val Loss: 0.001736\n",
      "Epoch: 65/100... Step: 66105... Loss: 0.000124... Val Loss: 0.001662\n",
      "Epoch: 65/100... Step: 66110... Loss: 0.000214... Val Loss: 0.001563\n",
      "Epoch: 65/100... Step: 66115... Loss: 0.000100... Val Loss: 0.001511\n",
      "Epoch: 65/100... Step: 66120... Loss: 0.000193... Val Loss: 0.001176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65/100... Step: 66125... Loss: 0.000187... Val Loss: 0.001382\n",
      "Epoch: 65/100... Step: 66130... Loss: 0.000128... Val Loss: 0.001435\n",
      "Epoch: 65/100... Step: 66135... Loss: 0.000142... Val Loss: 0.001503\n",
      "Epoch: 65/100... Step: 66140... Loss: 0.000125... Val Loss: 0.001366\n",
      "Epoch: 65/100... Step: 66145... Loss: 0.000200... Val Loss: 0.000909\n",
      "Epoch: 65/100... Step: 66150... Loss: 0.000122... Val Loss: 0.001095\n",
      "Epoch: 65/100... Step: 66155... Loss: 0.000078... Val Loss: 0.001127\n",
      "Epoch: 65/100... Step: 66160... Loss: 0.000075... Val Loss: 0.001153\n",
      "Epoch: 65/100... Step: 66165... Loss: 0.000092... Val Loss: 0.001017\n",
      "Epoch: 65/100... Step: 66170... Loss: 0.000065... Val Loss: 0.001113\n",
      "Epoch: 65/100... Step: 66175... Loss: 0.000158... Val Loss: 0.001345\n",
      "Epoch: 65/100... Step: 66180... Loss: 0.000087... Val Loss: 0.001377\n",
      "Epoch: 65/100... Step: 66185... Loss: 0.000077... Val Loss: 0.001501\n",
      "Epoch: 65/100... Step: 66190... Loss: 0.000112... Val Loss: 0.001560\n",
      "Epoch: 65/100... Step: 66195... Loss: 0.000101... Val Loss: 0.001678\n",
      "Epoch: 65/100... Step: 66200... Loss: 0.000054... Val Loss: 0.001701\n",
      "Epoch: 65/100... Step: 66205... Loss: 0.000097... Val Loss: 0.001534\n",
      "Epoch: 65/100... Step: 66210... Loss: 0.000110... Val Loss: 0.001473\n",
      "Epoch: 65/100... Step: 66215... Loss: 0.000069... Val Loss: 0.001341\n",
      "Epoch: 65/100... Step: 66220... Loss: 0.000050... Val Loss: 0.001382\n",
      "Epoch: 65/100... Step: 66225... Loss: 0.000080... Val Loss: 0.001360\n",
      "Epoch: 65/100... Step: 66230... Loss: 0.000062... Val Loss: 0.001312\n",
      "Epoch: 65/100... Step: 66235... Loss: 0.000089... Val Loss: 0.001176\n",
      "Epoch: 65/100... Step: 66240... Loss: 0.000081... Val Loss: 0.001140\n",
      "Epoch: 65/100... Step: 66245... Loss: 0.000047... Val Loss: 0.001235\n",
      "Epoch: 65/100... Step: 66250... Loss: 0.000060... Val Loss: 0.001208\n",
      "Epoch: 65/100... Step: 66255... Loss: 0.000077... Val Loss: 0.001268\n",
      "Epoch: 65/100... Step: 66260... Loss: 0.000056... Val Loss: 0.001210\n",
      "Epoch: 65/100... Step: 66265... Loss: 0.000075... Val Loss: 0.001190\n",
      "Epoch: 65/100... Step: 66270... Loss: 0.000059... Val Loss: 0.001223\n",
      "Epoch: 65/100... Step: 66275... Loss: 0.000114... Val Loss: 0.001095\n",
      "Epoch: 65/100... Step: 66280... Loss: 0.000105... Val Loss: 0.001048\n",
      "Epoch: 65/100... Step: 66285... Loss: 0.000097... Val Loss: 0.001112\n",
      "Epoch: 65/100... Step: 66290... Loss: 0.000118... Val Loss: 0.001222\n",
      "Epoch: 65/100... Step: 66295... Loss: 0.000069... Val Loss: 0.001145\n",
      "Epoch: 65/100... Step: 66300... Loss: 0.000096... Val Loss: 0.001187\n",
      "Epoch: 65/100... Step: 66305... Loss: 0.000097... Val Loss: 0.001176\n",
      "Epoch: 65/100... Step: 66310... Loss: 0.000090... Val Loss: 0.000907\n",
      "Epoch: 65/100... Step: 66315... Loss: 0.000101... Val Loss: 0.000888\n",
      "Epoch: 65/100... Step: 66320... Loss: 0.000070... Val Loss: 0.000905\n",
      "Epoch: 65/100... Step: 66325... Loss: 0.000038... Val Loss: 0.000904\n",
      "Epoch: 65/100... Step: 66330... Loss: 0.000044... Val Loss: 0.000911\n",
      "Epoch: 65/100... Step: 66335... Loss: 0.000111... Val Loss: 0.000825\n",
      "Epoch: 65/100... Step: 66340... Loss: 0.000076... Val Loss: 0.000602\n",
      "Epoch: 65/100... Step: 66345... Loss: 0.000119... Val Loss: 0.000562\n",
      "Epoch: 65/100... Step: 66350... Loss: 0.000099... Val Loss: 0.000557\n",
      "Epoch: 65/100... Step: 66355... Loss: 0.000127... Val Loss: 0.000798\n",
      "Epoch: 65/100... Step: 66360... Loss: 0.000104... Val Loss: 0.000521\n",
      "Epoch: 65/100... Step: 66365... Loss: 0.000055... Val Loss: 0.000673\n",
      "Epoch: 65/100... Step: 66370... Loss: 0.000057... Val Loss: 0.000665\n",
      "Epoch: 65/100... Step: 66375... Loss: 0.000089... Val Loss: 0.000478\n",
      "Epoch: 65/100... Step: 66380... Loss: 0.000077... Val Loss: 0.000413\n",
      "Epoch: 65/100... Step: 66385... Loss: 0.000094... Val Loss: 0.000494\n",
      "Epoch: 65/100... Step: 66390... Loss: 0.000095... Val Loss: 0.000598\n",
      "Epoch: 65/100... Step: 66395... Loss: 0.000247... Val Loss: 0.000528\n",
      "Epoch: 65/100... Step: 66400... Loss: 0.000412... Val Loss: 0.000304\n",
      "Epoch: 65/100... Step: 66405... Loss: 0.000191... Val Loss: 0.000663\n",
      "Epoch: 65/100... Step: 66410... Loss: 0.000152... Val Loss: 0.000935\n",
      "Epoch: 65/100... Step: 66415... Loss: 0.000100... Val Loss: 0.001022\n",
      "Epoch: 65/100... Step: 66420... Loss: 0.000185... Val Loss: 0.000949\n",
      "Epoch: 65/100... Step: 66425... Loss: 0.000132... Val Loss: 0.000900\n",
      "Epoch: 65/100... Step: 66430... Loss: 0.000102... Val Loss: 0.000997\n",
      "Epoch: 65/100... Step: 66435... Loss: 0.000129... Val Loss: 0.001071\n",
      "Epoch: 65/100... Step: 66440... Loss: 0.000082... Val Loss: 0.001079\n",
      "Epoch: 65/100... Step: 66445... Loss: 0.000075... Val Loss: 0.001137\n",
      "Epoch: 65/100... Step: 66450... Loss: 0.000151... Val Loss: 0.001154\n",
      "Epoch: 65/100... Step: 66455... Loss: 0.000290... Val Loss: 0.000590\n",
      "Epoch: 65/100... Step: 66460... Loss: 0.000364... Val Loss: 0.000411\n",
      "Epoch: 65/100... Step: 66465... Loss: 0.000860... Val Loss: 0.000446\n",
      "Epoch: 65/100... Step: 66470... Loss: 0.000967... Val Loss: 0.001770\n",
      "Epoch: 65/100... Step: 66475... Loss: 0.000488... Val Loss: 0.000784\n",
      "Epoch: 65/100... Step: 66480... Loss: 0.000196... Val Loss: 0.001015\n",
      "Epoch: 65/100... Step: 66485... Loss: 0.000294... Val Loss: 0.001546\n",
      "Epoch: 65/100... Step: 66490... Loss: 0.000313... Val Loss: 0.001143\n",
      "Epoch: 65/100... Step: 66495... Loss: 0.000236... Val Loss: 0.001195\n",
      "Epoch: 65/100... Step: 66500... Loss: 0.000432... Val Loss: 0.001033\n",
      "Epoch: 65/100... Step: 66505... Loss: 0.000176... Val Loss: 0.001256\n",
      "Epoch: 65/100... Step: 66510... Loss: 0.000210... Val Loss: 0.001385\n",
      "Epoch: 65/100... Step: 66515... Loss: 0.000320... Val Loss: 0.001433\n",
      "Epoch: 65/100... Step: 66520... Loss: 0.000139... Val Loss: 0.001063\n",
      "Epoch: 65/100... Step: 66525... Loss: 0.000197... Val Loss: 0.001153\n",
      "Epoch: 65/100... Step: 66530... Loss: 0.000224... Val Loss: 0.001163\n",
      "Epoch: 65/100... Step: 66535... Loss: 0.000440... Val Loss: 0.000750\n",
      "Epoch: 65/100... Step: 66540... Loss: 0.000146... Val Loss: 0.001298\n",
      "Epoch: 65/100... Step: 66545... Loss: 0.000151... Val Loss: 0.000907\n",
      "Epoch: 65/100... Step: 66550... Loss: 0.000212... Val Loss: 0.000712\n",
      "Epoch: 65/100... Step: 66555... Loss: 0.000275... Val Loss: 0.000278\n",
      "Epoch: 65/100... Step: 66560... Loss: 0.000156... Val Loss: 0.000401\n",
      "Epoch: 65/100... Step: 66565... Loss: 0.000208... Val Loss: 0.000400\n",
      "Epoch: 65/100... Step: 66570... Loss: 0.000227... Val Loss: 0.000388\n",
      "Epoch: 65/100... Step: 66575... Loss: 0.000097... Val Loss: 0.000678\n",
      "Epoch: 65/100... Step: 66580... Loss: 0.000235... Val Loss: 0.000534\n",
      "Epoch: 65/100... Step: 66585... Loss: 0.000184... Val Loss: 0.000552\n",
      "Epoch: 65/100... Step: 66590... Loss: 0.000222... Val Loss: 0.000611\n",
      "Epoch: 65/100... Step: 66595... Loss: 0.000090... Val Loss: 0.000837\n",
      "Epoch: 65/100... Step: 66600... Loss: 0.000181... Val Loss: 0.000923\n",
      "Epoch: 65/100... Step: 66605... Loss: 0.000267... Val Loss: 0.000486\n",
      "Epoch: 65/100... Step: 66610... Loss: 0.000117... Val Loss: 0.000596\n",
      "Epoch: 65/100... Step: 66615... Loss: 0.000290... Val Loss: 0.000750\n",
      "Epoch: 65/100... Step: 66620... Loss: 0.000423... Val Loss: 0.000802\n",
      "Epoch: 65/100... Step: 66625... Loss: 0.000659... Val Loss: 0.000565\n",
      "Epoch: 65/100... Step: 66630... Loss: 0.000277... Val Loss: 0.000680\n",
      "Epoch: 65/100... Step: 66635... Loss: 0.000385... Val Loss: 0.001131\n",
      "Epoch: 65/100... Step: 66640... Loss: 0.000089... Val Loss: 0.001118\n",
      "Epoch: 65/100... Step: 66645... Loss: 0.000151... Val Loss: 0.000887\n",
      "Epoch: 65/100... Step: 66650... Loss: 0.000204... Val Loss: 0.001142\n",
      "Epoch: 65/100... Step: 66655... Loss: 0.000156... Val Loss: 0.000916\n",
      "Epoch: 65/100... Step: 66660... Loss: 0.000060... Val Loss: 0.000988\n",
      "Epoch: 65/100... Step: 66665... Loss: 0.000208... Val Loss: 0.000923\n",
      "Epoch: 65/100... Step: 66670... Loss: 0.000248... Val Loss: 0.001208\n",
      "Epoch: 65/100... Step: 66675... Loss: 0.000118... Val Loss: 0.000835\n",
      "Epoch: 65/100... Step: 66680... Loss: 0.000093... Val Loss: 0.001085\n",
      "Epoch: 65/100... Step: 66685... Loss: 0.000194... Val Loss: 0.000775\n",
      "Epoch: 65/100... Step: 66690... Loss: 0.000157... Val Loss: 0.000583\n",
      "Epoch: 65/100... Step: 66695... Loss: 0.000204... Val Loss: 0.001145\n",
      "Epoch: 65/100... Step: 66700... Loss: 0.000172... Val Loss: 0.000945\n",
      "Epoch: 65/100... Step: 66705... Loss: 0.000166... Val Loss: 0.000990\n",
      "Epoch: 65/100... Step: 66710... Loss: 0.000133... Val Loss: 0.000799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65/100... Step: 66715... Loss: 0.000240... Val Loss: 0.000602\n",
      "Epoch: 65/100... Step: 66720... Loss: 0.000100... Val Loss: 0.000290\n",
      "Epoch: 65/100... Step: 66725... Loss: 0.000109... Val Loss: 0.000487\n",
      "Epoch: 65/100... Step: 66730... Loss: 0.000281... Val Loss: 0.000271\n",
      "Epoch: 65/100... Step: 66735... Loss: 0.000362... Val Loss: 0.000391\n",
      "Epoch: 65/100... Step: 66740... Loss: 0.000316... Val Loss: 0.000457\n",
      "Epoch: 65/100... Step: 66745... Loss: 0.000072... Val Loss: 0.000289\n",
      "Epoch: 65/100... Step: 66750... Loss: 0.000297... Val Loss: 0.000300\n",
      "Epoch: 65/100... Step: 66755... Loss: 0.000238... Val Loss: 0.000478\n",
      "Epoch: 65/100... Step: 66760... Loss: 0.000123... Val Loss: 0.000448\n",
      "Epoch: 65/100... Step: 66765... Loss: 0.000179... Val Loss: 0.000506\n",
      "Epoch: 65/100... Step: 66770... Loss: 0.000142... Val Loss: 0.000393\n",
      "Epoch: 65/100... Step: 66775... Loss: 0.000183... Val Loss: 0.000370\n",
      "Epoch: 65/100... Step: 66780... Loss: 0.000213... Val Loss: 0.000292\n",
      "Epoch: 65/100... Step: 66785... Loss: 0.000261... Val Loss: 0.000293\n",
      "Epoch: 65/100... Step: 66790... Loss: 0.000186... Val Loss: 0.000371\n",
      "Epoch: 65/100... Step: 66795... Loss: 0.000127... Val Loss: 0.000423\n",
      "Epoch: 65/100... Step: 66800... Loss: 0.000200... Val Loss: 0.000684\n",
      "Epoch: 65/100... Step: 66805... Loss: 0.000266... Val Loss: 0.000876\n",
      "Epoch: 65/100... Step: 66810... Loss: 0.000207... Val Loss: 0.000360\n",
      "Epoch: 65/100... Step: 66815... Loss: 0.000304... Val Loss: 0.000586\n",
      "Epoch: 65/100... Step: 66820... Loss: 0.000120... Val Loss: 0.000381\n",
      "Epoch: 65/100... Step: 66825... Loss: 0.000194... Val Loss: 0.000434\n",
      "Epoch: 65/100... Step: 66830... Loss: 0.000172... Val Loss: 0.000696\n",
      "Epoch: 65/100... Step: 66835... Loss: 0.000211... Val Loss: 0.000529\n",
      "Epoch: 65/100... Step: 66840... Loss: 0.000128... Val Loss: 0.000486\n",
      "Epoch: 65/100... Step: 66845... Loss: 0.000187... Val Loss: 0.000332\n",
      "Epoch: 65/100... Step: 66850... Loss: 0.000167... Val Loss: 0.000394\n",
      "Epoch: 65/100... Step: 66855... Loss: 0.000289... Val Loss: 0.000724\n",
      "Epoch: 65/100... Step: 66860... Loss: 0.000420... Val Loss: 0.000516\n",
      "Epoch: 65/100... Step: 66865... Loss: 0.000341... Val Loss: 0.000525\n",
      "Epoch: 65/100... Step: 66870... Loss: 0.000361... Val Loss: 0.000575\n",
      "Epoch: 65/100... Step: 66875... Loss: 0.000139... Val Loss: 0.000478\n",
      "Epoch: 65/100... Step: 66880... Loss: 0.000141... Val Loss: 0.000583\n",
      "Epoch: 65/100... Step: 66885... Loss: 0.000291... Val Loss: 0.000587\n",
      "Epoch: 65/100... Step: 66890... Loss: 0.000095... Val Loss: 0.000486\n",
      "Epoch: 65/100... Step: 66895... Loss: 0.000239... Val Loss: 0.000555\n",
      "Epoch: 65/100... Step: 66900... Loss: 0.000177... Val Loss: 0.000655\n",
      "Epoch: 65/100... Step: 66905... Loss: 0.000107... Val Loss: 0.000807\n",
      "Epoch: 65/100... Step: 66910... Loss: 0.000165... Val Loss: 0.000761\n",
      "Epoch: 65/100... Step: 66915... Loss: 0.000082... Val Loss: 0.000758\n",
      "Epoch: 65/100... Step: 66920... Loss: 0.000075... Val Loss: 0.000767\n",
      "Epoch: 65/100... Step: 66925... Loss: 0.000109... Val Loss: 0.000773\n",
      "Epoch: 65/100... Step: 66930... Loss: 0.000184... Val Loss: 0.000854\n",
      "Epoch: 65/100... Step: 66935... Loss: 0.000073... Val Loss: 0.000641\n",
      "Epoch: 65/100... Step: 66940... Loss: 0.000151... Val Loss: 0.000605\n",
      "Epoch: 65/100... Step: 66945... Loss: 0.000113... Val Loss: 0.000684\n",
      "Epoch: 65/100... Step: 66950... Loss: 0.000094... Val Loss: 0.000639\n",
      "Epoch: 65/100... Step: 66955... Loss: 0.000298... Val Loss: 0.000519\n",
      "Epoch: 65/100... Step: 66960... Loss: 0.000384... Val Loss: 0.000711\n",
      "Epoch: 65/100... Step: 66965... Loss: 0.000073... Val Loss: 0.000687\n",
      "Epoch: 65/100... Step: 66970... Loss: 0.000250... Val Loss: 0.000884\n",
      "Epoch: 65/100... Step: 66975... Loss: 0.000074... Val Loss: 0.000716\n",
      "Epoch: 65/100... Step: 66980... Loss: 0.000260... Val Loss: 0.000819\n",
      "Epoch: 65/100... Step: 66985... Loss: 0.000216... Val Loss: 0.000651\n",
      "Epoch: 65/100... Step: 66990... Loss: 0.000181... Val Loss: 0.001001\n",
      "Epoch: 65/100... Step: 66995... Loss: 0.000125... Val Loss: 0.000794\n",
      "Epoch: 65/100... Step: 67000... Loss: 0.000192... Val Loss: 0.001039\n",
      "Epoch: 65/100... Step: 67005... Loss: 0.000128... Val Loss: 0.001097\n",
      "Epoch: 65/100... Step: 67010... Loss: 0.000170... Val Loss: 0.000979\n",
      "Epoch: 65/100... Step: 67015... Loss: 0.000113... Val Loss: 0.001006\n",
      "Epoch: 65/100... Step: 67020... Loss: 0.000156... Val Loss: 0.001056\n",
      "Epoch: 65/100... Step: 67025... Loss: 0.000125... Val Loss: 0.000958\n",
      "Epoch: 65/100... Step: 67030... Loss: 0.000406... Val Loss: 0.000929\n",
      "Epoch: 65/100... Step: 67035... Loss: 0.000119... Val Loss: 0.001203\n",
      "Epoch: 65/100... Step: 67040... Loss: 0.000074... Val Loss: 0.001112\n",
      "Epoch: 65/100... Step: 67045... Loss: 0.000108... Val Loss: 0.001180\n",
      "Epoch: 65/100... Step: 67050... Loss: 0.000115... Val Loss: 0.001086\n",
      "Epoch: 65/100... Step: 67055... Loss: 0.000069... Val Loss: 0.000901\n",
      "Epoch: 65/100... Step: 67060... Loss: 0.000136... Val Loss: 0.000900\n",
      "Epoch: 65/100... Step: 67065... Loss: 0.000098... Val Loss: 0.001038\n",
      "Epoch: 65/100... Step: 67070... Loss: 0.000092... Val Loss: 0.001048\n",
      "Epoch: 65/100... Step: 67075... Loss: 0.000104... Val Loss: 0.001089\n",
      "Epoch: 65/100... Step: 67080... Loss: 0.000111... Val Loss: 0.000950\n",
      "Epoch: 66/100... Step: 67085... Loss: 0.000391... Val Loss: 0.001487\n",
      "Epoch: 66/100... Step: 67090... Loss: 0.000157... Val Loss: 0.001496\n",
      "Epoch: 66/100... Step: 67095... Loss: 0.000160... Val Loss: 0.001674\n",
      "Epoch: 66/100... Step: 67100... Loss: 0.000138... Val Loss: 0.001727\n",
      "Epoch: 66/100... Step: 67105... Loss: 0.000114... Val Loss: 0.001792\n",
      "Epoch: 66/100... Step: 67110... Loss: 0.000144... Val Loss: 0.001887\n",
      "Epoch: 66/100... Step: 67115... Loss: 0.000101... Val Loss: 0.001865\n",
      "Epoch: 66/100... Step: 67120... Loss: 0.000180... Val Loss: 0.001530\n",
      "Epoch: 66/100... Step: 67125... Loss: 0.000208... Val Loss: 0.001743\n",
      "Epoch: 66/100... Step: 67130... Loss: 0.000227... Val Loss: 0.001708\n",
      "Epoch: 66/100... Step: 67135... Loss: 0.000111... Val Loss: 0.001654\n",
      "Epoch: 66/100... Step: 67140... Loss: 0.000200... Val Loss: 0.001687\n",
      "Epoch: 66/100... Step: 67145... Loss: 0.000120... Val Loss: 0.001611\n",
      "Epoch: 66/100... Step: 67150... Loss: 0.000209... Val Loss: 0.001414\n",
      "Epoch: 66/100... Step: 67155... Loss: 0.000174... Val Loss: 0.001124\n",
      "Epoch: 66/100... Step: 67160... Loss: 0.000096... Val Loss: 0.001139\n",
      "Epoch: 66/100... Step: 67165... Loss: 0.000110... Val Loss: 0.001338\n",
      "Epoch: 66/100... Step: 67170... Loss: 0.000151... Val Loss: 0.001440\n",
      "Epoch: 66/100... Step: 67175... Loss: 0.000153... Val Loss: 0.001197\n",
      "Epoch: 66/100... Step: 67180... Loss: 0.000146... Val Loss: 0.000875\n",
      "Epoch: 66/100... Step: 67185... Loss: 0.000186... Val Loss: 0.001060\n",
      "Epoch: 66/100... Step: 67190... Loss: 0.000042... Val Loss: 0.001117\n",
      "Epoch: 66/100... Step: 67195... Loss: 0.000104... Val Loss: 0.001035\n",
      "Epoch: 66/100... Step: 67200... Loss: 0.000072... Val Loss: 0.000962\n",
      "Epoch: 66/100... Step: 67205... Loss: 0.000231... Val Loss: 0.001429\n",
      "Epoch: 66/100... Step: 67210... Loss: 0.000128... Val Loss: 0.001461\n",
      "Epoch: 66/100... Step: 67215... Loss: 0.000060... Val Loss: 0.001372\n",
      "Epoch: 66/100... Step: 67220... Loss: 0.000125... Val Loss: 0.001495\n",
      "Epoch: 66/100... Step: 67225... Loss: 0.000089... Val Loss: 0.001639\n",
      "Epoch: 66/100... Step: 67230... Loss: 0.000086... Val Loss: 0.001835\n",
      "Epoch: 66/100... Step: 67235... Loss: 0.000106... Val Loss: 0.001646\n",
      "Epoch: 66/100... Step: 67240... Loss: 0.000129... Val Loss: 0.001523\n",
      "Epoch: 66/100... Step: 67245... Loss: 0.000085... Val Loss: 0.001464\n",
      "Epoch: 66/100... Step: 67250... Loss: 0.000118... Val Loss: 0.001362\n",
      "Epoch: 66/100... Step: 67255... Loss: 0.000100... Val Loss: 0.001346\n",
      "Epoch: 66/100... Step: 67260... Loss: 0.000137... Val Loss: 0.001202\n",
      "Epoch: 66/100... Step: 67265... Loss: 0.000106... Val Loss: 0.001167\n",
      "Epoch: 66/100... Step: 67270... Loss: 0.000097... Val Loss: 0.001145\n",
      "Epoch: 66/100... Step: 67275... Loss: 0.000150... Val Loss: 0.001369\n",
      "Epoch: 66/100... Step: 67280... Loss: 0.000230... Val Loss: 0.000947\n",
      "Epoch: 66/100... Step: 67285... Loss: 0.000145... Val Loss: 0.001160\n",
      "Epoch: 66/100... Step: 67290... Loss: 0.000049... Val Loss: 0.001312\n",
      "Epoch: 66/100... Step: 67295... Loss: 0.000070... Val Loss: 0.001321\n",
      "Epoch: 66/100... Step: 67300... Loss: 0.000104... Val Loss: 0.001310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66/100... Step: 67305... Loss: 0.000086... Val Loss: 0.001282\n",
      "Epoch: 66/100... Step: 67310... Loss: 0.000121... Val Loss: 0.001106\n",
      "Epoch: 66/100... Step: 67315... Loss: 0.000084... Val Loss: 0.001147\n",
      "Epoch: 66/100... Step: 67320... Loss: 0.000119... Val Loss: 0.001158\n",
      "Epoch: 66/100... Step: 67325... Loss: 0.000085... Val Loss: 0.001018\n",
      "Epoch: 66/100... Step: 67330... Loss: 0.000066... Val Loss: 0.001068\n",
      "Epoch: 66/100... Step: 67335... Loss: 0.000058... Val Loss: 0.001118\n",
      "Epoch: 66/100... Step: 67340... Loss: 0.000087... Val Loss: 0.000927\n",
      "Epoch: 66/100... Step: 67345... Loss: 0.000105... Val Loss: 0.000932\n",
      "Epoch: 66/100... Step: 67350... Loss: 0.000098... Val Loss: 0.000919\n",
      "Epoch: 66/100... Step: 67355... Loss: 0.000103... Val Loss: 0.000831\n",
      "Epoch: 66/100... Step: 67360... Loss: 0.000056... Val Loss: 0.000863\n",
      "Epoch: 66/100... Step: 67365... Loss: 0.000073... Val Loss: 0.000837\n",
      "Epoch: 66/100... Step: 67370... Loss: 0.000149... Val Loss: 0.000784\n",
      "Epoch: 66/100... Step: 67375... Loss: 0.000051... Val Loss: 0.000708\n",
      "Epoch: 66/100... Step: 67380... Loss: 0.000063... Val Loss: 0.000617\n",
      "Epoch: 66/100... Step: 67385... Loss: 0.000158... Val Loss: 0.000604\n",
      "Epoch: 66/100... Step: 67390... Loss: 0.000199... Val Loss: 0.000717\n",
      "Epoch: 66/100... Step: 67395... Loss: 0.000113... Val Loss: 0.000691\n",
      "Epoch: 66/100... Step: 67400... Loss: 0.000098... Val Loss: 0.000586\n",
      "Epoch: 66/100... Step: 67405... Loss: 0.000125... Val Loss: 0.000503\n",
      "Epoch: 66/100... Step: 67410... Loss: 0.000068... Val Loss: 0.000465\n",
      "Epoch: 66/100... Step: 67415... Loss: 0.000095... Val Loss: 0.000532\n",
      "Epoch: 66/100... Step: 67420... Loss: 0.000107... Val Loss: 0.000680\n",
      "Epoch: 66/100... Step: 67425... Loss: 0.000188... Val Loss: 0.000474\n",
      "Epoch: 66/100... Step: 67430... Loss: 0.000034... Val Loss: 0.000395\n",
      "Epoch: 66/100... Step: 67435... Loss: 0.000226... Val Loss: 0.000492\n",
      "Epoch: 66/100... Step: 67440... Loss: 0.000125... Val Loss: 0.000745\n",
      "Epoch: 66/100... Step: 67445... Loss: 0.000099... Val Loss: 0.001064\n",
      "Epoch: 66/100... Step: 67450... Loss: 0.000058... Val Loss: 0.001001\n",
      "Epoch: 66/100... Step: 67455... Loss: 0.000050... Val Loss: 0.000992\n",
      "Epoch: 66/100... Step: 67460... Loss: 0.000060... Val Loss: 0.001158\n",
      "Epoch: 66/100... Step: 67465... Loss: 0.000083... Val Loss: 0.000985\n",
      "Epoch: 66/100... Step: 67470... Loss: 0.000094... Val Loss: 0.001028\n",
      "Epoch: 66/100... Step: 67475... Loss: 0.000045... Val Loss: 0.001073\n",
      "Epoch: 66/100... Step: 67480... Loss: 0.000048... Val Loss: 0.001119\n",
      "Epoch: 66/100... Step: 67485... Loss: 0.000305... Val Loss: 0.000973\n",
      "Epoch: 66/100... Step: 67490... Loss: 0.000119... Val Loss: 0.000277\n",
      "Epoch: 66/100... Step: 67495... Loss: 0.000721... Val Loss: 0.000265\n",
      "Epoch: 66/100... Step: 67500... Loss: 0.000285... Val Loss: 0.001812\n",
      "Epoch: 66/100... Step: 67505... Loss: 0.000188... Val Loss: 0.000576\n",
      "Epoch: 66/100... Step: 67510... Loss: 0.000365... Val Loss: 0.001526\n",
      "Epoch: 66/100... Step: 67515... Loss: 0.000268... Val Loss: 0.001037\n",
      "Epoch: 66/100... Step: 67520... Loss: 0.000137... Val Loss: 0.001030\n",
      "Epoch: 66/100... Step: 67525... Loss: 0.000076... Val Loss: 0.001083\n",
      "Epoch: 66/100... Step: 67530... Loss: 0.000158... Val Loss: 0.001305\n",
      "Epoch: 66/100... Step: 67535... Loss: 0.000174... Val Loss: 0.001362\n",
      "Epoch: 66/100... Step: 67540... Loss: 0.000220... Val Loss: 0.001416\n",
      "Epoch: 66/100... Step: 67545... Loss: 0.000152... Val Loss: 0.001466\n",
      "Epoch: 66/100... Step: 67550... Loss: 0.000156... Val Loss: 0.001155\n",
      "Epoch: 66/100... Step: 67555... Loss: 0.000250... Val Loss: 0.001140\n",
      "Epoch: 66/100... Step: 67560... Loss: 0.000241... Val Loss: 0.001120\n",
      "Epoch: 66/100... Step: 67565... Loss: 0.000240... Val Loss: 0.001088\n",
      "Epoch: 66/100... Step: 67570... Loss: 0.000074... Val Loss: 0.001080\n",
      "Epoch: 66/100... Step: 67575... Loss: 0.000148... Val Loss: 0.000961\n",
      "Epoch: 66/100... Step: 67580... Loss: 0.000221... Val Loss: 0.000588\n",
      "Epoch: 66/100... Step: 67585... Loss: 0.000257... Val Loss: 0.000782\n",
      "Epoch: 66/100... Step: 67590... Loss: 0.000136... Val Loss: 0.000353\n",
      "Epoch: 66/100... Step: 67595... Loss: 0.000111... Val Loss: 0.000530\n",
      "Epoch: 66/100... Step: 67600... Loss: 0.000082... Val Loss: 0.000462\n",
      "Epoch: 66/100... Step: 67605... Loss: 0.000084... Val Loss: 0.000676\n",
      "Epoch: 66/100... Step: 67610... Loss: 0.000146... Val Loss: 0.000830\n",
      "Epoch: 66/100... Step: 67615... Loss: 0.000073... Val Loss: 0.000652\n",
      "Epoch: 66/100... Step: 67620... Loss: 0.000189... Val Loss: 0.000724\n",
      "Epoch: 66/100... Step: 67625... Loss: 0.000127... Val Loss: 0.000868\n",
      "Epoch: 66/100... Step: 67630... Loss: 0.000104... Val Loss: 0.000821\n",
      "Epoch: 66/100... Step: 67635... Loss: 0.000100... Val Loss: 0.000794\n",
      "Epoch: 66/100... Step: 67640... Loss: 0.000078... Val Loss: 0.000655\n",
      "Epoch: 66/100... Step: 67645... Loss: 0.000034... Val Loss: 0.000872\n",
      "Epoch: 66/100... Step: 67650... Loss: 0.000153... Val Loss: 0.000846\n",
      "Epoch: 66/100... Step: 67655... Loss: 0.000133... Val Loss: 0.000919\n",
      "Epoch: 66/100... Step: 67660... Loss: 0.000085... Val Loss: 0.000848\n",
      "Epoch: 66/100... Step: 67665... Loss: 0.000153... Val Loss: 0.001048\n",
      "Epoch: 66/100... Step: 67670... Loss: 0.000143... Val Loss: 0.000821\n",
      "Epoch: 66/100... Step: 67675... Loss: 0.000187... Val Loss: 0.000812\n",
      "Epoch: 66/100... Step: 67680... Loss: 0.000126... Val Loss: 0.000933\n",
      "Epoch: 66/100... Step: 67685... Loss: 0.000067... Val Loss: 0.000999\n",
      "Epoch: 66/100... Step: 67690... Loss: 0.000073... Val Loss: 0.000895\n",
      "Epoch: 66/100... Step: 67695... Loss: 0.000078... Val Loss: 0.000958\n",
      "Epoch: 66/100... Step: 67700... Loss: 0.000083... Val Loss: 0.000715\n",
      "Epoch: 66/100... Step: 67705... Loss: 0.000110... Val Loss: 0.000919\n",
      "Epoch: 66/100... Step: 67710... Loss: 0.000063... Val Loss: 0.000785\n",
      "Epoch: 66/100... Step: 67715... Loss: 0.000282... Val Loss: 0.000707\n",
      "Epoch: 66/100... Step: 67720... Loss: 0.000173... Val Loss: 0.000824\n",
      "Epoch: 66/100... Step: 67725... Loss: 0.000163... Val Loss: 0.000517\n",
      "Epoch: 66/100... Step: 67730... Loss: 0.000195... Val Loss: 0.001011\n",
      "Epoch: 66/100... Step: 67735... Loss: 0.000072... Val Loss: 0.000936\n",
      "Epoch: 66/100... Step: 67740... Loss: 0.000243... Val Loss: 0.000736\n",
      "Epoch: 66/100... Step: 67745... Loss: 0.000155... Val Loss: 0.000698\n",
      "Epoch: 66/100... Step: 67750... Loss: 0.000227... Val Loss: 0.000334\n",
      "Epoch: 66/100... Step: 67755... Loss: 0.000117... Val Loss: 0.000490\n",
      "Epoch: 66/100... Step: 67760... Loss: 0.000125... Val Loss: 0.000481\n",
      "Epoch: 66/100... Step: 67765... Loss: 0.000233... Val Loss: 0.000267\n",
      "Epoch: 66/100... Step: 67770... Loss: 0.000136... Val Loss: 0.000284\n",
      "Epoch: 66/100... Step: 67775... Loss: 0.000112... Val Loss: 0.000304\n",
      "Epoch: 66/100... Step: 67780... Loss: 0.000134... Val Loss: 0.000276\n",
      "Epoch: 66/100... Step: 67785... Loss: 0.000114... Val Loss: 0.000284\n",
      "Epoch: 66/100... Step: 67790... Loss: 0.000202... Val Loss: 0.000277\n",
      "Epoch: 66/100... Step: 67795... Loss: 0.000225... Val Loss: 0.000362\n",
      "Epoch: 66/100... Step: 67800... Loss: 0.000138... Val Loss: 0.000521\n",
      "Epoch: 66/100... Step: 67805... Loss: 0.000258... Val Loss: 0.000320\n",
      "Epoch: 66/100... Step: 67810... Loss: 0.000201... Val Loss: 0.000317\n",
      "Epoch: 66/100... Step: 67815... Loss: 0.000284... Val Loss: 0.000416\n",
      "Epoch: 66/100... Step: 67820... Loss: 0.000291... Val Loss: 0.000385\n",
      "Epoch: 66/100... Step: 67825... Loss: 0.000129... Val Loss: 0.000368\n",
      "Epoch: 66/100... Step: 67830... Loss: 0.000159... Val Loss: 0.000268\n",
      "Epoch: 66/100... Step: 67835... Loss: 0.000158... Val Loss: 0.000502\n",
      "Epoch: 66/100... Step: 67840... Loss: 0.000116... Val Loss: 0.000345\n",
      "Epoch: 66/100... Step: 67845... Loss: 0.000248... Val Loss: 0.000318\n",
      "Epoch: 66/100... Step: 67850... Loss: 0.000229... Val Loss: 0.000361\n",
      "Epoch: 66/100... Step: 67855... Loss: 0.000116... Val Loss: 0.000390\n",
      "Epoch: 66/100... Step: 67860... Loss: 0.000218... Val Loss: 0.000549\n",
      "Epoch: 66/100... Step: 67865... Loss: 0.000194... Val Loss: 0.000570\n",
      "Epoch: 66/100... Step: 67870... Loss: 0.000281... Val Loss: 0.000317\n",
      "Epoch: 66/100... Step: 67875... Loss: 0.000226... Val Loss: 0.000389\n",
      "Epoch: 66/100... Step: 67880... Loss: 0.000109... Val Loss: 0.000524\n",
      "Epoch: 66/100... Step: 67885... Loss: 0.000282... Val Loss: 0.000498\n",
      "Epoch: 66/100... Step: 67890... Loss: 0.000124... Val Loss: 0.000721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66/100... Step: 67895... Loss: 0.000214... Val Loss: 0.000815\n",
      "Epoch: 66/100... Step: 67900... Loss: 0.000061... Val Loss: 0.000678\n",
      "Epoch: 66/100... Step: 67905... Loss: 0.000084... Val Loss: 0.000687\n",
      "Epoch: 66/100... Step: 67910... Loss: 0.000162... Val Loss: 0.000645\n",
      "Epoch: 66/100... Step: 67915... Loss: 0.000171... Val Loss: 0.000716\n",
      "Epoch: 66/100... Step: 67920... Loss: 0.000081... Val Loss: 0.000507\n",
      "Epoch: 66/100... Step: 67925... Loss: 0.000133... Val Loss: 0.000544\n",
      "Epoch: 66/100... Step: 67930... Loss: 0.000151... Val Loss: 0.000670\n",
      "Epoch: 66/100... Step: 67935... Loss: 0.000106... Val Loss: 0.000635\n",
      "Epoch: 66/100... Step: 67940... Loss: 0.000179... Val Loss: 0.000896\n",
      "Epoch: 66/100... Step: 67945... Loss: 0.000093... Val Loss: 0.000780\n",
      "Epoch: 66/100... Step: 67950... Loss: 0.000251... Val Loss: 0.000657\n",
      "Epoch: 66/100... Step: 67955... Loss: 0.000185... Val Loss: 0.001022\n",
      "Epoch: 66/100... Step: 67960... Loss: 0.000102... Val Loss: 0.000755\n",
      "Epoch: 66/100... Step: 67965... Loss: 0.000082... Val Loss: 0.000698\n",
      "Epoch: 66/100... Step: 67970... Loss: 0.000238... Val Loss: 0.000680\n",
      "Epoch: 66/100... Step: 67975... Loss: 0.000088... Val Loss: 0.000690\n",
      "Epoch: 66/100... Step: 67980... Loss: 0.000188... Val Loss: 0.000654\n",
      "Epoch: 66/100... Step: 67985... Loss: 0.000098... Val Loss: 0.000599\n",
      "Epoch: 66/100... Step: 67990... Loss: 0.000077... Val Loss: 0.000521\n",
      "Epoch: 66/100... Step: 67995... Loss: 0.000080... Val Loss: 0.000527\n",
      "Epoch: 66/100... Step: 68000... Loss: 0.000138... Val Loss: 0.000636\n",
      "Epoch: 66/100... Step: 68005... Loss: 0.000161... Val Loss: 0.000770\n",
      "Epoch: 66/100... Step: 68010... Loss: 0.000119... Val Loss: 0.000807\n",
      "Epoch: 66/100... Step: 68015... Loss: 0.000108... Val Loss: 0.000837\n",
      "Epoch: 66/100... Step: 68020... Loss: 0.000087... Val Loss: 0.001027\n",
      "Epoch: 66/100... Step: 68025... Loss: 0.000112... Val Loss: 0.000580\n",
      "Epoch: 66/100... Step: 68030... Loss: 0.000107... Val Loss: 0.000835\n",
      "Epoch: 66/100... Step: 68035... Loss: 0.000172... Val Loss: 0.001009\n",
      "Epoch: 66/100... Step: 68040... Loss: 0.000134... Val Loss: 0.000969\n",
      "Epoch: 66/100... Step: 68045... Loss: 0.000289... Val Loss: 0.001176\n",
      "Epoch: 66/100... Step: 68050... Loss: 0.000096... Val Loss: 0.001107\n",
      "Epoch: 66/100... Step: 68055... Loss: 0.000057... Val Loss: 0.001155\n",
      "Epoch: 66/100... Step: 68060... Loss: 0.000149... Val Loss: 0.000741\n",
      "Epoch: 66/100... Step: 68065... Loss: 0.000152... Val Loss: 0.001010\n",
      "Epoch: 66/100... Step: 68070... Loss: 0.000038... Val Loss: 0.001073\n",
      "Epoch: 66/100... Step: 68075... Loss: 0.000058... Val Loss: 0.000995\n",
      "Epoch: 66/100... Step: 68080... Loss: 0.000104... Val Loss: 0.001017\n",
      "Epoch: 66/100... Step: 68085... Loss: 0.000074... Val Loss: 0.000881\n",
      "Epoch: 66/100... Step: 68090... Loss: 0.000131... Val Loss: 0.000876\n",
      "Epoch: 66/100... Step: 68095... Loss: 0.000133... Val Loss: 0.000892\n",
      "Epoch: 66/100... Step: 68100... Loss: 0.000114... Val Loss: 0.001066\n",
      "Epoch: 66/100... Step: 68105... Loss: 0.000072... Val Loss: 0.000953\n",
      "Epoch: 66/100... Step: 68110... Loss: 0.000054... Val Loss: 0.000947\n",
      "Epoch: 67/100... Step: 68115... Loss: 0.000118... Val Loss: 0.001600\n",
      "Epoch: 67/100... Step: 68120... Loss: 0.000174... Val Loss: 0.001418\n",
      "Epoch: 67/100... Step: 68125... Loss: 0.000124... Val Loss: 0.001419\n",
      "Epoch: 67/100... Step: 68130... Loss: 0.000215... Val Loss: 0.001869\n",
      "Epoch: 67/100... Step: 68135... Loss: 0.000197... Val Loss: 0.001705\n",
      "Epoch: 67/100... Step: 68140... Loss: 0.000173... Val Loss: 0.001666\n",
      "Epoch: 67/100... Step: 68145... Loss: 0.000094... Val Loss: 0.001790\n",
      "Epoch: 67/100... Step: 68150... Loss: 0.000203... Val Loss: 0.001635\n",
      "Epoch: 67/100... Step: 68155... Loss: 0.000136... Val Loss: 0.001861\n",
      "Epoch: 67/100... Step: 68160... Loss: 0.000086... Val Loss: 0.001794\n",
      "Epoch: 67/100... Step: 68165... Loss: 0.000053... Val Loss: 0.001747\n",
      "Epoch: 67/100... Step: 68170... Loss: 0.000119... Val Loss: 0.001666\n",
      "Epoch: 67/100... Step: 68175... Loss: 0.000077... Val Loss: 0.001465\n",
      "Epoch: 67/100... Step: 68180... Loss: 0.000138... Val Loss: 0.001386\n",
      "Epoch: 67/100... Step: 68185... Loss: 0.000139... Val Loss: 0.001067\n",
      "Epoch: 67/100... Step: 68190... Loss: 0.000121... Val Loss: 0.001163\n",
      "Epoch: 67/100... Step: 68195... Loss: 0.000057... Val Loss: 0.001313\n",
      "Epoch: 67/100... Step: 68200... Loss: 0.000108... Val Loss: 0.001328\n",
      "Epoch: 67/100... Step: 68205... Loss: 0.000194... Val Loss: 0.001292\n",
      "Epoch: 67/100... Step: 68210... Loss: 0.000110... Val Loss: 0.001067\n",
      "Epoch: 67/100... Step: 68215... Loss: 0.000114... Val Loss: 0.001223\n",
      "Epoch: 67/100... Step: 68220... Loss: 0.000063... Val Loss: 0.001229\n",
      "Epoch: 67/100... Step: 68225... Loss: 0.000125... Val Loss: 0.001148\n",
      "Epoch: 67/100... Step: 68230... Loss: 0.000050... Val Loss: 0.001232\n",
      "Epoch: 67/100... Step: 68235... Loss: 0.000083... Val Loss: 0.001213\n",
      "Epoch: 67/100... Step: 68240... Loss: 0.000123... Val Loss: 0.001235\n",
      "Epoch: 67/100... Step: 68245... Loss: 0.000153... Val Loss: 0.001290\n",
      "Epoch: 67/100... Step: 68250... Loss: 0.000057... Val Loss: 0.001407\n",
      "Epoch: 67/100... Step: 68255... Loss: 0.000131... Val Loss: 0.001549\n",
      "Epoch: 67/100... Step: 68260... Loss: 0.000075... Val Loss: 0.001707\n",
      "Epoch: 67/100... Step: 68265... Loss: 0.000067... Val Loss: 0.001510\n",
      "Epoch: 67/100... Step: 68270... Loss: 0.000079... Val Loss: 0.001373\n",
      "Epoch: 67/100... Step: 68275... Loss: 0.000127... Val Loss: 0.001268\n",
      "Epoch: 67/100... Step: 68280... Loss: 0.000093... Val Loss: 0.001266\n",
      "Epoch: 67/100... Step: 68285... Loss: 0.000179... Val Loss: 0.001317\n",
      "Epoch: 67/100... Step: 68290... Loss: 0.000130... Val Loss: 0.001257\n",
      "Epoch: 67/100... Step: 68295... Loss: 0.000121... Val Loss: 0.001331\n",
      "Epoch: 67/100... Step: 68300... Loss: 0.000098... Val Loss: 0.001050\n",
      "Epoch: 67/100... Step: 68305... Loss: 0.000109... Val Loss: 0.001038\n",
      "Epoch: 67/100... Step: 68310... Loss: 0.000108... Val Loss: 0.001122\n",
      "Epoch: 67/100... Step: 68315... Loss: 0.000207... Val Loss: 0.001075\n",
      "Epoch: 67/100... Step: 68320... Loss: 0.000076... Val Loss: 0.001151\n",
      "Epoch: 67/100... Step: 68325... Loss: 0.000112... Val Loss: 0.001329\n",
      "Epoch: 67/100... Step: 68330... Loss: 0.000099... Val Loss: 0.001068\n",
      "Epoch: 67/100... Step: 68335... Loss: 0.000064... Val Loss: 0.001091\n",
      "Epoch: 67/100... Step: 68340... Loss: 0.000137... Val Loss: 0.001020\n",
      "Epoch: 67/100... Step: 68345... Loss: 0.000064... Val Loss: 0.001078\n",
      "Epoch: 67/100... Step: 68350... Loss: 0.000136... Val Loss: 0.001043\n",
      "Epoch: 67/100... Step: 68355... Loss: 0.000101... Val Loss: 0.001101\n",
      "Epoch: 67/100... Step: 68360... Loss: 0.000076... Val Loss: 0.001104\n",
      "Epoch: 67/100... Step: 68365... Loss: 0.000068... Val Loss: 0.001103\n",
      "Epoch: 67/100... Step: 68370... Loss: 0.000120... Val Loss: 0.001026\n",
      "Epoch: 67/100... Step: 68375... Loss: 0.000107... Val Loss: 0.000966\n",
      "Epoch: 67/100... Step: 68380... Loss: 0.000101... Val Loss: 0.000968\n",
      "Epoch: 67/100... Step: 68385... Loss: 0.000099... Val Loss: 0.000938\n",
      "Epoch: 67/100... Step: 68390... Loss: 0.000077... Val Loss: 0.000855\n",
      "Epoch: 67/100... Step: 68395... Loss: 0.000146... Val Loss: 0.001002\n",
      "Epoch: 67/100... Step: 68400... Loss: 0.000151... Val Loss: 0.000830\n",
      "Epoch: 67/100... Step: 68405... Loss: 0.000099... Val Loss: 0.000667\n",
      "Epoch: 67/100... Step: 68410... Loss: 0.000137... Val Loss: 0.000491\n",
      "Epoch: 67/100... Step: 68415... Loss: 0.000199... Val Loss: 0.000536\n",
      "Epoch: 67/100... Step: 68420... Loss: 0.000147... Val Loss: 0.000647\n",
      "Epoch: 67/100... Step: 68425... Loss: 0.000102... Val Loss: 0.000636\n",
      "Epoch: 67/100... Step: 68430... Loss: 0.000062... Val Loss: 0.000686\n",
      "Epoch: 67/100... Step: 68435... Loss: 0.000102... Val Loss: 0.000685\n",
      "Epoch: 67/100... Step: 68440... Loss: 0.000159... Val Loss: 0.000450\n",
      "Epoch: 67/100... Step: 68445... Loss: 0.000108... Val Loss: 0.000427\n",
      "Epoch: 67/100... Step: 68450... Loss: 0.000101... Val Loss: 0.000534\n",
      "Epoch: 67/100... Step: 68455... Loss: 0.000159... Val Loss: 0.000572\n",
      "Epoch: 67/100... Step: 68460... Loss: 0.000102... Val Loss: 0.000302\n",
      "Epoch: 67/100... Step: 68465... Loss: 0.000168... Val Loss: 0.000380\n",
      "Epoch: 67/100... Step: 68470... Loss: 0.000119... Val Loss: 0.000917\n",
      "Epoch: 67/100... Step: 68475... Loss: 0.000203... Val Loss: 0.000710\n",
      "Epoch: 67/100... Step: 68480... Loss: 0.000221... Val Loss: 0.000947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67/100... Step: 68485... Loss: 0.000121... Val Loss: 0.001127\n",
      "Epoch: 67/100... Step: 68490... Loss: 0.000135... Val Loss: 0.001136\n",
      "Epoch: 67/100... Step: 68495... Loss: 0.000158... Val Loss: 0.001062\n",
      "Epoch: 67/100... Step: 68500... Loss: 0.000066... Val Loss: 0.000860\n",
      "Epoch: 67/100... Step: 68505... Loss: 0.000098... Val Loss: 0.001000\n",
      "Epoch: 67/100... Step: 68510... Loss: 0.000078... Val Loss: 0.001022\n",
      "Epoch: 67/100... Step: 68515... Loss: 0.000169... Val Loss: 0.000974\n",
      "Epoch: 67/100... Step: 68520... Loss: 0.000388... Val Loss: 0.000710\n",
      "Epoch: 67/100... Step: 68525... Loss: 0.000570... Val Loss: 0.000261\n",
      "Epoch: 67/100... Step: 68530... Loss: 0.000547... Val Loss: 0.000957\n",
      "Epoch: 67/100... Step: 68535... Loss: 0.000264... Val Loss: 0.000998\n",
      "Epoch: 67/100... Step: 68540... Loss: 0.000154... Val Loss: 0.001303\n",
      "Epoch: 67/100... Step: 68545... Loss: 0.000226... Val Loss: 0.000871\n",
      "Epoch: 67/100... Step: 68550... Loss: 0.000286... Val Loss: 0.001361\n",
      "Epoch: 67/100... Step: 68555... Loss: 0.000102... Val Loss: 0.001225\n",
      "Epoch: 67/100... Step: 68560... Loss: 0.000202... Val Loss: 0.001375\n",
      "Epoch: 67/100... Step: 68565... Loss: 0.000106... Val Loss: 0.001384\n",
      "Epoch: 67/100... Step: 68570... Loss: 0.000141... Val Loss: 0.001327\n",
      "Epoch: 67/100... Step: 68575... Loss: 0.000074... Val Loss: 0.001322\n",
      "Epoch: 67/100... Step: 68580... Loss: 0.000180... Val Loss: 0.001333\n",
      "Epoch: 67/100... Step: 68585... Loss: 0.000077... Val Loss: 0.001113\n",
      "Epoch: 67/100... Step: 68590... Loss: 0.000151... Val Loss: 0.001275\n",
      "Epoch: 67/100... Step: 68595... Loss: 0.000122... Val Loss: 0.000996\n",
      "Epoch: 67/100... Step: 68600... Loss: 0.000267... Val Loss: 0.000747\n",
      "Epoch: 67/100... Step: 68605... Loss: 0.000215... Val Loss: 0.001104\n",
      "Epoch: 67/100... Step: 68610... Loss: 0.000145... Val Loss: 0.000534\n",
      "Epoch: 67/100... Step: 68615... Loss: 0.000202... Val Loss: 0.000461\n",
      "Epoch: 67/100... Step: 68620... Loss: 0.000173... Val Loss: 0.000730\n",
      "Epoch: 67/100... Step: 68625... Loss: 0.000120... Val Loss: 0.000295\n",
      "Epoch: 67/100... Step: 68630... Loss: 0.000170... Val Loss: 0.000492\n",
      "Epoch: 67/100... Step: 68635... Loss: 0.000179... Val Loss: 0.000552\n",
      "Epoch: 67/100... Step: 68640... Loss: 0.000162... Val Loss: 0.000665\n",
      "Epoch: 67/100... Step: 68645... Loss: 0.000191... Val Loss: 0.000684\n",
      "Epoch: 67/100... Step: 68650... Loss: 0.000051... Val Loss: 0.000392\n",
      "Epoch: 67/100... Step: 68655... Loss: 0.000274... Val Loss: 0.000737\n",
      "Epoch: 67/100... Step: 68660... Loss: 0.000188... Val Loss: 0.000954\n",
      "Epoch: 67/100... Step: 68665... Loss: 0.000154... Val Loss: 0.000795\n",
      "Epoch: 67/100... Step: 68670... Loss: 0.000108... Val Loss: 0.000679\n",
      "Epoch: 67/100... Step: 68675... Loss: 0.000099... Val Loss: 0.000780\n",
      "Epoch: 67/100... Step: 68680... Loss: 0.000111... Val Loss: 0.000861\n",
      "Epoch: 67/100... Step: 68685... Loss: 0.000168... Val Loss: 0.001065\n",
      "Epoch: 67/100... Step: 68690... Loss: 0.000135... Val Loss: 0.000896\n",
      "Epoch: 67/100... Step: 68695... Loss: 0.000150... Val Loss: 0.000981\n",
      "Epoch: 67/100... Step: 68700... Loss: 0.000206... Val Loss: 0.000903\n",
      "Epoch: 67/100... Step: 68705... Loss: 0.000092... Val Loss: 0.001174\n",
      "Epoch: 67/100... Step: 68710... Loss: 0.000063... Val Loss: 0.000796\n",
      "Epoch: 67/100... Step: 68715... Loss: 0.000087... Val Loss: 0.000960\n",
      "Epoch: 67/100... Step: 68720... Loss: 0.000145... Val Loss: 0.000844\n",
      "Epoch: 67/100... Step: 68725... Loss: 0.000088... Val Loss: 0.000818\n",
      "Epoch: 67/100... Step: 68730... Loss: 0.000109... Val Loss: 0.000630\n",
      "Epoch: 67/100... Step: 68735... Loss: 0.000105... Val Loss: 0.000904\n",
      "Epoch: 67/100... Step: 68740... Loss: 0.000194... Val Loss: 0.000907\n",
      "Epoch: 67/100... Step: 68745... Loss: 0.000119... Val Loss: 0.000954\n",
      "Epoch: 67/100... Step: 68750... Loss: 0.000275... Val Loss: 0.000715\n",
      "Epoch: 67/100... Step: 68755... Loss: 0.000117... Val Loss: 0.000571\n",
      "Epoch: 67/100... Step: 68760... Loss: 0.000314... Val Loss: 0.000481\n",
      "Epoch: 67/100... Step: 68765... Loss: 0.000296... Val Loss: 0.000976\n",
      "Epoch: 67/100... Step: 68770... Loss: 0.000083... Val Loss: 0.000857\n",
      "Epoch: 67/100... Step: 68775... Loss: 0.000090... Val Loss: 0.000708\n",
      "Epoch: 67/100... Step: 68780... Loss: 0.000128... Val Loss: 0.000413\n",
      "Epoch: 67/100... Step: 68785... Loss: 0.000063... Val Loss: 0.000312\n",
      "Epoch: 67/100... Step: 68790... Loss: 0.000145... Val Loss: 0.000333\n",
      "Epoch: 67/100... Step: 68795... Loss: 0.000232... Val Loss: 0.000266\n",
      "Epoch: 67/100... Step: 68800... Loss: 0.000201... Val Loss: 0.000519\n",
      "Epoch: 67/100... Step: 68805... Loss: 0.000082... Val Loss: 0.000303\n",
      "Epoch: 67/100... Step: 68810... Loss: 0.000075... Val Loss: 0.000281\n",
      "Epoch: 67/100... Step: 68815... Loss: 0.000137... Val Loss: 0.000318\n",
      "Epoch: 67/100... Step: 68820... Loss: 0.000132... Val Loss: 0.000362\n",
      "Epoch: 67/100... Step: 68825... Loss: 0.000064... Val Loss: 0.000449\n",
      "Epoch: 67/100... Step: 68830... Loss: 0.000059... Val Loss: 0.000515\n",
      "Epoch: 67/100... Step: 68835... Loss: 0.000050... Val Loss: 0.000469\n",
      "Epoch: 67/100... Step: 68840... Loss: 0.000065... Val Loss: 0.000341\n",
      "Epoch: 67/100... Step: 68845... Loss: 0.000099... Val Loss: 0.000457\n",
      "Epoch: 67/100... Step: 68850... Loss: 0.000191... Val Loss: 0.000268\n",
      "Epoch: 67/100... Step: 68855... Loss: 0.000164... Val Loss: 0.000329\n",
      "Epoch: 67/100... Step: 68860... Loss: 0.000082... Val Loss: 0.000341\n",
      "Epoch: 67/100... Step: 68865... Loss: 0.000074... Val Loss: 0.000548\n",
      "Epoch: 67/100... Step: 68870... Loss: 0.000079... Val Loss: 0.000297\n",
      "Epoch: 67/100... Step: 68875... Loss: 0.000165... Val Loss: 0.000297\n",
      "Epoch: 67/100... Step: 68880... Loss: 0.000109... Val Loss: 0.000364\n",
      "Epoch: 67/100... Step: 68885... Loss: 0.000031... Val Loss: 0.000335\n",
      "Epoch: 67/100... Step: 68890... Loss: 0.000221... Val Loss: 0.000475\n",
      "Epoch: 67/100... Step: 68895... Loss: 0.000261... Val Loss: 0.000670\n",
      "Epoch: 67/100... Step: 68900... Loss: 0.000098... Val Loss: 0.000707\n",
      "Epoch: 67/100... Step: 68905... Loss: 0.000215... Val Loss: 0.000375\n",
      "Epoch: 67/100... Step: 68910... Loss: 0.000334... Val Loss: 0.000321\n",
      "Epoch: 67/100... Step: 68915... Loss: 0.000259... Val Loss: 0.000440\n",
      "Epoch: 67/100... Step: 68920... Loss: 0.000209... Val Loss: 0.000802\n",
      "Epoch: 67/100... Step: 68925... Loss: 0.000220... Val Loss: 0.000695\n",
      "Epoch: 67/100... Step: 68930... Loss: 0.000182... Val Loss: 0.000733\n",
      "Epoch: 67/100... Step: 68935... Loss: 0.000123... Val Loss: 0.000691\n",
      "Epoch: 67/100... Step: 68940... Loss: 0.000084... Val Loss: 0.000841\n",
      "Epoch: 67/100... Step: 68945... Loss: 0.000305... Val Loss: 0.000492\n",
      "Epoch: 67/100... Step: 68950... Loss: 0.000168... Val Loss: 0.000483\n",
      "Epoch: 67/100... Step: 68955... Loss: 0.000211... Val Loss: 0.000721\n",
      "Epoch: 67/100... Step: 68960... Loss: 0.000113... Val Loss: 0.000699\n",
      "Epoch: 67/100... Step: 68965... Loss: 0.000109... Val Loss: 0.000571\n",
      "Epoch: 67/100... Step: 68970... Loss: 0.000114... Val Loss: 0.000613\n",
      "Epoch: 67/100... Step: 68975... Loss: 0.000054... Val Loss: 0.000762\n",
      "Epoch: 67/100... Step: 68980... Loss: 0.000097... Val Loss: 0.000709\n",
      "Epoch: 67/100... Step: 68985... Loss: 0.000094... Val Loss: 0.000639\n",
      "Epoch: 67/100... Step: 68990... Loss: 0.000079... Val Loss: 0.000762\n",
      "Epoch: 67/100... Step: 68995... Loss: 0.000073... Val Loss: 0.000713\n",
      "Epoch: 67/100... Step: 69000... Loss: 0.000077... Val Loss: 0.000639\n",
      "Epoch: 67/100... Step: 69005... Loss: 0.000134... Val Loss: 0.000714\n",
      "Epoch: 67/100... Step: 69010... Loss: 0.000091... Val Loss: 0.000686\n",
      "Epoch: 67/100... Step: 69015... Loss: 0.000123... Val Loss: 0.000639\n",
      "Epoch: 67/100... Step: 69020... Loss: 0.000133... Val Loss: 0.000480\n",
      "Epoch: 67/100... Step: 69025... Loss: 0.000188... Val Loss: 0.000519\n",
      "Epoch: 67/100... Step: 69030... Loss: 0.000103... Val Loss: 0.000654\n",
      "Epoch: 67/100... Step: 69035... Loss: 0.000067... Val Loss: 0.000721\n",
      "Epoch: 67/100... Step: 69040... Loss: 0.000095... Val Loss: 0.000777\n",
      "Epoch: 67/100... Step: 69045... Loss: 0.000135... Val Loss: 0.000922\n",
      "Epoch: 67/100... Step: 69050... Loss: 0.000040... Val Loss: 0.000942\n",
      "Epoch: 67/100... Step: 69055... Loss: 0.000075... Val Loss: 0.000735\n",
      "Epoch: 67/100... Step: 69060... Loss: 0.000073... Val Loss: 0.000730\n",
      "Epoch: 67/100... Step: 69065... Loss: 0.000072... Val Loss: 0.000934\n",
      "Epoch: 67/100... Step: 69070... Loss: 0.000049... Val Loss: 0.000965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67/100... Step: 69075... Loss: 0.000087... Val Loss: 0.000971\n",
      "Epoch: 67/100... Step: 69080... Loss: 0.000078... Val Loss: 0.000973\n",
      "Epoch: 67/100... Step: 69085... Loss: 0.000129... Val Loss: 0.000948\n",
      "Epoch: 67/100... Step: 69090... Loss: 0.000069... Val Loss: 0.001006\n",
      "Epoch: 67/100... Step: 69095... Loss: 0.000093... Val Loss: 0.001001\n",
      "Epoch: 67/100... Step: 69100... Loss: 0.000122... Val Loss: 0.001078\n",
      "Epoch: 67/100... Step: 69105... Loss: 0.000109... Val Loss: 0.001224\n",
      "Epoch: 67/100... Step: 69110... Loss: 0.000128... Val Loss: 0.001128\n",
      "Epoch: 67/100... Step: 69115... Loss: 0.000160... Val Loss: 0.001103\n",
      "Epoch: 67/100... Step: 69120... Loss: 0.000182... Val Loss: 0.000920\n",
      "Epoch: 67/100... Step: 69125... Loss: 0.000227... Val Loss: 0.000817\n",
      "Epoch: 67/100... Step: 69130... Loss: 0.000113... Val Loss: 0.001036\n",
      "Epoch: 67/100... Step: 69135... Loss: 0.000142... Val Loss: 0.001117\n",
      "Epoch: 67/100... Step: 69140... Loss: 0.000068... Val Loss: 0.000998\n",
      "Epoch: 68/100... Step: 69145... Loss: 0.000243... Val Loss: 0.001165\n",
      "Epoch: 68/100... Step: 69150... Loss: 0.000150... Val Loss: 0.001340\n",
      "Epoch: 68/100... Step: 69155... Loss: 0.000113... Val Loss: 0.001706\n",
      "Epoch: 68/100... Step: 69160... Loss: 0.000222... Val Loss: 0.001565\n",
      "Epoch: 68/100... Step: 69165... Loss: 0.000230... Val Loss: 0.001857\n",
      "Epoch: 68/100... Step: 69170... Loss: 0.000179... Val Loss: 0.001695\n",
      "Epoch: 68/100... Step: 69175... Loss: 0.000084... Val Loss: 0.001839\n",
      "Epoch: 68/100... Step: 69180... Loss: 0.000166... Val Loss: 0.001767\n",
      "Epoch: 68/100... Step: 69185... Loss: 0.000198... Val Loss: 0.001568\n",
      "Epoch: 68/100... Step: 69190... Loss: 0.000135... Val Loss: 0.001829\n",
      "Epoch: 68/100... Step: 69195... Loss: 0.000074... Val Loss: 0.001722\n",
      "Epoch: 68/100... Step: 69200... Loss: 0.000073... Val Loss: 0.001736\n",
      "Epoch: 68/100... Step: 69205... Loss: 0.000122... Val Loss: 0.001660\n",
      "Epoch: 68/100... Step: 69210... Loss: 0.000111... Val Loss: 0.001659\n",
      "Epoch: 68/100... Step: 69215... Loss: 0.000186... Val Loss: 0.001114\n",
      "Epoch: 68/100... Step: 69220... Loss: 0.000119... Val Loss: 0.001194\n",
      "Epoch: 68/100... Step: 69225... Loss: 0.000099... Val Loss: 0.001429\n",
      "Epoch: 68/100... Step: 69230... Loss: 0.000213... Val Loss: 0.001300\n",
      "Epoch: 68/100... Step: 69235... Loss: 0.000109... Val Loss: 0.001161\n",
      "Epoch: 68/100... Step: 69240... Loss: 0.000158... Val Loss: 0.001277\n",
      "Epoch: 68/100... Step: 69245... Loss: 0.000162... Val Loss: 0.000832\n",
      "Epoch: 68/100... Step: 69250... Loss: 0.000153... Val Loss: 0.001282\n",
      "Epoch: 68/100... Step: 69255... Loss: 0.000042... Val Loss: 0.001244\n",
      "Epoch: 68/100... Step: 69260... Loss: 0.000061... Val Loss: 0.001196\n",
      "Epoch: 68/100... Step: 69265... Loss: 0.000080... Val Loss: 0.001250\n",
      "Epoch: 68/100... Step: 69270... Loss: 0.000061... Val Loss: 0.001394\n",
      "Epoch: 68/100... Step: 69275... Loss: 0.000093... Val Loss: 0.001438\n",
      "Epoch: 68/100... Step: 69280... Loss: 0.000148... Val Loss: 0.001366\n",
      "Epoch: 68/100... Step: 69285... Loss: 0.000158... Val Loss: 0.001475\n",
      "Epoch: 68/100... Step: 69290... Loss: 0.000070... Val Loss: 0.001727\n",
      "Epoch: 68/100... Step: 69295... Loss: 0.000072... Val Loss: 0.001656\n",
      "Epoch: 68/100... Step: 69300... Loss: 0.000086... Val Loss: 0.001600\n",
      "Epoch: 68/100... Step: 69305... Loss: 0.000055... Val Loss: 0.001535\n",
      "Epoch: 68/100... Step: 69310... Loss: 0.000047... Val Loss: 0.001422\n",
      "Epoch: 68/100... Step: 69315... Loss: 0.000074... Val Loss: 0.001423\n",
      "Epoch: 68/100... Step: 69320... Loss: 0.000106... Val Loss: 0.001417\n",
      "Epoch: 68/100... Step: 69325... Loss: 0.000079... Val Loss: 0.001341\n",
      "Epoch: 68/100... Step: 69330... Loss: 0.000131... Val Loss: 0.001127\n",
      "Epoch: 68/100... Step: 69335... Loss: 0.000086... Val Loss: 0.001060\n",
      "Epoch: 68/100... Step: 69340... Loss: 0.000046... Val Loss: 0.001129\n",
      "Epoch: 68/100... Step: 69345... Loss: 0.000042... Val Loss: 0.001061\n",
      "Epoch: 68/100... Step: 69350... Loss: 0.000074... Val Loss: 0.001223\n",
      "Epoch: 68/100... Step: 69355... Loss: 0.000044... Val Loss: 0.001165\n",
      "Epoch: 68/100... Step: 69360... Loss: 0.000070... Val Loss: 0.001350\n",
      "Epoch: 68/100... Step: 69365... Loss: 0.000076... Val Loss: 0.001097\n",
      "Epoch: 68/100... Step: 69370... Loss: 0.000068... Val Loss: 0.001346\n",
      "Epoch: 68/100... Step: 69375... Loss: 0.000176... Val Loss: 0.001130\n",
      "Epoch: 68/100... Step: 69380... Loss: 0.000143... Val Loss: 0.001000\n",
      "Epoch: 68/100... Step: 69385... Loss: 0.000062... Val Loss: 0.001176\n",
      "Epoch: 68/100... Step: 69390... Loss: 0.000276... Val Loss: 0.001132\n",
      "Epoch: 68/100... Step: 69395... Loss: 0.000140... Val Loss: 0.001167\n",
      "Epoch: 68/100... Step: 69400... Loss: 0.000195... Val Loss: 0.001188\n",
      "Epoch: 68/100... Step: 69405... Loss: 0.000172... Val Loss: 0.000883\n",
      "Epoch: 68/100... Step: 69410... Loss: 0.000076... Val Loss: 0.000816\n",
      "Epoch: 68/100... Step: 69415... Loss: 0.000116... Val Loss: 0.000878\n",
      "Epoch: 68/100... Step: 69420... Loss: 0.000088... Val Loss: 0.000766\n",
      "Epoch: 68/100... Step: 69425... Loss: 0.000118... Val Loss: 0.000886\n",
      "Epoch: 68/100... Step: 69430... Loss: 0.000098... Val Loss: 0.000896\n",
      "Epoch: 68/100... Step: 69435... Loss: 0.000093... Val Loss: 0.000644\n",
      "Epoch: 68/100... Step: 69440... Loss: 0.000106... Val Loss: 0.000617\n",
      "Epoch: 68/100... Step: 69445... Loss: 0.000118... Val Loss: 0.000573\n",
      "Epoch: 68/100... Step: 69450... Loss: 0.000182... Val Loss: 0.000687\n",
      "Epoch: 68/100... Step: 69455... Loss: 0.000116... Val Loss: 0.000464\n",
      "Epoch: 68/100... Step: 69460... Loss: 0.000176... Val Loss: 0.000660\n",
      "Epoch: 68/100... Step: 69465... Loss: 0.000073... Val Loss: 0.000747\n",
      "Epoch: 68/100... Step: 69470... Loss: 0.000127... Val Loss: 0.000399\n",
      "Epoch: 68/100... Step: 69475... Loss: 0.000077... Val Loss: 0.000387\n",
      "Epoch: 68/100... Step: 69480... Loss: 0.000123... Val Loss: 0.000644\n",
      "Epoch: 68/100... Step: 69485... Loss: 0.000156... Val Loss: 0.000623\n",
      "Epoch: 68/100... Step: 69490... Loss: 0.000199... Val Loss: 0.000460\n",
      "Epoch: 68/100... Step: 69495... Loss: 0.000084... Val Loss: 0.000394\n",
      "Epoch: 68/100... Step: 69500... Loss: 0.000332... Val Loss: 0.000464\n",
      "Epoch: 68/100... Step: 69505... Loss: 0.000203... Val Loss: 0.000806\n",
      "Epoch: 68/100... Step: 69510... Loss: 0.000113... Val Loss: 0.001082\n",
      "Epoch: 68/100... Step: 69515... Loss: 0.000183... Val Loss: 0.000813\n",
      "Epoch: 68/100... Step: 69520... Loss: 0.000170... Val Loss: 0.001110\n",
      "Epoch: 68/100... Step: 69525... Loss: 0.000196... Val Loss: 0.001095\n",
      "Epoch: 68/100... Step: 69530... Loss: 0.000113... Val Loss: 0.000859\n",
      "Epoch: 68/100... Step: 69535... Loss: 0.000080... Val Loss: 0.000926\n",
      "Epoch: 68/100... Step: 69540... Loss: 0.000062... Val Loss: 0.000996\n",
      "Epoch: 68/100... Step: 69545... Loss: 0.000069... Val Loss: 0.001086\n",
      "Epoch: 68/100... Step: 69550... Loss: 0.000404... Val Loss: 0.000998\n",
      "Epoch: 68/100... Step: 69555... Loss: 0.000202... Val Loss: 0.000273\n",
      "Epoch: 68/100... Step: 69560... Loss: 0.000761... Val Loss: 0.000348\n",
      "Epoch: 68/100... Step: 69565... Loss: 0.000399... Val Loss: 0.001567\n",
      "Epoch: 68/100... Step: 69570... Loss: 0.000364... Val Loss: 0.000694\n",
      "Epoch: 68/100... Step: 69575... Loss: 0.000233... Val Loss: 0.001223\n",
      "Epoch: 68/100... Step: 69580... Loss: 0.000098... Val Loss: 0.001251\n",
      "Epoch: 68/100... Step: 69585... Loss: 0.000084... Val Loss: 0.001131\n",
      "Epoch: 68/100... Step: 69590... Loss: 0.000095... Val Loss: 0.001182\n",
      "Epoch: 68/100... Step: 69595... Loss: 0.000110... Val Loss: 0.001353\n",
      "Epoch: 68/100... Step: 69600... Loss: 0.000208... Val Loss: 0.001331\n",
      "Epoch: 68/100... Step: 69605... Loss: 0.000176... Val Loss: 0.001428\n",
      "Epoch: 68/100... Step: 69610... Loss: 0.000134... Val Loss: 0.001465\n",
      "Epoch: 68/100... Step: 69615... Loss: 0.000145... Val Loss: 0.001084\n",
      "Epoch: 68/100... Step: 69620... Loss: 0.000207... Val Loss: 0.001172\n",
      "Epoch: 68/100... Step: 69625... Loss: 0.000097... Val Loss: 0.001164\n",
      "Epoch: 68/100... Step: 69630... Loss: 0.000182... Val Loss: 0.001066\n",
      "Epoch: 68/100... Step: 69635... Loss: 0.000079... Val Loss: 0.001034\n",
      "Epoch: 68/100... Step: 69640... Loss: 0.000099... Val Loss: 0.000642\n",
      "Epoch: 68/100... Step: 69645... Loss: 0.000104... Val Loss: 0.000666\n",
      "Epoch: 68/100... Step: 69650... Loss: 0.000229... Val Loss: 0.000491\n",
      "Epoch: 68/100... Step: 69655... Loss: 0.000117... Val Loss: 0.000345\n",
      "Epoch: 68/100... Step: 69660... Loss: 0.000103... Val Loss: 0.000438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68/100... Step: 69665... Loss: 0.000103... Val Loss: 0.000503\n",
      "Epoch: 68/100... Step: 69670... Loss: 0.000089... Val Loss: 0.000639\n",
      "Epoch: 68/100... Step: 69675... Loss: 0.000177... Val Loss: 0.000704\n",
      "Epoch: 68/100... Step: 69680... Loss: 0.000087... Val Loss: 0.000579\n",
      "Epoch: 68/100... Step: 69685... Loss: 0.000202... Val Loss: 0.000698\n",
      "Epoch: 68/100... Step: 69690... Loss: 0.000092... Val Loss: 0.000885\n",
      "Epoch: 68/100... Step: 69695... Loss: 0.000072... Val Loss: 0.000863\n",
      "Epoch: 68/100... Step: 69700... Loss: 0.000154... Val Loss: 0.000734\n",
      "Epoch: 68/100... Step: 69705... Loss: 0.000058... Val Loss: 0.000789\n",
      "Epoch: 68/100... Step: 69710... Loss: 0.000081... Val Loss: 0.000794\n",
      "Epoch: 68/100... Step: 69715... Loss: 0.000093... Val Loss: 0.000901\n",
      "Epoch: 68/100... Step: 69720... Loss: 0.000130... Val Loss: 0.000858\n",
      "Epoch: 68/100... Step: 69725... Loss: 0.000171... Val Loss: 0.000833\n",
      "Epoch: 68/100... Step: 69730... Loss: 0.000097... Val Loss: 0.000734\n",
      "Epoch: 68/100... Step: 69735... Loss: 0.000080... Val Loss: 0.000806\n",
      "Epoch: 68/100... Step: 69740... Loss: 0.000072... Val Loss: 0.000870\n",
      "Epoch: 68/100... Step: 69745... Loss: 0.000098... Val Loss: 0.000872\n",
      "Epoch: 68/100... Step: 69750... Loss: 0.000095... Val Loss: 0.000936\n",
      "Epoch: 68/100... Step: 69755... Loss: 0.000076... Val Loss: 0.000959\n",
      "Epoch: 68/100... Step: 69760... Loss: 0.000093... Val Loss: 0.000790\n",
      "Epoch: 68/100... Step: 69765... Loss: 0.000130... Val Loss: 0.000872\n",
      "Epoch: 68/100... Step: 69770... Loss: 0.000058... Val Loss: 0.000902\n",
      "Epoch: 68/100... Step: 69775... Loss: 0.000125... Val Loss: 0.000799\n",
      "Epoch: 68/100... Step: 69780... Loss: 0.000066... Val Loss: 0.000941\n",
      "Epoch: 68/100... Step: 69785... Loss: 0.000107... Val Loss: 0.000743\n",
      "Epoch: 68/100... Step: 69790... Loss: 0.000069... Val Loss: 0.000544\n",
      "Epoch: 68/100... Step: 69795... Loss: 0.000171... Val Loss: 0.000732\n",
      "Epoch: 68/100... Step: 69800... Loss: 0.000055... Val Loss: 0.000809\n",
      "Epoch: 68/100... Step: 69805... Loss: 0.000114... Val Loss: 0.000776\n",
      "Epoch: 68/100... Step: 69810... Loss: 0.000169... Val Loss: 0.000514\n",
      "Epoch: 68/100... Step: 69815... Loss: 0.000088... Val Loss: 0.000354\n",
      "Epoch: 68/100... Step: 69820... Loss: 0.000133... Val Loss: 0.000391\n",
      "Epoch: 68/100... Step: 69825... Loss: 0.000153... Val Loss: 0.000406\n",
      "Epoch: 68/100... Step: 69830... Loss: 0.000118... Val Loss: 0.000267\n",
      "Epoch: 68/100... Step: 69835... Loss: 0.000185... Val Loss: 0.000255\n",
      "Epoch: 68/100... Step: 69840... Loss: 0.000227... Val Loss: 0.000374\n",
      "Epoch: 68/100... Step: 69845... Loss: 0.000118... Val Loss: 0.000353\n",
      "Epoch: 68/100... Step: 69850... Loss: 0.000156... Val Loss: 0.000266\n",
      "Epoch: 68/100... Step: 69855... Loss: 0.000160... Val Loss: 0.000443\n",
      "Epoch: 68/100... Step: 69860... Loss: 0.000074... Val Loss: 0.000563\n",
      "Epoch: 68/100... Step: 69865... Loss: 0.000123... Val Loss: 0.000488\n",
      "Epoch: 68/100... Step: 69870... Loss: 0.000183... Val Loss: 0.000339\n",
      "Epoch: 68/100... Step: 69875... Loss: 0.000108... Val Loss: 0.000389\n",
      "Epoch: 68/100... Step: 69880... Loss: 0.000159... Val Loss: 0.000301\n",
      "Epoch: 68/100... Step: 69885... Loss: 0.000192... Val Loss: 0.000478\n",
      "Epoch: 68/100... Step: 69890... Loss: 0.000081... Val Loss: 0.000514\n",
      "Epoch: 68/100... Step: 69895... Loss: 0.000065... Val Loss: 0.000361\n",
      "Epoch: 68/100... Step: 69900... Loss: 0.000087... Val Loss: 0.000373\n",
      "Epoch: 68/100... Step: 69905... Loss: 0.000084... Val Loss: 0.000424\n",
      "Epoch: 68/100... Step: 69910... Loss: 0.000070... Val Loss: 0.000364\n",
      "Epoch: 68/100... Step: 69915... Loss: 0.000147... Val Loss: 0.000373\n",
      "Epoch: 68/100... Step: 69920... Loss: 0.000203... Val Loss: 0.000448\n",
      "Epoch: 68/100... Step: 69925... Loss: 0.000154... Val Loss: 0.000550\n",
      "Epoch: 68/100... Step: 69930... Loss: 0.000066... Val Loss: 0.000621\n",
      "Epoch: 68/100... Step: 69935... Loss: 0.000092... Val Loss: 0.000483\n",
      "Epoch: 68/100... Step: 69940... Loss: 0.000066... Val Loss: 0.000417\n",
      "Epoch: 68/100... Step: 69945... Loss: 0.000119... Val Loss: 0.000438\n",
      "Epoch: 68/100... Step: 69950... Loss: 0.000111... Val Loss: 0.000695\n",
      "Epoch: 68/100... Step: 69955... Loss: 0.000151... Val Loss: 0.000790\n",
      "Epoch: 68/100... Step: 69960... Loss: 0.000064... Val Loss: 0.000710\n",
      "Epoch: 68/100... Step: 69965... Loss: 0.000072... Val Loss: 0.000576\n",
      "Epoch: 68/100... Step: 69970... Loss: 0.000054... Val Loss: 0.000604\n",
      "Epoch: 68/100... Step: 69975... Loss: 0.000053... Val Loss: 0.000698\n",
      "Epoch: 68/100... Step: 69980... Loss: 0.000049... Val Loss: 0.000626\n",
      "Epoch: 68/100... Step: 69985... Loss: 0.000050... Val Loss: 0.000592\n",
      "Epoch: 68/100... Step: 69990... Loss: 0.000028... Val Loss: 0.000626\n",
      "Epoch: 68/100... Step: 69995... Loss: 0.000050... Val Loss: 0.000594\n",
      "Epoch: 68/100... Step: 70000... Loss: 0.000110... Val Loss: 0.000685\n",
      "Epoch: 68/100... Step: 70005... Loss: 0.000089... Val Loss: 0.000652\n",
      "Epoch: 68/100... Step: 70010... Loss: 0.000059... Val Loss: 0.000866\n",
      "Epoch: 68/100... Step: 70015... Loss: 0.000073... Val Loss: 0.000703\n",
      "Epoch: 68/100... Step: 70020... Loss: 0.000094... Val Loss: 0.000830\n",
      "Epoch: 68/100... Step: 70025... Loss: 0.000116... Val Loss: 0.000705\n",
      "Epoch: 68/100... Step: 70030... Loss: 0.000160... Val Loss: 0.000693\n",
      "Epoch: 68/100... Step: 70035... Loss: 0.000171... Val Loss: 0.000657\n",
      "Epoch: 68/100... Step: 70040... Loss: 0.000102... Val Loss: 0.000623\n",
      "Epoch: 68/100... Step: 70045... Loss: 0.000103... Val Loss: 0.000602\n",
      "Epoch: 68/100... Step: 70050... Loss: 0.000108... Val Loss: 0.000526\n",
      "Epoch: 68/100... Step: 70055... Loss: 0.000135... Val Loss: 0.000529\n",
      "Epoch: 68/100... Step: 70060... Loss: 0.000135... Val Loss: 0.000630\n",
      "Epoch: 68/100... Step: 70065... Loss: 0.000080... Val Loss: 0.000739\n",
      "Epoch: 68/100... Step: 70070... Loss: 0.000094... Val Loss: 0.000773\n",
      "Epoch: 68/100... Step: 70075... Loss: 0.000122... Val Loss: 0.000848\n",
      "Epoch: 68/100... Step: 70080... Loss: 0.000087... Val Loss: 0.000987\n",
      "Epoch: 68/100... Step: 70085... Loss: 0.000176... Val Loss: 0.000968\n",
      "Epoch: 68/100... Step: 70090... Loss: 0.000067... Val Loss: 0.000740\n",
      "Epoch: 68/100... Step: 70095... Loss: 0.000188... Val Loss: 0.000980\n",
      "Epoch: 68/100... Step: 70100... Loss: 0.000108... Val Loss: 0.000937\n",
      "Epoch: 68/100... Step: 70105... Loss: 0.000102... Val Loss: 0.001101\n",
      "Epoch: 68/100... Step: 70110... Loss: 0.000182... Val Loss: 0.000938\n",
      "Epoch: 68/100... Step: 70115... Loss: 0.000182... Val Loss: 0.001038\n",
      "Epoch: 68/100... Step: 70120... Loss: 0.000066... Val Loss: 0.001117\n",
      "Epoch: 68/100... Step: 70125... Loss: 0.000166... Val Loss: 0.000814\n",
      "Epoch: 68/100... Step: 70130... Loss: 0.000123... Val Loss: 0.000918\n",
      "Epoch: 68/100... Step: 70135... Loss: 0.000059... Val Loss: 0.001013\n",
      "Epoch: 68/100... Step: 70140... Loss: 0.000049... Val Loss: 0.001034\n",
      "Epoch: 68/100... Step: 70145... Loss: 0.000128... Val Loss: 0.000896\n",
      "Epoch: 68/100... Step: 70150... Loss: 0.000123... Val Loss: 0.000816\n",
      "Epoch: 68/100... Step: 70155... Loss: 0.000161... Val Loss: 0.000948\n",
      "Epoch: 68/100... Step: 70160... Loss: 0.000148... Val Loss: 0.001059\n",
      "Epoch: 68/100... Step: 70165... Loss: 0.000091... Val Loss: 0.001192\n",
      "Epoch: 68/100... Step: 70170... Loss: 0.000128... Val Loss: 0.001028\n",
      "Epoch: 68/100... Step: 70175... Loss: 0.000216... Val Loss: 0.000937\n",
      "Epoch: 69/100... Step: 70180... Loss: 0.000155... Val Loss: 0.001647\n",
      "Epoch: 69/100... Step: 70185... Loss: 0.000210... Val Loss: 0.001383\n",
      "Epoch: 69/100... Step: 70190... Loss: 0.000119... Val Loss: 0.001503\n",
      "Epoch: 69/100... Step: 70195... Loss: 0.000184... Val Loss: 0.001918\n",
      "Epoch: 69/100... Step: 70200... Loss: 0.000132... Val Loss: 0.001686\n",
      "Epoch: 69/100... Step: 70205... Loss: 0.000092... Val Loss: 0.001803\n",
      "Epoch: 69/100... Step: 70210... Loss: 0.000153... Val Loss: 0.001789\n",
      "Epoch: 69/100... Step: 70215... Loss: 0.000158... Val Loss: 0.001679\n",
      "Epoch: 69/100... Step: 70220... Loss: 0.000228... Val Loss: 0.001861\n",
      "Epoch: 69/100... Step: 70225... Loss: 0.000096... Val Loss: 0.001732\n",
      "Epoch: 69/100... Step: 70230... Loss: 0.000118... Val Loss: 0.001620\n",
      "Epoch: 69/100... Step: 70235... Loss: 0.000101... Val Loss: 0.001626\n",
      "Epoch: 69/100... Step: 70240... Loss: 0.000060... Val Loss: 0.001571\n",
      "Epoch: 69/100... Step: 70245... Loss: 0.000214... Val Loss: 0.001488\n",
      "Epoch: 69/100... Step: 70250... Loss: 0.000159... Val Loss: 0.000898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69/100... Step: 70255... Loss: 0.000194... Val Loss: 0.001421\n",
      "Epoch: 69/100... Step: 70260... Loss: 0.000041... Val Loss: 0.001440\n",
      "Epoch: 69/100... Step: 70265... Loss: 0.000098... Val Loss: 0.001385\n",
      "Epoch: 69/100... Step: 70270... Loss: 0.000109... Val Loss: 0.001297\n",
      "Epoch: 69/100... Step: 70275... Loss: 0.000167... Val Loss: 0.000985\n",
      "Epoch: 69/100... Step: 70280... Loss: 0.000122... Val Loss: 0.001107\n",
      "Epoch: 69/100... Step: 70285... Loss: 0.000054... Val Loss: 0.001041\n",
      "Epoch: 69/100... Step: 70290... Loss: 0.000030... Val Loss: 0.001103\n",
      "Epoch: 69/100... Step: 70295... Loss: 0.000089... Val Loss: 0.001115\n",
      "Epoch: 69/100... Step: 70300... Loss: 0.000117... Val Loss: 0.001088\n",
      "Epoch: 69/100... Step: 70305... Loss: 0.000071... Val Loss: 0.001400\n",
      "Epoch: 69/100... Step: 70310... Loss: 0.000093... Val Loss: 0.001305\n",
      "Epoch: 69/100... Step: 70315... Loss: 0.000110... Val Loss: 0.001389\n",
      "Epoch: 69/100... Step: 70320... Loss: 0.000122... Val Loss: 0.001632\n",
      "Epoch: 69/100... Step: 70325... Loss: 0.000165... Val Loss: 0.001909\n",
      "Epoch: 69/100... Step: 70330... Loss: 0.000051... Val Loss: 0.001611\n",
      "Epoch: 69/100... Step: 70335... Loss: 0.000102... Val Loss: 0.001478\n",
      "Epoch: 69/100... Step: 70340... Loss: 0.000062... Val Loss: 0.001472\n",
      "Epoch: 69/100... Step: 70345... Loss: 0.000144... Val Loss: 0.001407\n",
      "Epoch: 69/100... Step: 70350... Loss: 0.000125... Val Loss: 0.001329\n",
      "Epoch: 69/100... Step: 70355... Loss: 0.000066... Val Loss: 0.001365\n",
      "Epoch: 69/100... Step: 70360... Loss: 0.000082... Val Loss: 0.001192\n",
      "Epoch: 69/100... Step: 70365... Loss: 0.000078... Val Loss: 0.001146\n",
      "Epoch: 69/100... Step: 70370... Loss: 0.000056... Val Loss: 0.001139\n",
      "Epoch: 69/100... Step: 70375... Loss: 0.000057... Val Loss: 0.001126\n",
      "Epoch: 69/100... Step: 70380... Loss: 0.000107... Val Loss: 0.001216\n",
      "Epoch: 69/100... Step: 70385... Loss: 0.000105... Val Loss: 0.001106\n",
      "Epoch: 69/100... Step: 70390... Loss: 0.000081... Val Loss: 0.001335\n",
      "Epoch: 69/100... Step: 70395... Loss: 0.000154... Val Loss: 0.001351\n",
      "Epoch: 69/100... Step: 70400... Loss: 0.000081... Val Loss: 0.001292\n",
      "Epoch: 69/100... Step: 70405... Loss: 0.000124... Val Loss: 0.001119\n",
      "Epoch: 69/100... Step: 70410... Loss: 0.000040... Val Loss: 0.001039\n",
      "Epoch: 69/100... Step: 70415... Loss: 0.000107... Val Loss: 0.001122\n",
      "Epoch: 69/100... Step: 70420... Loss: 0.000132... Val Loss: 0.001152\n",
      "Epoch: 69/100... Step: 70425... Loss: 0.000076... Val Loss: 0.001200\n",
      "Epoch: 69/100... Step: 70430... Loss: 0.000055... Val Loss: 0.001090\n",
      "Epoch: 69/100... Step: 70435... Loss: 0.000099... Val Loss: 0.001073\n",
      "Epoch: 69/100... Step: 70440... Loss: 0.000132... Val Loss: 0.000976\n",
      "Epoch: 69/100... Step: 70445... Loss: 0.000083... Val Loss: 0.000951\n",
      "Epoch: 69/100... Step: 70450... Loss: 0.000135... Val Loss: 0.001010\n",
      "Epoch: 69/100... Step: 70455... Loss: 0.000075... Val Loss: 0.001010\n",
      "Epoch: 69/100... Step: 70460... Loss: 0.000118... Val Loss: 0.000985\n",
      "Epoch: 69/100... Step: 70465... Loss: 0.000179... Val Loss: 0.000795\n",
      "Epoch: 69/100... Step: 70470... Loss: 0.000060... Val Loss: 0.000648\n",
      "Epoch: 69/100... Step: 70475... Loss: 0.000071... Val Loss: 0.000566\n",
      "Epoch: 69/100... Step: 70480... Loss: 0.000162... Val Loss: 0.000607\n",
      "Epoch: 69/100... Step: 70485... Loss: 0.000172... Val Loss: 0.000698\n",
      "Epoch: 69/100... Step: 70490... Loss: 0.000033... Val Loss: 0.000643\n",
      "Epoch: 69/100... Step: 70495... Loss: 0.000103... Val Loss: 0.000716\n",
      "Epoch: 69/100... Step: 70500... Loss: 0.000127... Val Loss: 0.000600\n",
      "Epoch: 69/100... Step: 70505... Loss: 0.000104... Val Loss: 0.000431\n",
      "Epoch: 69/100... Step: 70510... Loss: 0.000129... Val Loss: 0.000499\n",
      "Epoch: 69/100... Step: 70515... Loss: 0.000153... Val Loss: 0.000596\n",
      "Epoch: 69/100... Step: 70520... Loss: 0.000237... Val Loss: 0.000528\n",
      "Epoch: 69/100... Step: 70525... Loss: 0.000132... Val Loss: 0.000352\n",
      "Epoch: 69/100... Step: 70530... Loss: 0.000241... Val Loss: 0.000376\n",
      "Epoch: 69/100... Step: 70535... Loss: 0.000168... Val Loss: 0.000818\n",
      "Epoch: 69/100... Step: 70540... Loss: 0.000110... Val Loss: 0.001024\n",
      "Epoch: 69/100... Step: 70545... Loss: 0.000174... Val Loss: 0.000935\n",
      "Epoch: 69/100... Step: 70550... Loss: 0.000079... Val Loss: 0.000893\n",
      "Epoch: 69/100... Step: 70555... Loss: 0.000092... Val Loss: 0.001020\n",
      "Epoch: 69/100... Step: 70560... Loss: 0.000110... Val Loss: 0.001041\n",
      "Epoch: 69/100... Step: 70565... Loss: 0.000122... Val Loss: 0.001044\n",
      "Epoch: 69/100... Step: 70570... Loss: 0.000112... Val Loss: 0.001064\n",
      "Epoch: 69/100... Step: 70575... Loss: 0.000067... Val Loss: 0.001210\n",
      "Epoch: 69/100... Step: 70580... Loss: 0.000354... Val Loss: 0.000978\n",
      "Epoch: 69/100... Step: 70585... Loss: 0.000280... Val Loss: 0.000393\n",
      "Epoch: 69/100... Step: 70590... Loss: 0.000735... Val Loss: 0.000326\n",
      "Epoch: 69/100... Step: 70595... Loss: 0.000628... Val Loss: 0.001127\n",
      "Epoch: 69/100... Step: 70600... Loss: 0.000289... Val Loss: 0.000827\n",
      "Epoch: 69/100... Step: 70605... Loss: 0.000248... Val Loss: 0.001192\n",
      "Epoch: 69/100... Step: 70610... Loss: 0.000121... Val Loss: 0.000909\n",
      "Epoch: 69/100... Step: 70615... Loss: 0.000264... Val Loss: 0.001377\n",
      "Epoch: 69/100... Step: 70620... Loss: 0.000194... Val Loss: 0.001232\n",
      "Epoch: 69/100... Step: 70625... Loss: 0.000203... Val Loss: 0.001191\n",
      "Epoch: 69/100... Step: 70630... Loss: 0.000270... Val Loss: 0.001203\n",
      "Epoch: 69/100... Step: 70635... Loss: 0.000294... Val Loss: 0.001369\n",
      "Epoch: 69/100... Step: 70640... Loss: 0.000271... Val Loss: 0.001662\n",
      "Epoch: 69/100... Step: 70645... Loss: 0.000202... Val Loss: 0.001236\n",
      "Epoch: 69/100... Step: 70650... Loss: 0.000224... Val Loss: 0.001053\n",
      "Epoch: 69/100... Step: 70655... Loss: 0.000117... Val Loss: 0.001127\n",
      "Epoch: 69/100... Step: 70660... Loss: 0.000334... Val Loss: 0.000970\n",
      "Epoch: 69/100... Step: 70665... Loss: 0.000180... Val Loss: 0.000998\n",
      "Epoch: 69/100... Step: 70670... Loss: 0.000135... Val Loss: 0.000642\n",
      "Epoch: 69/100... Step: 70675... Loss: 0.000178... Val Loss: 0.000981\n",
      "Epoch: 69/100... Step: 70680... Loss: 0.000225... Val Loss: 0.000393\n",
      "Epoch: 69/100... Step: 70685... Loss: 0.000128... Val Loss: 0.000549\n",
      "Epoch: 69/100... Step: 70690... Loss: 0.000192... Val Loss: 0.000576\n",
      "Epoch: 69/100... Step: 70695... Loss: 0.000172... Val Loss: 0.000528\n",
      "Epoch: 69/100... Step: 70700... Loss: 0.000254... Val Loss: 0.000750\n",
      "Epoch: 69/100... Step: 70705... Loss: 0.000230... Val Loss: 0.000741\n",
      "Epoch: 69/100... Step: 70710... Loss: 0.000294... Val Loss: 0.000574\n",
      "Epoch: 69/100... Step: 70715... Loss: 0.000267... Val Loss: 0.000505\n",
      "Epoch: 69/100... Step: 70720... Loss: 0.000164... Val Loss: 0.000785\n",
      "Epoch: 69/100... Step: 70725... Loss: 0.000099... Val Loss: 0.000885\n",
      "Epoch: 69/100... Step: 70730... Loss: 0.000111... Val Loss: 0.000832\n",
      "Epoch: 69/100... Step: 70735... Loss: 0.000085... Val Loss: 0.000693\n",
      "Epoch: 69/100... Step: 70740... Loss: 0.000147... Val Loss: 0.000768\n",
      "Epoch: 69/100... Step: 70745... Loss: 0.000081... Val Loss: 0.000750\n",
      "Epoch: 69/100... Step: 70750... Loss: 0.000120... Val Loss: 0.000931\n",
      "Epoch: 69/100... Step: 70755... Loss: 0.000107... Val Loss: 0.000839\n",
      "Epoch: 69/100... Step: 70760... Loss: 0.000188... Val Loss: 0.000729\n",
      "Epoch: 69/100... Step: 70765... Loss: 0.000142... Val Loss: 0.000928\n",
      "Epoch: 69/100... Step: 70770... Loss: 0.000132... Val Loss: 0.001111\n",
      "Epoch: 69/100... Step: 70775... Loss: 0.000339... Val Loss: 0.000816\n",
      "Epoch: 69/100... Step: 70780... Loss: 0.000131... Val Loss: 0.000928\n",
      "Epoch: 69/100... Step: 70785... Loss: 0.000159... Val Loss: 0.000820\n",
      "Epoch: 69/100... Step: 70790... Loss: 0.000069... Val Loss: 0.000731\n",
      "Epoch: 69/100... Step: 70795... Loss: 0.000058... Val Loss: 0.000811\n",
      "Epoch: 69/100... Step: 70800... Loss: 0.000094... Val Loss: 0.000936\n",
      "Epoch: 69/100... Step: 70805... Loss: 0.000106... Val Loss: 0.000828\n",
      "Epoch: 69/100... Step: 70810... Loss: 0.000078... Val Loss: 0.000796\n",
      "Epoch: 69/100... Step: 70815... Loss: 0.000168... Val Loss: 0.000818\n",
      "Epoch: 69/100... Step: 70820... Loss: 0.000264... Val Loss: 0.000491\n",
      "Epoch: 69/100... Step: 70825... Loss: 0.000152... Val Loss: 0.000657\n",
      "Epoch: 69/100... Step: 70830... Loss: 0.000125... Val Loss: 0.000942\n",
      "Epoch: 69/100... Step: 70835... Loss: 0.000107... Val Loss: 0.000946\n",
      "Epoch: 69/100... Step: 70840... Loss: 0.000123... Val Loss: 0.000601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69/100... Step: 70845... Loss: 0.000108... Val Loss: 0.000335\n",
      "Epoch: 69/100... Step: 70850... Loss: 0.000081... Val Loss: 0.000335\n",
      "Epoch: 69/100... Step: 70855... Loss: 0.000149... Val Loss: 0.000293\n",
      "Epoch: 69/100... Step: 70860... Loss: 0.000143... Val Loss: 0.000322\n",
      "Epoch: 69/100... Step: 70865... Loss: 0.000122... Val Loss: 0.000339\n",
      "Epoch: 69/100... Step: 70870... Loss: 0.000117... Val Loss: 0.000385\n",
      "Epoch: 69/100... Step: 70875... Loss: 0.000091... Val Loss: 0.000309\n",
      "Epoch: 69/100... Step: 70880... Loss: 0.000057... Val Loss: 0.000316\n",
      "Epoch: 69/100... Step: 70885... Loss: 0.000053... Val Loss: 0.000313\n",
      "Epoch: 69/100... Step: 70890... Loss: 0.000044... Val Loss: 0.000399\n",
      "Epoch: 69/100... Step: 70895... Loss: 0.000078... Val Loss: 0.000386\n",
      "Epoch: 69/100... Step: 70900... Loss: 0.000133... Val Loss: 0.000378\n",
      "Epoch: 69/100... Step: 70905... Loss: 0.000096... Val Loss: 0.000359\n",
      "Epoch: 69/100... Step: 70910... Loss: 0.000088... Val Loss: 0.000378\n",
      "Epoch: 69/100... Step: 70915... Loss: 0.000238... Val Loss: 0.000269\n",
      "Epoch: 69/100... Step: 70920... Loss: 0.000186... Val Loss: 0.000500\n",
      "Epoch: 69/100... Step: 70925... Loss: 0.000052... Val Loss: 0.000277\n",
      "Epoch: 69/100... Step: 70930... Loss: 0.000038... Val Loss: 0.000264\n",
      "Epoch: 69/100... Step: 70935... Loss: 0.000126... Val Loss: 0.000437\n",
      "Epoch: 69/100... Step: 70940... Loss: 0.000140... Val Loss: 0.000358\n",
      "Epoch: 69/100... Step: 70945... Loss: 0.000064... Val Loss: 0.000489\n",
      "Epoch: 69/100... Step: 70950... Loss: 0.000044... Val Loss: 0.000529\n",
      "Epoch: 69/100... Step: 70955... Loss: 0.000152... Val Loss: 0.000360\n",
      "Epoch: 69/100... Step: 70960... Loss: 0.000123... Val Loss: 0.000529\n",
      "Epoch: 69/100... Step: 70965... Loss: 0.000283... Val Loss: 0.000596\n",
      "Epoch: 69/100... Step: 70970... Loss: 0.000150... Val Loss: 0.000383\n",
      "Epoch: 69/100... Step: 70975... Loss: 0.000136... Val Loss: 0.000378\n",
      "Epoch: 69/100... Step: 70980... Loss: 0.000145... Val Loss: 0.000560\n",
      "Epoch: 69/100... Step: 70985... Loss: 0.000076... Val Loss: 0.000796\n",
      "Epoch: 69/100... Step: 70990... Loss: 0.000080... Val Loss: 0.000635\n",
      "Epoch: 69/100... Step: 70995... Loss: 0.000059... Val Loss: 0.000542\n",
      "Epoch: 69/100... Step: 71000... Loss: 0.000034... Val Loss: 0.000615\n",
      "Epoch: 69/100... Step: 71005... Loss: 0.000110... Val Loss: 0.000661\n",
      "Epoch: 69/100... Step: 71010... Loss: 0.000063... Val Loss: 0.000608\n",
      "Epoch: 69/100... Step: 71015... Loss: 0.000116... Val Loss: 0.000597\n",
      "Epoch: 69/100... Step: 71020... Loss: 0.000140... Val Loss: 0.000582\n",
      "Epoch: 69/100... Step: 71025... Loss: 0.000134... Val Loss: 0.000498\n",
      "Epoch: 69/100... Step: 71030... Loss: 0.000081... Val Loss: 0.000556\n",
      "Epoch: 69/100... Step: 71035... Loss: 0.000102... Val Loss: 0.000662\n",
      "Epoch: 69/100... Step: 71040... Loss: 0.000045... Val Loss: 0.000696\n",
      "Epoch: 69/100... Step: 71045... Loss: 0.000159... Val Loss: 0.000818\n",
      "Epoch: 69/100... Step: 71050... Loss: 0.000222... Val Loss: 0.001015\n",
      "Epoch: 69/100... Step: 71055... Loss: 0.000107... Val Loss: 0.000912\n",
      "Epoch: 69/100... Step: 71060... Loss: 0.000100... Val Loss: 0.000688\n",
      "Epoch: 69/100... Step: 71065... Loss: 0.000161... Val Loss: 0.000587\n",
      "Epoch: 69/100... Step: 71070... Loss: 0.000100... Val Loss: 0.000710\n",
      "Epoch: 69/100... Step: 71075... Loss: 0.000083... Val Loss: 0.000595\n",
      "Epoch: 69/100... Step: 71080... Loss: 0.000149... Val Loss: 0.000595\n",
      "Epoch: 69/100... Step: 71085... Loss: 0.000137... Val Loss: 0.000601\n",
      "Epoch: 69/100... Step: 71090... Loss: 0.000108... Val Loss: 0.000672\n",
      "Epoch: 69/100... Step: 71095... Loss: 0.000114... Val Loss: 0.000747\n",
      "Epoch: 69/100... Step: 71100... Loss: 0.000070... Val Loss: 0.000733\n",
      "Epoch: 69/100... Step: 71105... Loss: 0.000077... Val Loss: 0.000830\n",
      "Epoch: 69/100... Step: 71110... Loss: 0.000092... Val Loss: 0.000819\n",
      "Epoch: 69/100... Step: 71115... Loss: 0.000088... Val Loss: 0.000972\n",
      "Epoch: 69/100... Step: 71120... Loss: 0.000073... Val Loss: 0.000884\n",
      "Epoch: 69/100... Step: 71125... Loss: 0.000131... Val Loss: 0.000775\n",
      "Epoch: 69/100... Step: 71130... Loss: 0.000079... Val Loss: 0.000962\n",
      "Epoch: 69/100... Step: 71135... Loss: 0.000050... Val Loss: 0.000969\n",
      "Epoch: 69/100... Step: 71140... Loss: 0.000098... Val Loss: 0.001088\n",
      "Epoch: 69/100... Step: 71145... Loss: 0.000110... Val Loss: 0.001002\n",
      "Epoch: 69/100... Step: 71150... Loss: 0.000050... Val Loss: 0.000981\n",
      "Epoch: 69/100... Step: 71155... Loss: 0.000078... Val Loss: 0.000797\n",
      "Epoch: 69/100... Step: 71160... Loss: 0.000204... Val Loss: 0.000871\n",
      "Epoch: 69/100... Step: 71165... Loss: 0.000105... Val Loss: 0.001096\n",
      "Epoch: 69/100... Step: 71170... Loss: 0.000164... Val Loss: 0.000993\n",
      "Epoch: 69/100... Step: 71175... Loss: 0.000116... Val Loss: 0.001039\n",
      "Epoch: 69/100... Step: 71180... Loss: 0.000094... Val Loss: 0.000990\n",
      "Epoch: 69/100... Step: 71185... Loss: 0.000066... Val Loss: 0.000985\n",
      "Epoch: 69/100... Step: 71190... Loss: 0.000098... Val Loss: 0.000896\n",
      "Epoch: 69/100... Step: 71195... Loss: 0.000138... Val Loss: 0.000949\n",
      "Epoch: 69/100... Step: 71200... Loss: 0.000102... Val Loss: 0.001017\n",
      "Epoch: 69/100... Step: 71205... Loss: 0.000076... Val Loss: 0.000872\n",
      "Epoch: 70/100... Step: 71210... Loss: 0.000134... Val Loss: 0.001380\n",
      "Epoch: 70/100... Step: 71215... Loss: 0.000136... Val Loss: 0.001324\n",
      "Epoch: 70/100... Step: 71220... Loss: 0.000154... Val Loss: 0.001405\n",
      "Epoch: 70/100... Step: 71225... Loss: 0.000252... Val Loss: 0.001919\n",
      "Epoch: 70/100... Step: 71230... Loss: 0.000266... Val Loss: 0.001658\n",
      "Epoch: 70/100... Step: 71235... Loss: 0.000123... Val Loss: 0.001689\n",
      "Epoch: 70/100... Step: 71240... Loss: 0.000164... Val Loss: 0.001844\n",
      "Epoch: 70/100... Step: 71245... Loss: 0.000237... Val Loss: 0.001794\n",
      "Epoch: 70/100... Step: 71250... Loss: 0.000239... Val Loss: 0.001673\n",
      "Epoch: 70/100... Step: 71255... Loss: 0.000111... Val Loss: 0.001666\n",
      "Epoch: 70/100... Step: 71260... Loss: 0.000100... Val Loss: 0.001641\n",
      "Epoch: 70/100... Step: 71265... Loss: 0.000132... Val Loss: 0.001772\n",
      "Epoch: 70/100... Step: 71270... Loss: 0.000124... Val Loss: 0.001692\n",
      "Epoch: 70/100... Step: 71275... Loss: 0.000195... Val Loss: 0.001587\n",
      "Epoch: 70/100... Step: 71280... Loss: 0.000181... Val Loss: 0.001062\n",
      "Epoch: 70/100... Step: 71285... Loss: 0.000135... Val Loss: 0.001329\n",
      "Epoch: 70/100... Step: 71290... Loss: 0.000149... Val Loss: 0.001383\n",
      "Epoch: 70/100... Step: 71295... Loss: 0.000183... Val Loss: 0.001413\n",
      "Epoch: 70/100... Step: 71300... Loss: 0.000132... Val Loss: 0.001412\n",
      "Epoch: 70/100... Step: 71305... Loss: 0.000165... Val Loss: 0.000984\n",
      "Epoch: 70/100... Step: 71310... Loss: 0.000084... Val Loss: 0.001056\n",
      "Epoch: 70/100... Step: 71315... Loss: 0.000193... Val Loss: 0.001175\n",
      "Epoch: 70/100... Step: 71320... Loss: 0.000119... Val Loss: 0.001142\n",
      "Epoch: 70/100... Step: 71325... Loss: 0.000070... Val Loss: 0.001191\n",
      "Epoch: 70/100... Step: 71330... Loss: 0.000175... Val Loss: 0.001301\n",
      "Epoch: 70/100... Step: 71335... Loss: 0.000053... Val Loss: 0.001269\n",
      "Epoch: 70/100... Step: 71340... Loss: 0.000136... Val Loss: 0.001292\n",
      "Epoch: 70/100... Step: 71345... Loss: 0.000052... Val Loss: 0.001515\n",
      "Epoch: 70/100... Step: 71350... Loss: 0.000185... Val Loss: 0.001416\n",
      "Epoch: 70/100... Step: 71355... Loss: 0.000093... Val Loss: 0.001645\n",
      "Epoch: 70/100... Step: 71360... Loss: 0.000128... Val Loss: 0.001595\n",
      "Epoch: 70/100... Step: 71365... Loss: 0.000133... Val Loss: 0.001506\n",
      "Epoch: 70/100... Step: 71370... Loss: 0.000122... Val Loss: 0.001420\n",
      "Epoch: 70/100... Step: 71375... Loss: 0.000070... Val Loss: 0.001307\n",
      "Epoch: 70/100... Step: 71380... Loss: 0.000079... Val Loss: 0.001316\n",
      "Epoch: 70/100... Step: 71385... Loss: 0.000064... Val Loss: 0.001362\n",
      "Epoch: 70/100... Step: 71390... Loss: 0.000101... Val Loss: 0.001398\n",
      "Epoch: 70/100... Step: 71395... Loss: 0.000079... Val Loss: 0.001219\n",
      "Epoch: 70/100... Step: 71400... Loss: 0.000113... Val Loss: 0.001244\n",
      "Epoch: 70/100... Step: 71405... Loss: 0.000102... Val Loss: 0.001213\n",
      "Epoch: 70/100... Step: 71410... Loss: 0.000098... Val Loss: 0.001219\n",
      "Epoch: 70/100... Step: 71415... Loss: 0.000110... Val Loss: 0.001116\n",
      "Epoch: 70/100... Step: 71420... Loss: 0.000085... Val Loss: 0.001388\n",
      "Epoch: 70/100... Step: 71425... Loss: 0.000067... Val Loss: 0.001410\n",
      "Epoch: 70/100... Step: 71430... Loss: 0.000172... Val Loss: 0.001098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70/100... Step: 71435... Loss: 0.000123... Val Loss: 0.001321\n",
      "Epoch: 70/100... Step: 71440... Loss: 0.000258... Val Loss: 0.001099\n",
      "Epoch: 70/100... Step: 71445... Loss: 0.000244... Val Loss: 0.001085\n",
      "Epoch: 70/100... Step: 71450... Loss: 0.000185... Val Loss: 0.001148\n",
      "Epoch: 70/100... Step: 71455... Loss: 0.000088... Val Loss: 0.001048\n",
      "Epoch: 70/100... Step: 71460... Loss: 0.000190... Val Loss: 0.001178\n",
      "Epoch: 70/100... Step: 71465... Loss: 0.000203... Val Loss: 0.001178\n",
      "Epoch: 70/100... Step: 71470... Loss: 0.000146... Val Loss: 0.000938\n",
      "Epoch: 70/100... Step: 71475... Loss: 0.000088... Val Loss: 0.000927\n",
      "Epoch: 70/100... Step: 71480... Loss: 0.000058... Val Loss: 0.000908\n",
      "Epoch: 70/100... Step: 71485... Loss: 0.000085... Val Loss: 0.000833\n",
      "Epoch: 70/100... Step: 71490... Loss: 0.000121... Val Loss: 0.000895\n",
      "Epoch: 70/100... Step: 71495... Loss: 0.000175... Val Loss: 0.000795\n",
      "Epoch: 70/100... Step: 71500... Loss: 0.000072... Val Loss: 0.000596\n",
      "Epoch: 70/100... Step: 71505... Loss: 0.000062... Val Loss: 0.000569\n",
      "Epoch: 70/100... Step: 71510... Loss: 0.000115... Val Loss: 0.000588\n",
      "Epoch: 70/100... Step: 71515... Loss: 0.000151... Val Loss: 0.000747\n",
      "Epoch: 70/100... Step: 71520... Loss: 0.000095... Val Loss: 0.000552\n",
      "Epoch: 70/100... Step: 71525... Loss: 0.000041... Val Loss: 0.000609\n",
      "Epoch: 70/100... Step: 71530... Loss: 0.000049... Val Loss: 0.000672\n",
      "Epoch: 70/100... Step: 71535... Loss: 0.000089... Val Loss: 0.000420\n",
      "Epoch: 70/100... Step: 71540... Loss: 0.000077... Val Loss: 0.000424\n",
      "Epoch: 70/100... Step: 71545... Loss: 0.000248... Val Loss: 0.000579\n",
      "Epoch: 70/100... Step: 71550... Loss: 0.000170... Val Loss: 0.000608\n",
      "Epoch: 70/100... Step: 71555... Loss: 0.000232... Val Loss: 0.000507\n",
      "Epoch: 70/100... Step: 71560... Loss: 0.000167... Val Loss: 0.000418\n",
      "Epoch: 70/100... Step: 71565... Loss: 0.000140... Val Loss: 0.000700\n",
      "Epoch: 70/100... Step: 71570... Loss: 0.000211... Val Loss: 0.000974\n",
      "Epoch: 70/100... Step: 71575... Loss: 0.000210... Val Loss: 0.001078\n",
      "Epoch: 70/100... Step: 71580... Loss: 0.000063... Val Loss: 0.000966\n",
      "Epoch: 70/100... Step: 71585... Loss: 0.000131... Val Loss: 0.001059\n",
      "Epoch: 70/100... Step: 71590... Loss: 0.000077... Val Loss: 0.001032\n",
      "Epoch: 70/100... Step: 71595... Loss: 0.000099... Val Loss: 0.001028\n",
      "Epoch: 70/100... Step: 71600... Loss: 0.000113... Val Loss: 0.001092\n",
      "Epoch: 70/100... Step: 71605... Loss: 0.000075... Val Loss: 0.001163\n",
      "Epoch: 70/100... Step: 71610... Loss: 0.000107... Val Loss: 0.001209\n",
      "Epoch: 70/100... Step: 71615... Loss: 0.000361... Val Loss: 0.000689\n",
      "Epoch: 70/100... Step: 71620... Loss: 0.000424... Val Loss: 0.000344\n",
      "Epoch: 70/100... Step: 71625... Loss: 0.000968... Val Loss: 0.000332\n",
      "Epoch: 70/100... Step: 71630... Loss: 0.000685... Val Loss: 0.001504\n",
      "Epoch: 70/100... Step: 71635... Loss: 0.000174... Val Loss: 0.000979\n",
      "Epoch: 70/100... Step: 71640... Loss: 0.000134... Val Loss: 0.000974\n",
      "Epoch: 70/100... Step: 71645... Loss: 0.000181... Val Loss: 0.001380\n",
      "Epoch: 70/100... Step: 71650... Loss: 0.000118... Val Loss: 0.001225\n",
      "Epoch: 70/100... Step: 71655... Loss: 0.000100... Val Loss: 0.001217\n",
      "Epoch: 70/100... Step: 71660... Loss: 0.000193... Val Loss: 0.001344\n",
      "Epoch: 70/100... Step: 71665... Loss: 0.000126... Val Loss: 0.001269\n",
      "Epoch: 70/100... Step: 71670... Loss: 0.000271... Val Loss: 0.001510\n",
      "Epoch: 70/100... Step: 71675... Loss: 0.000126... Val Loss: 0.001510\n",
      "Epoch: 70/100... Step: 71680... Loss: 0.000113... Val Loss: 0.001090\n",
      "Epoch: 70/100... Step: 71685... Loss: 0.000093... Val Loss: 0.001121\n",
      "Epoch: 70/100... Step: 71690... Loss: 0.000147... Val Loss: 0.001124\n",
      "Epoch: 70/100... Step: 71695... Loss: 0.000127... Val Loss: 0.000863\n",
      "Epoch: 70/100... Step: 71700... Loss: 0.000100... Val Loss: 0.000893\n",
      "Epoch: 70/100... Step: 71705... Loss: 0.000173... Val Loss: 0.000660\n",
      "Epoch: 70/100... Step: 71710... Loss: 0.000101... Val Loss: 0.000610\n",
      "Epoch: 70/100... Step: 71715... Loss: 0.000198... Val Loss: 0.000311\n",
      "Epoch: 70/100... Step: 71720... Loss: 0.000111... Val Loss: 0.000468\n",
      "Epoch: 70/100... Step: 71725... Loss: 0.000089... Val Loss: 0.000380\n",
      "Epoch: 70/100... Step: 71730... Loss: 0.000156... Val Loss: 0.000525\n",
      "Epoch: 70/100... Step: 71735... Loss: 0.000063... Val Loss: 0.000589\n",
      "Epoch: 70/100... Step: 71740... Loss: 0.000125... Val Loss: 0.000545\n",
      "Epoch: 70/100... Step: 71745... Loss: 0.000083... Val Loss: 0.000477\n",
      "Epoch: 70/100... Step: 71750... Loss: 0.000197... Val Loss: 0.000881\n",
      "Epoch: 70/100... Step: 71755... Loss: 0.000060... Val Loss: 0.001011\n",
      "Epoch: 70/100... Step: 71760... Loss: 0.000205... Val Loss: 0.000739\n",
      "Epoch: 70/100... Step: 71765... Loss: 0.000105... Val Loss: 0.000669\n",
      "Epoch: 70/100... Step: 71770... Loss: 0.000127... Val Loss: 0.000864\n",
      "Epoch: 70/100... Step: 71775... Loss: 0.000146... Val Loss: 0.000828\n",
      "Epoch: 70/100... Step: 71780... Loss: 0.000092... Val Loss: 0.001015\n",
      "Epoch: 70/100... Step: 71785... Loss: 0.000131... Val Loss: 0.000791\n",
      "Epoch: 70/100... Step: 71790... Loss: 0.000112... Val Loss: 0.000746\n",
      "Epoch: 70/100... Step: 71795... Loss: 0.000110... Val Loss: 0.000710\n",
      "Epoch: 70/100... Step: 71800... Loss: 0.000117... Val Loss: 0.000886\n",
      "Epoch: 70/100... Step: 71805... Loss: 0.000112... Val Loss: 0.000941\n",
      "Epoch: 70/100... Step: 71810... Loss: 0.000138... Val Loss: 0.000991\n",
      "Epoch: 70/100... Step: 71815... Loss: 0.000129... Val Loss: 0.000960\n",
      "Epoch: 70/100... Step: 71820... Loss: 0.000111... Val Loss: 0.001018\n",
      "Epoch: 70/100... Step: 71825... Loss: 0.000115... Val Loss: 0.000823\n",
      "Epoch: 70/100... Step: 71830... Loss: 0.000083... Val Loss: 0.000953\n",
      "Epoch: 70/100... Step: 71835... Loss: 0.000118... Val Loss: 0.000865\n",
      "Epoch: 70/100... Step: 71840... Loss: 0.000041... Val Loss: 0.000828\n",
      "Epoch: 70/100... Step: 71845... Loss: 0.000064... Val Loss: 0.000852\n",
      "Epoch: 70/100... Step: 71850... Loss: 0.000082... Val Loss: 0.000495\n",
      "Epoch: 70/100... Step: 71855... Loss: 0.000193... Val Loss: 0.000557\n",
      "Epoch: 70/100... Step: 71860... Loss: 0.000184... Val Loss: 0.000962\n",
      "Epoch: 70/100... Step: 71865... Loss: 0.000123... Val Loss: 0.000881\n",
      "Epoch: 70/100... Step: 71870... Loss: 0.000084... Val Loss: 0.000694\n",
      "Epoch: 70/100... Step: 71875... Loss: 0.000155... Val Loss: 0.000536\n",
      "Epoch: 70/100... Step: 71880... Loss: 0.000077... Val Loss: 0.000468\n",
      "Epoch: 70/100... Step: 71885... Loss: 0.000061... Val Loss: 0.000342\n",
      "Epoch: 70/100... Step: 71890... Loss: 0.000131... Val Loss: 0.000269\n",
      "Epoch: 70/100... Step: 71895... Loss: 0.000116... Val Loss: 0.000306\n",
      "Epoch: 70/100... Step: 71900... Loss: 0.000055... Val Loss: 0.000314\n",
      "Epoch: 70/100... Step: 71905... Loss: 0.000085... Val Loss: 0.000302\n",
      "Epoch: 70/100... Step: 71910... Loss: 0.000073... Val Loss: 0.000277\n",
      "Epoch: 70/100... Step: 71915... Loss: 0.000066... Val Loss: 0.000313\n",
      "Epoch: 70/100... Step: 71920... Loss: 0.000056... Val Loss: 0.000355\n",
      "Epoch: 70/100... Step: 71925... Loss: 0.000078... Val Loss: 0.000403\n",
      "Epoch: 70/100... Step: 71930... Loss: 0.000073... Val Loss: 0.000449\n",
      "Epoch: 70/100... Step: 71935... Loss: 0.000099... Val Loss: 0.000329\n",
      "Epoch: 70/100... Step: 71940... Loss: 0.000069... Val Loss: 0.000318\n",
      "Epoch: 70/100... Step: 71945... Loss: 0.000180... Val Loss: 0.000336\n",
      "Epoch: 70/100... Step: 71950... Loss: 0.000152... Val Loss: 0.000455\n",
      "Epoch: 70/100... Step: 71955... Loss: 0.000083... Val Loss: 0.000312\n",
      "Epoch: 70/100... Step: 71960... Loss: 0.000087... Val Loss: 0.000337\n",
      "Epoch: 70/100... Step: 71965... Loss: 0.000081... Val Loss: 0.000371\n",
      "Epoch: 70/100... Step: 71970... Loss: 0.000107... Val Loss: 0.000417\n",
      "Epoch: 70/100... Step: 71975... Loss: 0.000089... Val Loss: 0.000400\n",
      "Epoch: 70/100... Step: 71980... Loss: 0.000066... Val Loss: 0.000408\n",
      "Epoch: 70/100... Step: 71985... Loss: 0.000092... Val Loss: 0.000438\n",
      "Epoch: 70/100... Step: 71990... Loss: 0.000075... Val Loss: 0.000530\n",
      "Epoch: 70/100... Step: 71995... Loss: 0.000053... Val Loss: 0.000592\n",
      "Epoch: 70/100... Step: 72000... Loss: 0.000080... Val Loss: 0.000405\n",
      "Epoch: 70/100... Step: 72005... Loss: 0.000068... Val Loss: 0.000345\n",
      "Epoch: 70/100... Step: 72010... Loss: 0.000128... Val Loss: 0.000454\n",
      "Epoch: 70/100... Step: 72015... Loss: 0.000100... Val Loss: 0.000780\n",
      "Epoch: 70/100... Step: 72020... Loss: 0.000072... Val Loss: 0.000738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70/100... Step: 72025... Loss: 0.000055... Val Loss: 0.000683\n",
      "Epoch: 70/100... Step: 72030... Loss: 0.000076... Val Loss: 0.000659\n",
      "Epoch: 70/100... Step: 72035... Loss: 0.000049... Val Loss: 0.000625\n",
      "Epoch: 70/100... Step: 72040... Loss: 0.000103... Val Loss: 0.000621\n",
      "Epoch: 70/100... Step: 72045... Loss: 0.000059... Val Loss: 0.000614\n",
      "Epoch: 70/100... Step: 72050... Loss: 0.000048... Val Loss: 0.000610\n",
      "Epoch: 70/100... Step: 72055... Loss: 0.000047... Val Loss: 0.000593\n",
      "Epoch: 70/100... Step: 72060... Loss: 0.000028... Val Loss: 0.000576\n",
      "Epoch: 70/100... Step: 72065... Loss: 0.000053... Val Loss: 0.000702\n",
      "Epoch: 70/100... Step: 72070... Loss: 0.000044... Val Loss: 0.000738\n",
      "Epoch: 70/100... Step: 72075... Loss: 0.000036... Val Loss: 0.000787\n",
      "Epoch: 70/100... Step: 72080... Loss: 0.000052... Val Loss: 0.000777\n",
      "Epoch: 70/100... Step: 72085... Loss: 0.000058... Val Loss: 0.000886\n",
      "Epoch: 70/100... Step: 72090... Loss: 0.000101... Val Loss: 0.000811\n",
      "Epoch: 70/100... Step: 72095... Loss: 0.000072... Val Loss: 0.000672\n",
      "Epoch: 70/100... Step: 72100... Loss: 0.000082... Val Loss: 0.000701\n",
      "Epoch: 70/100... Step: 72105... Loss: 0.000050... Val Loss: 0.000671\n",
      "Epoch: 70/100... Step: 72110... Loss: 0.000056... Val Loss: 0.000641\n",
      "Epoch: 70/100... Step: 72115... Loss: 0.000069... Val Loss: 0.000537\n",
      "Epoch: 70/100... Step: 72120... Loss: 0.000067... Val Loss: 0.000507\n",
      "Epoch: 70/100... Step: 72125... Loss: 0.000075... Val Loss: 0.000566\n",
      "Epoch: 70/100... Step: 72130... Loss: 0.000083... Val Loss: 0.000652\n",
      "Epoch: 70/100... Step: 72135... Loss: 0.000096... Val Loss: 0.000691\n",
      "Epoch: 70/100... Step: 72140... Loss: 0.000041... Val Loss: 0.000872\n",
      "Epoch: 70/100... Step: 72145... Loss: 0.000072... Val Loss: 0.000913\n",
      "Epoch: 70/100... Step: 72150... Loss: 0.000048... Val Loss: 0.000747\n",
      "Epoch: 70/100... Step: 72155... Loss: 0.000046... Val Loss: 0.000768\n",
      "Epoch: 70/100... Step: 72160... Loss: 0.000086... Val Loss: 0.000847\n",
      "Epoch: 70/100... Step: 72165... Loss: 0.000103... Val Loss: 0.000944\n",
      "Epoch: 70/100... Step: 72170... Loss: 0.000046... Val Loss: 0.000961\n",
      "Epoch: 70/100... Step: 72175... Loss: 0.000111... Val Loss: 0.000912\n",
      "Epoch: 70/100... Step: 72180... Loss: 0.000086... Val Loss: 0.001035\n",
      "Epoch: 70/100... Step: 72185... Loss: 0.000067... Val Loss: 0.000944\n",
      "Epoch: 70/100... Step: 72190... Loss: 0.000090... Val Loss: 0.000947\n",
      "Epoch: 70/100... Step: 72195... Loss: 0.000057... Val Loss: 0.001182\n",
      "Epoch: 70/100... Step: 72200... Loss: 0.000054... Val Loss: 0.001105\n",
      "Epoch: 70/100... Step: 72205... Loss: 0.000158... Val Loss: 0.000998\n",
      "Epoch: 70/100... Step: 72210... Loss: 0.000108... Val Loss: 0.000928\n",
      "Epoch: 70/100... Step: 72215... Loss: 0.000115... Val Loss: 0.000860\n",
      "Epoch: 70/100... Step: 72220... Loss: 0.000081... Val Loss: 0.000964\n",
      "Epoch: 70/100... Step: 72225... Loss: 0.000060... Val Loss: 0.001017\n",
      "Epoch: 70/100... Step: 72230... Loss: 0.000086... Val Loss: 0.001087\n",
      "Epoch: 70/100... Step: 72235... Loss: 0.000048... Val Loss: 0.001032\n",
      "Epoch: 70/100... Step: 72240... Loss: 0.000084... Val Loss: 0.000942\n",
      "Epoch: 71/100... Step: 72245... Loss: 0.000229... Val Loss: 0.001661\n",
      "Epoch: 71/100... Step: 72250... Loss: 0.000120... Val Loss: 0.001460\n",
      "Epoch: 71/100... Step: 72255... Loss: 0.000087... Val Loss: 0.001578\n",
      "Epoch: 71/100... Step: 72260... Loss: 0.000140... Val Loss: 0.001809\n",
      "Epoch: 71/100... Step: 72265... Loss: 0.000098... Val Loss: 0.001607\n",
      "Epoch: 71/100... Step: 72270... Loss: 0.000089... Val Loss: 0.001932\n",
      "Epoch: 71/100... Step: 72275... Loss: 0.000094... Val Loss: 0.001885\n",
      "Epoch: 71/100... Step: 72280... Loss: 0.000110... Val Loss: 0.001538\n",
      "Epoch: 71/100... Step: 72285... Loss: 0.000094... Val Loss: 0.001866\n",
      "Epoch: 71/100... Step: 72290... Loss: 0.000126... Val Loss: 0.001610\n",
      "Epoch: 71/100... Step: 72295... Loss: 0.000119... Val Loss: 0.001791\n",
      "Epoch: 71/100... Step: 72300... Loss: 0.000102... Val Loss: 0.001627\n",
      "Epoch: 71/100... Step: 72305... Loss: 0.000054... Val Loss: 0.001550\n",
      "Epoch: 71/100... Step: 72310... Loss: 0.000220... Val Loss: 0.001366\n",
      "Epoch: 71/100... Step: 72315... Loss: 0.000127... Val Loss: 0.001091\n",
      "Epoch: 71/100... Step: 72320... Loss: 0.000083... Val Loss: 0.001296\n",
      "Epoch: 71/100... Step: 72325... Loss: 0.000055... Val Loss: 0.001350\n",
      "Epoch: 71/100... Step: 72330... Loss: 0.000042... Val Loss: 0.001265\n",
      "Epoch: 71/100... Step: 72335... Loss: 0.000128... Val Loss: 0.001257\n",
      "Epoch: 71/100... Step: 72340... Loss: 0.000097... Val Loss: 0.000925\n",
      "Epoch: 71/100... Step: 72345... Loss: 0.000118... Val Loss: 0.001129\n",
      "Epoch: 71/100... Step: 72350... Loss: 0.000094... Val Loss: 0.001159\n",
      "Epoch: 71/100... Step: 72355... Loss: 0.000048... Val Loss: 0.001175\n",
      "Epoch: 71/100... Step: 72360... Loss: 0.000065... Val Loss: 0.001178\n",
      "Epoch: 71/100... Step: 72365... Loss: 0.000071... Val Loss: 0.001244\n",
      "Epoch: 71/100... Step: 72370... Loss: 0.000072... Val Loss: 0.001453\n",
      "Epoch: 71/100... Step: 72375... Loss: 0.000048... Val Loss: 0.001329\n",
      "Epoch: 71/100... Step: 72380... Loss: 0.000086... Val Loss: 0.001414\n",
      "Epoch: 71/100... Step: 72385... Loss: 0.000092... Val Loss: 0.001816\n",
      "Epoch: 71/100... Step: 72390... Loss: 0.000083... Val Loss: 0.001672\n",
      "Epoch: 71/100... Step: 72395... Loss: 0.000099... Val Loss: 0.001618\n",
      "Epoch: 71/100... Step: 72400... Loss: 0.000068... Val Loss: 0.001441\n",
      "Epoch: 71/100... Step: 72405... Loss: 0.000073... Val Loss: 0.001389\n",
      "Epoch: 71/100... Step: 72410... Loss: 0.000047... Val Loss: 0.001379\n",
      "Epoch: 71/100... Step: 72415... Loss: 0.000052... Val Loss: 0.001381\n",
      "Epoch: 71/100... Step: 72420... Loss: 0.000045... Val Loss: 0.001372\n",
      "Epoch: 71/100... Step: 72425... Loss: 0.000088... Val Loss: 0.001269\n",
      "Epoch: 71/100... Step: 72430... Loss: 0.000123... Val Loss: 0.001050\n",
      "Epoch: 71/100... Step: 72435... Loss: 0.000077... Val Loss: 0.001072\n",
      "Epoch: 71/100... Step: 72440... Loss: 0.000053... Val Loss: 0.001059\n",
      "Epoch: 71/100... Step: 72445... Loss: 0.000126... Val Loss: 0.001308\n",
      "Epoch: 71/100... Step: 72450... Loss: 0.000045... Val Loss: 0.001384\n",
      "Epoch: 71/100... Step: 72455... Loss: 0.000097... Val Loss: 0.001255\n",
      "Epoch: 71/100... Step: 72460... Loss: 0.000077... Val Loss: 0.001178\n",
      "Epoch: 71/100... Step: 72465... Loss: 0.000066... Val Loss: 0.001220\n",
      "Epoch: 71/100... Step: 72470... Loss: 0.000106... Val Loss: 0.001115\n",
      "Epoch: 71/100... Step: 72475... Loss: 0.000033... Val Loss: 0.001091\n",
      "Epoch: 71/100... Step: 72480... Loss: 0.000070... Val Loss: 0.001175\n",
      "Epoch: 71/100... Step: 72485... Loss: 0.000069... Val Loss: 0.001079\n",
      "Epoch: 71/100... Step: 72490... Loss: 0.000069... Val Loss: 0.001174\n",
      "Epoch: 71/100... Step: 72495... Loss: 0.000103... Val Loss: 0.001186\n",
      "Epoch: 71/100... Step: 72500... Loss: 0.000081... Val Loss: 0.000925\n",
      "Epoch: 71/100... Step: 72505... Loss: 0.000105... Val Loss: 0.000979\n",
      "Epoch: 71/100... Step: 72510... Loss: 0.000048... Val Loss: 0.000966\n",
      "Epoch: 71/100... Step: 72515... Loss: 0.000084... Val Loss: 0.000920\n",
      "Epoch: 71/100... Step: 72520... Loss: 0.000054... Val Loss: 0.000986\n",
      "Epoch: 71/100... Step: 72525... Loss: 0.000057... Val Loss: 0.000975\n",
      "Epoch: 71/100... Step: 72530... Loss: 0.000100... Val Loss: 0.000659\n",
      "Epoch: 71/100... Step: 72535... Loss: 0.000068... Val Loss: 0.000638\n",
      "Epoch: 71/100... Step: 72540... Loss: 0.000066... Val Loss: 0.000599\n",
      "Epoch: 71/100... Step: 72545... Loss: 0.000153... Val Loss: 0.000653\n",
      "Epoch: 71/100... Step: 72550... Loss: 0.000150... Val Loss: 0.000595\n",
      "Epoch: 71/100... Step: 72555... Loss: 0.000029... Val Loss: 0.000661\n",
      "Epoch: 71/100... Step: 72560... Loss: 0.000045... Val Loss: 0.000645\n",
      "Epoch: 71/100... Step: 72565... Loss: 0.000103... Val Loss: 0.000523\n",
      "Epoch: 71/100... Step: 72570... Loss: 0.000048... Val Loss: 0.000434\n",
      "Epoch: 71/100... Step: 72575... Loss: 0.000044... Val Loss: 0.000494\n",
      "Epoch: 71/100... Step: 72580... Loss: 0.000130... Val Loss: 0.000657\n",
      "Epoch: 71/100... Step: 72585... Loss: 0.000188... Val Loss: 0.000481\n",
      "Epoch: 71/100... Step: 72590... Loss: 0.000097... Val Loss: 0.000338\n",
      "Epoch: 71/100... Step: 72595... Loss: 0.000182... Val Loss: 0.000565\n",
      "Epoch: 71/100... Step: 72600... Loss: 0.000072... Val Loss: 0.000718\n",
      "Epoch: 71/100... Step: 72605... Loss: 0.000099... Val Loss: 0.000968\n",
      "Epoch: 71/100... Step: 72610... Loss: 0.000076... Val Loss: 0.000982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71/100... Step: 72615... Loss: 0.000035... Val Loss: 0.001019\n",
      "Epoch: 71/100... Step: 72620... Loss: 0.000100... Val Loss: 0.001085\n",
      "Epoch: 71/100... Step: 72625... Loss: 0.000075... Val Loss: 0.001008\n",
      "Epoch: 71/100... Step: 72630... Loss: 0.000046... Val Loss: 0.000999\n",
      "Epoch: 71/100... Step: 72635... Loss: 0.000053... Val Loss: 0.001092\n",
      "Epoch: 71/100... Step: 72640... Loss: 0.000038... Val Loss: 0.001144\n",
      "Epoch: 71/100... Step: 72645... Loss: 0.000305... Val Loss: 0.001032\n",
      "Epoch: 71/100... Step: 72650... Loss: 0.000231... Val Loss: 0.000362\n",
      "Epoch: 71/100... Step: 72655... Loss: 0.000705... Val Loss: 0.000303\n",
      "Epoch: 71/100... Step: 72660... Loss: 0.000318... Val Loss: 0.001770\n",
      "Epoch: 71/100... Step: 72665... Loss: 0.000276... Val Loss: 0.000647\n",
      "Epoch: 71/100... Step: 72670... Loss: 0.000274... Val Loss: 0.001328\n",
      "Epoch: 71/100... Step: 72675... Loss: 0.000189... Val Loss: 0.001134\n",
      "Epoch: 71/100... Step: 72680... Loss: 0.000071... Val Loss: 0.001073\n",
      "Epoch: 71/100... Step: 72685... Loss: 0.000136... Val Loss: 0.001129\n",
      "Epoch: 71/100... Step: 72690... Loss: 0.000151... Val Loss: 0.001246\n",
      "Epoch: 71/100... Step: 72695... Loss: 0.000175... Val Loss: 0.001449\n",
      "Epoch: 71/100... Step: 72700... Loss: 0.000150... Val Loss: 0.001426\n",
      "Epoch: 71/100... Step: 72705... Loss: 0.000101... Val Loss: 0.001512\n",
      "Epoch: 71/100... Step: 72710... Loss: 0.000140... Val Loss: 0.001075\n",
      "Epoch: 71/100... Step: 72715... Loss: 0.000177... Val Loss: 0.001076\n",
      "Epoch: 71/100... Step: 72720... Loss: 0.000119... Val Loss: 0.001210\n",
      "Epoch: 71/100... Step: 72725... Loss: 0.000177... Val Loss: 0.001164\n",
      "Epoch: 71/100... Step: 72730... Loss: 0.000180... Val Loss: 0.000746\n",
      "Epoch: 71/100... Step: 72735... Loss: 0.000210... Val Loss: 0.000918\n",
      "Epoch: 71/100... Step: 72740... Loss: 0.000059... Val Loss: 0.000696\n",
      "Epoch: 71/100... Step: 72745... Loss: 0.000316... Val Loss: 0.000520\n",
      "Epoch: 71/100... Step: 72750... Loss: 0.000204... Val Loss: 0.000349\n",
      "Epoch: 71/100... Step: 72755... Loss: 0.000120... Val Loss: 0.000332\n",
      "Epoch: 71/100... Step: 72760... Loss: 0.000160... Val Loss: 0.000563\n",
      "Epoch: 71/100... Step: 72765... Loss: 0.000115... Val Loss: 0.000728\n",
      "Epoch: 71/100... Step: 72770... Loss: 0.000121... Val Loss: 0.000728\n",
      "Epoch: 71/100... Step: 72775... Loss: 0.000094... Val Loss: 0.000503\n",
      "Epoch: 71/100... Step: 72780... Loss: 0.000073... Val Loss: 0.000578\n",
      "Epoch: 71/100... Step: 72785... Loss: 0.000128... Val Loss: 0.000930\n",
      "Epoch: 71/100... Step: 72790... Loss: 0.000125... Val Loss: 0.000893\n",
      "Epoch: 71/100... Step: 72795... Loss: 0.000110... Val Loss: 0.000818\n",
      "Epoch: 71/100... Step: 72800... Loss: 0.000064... Val Loss: 0.000710\n",
      "Epoch: 71/100... Step: 72805... Loss: 0.000039... Val Loss: 0.000734\n",
      "Epoch: 71/100... Step: 72810... Loss: 0.000156... Val Loss: 0.000847\n",
      "Epoch: 71/100... Step: 72815... Loss: 0.000277... Val Loss: 0.000723\n",
      "Epoch: 71/100... Step: 72820... Loss: 0.000107... Val Loss: 0.000870\n",
      "Epoch: 71/100... Step: 72825... Loss: 0.000051... Val Loss: 0.000969\n",
      "Epoch: 71/100... Step: 72830... Loss: 0.000271... Val Loss: 0.000762\n",
      "Epoch: 71/100... Step: 72835... Loss: 0.000111... Val Loss: 0.000893\n",
      "Epoch: 71/100... Step: 72840... Loss: 0.000182... Val Loss: 0.001030\n",
      "Epoch: 71/100... Step: 72845... Loss: 0.000187... Val Loss: 0.000944\n",
      "Epoch: 71/100... Step: 72850... Loss: 0.000075... Val Loss: 0.000809\n",
      "Epoch: 71/100... Step: 72855... Loss: 0.000122... Val Loss: 0.000761\n",
      "Epoch: 71/100... Step: 72860... Loss: 0.000112... Val Loss: 0.000942\n",
      "Epoch: 71/100... Step: 72865... Loss: 0.000114... Val Loss: 0.000981\n",
      "Epoch: 71/100... Step: 72870... Loss: 0.000089... Val Loss: 0.000914\n",
      "Epoch: 71/100... Step: 72875... Loss: 0.000097... Val Loss: 0.000862\n",
      "Epoch: 71/100... Step: 72880... Loss: 0.000106... Val Loss: 0.000746\n",
      "Epoch: 71/100... Step: 72885... Loss: 0.000044... Val Loss: 0.000718\n",
      "Epoch: 71/100... Step: 72890... Loss: 0.000189... Val Loss: 0.000775\n",
      "Epoch: 71/100... Step: 72895... Loss: 0.000089... Val Loss: 0.000839\n",
      "Epoch: 71/100... Step: 72900... Loss: 0.000106... Val Loss: 0.000787\n",
      "Epoch: 71/100... Step: 72905... Loss: 0.000163... Val Loss: 0.000666\n",
      "Epoch: 71/100... Step: 72910... Loss: 0.000113... Val Loss: 0.000336\n",
      "Epoch: 71/100... Step: 72915... Loss: 0.000064... Val Loss: 0.000340\n",
      "Epoch: 71/100... Step: 72920... Loss: 0.000131... Val Loss: 0.000289\n",
      "Epoch: 71/100... Step: 72925... Loss: 0.000081... Val Loss: 0.000274\n",
      "Epoch: 71/100... Step: 72930... Loss: 0.000096... Val Loss: 0.000376\n",
      "Epoch: 71/100... Step: 72935... Loss: 0.000077... Val Loss: 0.000343\n",
      "Epoch: 71/100... Step: 72940... Loss: 0.000071... Val Loss: 0.000315\n",
      "Epoch: 71/100... Step: 72945... Loss: 0.000059... Val Loss: 0.000300\n",
      "Epoch: 71/100... Step: 72950... Loss: 0.000085... Val Loss: 0.000327\n",
      "Epoch: 71/100... Step: 72955... Loss: 0.000038... Val Loss: 0.000396\n",
      "Epoch: 71/100... Step: 72960... Loss: 0.000044... Val Loss: 0.000434\n",
      "Epoch: 71/100... Step: 72965... Loss: 0.000096... Val Loss: 0.000322\n",
      "Epoch: 71/100... Step: 72970... Loss: 0.000128... Val Loss: 0.000329\n",
      "Epoch: 71/100... Step: 72975... Loss: 0.000121... Val Loss: 0.000402\n",
      "Epoch: 71/100... Step: 72980... Loss: 0.000254... Val Loss: 0.000266\n",
      "Epoch: 71/100... Step: 72985... Loss: 0.000088... Val Loss: 0.000413\n",
      "Epoch: 71/100... Step: 72990... Loss: 0.000149... Val Loss: 0.000511\n",
      "Epoch: 71/100... Step: 72995... Loss: 0.000088... Val Loss: 0.000413\n",
      "Epoch: 71/100... Step: 73000... Loss: 0.000065... Val Loss: 0.000445\n",
      "Epoch: 71/100... Step: 73005... Loss: 0.000078... Val Loss: 0.000353\n",
      "Epoch: 71/100... Step: 73010... Loss: 0.000127... Val Loss: 0.000429\n",
      "Epoch: 71/100... Step: 73015... Loss: 0.000087... Val Loss: 0.000420\n",
      "Epoch: 71/100... Step: 73020... Loss: 0.000108... Val Loss: 0.000478\n",
      "Epoch: 71/100... Step: 73025... Loss: 0.000060... Val Loss: 0.000517\n",
      "Epoch: 71/100... Step: 73030... Loss: 0.000107... Val Loss: 0.000354\n",
      "Epoch: 71/100... Step: 73035... Loss: 0.000056... Val Loss: 0.000363\n",
      "Epoch: 71/100... Step: 73040... Loss: 0.000074... Val Loss: 0.000376\n",
      "Epoch: 71/100... Step: 73045... Loss: 0.000142... Val Loss: 0.000684\n",
      "Epoch: 71/100... Step: 73050... Loss: 0.000111... Val Loss: 0.000800\n",
      "Epoch: 71/100... Step: 73055... Loss: 0.000095... Val Loss: 0.000626\n",
      "Epoch: 71/100... Step: 73060... Loss: 0.000112... Val Loss: 0.000703\n",
      "Epoch: 71/100... Step: 73065... Loss: 0.000032... Val Loss: 0.000574\n",
      "Epoch: 71/100... Step: 73070... Loss: 0.000172... Val Loss: 0.000689\n",
      "Epoch: 71/100... Step: 73075... Loss: 0.000290... Val Loss: 0.000712\n",
      "Epoch: 71/100... Step: 73080... Loss: 0.000110... Val Loss: 0.000503\n",
      "Epoch: 71/100... Step: 73085... Loss: 0.000111... Val Loss: 0.000502\n",
      "Epoch: 71/100... Step: 73090... Loss: 0.000065... Val Loss: 0.000497\n",
      "Epoch: 71/100... Step: 73095... Loss: 0.000178... Val Loss: 0.000749\n",
      "Epoch: 71/100... Step: 73100... Loss: 0.000111... Val Loss: 0.000933\n",
      "Epoch: 71/100... Step: 73105... Loss: 0.000072... Val Loss: 0.000825\n",
      "Epoch: 71/100... Step: 73110... Loss: 0.000119... Val Loss: 0.000771\n",
      "Epoch: 71/100... Step: 73115... Loss: 0.000122... Val Loss: 0.000751\n",
      "Epoch: 71/100... Step: 73120... Loss: 0.000154... Val Loss: 0.000782\n",
      "Epoch: 71/100... Step: 73125... Loss: 0.000157... Val Loss: 0.000779\n",
      "Epoch: 71/100... Step: 73130... Loss: 0.000112... Val Loss: 0.000687\n",
      "Epoch: 71/100... Step: 73135... Loss: 0.000083... Val Loss: 0.000719\n",
      "Epoch: 71/100... Step: 73140... Loss: 0.000031... Val Loss: 0.000785\n",
      "Epoch: 71/100... Step: 73145... Loss: 0.000130... Val Loss: 0.000688\n",
      "Epoch: 71/100... Step: 73150... Loss: 0.000128... Val Loss: 0.000551\n",
      "Epoch: 71/100... Step: 73155... Loss: 0.000135... Val Loss: 0.000543\n",
      "Epoch: 71/100... Step: 73160... Loss: 0.000079... Val Loss: 0.000616\n",
      "Epoch: 71/100... Step: 73165... Loss: 0.000143... Val Loss: 0.000669\n",
      "Epoch: 71/100... Step: 73170... Loss: 0.000126... Val Loss: 0.000818\n",
      "Epoch: 71/100... Step: 73175... Loss: 0.000189... Val Loss: 0.000817\n",
      "Epoch: 71/100... Step: 73180... Loss: 0.000065... Val Loss: 0.000971\n",
      "Epoch: 71/100... Step: 73185... Loss: 0.000114... Val Loss: 0.000798\n",
      "Epoch: 71/100... Step: 73190... Loss: 0.000165... Val Loss: 0.000744\n",
      "Epoch: 71/100... Step: 73195... Loss: 0.000164... Val Loss: 0.001028\n",
      "Epoch: 71/100... Step: 73200... Loss: 0.000114... Val Loss: 0.000892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71/100... Step: 73205... Loss: 0.000044... Val Loss: 0.001020\n",
      "Epoch: 71/100... Step: 73210... Loss: 0.000164... Val Loss: 0.000908\n",
      "Epoch: 71/100... Step: 73215... Loss: 0.000072... Val Loss: 0.000861\n",
      "Epoch: 71/100... Step: 73220... Loss: 0.000127... Val Loss: 0.000995\n",
      "Epoch: 71/100... Step: 73225... Loss: 0.000102... Val Loss: 0.001092\n",
      "Epoch: 71/100... Step: 73230... Loss: 0.000049... Val Loss: 0.001096\n",
      "Epoch: 71/100... Step: 73235... Loss: 0.000064... Val Loss: 0.000991\n",
      "Epoch: 71/100... Step: 73240... Loss: 0.000134... Val Loss: 0.000980\n",
      "Epoch: 71/100... Step: 73245... Loss: 0.000061... Val Loss: 0.000931\n",
      "Epoch: 71/100... Step: 73250... Loss: 0.000077... Val Loss: 0.000828\n",
      "Epoch: 71/100... Step: 73255... Loss: 0.000128... Val Loss: 0.000916\n",
      "Epoch: 71/100... Step: 73260... Loss: 0.000055... Val Loss: 0.001065\n",
      "Epoch: 71/100... Step: 73265... Loss: 0.000080... Val Loss: 0.000962\n",
      "Epoch: 71/100... Step: 73270... Loss: 0.000078... Val Loss: 0.000924\n",
      "Epoch: 72/100... Step: 73275... Loss: 0.000137... Val Loss: 0.001538\n",
      "Epoch: 72/100... Step: 73280... Loss: 0.000137... Val Loss: 0.001283\n",
      "Epoch: 72/100... Step: 73285... Loss: 0.000156... Val Loss: 0.001654\n",
      "Epoch: 72/100... Step: 73290... Loss: 0.000263... Val Loss: 0.001851\n",
      "Epoch: 72/100... Step: 73295... Loss: 0.000222... Val Loss: 0.001636\n",
      "Epoch: 72/100... Step: 73300... Loss: 0.000156... Val Loss: 0.001879\n",
      "Epoch: 72/100... Step: 73305... Loss: 0.000144... Val Loss: 0.001936\n",
      "Epoch: 72/100... Step: 73310... Loss: 0.000160... Val Loss: 0.001580\n",
      "Epoch: 72/100... Step: 73315... Loss: 0.000149... Val Loss: 0.001725\n",
      "Epoch: 72/100... Step: 73320... Loss: 0.000053... Val Loss: 0.001744\n",
      "Epoch: 72/100... Step: 73325... Loss: 0.000059... Val Loss: 0.001678\n",
      "Epoch: 72/100... Step: 73330... Loss: 0.000098... Val Loss: 0.001641\n",
      "Epoch: 72/100... Step: 73335... Loss: 0.000068... Val Loss: 0.001542\n",
      "Epoch: 72/100... Step: 73340... Loss: 0.000128... Val Loss: 0.001465\n",
      "Epoch: 72/100... Step: 73345... Loss: 0.000178... Val Loss: 0.001055\n",
      "Epoch: 72/100... Step: 73350... Loss: 0.000126... Val Loss: 0.001361\n",
      "Epoch: 72/100... Step: 73355... Loss: 0.000096... Val Loss: 0.001445\n",
      "Epoch: 72/100... Step: 73360... Loss: 0.000055... Val Loss: 0.001371\n",
      "Epoch: 72/100... Step: 73365... Loss: 0.000114... Val Loss: 0.001252\n",
      "Epoch: 72/100... Step: 73370... Loss: 0.000111... Val Loss: 0.000950\n",
      "Epoch: 72/100... Step: 73375... Loss: 0.000100... Val Loss: 0.001150\n",
      "Epoch: 72/100... Step: 73380... Loss: 0.000048... Val Loss: 0.001184\n",
      "Epoch: 72/100... Step: 73385... Loss: 0.000062... Val Loss: 0.001174\n",
      "Epoch: 72/100... Step: 73390... Loss: 0.000045... Val Loss: 0.001103\n",
      "Epoch: 72/100... Step: 73395... Loss: 0.000097... Val Loss: 0.001191\n",
      "Epoch: 72/100... Step: 73400... Loss: 0.000138... Val Loss: 0.001473\n",
      "Epoch: 72/100... Step: 73405... Loss: 0.000099... Val Loss: 0.001416\n",
      "Epoch: 72/100... Step: 73410... Loss: 0.000113... Val Loss: 0.001382\n",
      "Epoch: 72/100... Step: 73415... Loss: 0.000121... Val Loss: 0.001552\n",
      "Epoch: 72/100... Step: 73420... Loss: 0.000094... Val Loss: 0.001725\n",
      "Epoch: 72/100... Step: 73425... Loss: 0.000081... Val Loss: 0.001568\n",
      "Epoch: 72/100... Step: 73430... Loss: 0.000059... Val Loss: 0.001442\n",
      "Epoch: 72/100... Step: 73435... Loss: 0.000087... Val Loss: 0.001384\n",
      "Epoch: 72/100... Step: 73440... Loss: 0.000047... Val Loss: 0.001326\n",
      "Epoch: 72/100... Step: 73445... Loss: 0.000102... Val Loss: 0.001365\n",
      "Epoch: 72/100... Step: 73450... Loss: 0.000141... Val Loss: 0.001375\n",
      "Epoch: 72/100... Step: 73455... Loss: 0.000075... Val Loss: 0.001327\n",
      "Epoch: 72/100... Step: 73460... Loss: 0.000142... Val Loss: 0.001059\n",
      "Epoch: 72/100... Step: 73465... Loss: 0.000060... Val Loss: 0.001184\n",
      "Epoch: 72/100... Step: 73470... Loss: 0.000143... Val Loss: 0.001170\n",
      "Epoch: 72/100... Step: 73475... Loss: 0.000077... Val Loss: 0.001194\n",
      "Epoch: 72/100... Step: 73480... Loss: 0.000072... Val Loss: 0.001260\n",
      "Epoch: 72/100... Step: 73485... Loss: 0.000073... Val Loss: 0.001325\n",
      "Epoch: 72/100... Step: 73490... Loss: 0.000043... Val Loss: 0.001273\n",
      "Epoch: 72/100... Step: 73495... Loss: 0.000059... Val Loss: 0.001185\n",
      "Epoch: 72/100... Step: 73500... Loss: 0.000139... Val Loss: 0.001144\n",
      "Epoch: 72/100... Step: 73505... Loss: 0.000090... Val Loss: 0.001057\n",
      "Epoch: 72/100... Step: 73510... Loss: 0.000112... Val Loss: 0.001167\n",
      "Epoch: 72/100... Step: 73515... Loss: 0.000073... Val Loss: 0.001138\n",
      "Epoch: 72/100... Step: 73520... Loss: 0.000113... Val Loss: 0.001066\n",
      "Epoch: 72/100... Step: 73525... Loss: 0.000033... Val Loss: 0.001189\n",
      "Epoch: 72/100... Step: 73530... Loss: 0.000174... Val Loss: 0.001134\n",
      "Epoch: 72/100... Step: 73535... Loss: 0.000119... Val Loss: 0.000828\n",
      "Epoch: 72/100... Step: 73540... Loss: 0.000069... Val Loss: 0.000839\n",
      "Epoch: 72/100... Step: 73545... Loss: 0.000059... Val Loss: 0.000852\n",
      "Epoch: 72/100... Step: 73550... Loss: 0.000043... Val Loss: 0.000905\n",
      "Epoch: 72/100... Step: 73555... Loss: 0.000032... Val Loss: 0.000968\n",
      "Epoch: 72/100... Step: 73560... Loss: 0.000126... Val Loss: 0.000796\n",
      "Epoch: 72/100... Step: 73565... Loss: 0.000104... Val Loss: 0.000612\n",
      "Epoch: 72/100... Step: 73570... Loss: 0.000092... Val Loss: 0.000531\n",
      "Epoch: 72/100... Step: 73575... Loss: 0.000140... Val Loss: 0.000600\n",
      "Epoch: 72/100... Step: 73580... Loss: 0.000152... Val Loss: 0.000773\n",
      "Epoch: 72/100... Step: 73585... Loss: 0.000108... Val Loss: 0.000520\n",
      "Epoch: 72/100... Step: 73590... Loss: 0.000136... Val Loss: 0.000549\n",
      "Epoch: 72/100... Step: 73595... Loss: 0.000094... Val Loss: 0.000595\n",
      "Epoch: 72/100... Step: 73600... Loss: 0.000115... Val Loss: 0.000459\n",
      "Epoch: 72/100... Step: 73605... Loss: 0.000078... Val Loss: 0.000472\n",
      "Epoch: 72/100... Step: 73610... Loss: 0.000098... Val Loss: 0.000481\n",
      "Epoch: 72/100... Step: 73615... Loss: 0.000188... Val Loss: 0.000742\n",
      "Epoch: 72/100... Step: 73620... Loss: 0.000104... Val Loss: 0.000294\n",
      "Epoch: 72/100... Step: 73625... Loss: 0.000184... Val Loss: 0.000419\n",
      "Epoch: 72/100... Step: 73630... Loss: 0.000106... Val Loss: 0.000779\n",
      "Epoch: 72/100... Step: 73635... Loss: 0.000127... Val Loss: 0.000761\n",
      "Epoch: 72/100... Step: 73640... Loss: 0.000106... Val Loss: 0.001056\n",
      "Epoch: 72/100... Step: 73645... Loss: 0.000053... Val Loss: 0.001040\n",
      "Epoch: 72/100... Step: 73650... Loss: 0.000057... Val Loss: 0.001060\n",
      "Epoch: 72/100... Step: 73655... Loss: 0.000084... Val Loss: 0.001049\n",
      "Epoch: 72/100... Step: 73660... Loss: 0.000054... Val Loss: 0.000969\n",
      "Epoch: 72/100... Step: 73665... Loss: 0.000071... Val Loss: 0.000944\n",
      "Epoch: 72/100... Step: 73670... Loss: 0.000087... Val Loss: 0.000984\n",
      "Epoch: 72/100... Step: 73675... Loss: 0.000191... Val Loss: 0.001084\n",
      "Epoch: 72/100... Step: 73680... Loss: 0.000287... Val Loss: 0.000544\n",
      "Epoch: 72/100... Step: 73685... Loss: 0.000577... Val Loss: 0.000282\n",
      "Epoch: 72/100... Step: 73690... Loss: 0.000961... Val Loss: 0.000321\n",
      "Epoch: 72/100... Step: 73695... Loss: 0.000695... Val Loss: 0.001597\n",
      "Epoch: 72/100... Step: 73700... Loss: 0.000359... Val Loss: 0.000694\n",
      "Epoch: 72/100... Step: 73705... Loss: 0.000228... Val Loss: 0.001315\n",
      "Epoch: 72/100... Step: 73710... Loss: 0.000180... Val Loss: 0.001147\n",
      "Epoch: 72/100... Step: 73715... Loss: 0.000126... Val Loss: 0.001029\n",
      "Epoch: 72/100... Step: 73720... Loss: 0.000142... Val Loss: 0.001220\n",
      "Epoch: 72/100... Step: 73725... Loss: 0.000114... Val Loss: 0.001522\n",
      "Epoch: 72/100... Step: 73730... Loss: 0.000137... Val Loss: 0.001300\n",
      "Epoch: 72/100... Step: 73735... Loss: 0.000180... Val Loss: 0.001413\n",
      "Epoch: 72/100... Step: 73740... Loss: 0.000175... Val Loss: 0.001318\n",
      "Epoch: 72/100... Step: 73745... Loss: 0.000228... Val Loss: 0.001114\n",
      "Epoch: 72/100... Step: 73750... Loss: 0.000199... Val Loss: 0.001235\n",
      "Epoch: 72/100... Step: 73755... Loss: 0.000159... Val Loss: 0.001135\n",
      "Epoch: 72/100... Step: 73760... Loss: 0.000119... Val Loss: 0.000769\n",
      "Epoch: 72/100... Step: 73765... Loss: 0.000109... Val Loss: 0.001056\n",
      "Epoch: 72/100... Step: 73770... Loss: 0.000212... Val Loss: 0.000797\n",
      "Epoch: 72/100... Step: 73775... Loss: 0.000276... Val Loss: 0.000429\n",
      "Epoch: 72/100... Step: 73780... Loss: 0.000220... Val Loss: 0.000505\n",
      "Epoch: 72/100... Step: 73785... Loss: 0.000135... Val Loss: 0.000421\n",
      "Epoch: 72/100... Step: 73790... Loss: 0.000132... Val Loss: 0.000423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72/100... Step: 73795... Loss: 0.000113... Val Loss: 0.000533\n",
      "Epoch: 72/100... Step: 73800... Loss: 0.000097... Val Loss: 0.000668\n",
      "Epoch: 72/100... Step: 73805... Loss: 0.000202... Val Loss: 0.000689\n",
      "Epoch: 72/100... Step: 73810... Loss: 0.000102... Val Loss: 0.000595\n",
      "Epoch: 72/100... Step: 73815... Loss: 0.000121... Val Loss: 0.000822\n",
      "Epoch: 72/100... Step: 73820... Loss: 0.000097... Val Loss: 0.000836\n",
      "Epoch: 72/100... Step: 73825... Loss: 0.000158... Val Loss: 0.000849\n",
      "Epoch: 72/100... Step: 73830... Loss: 0.000162... Val Loss: 0.000858\n",
      "Epoch: 72/100... Step: 73835... Loss: 0.000060... Val Loss: 0.000818\n",
      "Epoch: 72/100... Step: 73840... Loss: 0.000137... Val Loss: 0.000761\n",
      "Epoch: 72/100... Step: 73845... Loss: 0.000072... Val Loss: 0.000917\n",
      "Epoch: 72/100... Step: 73850... Loss: 0.000132... Val Loss: 0.000845\n",
      "Epoch: 72/100... Step: 73855... Loss: 0.000081... Val Loss: 0.000822\n",
      "Epoch: 72/100... Step: 73860... Loss: 0.000054... Val Loss: 0.000856\n",
      "Epoch: 72/100... Step: 73865... Loss: 0.000142... Val Loss: 0.000886\n",
      "Epoch: 72/100... Step: 73870... Loss: 0.000075... Val Loss: 0.000962\n",
      "Epoch: 72/100... Step: 73875... Loss: 0.000153... Val Loss: 0.000958\n",
      "Epoch: 72/100... Step: 73880... Loss: 0.000095... Val Loss: 0.000953\n",
      "Epoch: 72/100... Step: 73885... Loss: 0.000145... Val Loss: 0.000908\n",
      "Epoch: 72/100... Step: 73890... Loss: 0.000046... Val Loss: 0.000715\n",
      "Epoch: 72/100... Step: 73895... Loss: 0.000091... Val Loss: 0.000838\n",
      "Epoch: 72/100... Step: 73900... Loss: 0.000110... Val Loss: 0.000961\n",
      "Epoch: 72/100... Step: 73905... Loss: 0.000112... Val Loss: 0.000900\n",
      "Epoch: 72/100... Step: 73910... Loss: 0.000107... Val Loss: 0.000798\n",
      "Epoch: 72/100... Step: 73915... Loss: 0.000131... Val Loss: 0.000687\n",
      "Epoch: 72/100... Step: 73920... Loss: 0.000124... Val Loss: 0.000556\n",
      "Epoch: 72/100... Step: 73925... Loss: 0.000078... Val Loss: 0.001021\n",
      "Epoch: 72/100... Step: 73930... Loss: 0.000144... Val Loss: 0.000755\n",
      "Epoch: 72/100... Step: 73935... Loss: 0.000115... Val Loss: 0.000664\n",
      "Epoch: 72/100... Step: 73940... Loss: 0.000138... Val Loss: 0.000465\n",
      "Epoch: 72/100... Step: 73945... Loss: 0.000053... Val Loss: 0.000342\n",
      "Epoch: 72/100... Step: 73950... Loss: 0.000081... Val Loss: 0.000354\n",
      "Epoch: 72/100... Step: 73955... Loss: 0.000111... Val Loss: 0.000268\n",
      "Epoch: 72/100... Step: 73960... Loss: 0.000140... Val Loss: 0.000320\n",
      "Epoch: 72/100... Step: 73965... Loss: 0.000077... Val Loss: 0.000332\n",
      "Epoch: 72/100... Step: 73970... Loss: 0.000151... Val Loss: 0.000346\n",
      "Epoch: 72/100... Step: 73975... Loss: 0.000070... Val Loss: 0.000283\n",
      "Epoch: 72/100... Step: 73980... Loss: 0.000052... Val Loss: 0.000337\n",
      "Epoch: 72/100... Step: 73985... Loss: 0.000084... Val Loss: 0.000395\n",
      "Epoch: 72/100... Step: 73990... Loss: 0.000073... Val Loss: 0.000448\n",
      "Epoch: 72/100... Step: 73995... Loss: 0.000055... Val Loss: 0.000409\n",
      "Epoch: 72/100... Step: 74000... Loss: 0.000051... Val Loss: 0.000368\n",
      "Epoch: 72/100... Step: 74005... Loss: 0.000130... Val Loss: 0.000384\n",
      "Epoch: 72/100... Step: 74010... Loss: 0.000179... Val Loss: 0.000266\n",
      "Epoch: 72/100... Step: 74015... Loss: 0.000066... Val Loss: 0.000304\n",
      "Epoch: 72/100... Step: 74020... Loss: 0.000065... Val Loss: 0.000383\n",
      "Epoch: 72/100... Step: 74025... Loss: 0.000085... Val Loss: 0.000395\n",
      "Epoch: 72/100... Step: 74030... Loss: 0.000084... Val Loss: 0.000407\n",
      "Epoch: 72/100... Step: 74035... Loss: 0.000081... Val Loss: 0.000341\n",
      "Epoch: 72/100... Step: 74040... Loss: 0.000067... Val Loss: 0.000428\n",
      "Epoch: 72/100... Step: 74045... Loss: 0.000047... Val Loss: 0.000407\n",
      "Epoch: 72/100... Step: 74050... Loss: 0.000111... Val Loss: 0.000510\n",
      "Epoch: 72/100... Step: 74055... Loss: 0.000074... Val Loss: 0.000572\n",
      "Epoch: 72/100... Step: 74060... Loss: 0.000119... Val Loss: 0.000518\n",
      "Epoch: 72/100... Step: 74065... Loss: 0.000074... Val Loss: 0.000428\n",
      "Epoch: 72/100... Step: 74070... Loss: 0.000048... Val Loss: 0.000399\n",
      "Epoch: 72/100... Step: 74075... Loss: 0.000137... Val Loss: 0.000455\n",
      "Epoch: 72/100... Step: 74080... Loss: 0.000115... Val Loss: 0.000771\n",
      "Epoch: 72/100... Step: 74085... Loss: 0.000126... Val Loss: 0.000752\n",
      "Epoch: 72/100... Step: 74090... Loss: 0.000039... Val Loss: 0.000684\n",
      "Epoch: 72/100... Step: 74095... Loss: 0.000040... Val Loss: 0.000639\n",
      "Epoch: 72/100... Step: 74100... Loss: 0.000052... Val Loss: 0.000667\n",
      "Epoch: 72/100... Step: 74105... Loss: 0.000089... Val Loss: 0.000700\n",
      "Epoch: 72/100... Step: 74110... Loss: 0.000055... Val Loss: 0.000628\n",
      "Epoch: 72/100... Step: 74115... Loss: 0.000080... Val Loss: 0.000628\n",
      "Epoch: 72/100... Step: 74120... Loss: 0.000077... Val Loss: 0.000676\n",
      "Epoch: 72/100... Step: 74125... Loss: 0.000066... Val Loss: 0.000705\n",
      "Epoch: 72/100... Step: 74130... Loss: 0.000059... Val Loss: 0.000758\n",
      "Epoch: 72/100... Step: 74135... Loss: 0.000059... Val Loss: 0.000768\n",
      "Epoch: 72/100... Step: 74140... Loss: 0.000044... Val Loss: 0.000718\n",
      "Epoch: 72/100... Step: 74145... Loss: 0.000077... Val Loss: 0.000774\n",
      "Epoch: 72/100... Step: 74150... Loss: 0.000114... Val Loss: 0.000720\n",
      "Epoch: 72/100... Step: 74155... Loss: 0.000086... Val Loss: 0.000784\n",
      "Epoch: 72/100... Step: 74160... Loss: 0.000096... Val Loss: 0.000634\n",
      "Epoch: 72/100... Step: 74165... Loss: 0.000102... Val Loss: 0.000721\n",
      "Epoch: 72/100... Step: 74170... Loss: 0.000055... Val Loss: 0.000728\n",
      "Epoch: 72/100... Step: 74175... Loss: 0.000056... Val Loss: 0.000702\n",
      "Epoch: 72/100... Step: 74180... Loss: 0.000045... Val Loss: 0.000558\n",
      "Epoch: 72/100... Step: 74185... Loss: 0.000062... Val Loss: 0.000597\n",
      "Epoch: 72/100... Step: 74190... Loss: 0.000048... Val Loss: 0.000612\n",
      "Epoch: 72/100... Step: 74195... Loss: 0.000126... Val Loss: 0.000642\n",
      "Epoch: 72/100... Step: 74200... Loss: 0.000037... Val Loss: 0.000773\n",
      "Epoch: 72/100... Step: 74205... Loss: 0.000048... Val Loss: 0.000886\n",
      "Epoch: 72/100... Step: 74210... Loss: 0.000095... Val Loss: 0.000766\n",
      "Epoch: 72/100... Step: 74215... Loss: 0.000064... Val Loss: 0.000963\n",
      "Epoch: 72/100... Step: 74220... Loss: 0.000062... Val Loss: 0.000809\n",
      "Epoch: 72/100... Step: 74225... Loss: 0.000129... Val Loss: 0.000857\n",
      "Epoch: 72/100... Step: 74230... Loss: 0.000061... Val Loss: 0.001008\n",
      "Epoch: 72/100... Step: 74235... Loss: 0.000064... Val Loss: 0.000943\n",
      "Epoch: 72/100... Step: 74240... Loss: 0.000059... Val Loss: 0.000944\n",
      "Epoch: 72/100... Step: 74245... Loss: 0.000128... Val Loss: 0.000920\n",
      "Epoch: 72/100... Step: 74250... Loss: 0.000047... Val Loss: 0.000854\n",
      "Epoch: 72/100... Step: 74255... Loss: 0.000097... Val Loss: 0.001002\n",
      "Epoch: 72/100... Step: 74260... Loss: 0.000036... Val Loss: 0.001052\n",
      "Epoch: 72/100... Step: 74265... Loss: 0.000037... Val Loss: 0.001065\n",
      "Epoch: 72/100... Step: 74270... Loss: 0.000064... Val Loss: 0.001030\n",
      "Epoch: 72/100... Step: 74275... Loss: 0.000066... Val Loss: 0.001002\n",
      "Epoch: 72/100... Step: 74280... Loss: 0.000068... Val Loss: 0.000860\n",
      "Epoch: 72/100... Step: 74285... Loss: 0.000102... Val Loss: 0.000895\n",
      "Epoch: 72/100... Step: 74290... Loss: 0.000065... Val Loss: 0.001000\n",
      "Epoch: 72/100... Step: 74295... Loss: 0.000074... Val Loss: 0.001102\n",
      "Epoch: 72/100... Step: 74300... Loss: 0.000046... Val Loss: 0.001008\n",
      "Epoch: 73/100... Step: 74305... Loss: 0.000340... Val Loss: 0.001006\n",
      "Epoch: 73/100... Step: 74310... Loss: 0.000252... Val Loss: 0.001639\n",
      "Epoch: 73/100... Step: 74315... Loss: 0.000194... Val Loss: 0.001397\n",
      "Epoch: 73/100... Step: 74320... Loss: 0.000165... Val Loss: 0.001747\n",
      "Epoch: 73/100... Step: 74325... Loss: 0.000178... Val Loss: 0.001834\n",
      "Epoch: 73/100... Step: 74330... Loss: 0.000170... Val Loss: 0.001558\n",
      "Epoch: 73/100... Step: 74335... Loss: 0.000090... Val Loss: 0.002015\n",
      "Epoch: 73/100... Step: 74340... Loss: 0.000161... Val Loss: 0.001629\n",
      "Epoch: 73/100... Step: 74345... Loss: 0.000124... Val Loss: 0.001643\n",
      "Epoch: 73/100... Step: 74350... Loss: 0.000075... Val Loss: 0.001848\n",
      "Epoch: 73/100... Step: 74355... Loss: 0.000058... Val Loss: 0.001734\n",
      "Epoch: 73/100... Step: 74360... Loss: 0.000073... Val Loss: 0.001706\n",
      "Epoch: 73/100... Step: 74365... Loss: 0.000124... Val Loss: 0.001664\n",
      "Epoch: 73/100... Step: 74370... Loss: 0.000050... Val Loss: 0.001593\n",
      "Epoch: 73/100... Step: 74375... Loss: 0.000219... Val Loss: 0.001275\n",
      "Epoch: 73/100... Step: 74380... Loss: 0.000149... Val Loss: 0.001146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73/100... Step: 74385... Loss: 0.000086... Val Loss: 0.001310\n",
      "Epoch: 73/100... Step: 74390... Loss: 0.000098... Val Loss: 0.001362\n",
      "Epoch: 73/100... Step: 74395... Loss: 0.000081... Val Loss: 0.001298\n",
      "Epoch: 73/100... Step: 74400... Loss: 0.000155... Val Loss: 0.001230\n",
      "Epoch: 73/100... Step: 74405... Loss: 0.000160... Val Loss: 0.000938\n",
      "Epoch: 73/100... Step: 74410... Loss: 0.000093... Val Loss: 0.001146\n",
      "Epoch: 73/100... Step: 74415... Loss: 0.000064... Val Loss: 0.001222\n",
      "Epoch: 73/100... Step: 74420... Loss: 0.000055... Val Loss: 0.001154\n",
      "Epoch: 73/100... Step: 74425... Loss: 0.000069... Val Loss: 0.001210\n",
      "Epoch: 73/100... Step: 74430... Loss: 0.000067... Val Loss: 0.001324\n",
      "Epoch: 73/100... Step: 74435... Loss: 0.000115... Val Loss: 0.001287\n",
      "Epoch: 73/100... Step: 74440... Loss: 0.000048... Val Loss: 0.001346\n",
      "Epoch: 73/100... Step: 74445... Loss: 0.000129... Val Loss: 0.001534\n",
      "Epoch: 73/100... Step: 74450... Loss: 0.000106... Val Loss: 0.001708\n",
      "Epoch: 73/100... Step: 74455... Loss: 0.000118... Val Loss: 0.001623\n",
      "Epoch: 73/100... Step: 74460... Loss: 0.000121... Val Loss: 0.001573\n",
      "Epoch: 73/100... Step: 74465... Loss: 0.000061... Val Loss: 0.001502\n",
      "Epoch: 73/100... Step: 74470... Loss: 0.000052... Val Loss: 0.001422\n",
      "Epoch: 73/100... Step: 74475... Loss: 0.000083... Val Loss: 0.001376\n",
      "Epoch: 73/100... Step: 74480... Loss: 0.000083... Val Loss: 0.001280\n",
      "Epoch: 73/100... Step: 74485... Loss: 0.000073... Val Loss: 0.001273\n",
      "Epoch: 73/100... Step: 74490... Loss: 0.000096... Val Loss: 0.001282\n",
      "Epoch: 73/100... Step: 74495... Loss: 0.000122... Val Loss: 0.001078\n",
      "Epoch: 73/100... Step: 74500... Loss: 0.000035... Val Loss: 0.001101\n",
      "Epoch: 73/100... Step: 74505... Loss: 0.000038... Val Loss: 0.001083\n",
      "Epoch: 73/100... Step: 74510... Loss: 0.000087... Val Loss: 0.001275\n",
      "Epoch: 73/100... Step: 74515... Loss: 0.000053... Val Loss: 0.001319\n",
      "Epoch: 73/100... Step: 74520... Loss: 0.000045... Val Loss: 0.001311\n",
      "Epoch: 73/100... Step: 74525... Loss: 0.000041... Val Loss: 0.001239\n",
      "Epoch: 73/100... Step: 74530... Loss: 0.000076... Val Loss: 0.001228\n",
      "Epoch: 73/100... Step: 74535... Loss: 0.000105... Val Loss: 0.001032\n",
      "Epoch: 73/100... Step: 74540... Loss: 0.000053... Val Loss: 0.001071\n",
      "Epoch: 73/100... Step: 74545... Loss: 0.000055... Val Loss: 0.001247\n",
      "Epoch: 73/100... Step: 74550... Loss: 0.000068... Val Loss: 0.001019\n",
      "Epoch: 73/100... Step: 74555... Loss: 0.000058... Val Loss: 0.001184\n",
      "Epoch: 73/100... Step: 74560... Loss: 0.000072... Val Loss: 0.001140\n",
      "Epoch: 73/100... Step: 74565... Loss: 0.000064... Val Loss: 0.000930\n",
      "Epoch: 73/100... Step: 74570... Loss: 0.000061... Val Loss: 0.000927\n",
      "Epoch: 73/100... Step: 74575... Loss: 0.000054... Val Loss: 0.000863\n",
      "Epoch: 73/100... Step: 74580... Loss: 0.000031... Val Loss: 0.000875\n",
      "Epoch: 73/100... Step: 74585... Loss: 0.000057... Val Loss: 0.000955\n",
      "Epoch: 73/100... Step: 74590... Loss: 0.000090... Val Loss: 0.000898\n",
      "Epoch: 73/100... Step: 74595... Loss: 0.000111... Val Loss: 0.000680\n",
      "Epoch: 73/100... Step: 74600... Loss: 0.000067... Val Loss: 0.000683\n",
      "Epoch: 73/100... Step: 74605... Loss: 0.000115... Val Loss: 0.000471\n",
      "Epoch: 73/100... Step: 74610... Loss: 0.000128... Val Loss: 0.000815\n",
      "Epoch: 73/100... Step: 74615... Loss: 0.000075... Val Loss: 0.000477\n",
      "Epoch: 73/100... Step: 74620... Loss: 0.000088... Val Loss: 0.000690\n",
      "Epoch: 73/100... Step: 74625... Loss: 0.000053... Val Loss: 0.000648\n",
      "Epoch: 73/100... Step: 74630... Loss: 0.000101... Val Loss: 0.000482\n",
      "Epoch: 73/100... Step: 74635... Loss: 0.000043... Val Loss: 0.000469\n",
      "Epoch: 73/100... Step: 74640... Loss: 0.000072... Val Loss: 0.000518\n",
      "Epoch: 73/100... Step: 74645... Loss: 0.000054... Val Loss: 0.000672\n",
      "Epoch: 73/100... Step: 74650... Loss: 0.000208... Val Loss: 0.000519\n",
      "Epoch: 73/100... Step: 74655... Loss: 0.000193... Val Loss: 0.000283\n",
      "Epoch: 73/100... Step: 74660... Loss: 0.000132... Val Loss: 0.000751\n",
      "Epoch: 73/100... Step: 74665... Loss: 0.000108... Val Loss: 0.000657\n",
      "Epoch: 73/100... Step: 74670... Loss: 0.000173... Val Loss: 0.001142\n",
      "Epoch: 73/100... Step: 74675... Loss: 0.000088... Val Loss: 0.000945\n",
      "Epoch: 73/100... Step: 74680... Loss: 0.000047... Val Loss: 0.000959\n",
      "Epoch: 73/100... Step: 74685... Loss: 0.000063... Val Loss: 0.001110\n",
      "Epoch: 73/100... Step: 74690... Loss: 0.000077... Val Loss: 0.001002\n",
      "Epoch: 73/100... Step: 74695... Loss: 0.000057... Val Loss: 0.000991\n",
      "Epoch: 73/100... Step: 74700... Loss: 0.000042... Val Loss: 0.001072\n",
      "Epoch: 73/100... Step: 74705... Loss: 0.000046... Val Loss: 0.001113\n",
      "Epoch: 73/100... Step: 74710... Loss: 0.000378... Val Loss: 0.000983\n",
      "Epoch: 73/100... Step: 74715... Loss: 0.000175... Val Loss: 0.000290\n",
      "Epoch: 73/100... Step: 74720... Loss: 0.000663... Val Loss: 0.000565\n",
      "Epoch: 73/100... Step: 74725... Loss: 0.000421... Val Loss: 0.001406\n",
      "Epoch: 73/100... Step: 74730... Loss: 0.000199... Val Loss: 0.000956\n",
      "Epoch: 73/100... Step: 74735... Loss: 0.000235... Val Loss: 0.001115\n",
      "Epoch: 73/100... Step: 74740... Loss: 0.000121... Val Loss: 0.001306\n",
      "Epoch: 73/100... Step: 74745... Loss: 0.000081... Val Loss: 0.000977\n",
      "Epoch: 73/100... Step: 74750... Loss: 0.000086... Val Loss: 0.001356\n",
      "Epoch: 73/100... Step: 74755... Loss: 0.000139... Val Loss: 0.001224\n",
      "Epoch: 73/100... Step: 74760... Loss: 0.000215... Val Loss: 0.001456\n",
      "Epoch: 73/100... Step: 74765... Loss: 0.000174... Val Loss: 0.001336\n",
      "Epoch: 73/100... Step: 74770... Loss: 0.000132... Val Loss: 0.001447\n",
      "Epoch: 73/100... Step: 74775... Loss: 0.000201... Val Loss: 0.001128\n",
      "Epoch: 73/100... Step: 74780... Loss: 0.000146... Val Loss: 0.001170\n",
      "Epoch: 73/100... Step: 74785... Loss: 0.000077... Val Loss: 0.001170\n",
      "Epoch: 73/100... Step: 74790... Loss: 0.000184... Val Loss: 0.001032\n",
      "Epoch: 73/100... Step: 74795... Loss: 0.000173... Val Loss: 0.000950\n",
      "Epoch: 73/100... Step: 74800... Loss: 0.000103... Val Loss: 0.000760\n",
      "Epoch: 73/100... Step: 74805... Loss: 0.000060... Val Loss: 0.000748\n",
      "Epoch: 73/100... Step: 74810... Loss: 0.000159... Val Loss: 0.000395\n",
      "Epoch: 73/100... Step: 74815... Loss: 0.000117... Val Loss: 0.000373\n",
      "Epoch: 73/100... Step: 74820... Loss: 0.000108... Val Loss: 0.000386\n",
      "Epoch: 73/100... Step: 74825... Loss: 0.000077... Val Loss: 0.000474\n",
      "Epoch: 73/100... Step: 74830... Loss: 0.000118... Val Loss: 0.000602\n",
      "Epoch: 73/100... Step: 74835... Loss: 0.000091... Val Loss: 0.000699\n",
      "Epoch: 73/100... Step: 74840... Loss: 0.000082... Val Loss: 0.000634\n",
      "Epoch: 73/100... Step: 74845... Loss: 0.000077... Val Loss: 0.000631\n",
      "Epoch: 73/100... Step: 74850... Loss: 0.000148... Val Loss: 0.000940\n",
      "Epoch: 73/100... Step: 74855... Loss: 0.000083... Val Loss: 0.000938\n",
      "Epoch: 73/100... Step: 74860... Loss: 0.000082... Val Loss: 0.000676\n",
      "Epoch: 73/100... Step: 74865... Loss: 0.000081... Val Loss: 0.000693\n",
      "Epoch: 73/100... Step: 74870... Loss: 0.000069... Val Loss: 0.000757\n",
      "Epoch: 73/100... Step: 74875... Loss: 0.000061... Val Loss: 0.000913\n",
      "Epoch: 73/100... Step: 74880... Loss: 0.000100... Val Loss: 0.000923\n",
      "Epoch: 73/100... Step: 74885... Loss: 0.000103... Val Loss: 0.000774\n",
      "Epoch: 73/100... Step: 74890... Loss: 0.000095... Val Loss: 0.000818\n",
      "Epoch: 73/100... Step: 74895... Loss: 0.000046... Val Loss: 0.000832\n",
      "Epoch: 73/100... Step: 74900... Loss: 0.000100... Val Loss: 0.000851\n",
      "Epoch: 73/100... Step: 74905... Loss: 0.000079... Val Loss: 0.001024\n",
      "Epoch: 73/100... Step: 74910... Loss: 0.000048... Val Loss: 0.001085\n",
      "Epoch: 73/100... Step: 74915... Loss: 0.000109... Val Loss: 0.000804\n",
      "Epoch: 73/100... Step: 74920... Loss: 0.000040... Val Loss: 0.000858\n",
      "Epoch: 73/100... Step: 74925... Loss: 0.000060... Val Loss: 0.000921\n",
      "Epoch: 73/100... Step: 74930... Loss: 0.000076... Val Loss: 0.000949\n",
      "Epoch: 73/100... Step: 74935... Loss: 0.000064... Val Loss: 0.000908\n",
      "Epoch: 73/100... Step: 74940... Loss: 0.000036... Val Loss: 0.000820\n",
      "Epoch: 73/100... Step: 74945... Loss: 0.000095... Val Loss: 0.000730\n",
      "Epoch: 73/100... Step: 74950... Loss: 0.000080... Val Loss: 0.000581\n",
      "Epoch: 73/100... Step: 74955... Loss: 0.000218... Val Loss: 0.000678\n",
      "Epoch: 73/100... Step: 74960... Loss: 0.000232... Val Loss: 0.001066\n",
      "Epoch: 73/100... Step: 74965... Loss: 0.000150... Val Loss: 0.000692\n",
      "Epoch: 73/100... Step: 74970... Loss: 0.000190... Val Loss: 0.000630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73/100... Step: 74975... Loss: 0.000150... Val Loss: 0.000321\n",
      "Epoch: 73/100... Step: 74980... Loss: 0.000051... Val Loss: 0.000303\n",
      "Epoch: 73/100... Step: 74985... Loss: 0.000179... Val Loss: 0.000350\n",
      "Epoch: 73/100... Step: 74990... Loss: 0.000188... Val Loss: 0.000271\n",
      "Epoch: 73/100... Step: 74995... Loss: 0.000120... Val Loss: 0.000379\n",
      "Epoch: 73/100... Step: 75000... Loss: 0.000070... Val Loss: 0.000359\n",
      "Epoch: 73/100... Step: 75005... Loss: 0.000110... Val Loss: 0.000272\n",
      "Epoch: 73/100... Step: 75010... Loss: 0.000068... Val Loss: 0.000298\n",
      "Epoch: 73/100... Step: 75015... Loss: 0.000050... Val Loss: 0.000348\n",
      "Epoch: 73/100... Step: 75020... Loss: 0.000040... Val Loss: 0.000413\n",
      "Epoch: 73/100... Step: 75025... Loss: 0.000056... Val Loss: 0.000423\n",
      "Epoch: 73/100... Step: 75030... Loss: 0.000063... Val Loss: 0.000398\n",
      "Epoch: 73/100... Step: 75035... Loss: 0.000097... Val Loss: 0.000471\n",
      "Epoch: 73/100... Step: 75040... Loss: 0.000142... Val Loss: 0.000295\n",
      "Epoch: 73/100... Step: 75045... Loss: 0.000223... Val Loss: 0.000296\n",
      "Epoch: 73/100... Step: 75050... Loss: 0.000041... Val Loss: 0.000277\n",
      "Epoch: 73/100... Step: 75055... Loss: 0.000040... Val Loss: 0.000393\n",
      "Epoch: 73/100... Step: 75060... Loss: 0.000048... Val Loss: 0.000420\n",
      "Epoch: 73/100... Step: 75065... Loss: 0.000087... Val Loss: 0.000331\n",
      "Epoch: 73/100... Step: 75070... Loss: 0.000073... Val Loss: 0.000313\n",
      "Epoch: 73/100... Step: 75075... Loss: 0.000110... Val Loss: 0.000462\n",
      "Epoch: 73/100... Step: 75080... Loss: 0.000084... Val Loss: 0.000479\n",
      "Epoch: 73/100... Step: 75085... Loss: 0.000117... Val Loss: 0.000506\n",
      "Epoch: 73/100... Step: 75090... Loss: 0.000082... Val Loss: 0.000489\n",
      "Epoch: 73/100... Step: 75095... Loss: 0.000136... Val Loss: 0.000500\n",
      "Epoch: 73/100... Step: 75100... Loss: 0.000084... Val Loss: 0.000407\n",
      "Epoch: 73/100... Step: 75105... Loss: 0.000114... Val Loss: 0.000403\n",
      "Epoch: 73/100... Step: 75110... Loss: 0.000111... Val Loss: 0.000711\n",
      "Epoch: 73/100... Step: 75115... Loss: 0.000053... Val Loss: 0.000707\n",
      "Epoch: 73/100... Step: 75120... Loss: 0.000052... Val Loss: 0.000669\n",
      "Epoch: 73/100... Step: 75125... Loss: 0.000062... Val Loss: 0.000617\n",
      "Epoch: 73/100... Step: 75130... Loss: 0.000063... Val Loss: 0.000624\n",
      "Epoch: 73/100... Step: 75135... Loss: 0.000054... Val Loss: 0.000647\n",
      "Epoch: 73/100... Step: 75140... Loss: 0.000046... Val Loss: 0.000623\n",
      "Epoch: 73/100... Step: 75145... Loss: 0.000082... Val Loss: 0.000612\n",
      "Epoch: 73/100... Step: 75150... Loss: 0.000087... Val Loss: 0.000631\n",
      "Epoch: 73/100... Step: 75155... Loss: 0.000120... Val Loss: 0.000580\n",
      "Epoch: 73/100... Step: 75160... Loss: 0.000085... Val Loss: 0.000632\n",
      "Epoch: 73/100... Step: 75165... Loss: 0.000076... Val Loss: 0.000792\n",
      "Epoch: 73/100... Step: 75170... Loss: 0.000067... Val Loss: 0.000884\n",
      "Epoch: 73/100... Step: 75175... Loss: 0.000109... Val Loss: 0.000642\n",
      "Epoch: 73/100... Step: 75180... Loss: 0.000084... Val Loss: 0.000702\n",
      "Epoch: 73/100... Step: 75185... Loss: 0.000169... Val Loss: 0.000847\n",
      "Epoch: 73/100... Step: 75190... Loss: 0.000078... Val Loss: 0.000633\n",
      "Epoch: 73/100... Step: 75195... Loss: 0.000071... Val Loss: 0.000716\n",
      "Epoch: 73/100... Step: 75200... Loss: 0.000077... Val Loss: 0.000731\n",
      "Epoch: 73/100... Step: 75205... Loss: 0.000093... Val Loss: 0.000739\n",
      "Epoch: 73/100... Step: 75210... Loss: 0.000086... Val Loss: 0.000617\n",
      "Epoch: 73/100... Step: 75215... Loss: 0.000106... Val Loss: 0.000585\n",
      "Epoch: 73/100... Step: 75220... Loss: 0.000067... Val Loss: 0.000587\n",
      "Epoch: 73/100... Step: 75225... Loss: 0.000068... Val Loss: 0.000623\n",
      "Epoch: 73/100... Step: 75230... Loss: 0.000081... Val Loss: 0.000731\n",
      "Epoch: 73/100... Step: 75235... Loss: 0.000074... Val Loss: 0.000766\n",
      "Epoch: 73/100... Step: 75240... Loss: 0.000044... Val Loss: 0.000837\n",
      "Epoch: 73/100... Step: 75245... Loss: 0.000058... Val Loss: 0.000997\n",
      "Epoch: 73/100... Step: 75250... Loss: 0.000083... Val Loss: 0.000727\n",
      "Epoch: 73/100... Step: 75255... Loss: 0.000089... Val Loss: 0.000880\n",
      "Epoch: 73/100... Step: 75260... Loss: 0.000036... Val Loss: 0.000972\n",
      "Epoch: 73/100... Step: 75265... Loss: 0.000083... Val Loss: 0.000983\n",
      "Epoch: 73/100... Step: 75270... Loss: 0.000026... Val Loss: 0.000985\n",
      "Epoch: 73/100... Step: 75275... Loss: 0.000063... Val Loss: 0.001008\n",
      "Epoch: 73/100... Step: 75280... Loss: 0.000073... Val Loss: 0.000996\n",
      "Epoch: 73/100... Step: 75285... Loss: 0.000067... Val Loss: 0.000906\n",
      "Epoch: 73/100... Step: 75290... Loss: 0.000120... Val Loss: 0.001048\n",
      "Epoch: 73/100... Step: 75295... Loss: 0.000076... Val Loss: 0.001089\n",
      "Epoch: 73/100... Step: 75300... Loss: 0.000080... Val Loss: 0.000972\n",
      "Epoch: 73/100... Step: 75305... Loss: 0.000088... Val Loss: 0.000965\n",
      "Epoch: 73/100... Step: 75310... Loss: 0.000096... Val Loss: 0.000999\n",
      "Epoch: 73/100... Step: 75315... Loss: 0.000111... Val Loss: 0.000852\n",
      "Epoch: 73/100... Step: 75320... Loss: 0.000042... Val Loss: 0.000927\n",
      "Epoch: 73/100... Step: 75325... Loss: 0.000089... Val Loss: 0.001067\n",
      "Epoch: 73/100... Step: 75330... Loss: 0.000053... Val Loss: 0.001046\n",
      "Epoch: 73/100... Step: 75335... Loss: 0.000065... Val Loss: 0.000992\n",
      "Epoch: 74/100... Step: 75340... Loss: 0.000131... Val Loss: 0.001511\n",
      "Epoch: 74/100... Step: 75345... Loss: 0.000098... Val Loss: 0.001376\n",
      "Epoch: 74/100... Step: 75350... Loss: 0.000124... Val Loss: 0.001706\n",
      "Epoch: 74/100... Step: 75355... Loss: 0.000165... Val Loss: 0.001686\n",
      "Epoch: 74/100... Step: 75360... Loss: 0.000149... Val Loss: 0.001705\n",
      "Epoch: 74/100... Step: 75365... Loss: 0.000105... Val Loss: 0.001847\n",
      "Epoch: 74/100... Step: 75370... Loss: 0.000060... Val Loss: 0.001776\n",
      "Epoch: 74/100... Step: 75375... Loss: 0.000114... Val Loss: 0.001617\n",
      "Epoch: 74/100... Step: 75380... Loss: 0.000170... Val Loss: 0.001685\n",
      "Epoch: 74/100... Step: 75385... Loss: 0.000109... Val Loss: 0.001740\n",
      "Epoch: 74/100... Step: 75390... Loss: 0.000064... Val Loss: 0.001812\n",
      "Epoch: 74/100... Step: 75395... Loss: 0.000096... Val Loss: 0.001583\n",
      "Epoch: 74/100... Step: 75400... Loss: 0.000034... Val Loss: 0.001556\n",
      "Epoch: 74/100... Step: 75405... Loss: 0.000170... Val Loss: 0.001439\n",
      "Epoch: 74/100... Step: 75410... Loss: 0.000085... Val Loss: 0.001019\n",
      "Epoch: 74/100... Step: 75415... Loss: 0.000097... Val Loss: 0.001376\n",
      "Epoch: 74/100... Step: 75420... Loss: 0.000035... Val Loss: 0.001381\n",
      "Epoch: 74/100... Step: 75425... Loss: 0.000064... Val Loss: 0.001304\n",
      "Epoch: 74/100... Step: 75430... Loss: 0.000146... Val Loss: 0.001247\n",
      "Epoch: 74/100... Step: 75435... Loss: 0.000073... Val Loss: 0.000925\n",
      "Epoch: 74/100... Step: 75440... Loss: 0.000099... Val Loss: 0.001183\n",
      "Epoch: 74/100... Step: 75445... Loss: 0.000072... Val Loss: 0.001175\n",
      "Epoch: 74/100... Step: 75450... Loss: 0.000066... Val Loss: 0.001212\n",
      "Epoch: 74/100... Step: 75455... Loss: 0.000070... Val Loss: 0.001225\n",
      "Epoch: 74/100... Step: 75460... Loss: 0.000071... Val Loss: 0.001186\n",
      "Epoch: 74/100... Step: 75465... Loss: 0.000107... Val Loss: 0.001345\n",
      "Epoch: 74/100... Step: 75470... Loss: 0.000049... Val Loss: 0.001424\n",
      "Epoch: 74/100... Step: 75475... Loss: 0.000091... Val Loss: 0.001360\n",
      "Epoch: 74/100... Step: 75480... Loss: 0.000138... Val Loss: 0.001708\n",
      "Epoch: 74/100... Step: 75485... Loss: 0.000081... Val Loss: 0.001618\n",
      "Epoch: 74/100... Step: 75490... Loss: 0.000085... Val Loss: 0.001534\n",
      "Epoch: 74/100... Step: 75495... Loss: 0.000075... Val Loss: 0.001504\n",
      "Epoch: 74/100... Step: 75500... Loss: 0.000084... Val Loss: 0.001463\n",
      "Epoch: 74/100... Step: 75505... Loss: 0.000062... Val Loss: 0.001328\n",
      "Epoch: 74/100... Step: 75510... Loss: 0.000088... Val Loss: 0.001310\n",
      "Epoch: 74/100... Step: 75515... Loss: 0.000118... Val Loss: 0.001312\n",
      "Epoch: 74/100... Step: 75520... Loss: 0.000112... Val Loss: 0.001338\n",
      "Epoch: 74/100... Step: 75525... Loss: 0.000110... Val Loss: 0.001098\n",
      "Epoch: 74/100... Step: 75530... Loss: 0.000075... Val Loss: 0.001066\n",
      "Epoch: 74/100... Step: 75535... Loss: 0.000051... Val Loss: 0.001079\n",
      "Epoch: 74/100... Step: 75540... Loss: 0.000075... Val Loss: 0.001242\n",
      "Epoch: 74/100... Step: 75545... Loss: 0.000060... Val Loss: 0.001353\n",
      "Epoch: 74/100... Step: 75550... Loss: 0.000100... Val Loss: 0.001249\n",
      "Epoch: 74/100... Step: 75555... Loss: 0.000059... Val Loss: 0.001158\n",
      "Epoch: 74/100... Step: 75560... Loss: 0.000106... Val Loss: 0.001227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74/100... Step: 75565... Loss: 0.000104... Val Loss: 0.001167\n",
      "Epoch: 74/100... Step: 75570... Loss: 0.000086... Val Loss: 0.000972\n",
      "Epoch: 74/100... Step: 75575... Loss: 0.000124... Val Loss: 0.001232\n",
      "Epoch: 74/100... Step: 75580... Loss: 0.000076... Val Loss: 0.001139\n",
      "Epoch: 74/100... Step: 75585... Loss: 0.000066... Val Loss: 0.001077\n",
      "Epoch: 74/100... Step: 75590... Loss: 0.000096... Val Loss: 0.001184\n",
      "Epoch: 74/100... Step: 75595... Loss: 0.000087... Val Loss: 0.000999\n",
      "Epoch: 74/100... Step: 75600... Loss: 0.000070... Val Loss: 0.000921\n",
      "Epoch: 74/100... Step: 75605... Loss: 0.000058... Val Loss: 0.000949\n",
      "Epoch: 74/100... Step: 75610... Loss: 0.000044... Val Loss: 0.000916\n",
      "Epoch: 74/100... Step: 75615... Loss: 0.000043... Val Loss: 0.001004\n",
      "Epoch: 74/100... Step: 75620... Loss: 0.000046... Val Loss: 0.000980\n",
      "Epoch: 74/100... Step: 75625... Loss: 0.000118... Val Loss: 0.000756\n",
      "Epoch: 74/100... Step: 75630... Loss: 0.000076... Val Loss: 0.000661\n",
      "Epoch: 74/100... Step: 75635... Loss: 0.000102... Val Loss: 0.000488\n",
      "Epoch: 74/100... Step: 75640... Loss: 0.000119... Val Loss: 0.000679\n",
      "Epoch: 74/100... Step: 75645... Loss: 0.000141... Val Loss: 0.000698\n",
      "Epoch: 74/100... Step: 75650... Loss: 0.000130... Val Loss: 0.000497\n",
      "Epoch: 74/100... Step: 75655... Loss: 0.000068... Val Loss: 0.000621\n",
      "Epoch: 74/100... Step: 75660... Loss: 0.000116... Val Loss: 0.000679\n",
      "Epoch: 74/100... Step: 75665... Loss: 0.000117... Val Loss: 0.000351\n",
      "Epoch: 74/100... Step: 75670... Loss: 0.000076... Val Loss: 0.000485\n",
      "Epoch: 74/100... Step: 75675... Loss: 0.000113... Val Loss: 0.000583\n",
      "Epoch: 74/100... Step: 75680... Loss: 0.000175... Val Loss: 0.000607\n",
      "Epoch: 74/100... Step: 75685... Loss: 0.000120... Val Loss: 0.000293\n",
      "Epoch: 74/100... Step: 75690... Loss: 0.000196... Val Loss: 0.000531\n",
      "Epoch: 74/100... Step: 75695... Loss: 0.000131... Val Loss: 0.000812\n",
      "Epoch: 74/100... Step: 75700... Loss: 0.000145... Val Loss: 0.000860\n",
      "Epoch: 74/100... Step: 75705... Loss: 0.000048... Val Loss: 0.000998\n",
      "Epoch: 74/100... Step: 75710... Loss: 0.000035... Val Loss: 0.000986\n",
      "Epoch: 74/100... Step: 75715... Loss: 0.000065... Val Loss: 0.001087\n",
      "Epoch: 74/100... Step: 75720... Loss: 0.000077... Val Loss: 0.001025\n",
      "Epoch: 74/100... Step: 75725... Loss: 0.000070... Val Loss: 0.000943\n",
      "Epoch: 74/100... Step: 75730... Loss: 0.000051... Val Loss: 0.001000\n",
      "Epoch: 74/100... Step: 75735... Loss: 0.000062... Val Loss: 0.001040\n",
      "Epoch: 74/100... Step: 75740... Loss: 0.000236... Val Loss: 0.001008\n",
      "Epoch: 74/100... Step: 75745... Loss: 0.000268... Val Loss: 0.000519\n",
      "Epoch: 74/100... Step: 75750... Loss: 0.000669... Val Loss: 0.000297\n",
      "Epoch: 74/100... Step: 75755... Loss: 0.000270... Val Loss: 0.001373\n",
      "Epoch: 74/100... Step: 75760... Loss: 0.000173... Val Loss: 0.000870\n",
      "Epoch: 74/100... Step: 75765... Loss: 0.000137... Val Loss: 0.001179\n",
      "Epoch: 74/100... Step: 75770... Loss: 0.000145... Val Loss: 0.001077\n",
      "Epoch: 74/100... Step: 75775... Loss: 0.000193... Val Loss: 0.001230\n",
      "Epoch: 74/100... Step: 75780... Loss: 0.000092... Val Loss: 0.001088\n",
      "Epoch: 74/100... Step: 75785... Loss: 0.000175... Val Loss: 0.001263\n",
      "Epoch: 74/100... Step: 75790... Loss: 0.000106... Val Loss: 0.001469\n",
      "Epoch: 74/100... Step: 75795... Loss: 0.000134... Val Loss: 0.001353\n",
      "Epoch: 74/100... Step: 75800... Loss: 0.000105... Val Loss: 0.001444\n",
      "Epoch: 74/100... Step: 75805... Loss: 0.000162... Val Loss: 0.001353\n",
      "Epoch: 74/100... Step: 75810... Loss: 0.000128... Val Loss: 0.001098\n",
      "Epoch: 74/100... Step: 75815... Loss: 0.000125... Val Loss: 0.001161\n",
      "Epoch: 74/100... Step: 75820... Loss: 0.000152... Val Loss: 0.001086\n",
      "Epoch: 74/100... Step: 75825... Loss: 0.000123... Val Loss: 0.000825\n",
      "Epoch: 74/100... Step: 75830... Loss: 0.000077... Val Loss: 0.000926\n",
      "Epoch: 74/100... Step: 75835... Loss: 0.000087... Val Loss: 0.000794\n",
      "Epoch: 74/100... Step: 75840... Loss: 0.000151... Val Loss: 0.000607\n",
      "Epoch: 74/100... Step: 75845... Loss: 0.000132... Val Loss: 0.000294\n",
      "Epoch: 74/100... Step: 75850... Loss: 0.000149... Val Loss: 0.000514\n",
      "Epoch: 74/100... Step: 75855... Loss: 0.000123... Val Loss: 0.000492\n",
      "Epoch: 74/100... Step: 75860... Loss: 0.000057... Val Loss: 0.000527\n",
      "Epoch: 74/100... Step: 75865... Loss: 0.000065... Val Loss: 0.000760\n",
      "Epoch: 74/100... Step: 75870... Loss: 0.000118... Val Loss: 0.000663\n",
      "Epoch: 74/100... Step: 75875... Loss: 0.000123... Val Loss: 0.000631\n",
      "Epoch: 74/100... Step: 75880... Loss: 0.000136... Val Loss: 0.000753\n",
      "Epoch: 74/100... Step: 75885... Loss: 0.000083... Val Loss: 0.000881\n",
      "Epoch: 74/100... Step: 75890... Loss: 0.000133... Val Loss: 0.000891\n",
      "Epoch: 74/100... Step: 75895... Loss: 0.000126... Val Loss: 0.000686\n",
      "Epoch: 74/100... Step: 75900... Loss: 0.000060... Val Loss: 0.000696\n",
      "Epoch: 74/100... Step: 75905... Loss: 0.000077... Val Loss: 0.000885\n",
      "Epoch: 74/100... Step: 75910... Loss: 0.000103... Val Loss: 0.001004\n",
      "Epoch: 74/100... Step: 75915... Loss: 0.000098... Val Loss: 0.000712\n",
      "Epoch: 74/100... Step: 75920... Loss: 0.000072... Val Loss: 0.000863\n",
      "Epoch: 74/100... Step: 75925... Loss: 0.000080... Val Loss: 0.000828\n",
      "Epoch: 74/100... Step: 75930... Loss: 0.000067... Val Loss: 0.000945\n",
      "Epoch: 74/100... Step: 75935... Loss: 0.000102... Val Loss: 0.000962\n",
      "Epoch: 74/100... Step: 75940... Loss: 0.000069... Val Loss: 0.000936\n",
      "Epoch: 74/100... Step: 75945... Loss: 0.000052... Val Loss: 0.000911\n",
      "Epoch: 74/100... Step: 75950... Loss: 0.000025... Val Loss: 0.000887\n",
      "Epoch: 74/100... Step: 75955... Loss: 0.000093... Val Loss: 0.000835\n",
      "Epoch: 74/100... Step: 75960... Loss: 0.000034... Val Loss: 0.000967\n",
      "Epoch: 74/100... Step: 75965... Loss: 0.000076... Val Loss: 0.000933\n",
      "Epoch: 74/100... Step: 75970... Loss: 0.000096... Val Loss: 0.000792\n",
      "Epoch: 74/100... Step: 75975... Loss: 0.000108... Val Loss: 0.000842\n",
      "Epoch: 74/100... Step: 75980... Loss: 0.000126... Val Loss: 0.000679\n",
      "Epoch: 74/100... Step: 75985... Loss: 0.000141... Val Loss: 0.000539\n",
      "Epoch: 74/100... Step: 75990... Loss: 0.000043... Val Loss: 0.001050\n",
      "Epoch: 74/100... Step: 75995... Loss: 0.000059... Val Loss: 0.000708\n",
      "Epoch: 74/100... Step: 76000... Loss: 0.000138... Val Loss: 0.000793\n",
      "Epoch: 74/100... Step: 76005... Loss: 0.000092... Val Loss: 0.000296\n",
      "Epoch: 74/100... Step: 76010... Loss: 0.000157... Val Loss: 0.000484\n",
      "Epoch: 74/100... Step: 76015... Loss: 0.000079... Val Loss: 0.000354\n",
      "Epoch: 74/100... Step: 76020... Loss: 0.000109... Val Loss: 0.000259\n",
      "Epoch: 74/100... Step: 76025... Loss: 0.000059... Val Loss: 0.000409\n",
      "Epoch: 74/100... Step: 76030... Loss: 0.000086... Val Loss: 0.000307\n",
      "Epoch: 74/100... Step: 76035... Loss: 0.000081... Val Loss: 0.000269\n",
      "Epoch: 74/100... Step: 76040... Loss: 0.000114... Val Loss: 0.000337\n",
      "Epoch: 74/100... Step: 76045... Loss: 0.000057... Val Loss: 0.000399\n",
      "Epoch: 74/100... Step: 76050... Loss: 0.000072... Val Loss: 0.000379\n",
      "Epoch: 74/100... Step: 76055... Loss: 0.000069... Val Loss: 0.000425\n",
      "Epoch: 74/100... Step: 76060... Loss: 0.000042... Val Loss: 0.000365\n",
      "Epoch: 74/100... Step: 76065... Loss: 0.000084... Val Loss: 0.000372\n",
      "Epoch: 74/100... Step: 76070... Loss: 0.000083... Val Loss: 0.000391\n",
      "Epoch: 74/100... Step: 76075... Loss: 0.000208... Val Loss: 0.000261\n",
      "Epoch: 74/100... Step: 76080... Loss: 0.000126... Val Loss: 0.000367\n",
      "Epoch: 74/100... Step: 76085... Loss: 0.000089... Val Loss: 0.000562\n",
      "Epoch: 74/100... Step: 76090... Loss: 0.000075... Val Loss: 0.000356\n",
      "Epoch: 74/100... Step: 76095... Loss: 0.000114... Val Loss: 0.000329\n",
      "Epoch: 74/100... Step: 76100... Loss: 0.000067... Val Loss: 0.000345\n",
      "Epoch: 74/100... Step: 76105... Loss: 0.000082... Val Loss: 0.000466\n",
      "Epoch: 74/100... Step: 76110... Loss: 0.000106... Val Loss: 0.000468\n",
      "Epoch: 74/100... Step: 76115... Loss: 0.000162... Val Loss: 0.000438\n",
      "Epoch: 74/100... Step: 76120... Loss: 0.000165... Val Loss: 0.000563\n",
      "Epoch: 74/100... Step: 76125... Loss: 0.000100... Val Loss: 0.000594\n",
      "Epoch: 74/100... Step: 76130... Loss: 0.000095... Val Loss: 0.000332\n",
      "Epoch: 74/100... Step: 76135... Loss: 0.000068... Val Loss: 0.000360\n",
      "Epoch: 74/100... Step: 76140... Loss: 0.000153... Val Loss: 0.000590\n",
      "Epoch: 74/100... Step: 76145... Loss: 0.000066... Val Loss: 0.000712\n",
      "Epoch: 74/100... Step: 76150... Loss: 0.000077... Val Loss: 0.000735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74/100... Step: 76155... Loss: 0.000038... Val Loss: 0.000644\n",
      "Epoch: 74/100... Step: 76160... Loss: 0.000053... Val Loss: 0.000621\n",
      "Epoch: 74/100... Step: 76165... Loss: 0.000041... Val Loss: 0.000615\n",
      "Epoch: 74/100... Step: 76170... Loss: 0.000059... Val Loss: 0.000638\n",
      "Epoch: 74/100... Step: 76175... Loss: 0.000063... Val Loss: 0.000619\n",
      "Epoch: 74/100... Step: 76180... Loss: 0.000042... Val Loss: 0.000598\n",
      "Epoch: 74/100... Step: 76185... Loss: 0.000025... Val Loss: 0.000618\n",
      "Epoch: 74/100... Step: 76190... Loss: 0.000063... Val Loss: 0.000622\n",
      "Epoch: 74/100... Step: 76195... Loss: 0.000048... Val Loss: 0.000734\n",
      "Epoch: 74/100... Step: 76200... Loss: 0.000033... Val Loss: 0.000739\n",
      "Epoch: 74/100... Step: 76205... Loss: 0.000053... Val Loss: 0.000734\n",
      "Epoch: 74/100... Step: 76210... Loss: 0.000049... Val Loss: 0.000775\n",
      "Epoch: 74/100... Step: 76215... Loss: 0.000081... Val Loss: 0.000761\n",
      "Epoch: 74/100... Step: 76220... Loss: 0.000119... Val Loss: 0.000761\n",
      "Epoch: 74/100... Step: 76225... Loss: 0.000078... Val Loss: 0.000639\n",
      "Epoch: 74/100... Step: 76230... Loss: 0.000065... Val Loss: 0.000688\n",
      "Epoch: 74/100... Step: 76235... Loss: 0.000057... Val Loss: 0.000717\n",
      "Epoch: 74/100... Step: 76240... Loss: 0.000065... Val Loss: 0.000647\n",
      "Epoch: 74/100... Step: 76245... Loss: 0.000051... Val Loss: 0.000544\n",
      "Epoch: 74/100... Step: 76250... Loss: 0.000038... Val Loss: 0.000563\n",
      "Epoch: 74/100... Step: 76255... Loss: 0.000034... Val Loss: 0.000665\n",
      "Epoch: 74/100... Step: 76260... Loss: 0.000061... Val Loss: 0.000704\n",
      "Epoch: 74/100... Step: 76265... Loss: 0.000036... Val Loss: 0.000778\n",
      "Epoch: 74/100... Step: 76270... Loss: 0.000041... Val Loss: 0.000824\n",
      "Epoch: 74/100... Step: 76275... Loss: 0.000047... Val Loss: 0.000860\n",
      "Epoch: 74/100... Step: 76280... Loss: 0.000062... Val Loss: 0.000848\n",
      "Epoch: 74/100... Step: 76285... Loss: 0.000042... Val Loss: 0.000764\n",
      "Epoch: 74/100... Step: 76290... Loss: 0.000068... Val Loss: 0.000986\n",
      "Epoch: 74/100... Step: 76295... Loss: 0.000046... Val Loss: 0.001033\n",
      "Epoch: 74/100... Step: 76300... Loss: 0.000033... Val Loss: 0.000982\n",
      "Epoch: 74/100... Step: 76305... Loss: 0.000025... Val Loss: 0.001007\n",
      "Epoch: 74/100... Step: 76310... Loss: 0.000032... Val Loss: 0.000964\n",
      "Epoch: 74/100... Step: 76315... Loss: 0.000043... Val Loss: 0.000887\n",
      "Epoch: 74/100... Step: 76320... Loss: 0.000098... Val Loss: 0.001012\n",
      "Epoch: 74/100... Step: 76325... Loss: 0.000034... Val Loss: 0.001026\n",
      "Epoch: 74/100... Step: 76330... Loss: 0.000051... Val Loss: 0.001000\n",
      "Epoch: 74/100... Step: 76335... Loss: 0.000051... Val Loss: 0.001006\n",
      "Epoch: 74/100... Step: 76340... Loss: 0.000076... Val Loss: 0.001014\n",
      "Epoch: 74/100... Step: 76345... Loss: 0.000081... Val Loss: 0.000861\n",
      "Epoch: 74/100... Step: 76350... Loss: 0.000069... Val Loss: 0.000936\n",
      "Epoch: 74/100... Step: 76355... Loss: 0.000065... Val Loss: 0.001053\n",
      "Epoch: 74/100... Step: 76360... Loss: 0.000060... Val Loss: 0.001090\n",
      "Epoch: 74/100... Step: 76365... Loss: 0.000069... Val Loss: 0.000975\n",
      "Epoch: 75/100... Step: 76370... Loss: 0.000270... Val Loss: 0.001129\n",
      "Epoch: 75/100... Step: 76375... Loss: 0.000193... Val Loss: 0.001640\n",
      "Epoch: 75/100... Step: 76380... Loss: 0.000153... Val Loss: 0.001464\n",
      "Epoch: 75/100... Step: 76385... Loss: 0.000174... Val Loss: 0.001733\n",
      "Epoch: 75/100... Step: 76390... Loss: 0.000154... Val Loss: 0.001724\n",
      "Epoch: 75/100... Step: 76395... Loss: 0.000113... Val Loss: 0.001696\n",
      "Epoch: 75/100... Step: 76400... Loss: 0.000099... Val Loss: 0.001915\n",
      "Epoch: 75/100... Step: 76405... Loss: 0.000155... Val Loss: 0.001788\n",
      "Epoch: 75/100... Step: 76410... Loss: 0.000148... Val Loss: 0.001533\n",
      "Epoch: 75/100... Step: 76415... Loss: 0.000089... Val Loss: 0.001943\n",
      "Epoch: 75/100... Step: 76420... Loss: 0.000105... Val Loss: 0.001575\n",
      "Epoch: 75/100... Step: 76425... Loss: 0.000135... Val Loss: 0.001830\n",
      "Epoch: 75/100... Step: 76430... Loss: 0.000090... Val Loss: 0.001531\n",
      "Epoch: 75/100... Step: 76435... Loss: 0.000110... Val Loss: 0.001512\n",
      "Epoch: 75/100... Step: 76440... Loss: 0.000203... Val Loss: 0.001267\n",
      "Epoch: 75/100... Step: 76445... Loss: 0.000179... Val Loss: 0.001078\n",
      "Epoch: 75/100... Step: 76450... Loss: 0.000119... Val Loss: 0.001434\n",
      "Epoch: 75/100... Step: 76455... Loss: 0.000023... Val Loss: 0.001419\n",
      "Epoch: 75/100... Step: 76460... Loss: 0.000076... Val Loss: 0.001269\n",
      "Epoch: 75/100... Step: 76465... Loss: 0.000160... Val Loss: 0.001138\n",
      "Epoch: 75/100... Step: 76470... Loss: 0.000134... Val Loss: 0.001060\n",
      "Epoch: 75/100... Step: 76475... Loss: 0.000052... Val Loss: 0.001108\n",
      "Epoch: 75/100... Step: 76480... Loss: 0.000095... Val Loss: 0.001300\n",
      "Epoch: 75/100... Step: 76485... Loss: 0.000124... Val Loss: 0.001063\n",
      "Epoch: 75/100... Step: 76490... Loss: 0.000075... Val Loss: 0.001204\n",
      "Epoch: 75/100... Step: 76495... Loss: 0.000054... Val Loss: 0.001406\n",
      "Epoch: 75/100... Step: 76500... Loss: 0.000085... Val Loss: 0.001370\n",
      "Epoch: 75/100... Step: 76505... Loss: 0.000085... Val Loss: 0.001336\n",
      "Epoch: 75/100... Step: 76510... Loss: 0.000122... Val Loss: 0.001543\n",
      "Epoch: 75/100... Step: 76515... Loss: 0.000096... Val Loss: 0.001801\n",
      "Epoch: 75/100... Step: 76520... Loss: 0.000074... Val Loss: 0.001613\n",
      "Epoch: 75/100... Step: 76525... Loss: 0.000102... Val Loss: 0.001506\n",
      "Epoch: 75/100... Step: 76530... Loss: 0.000045... Val Loss: 0.001475\n",
      "Epoch: 75/100... Step: 76535... Loss: 0.000040... Val Loss: 0.001339\n",
      "Epoch: 75/100... Step: 76540... Loss: 0.000057... Val Loss: 0.001359\n",
      "Epoch: 75/100... Step: 76545... Loss: 0.000062... Val Loss: 0.001394\n",
      "Epoch: 75/100... Step: 76550... Loss: 0.000071... Val Loss: 0.001347\n",
      "Epoch: 75/100... Step: 76555... Loss: 0.000075... Val Loss: 0.001138\n",
      "Epoch: 75/100... Step: 76560... Loss: 0.000051... Val Loss: 0.001177\n",
      "Epoch: 75/100... Step: 76565... Loss: 0.000052... Val Loss: 0.001161\n",
      "Epoch: 75/100... Step: 76570... Loss: 0.000034... Val Loss: 0.001182\n",
      "Epoch: 75/100... Step: 76575... Loss: 0.000062... Val Loss: 0.001242\n",
      "Epoch: 75/100... Step: 76580... Loss: 0.000062... Val Loss: 0.001277\n",
      "Epoch: 75/100... Step: 76585... Loss: 0.000041... Val Loss: 0.001303\n",
      "Epoch: 75/100... Step: 76590... Loss: 0.000054... Val Loss: 0.001214\n",
      "Epoch: 75/100... Step: 76595... Loss: 0.000080... Val Loss: 0.001200\n",
      "Epoch: 75/100... Step: 76600... Loss: 0.000088... Val Loss: 0.000984\n",
      "Epoch: 75/100... Step: 76605... Loss: 0.000073... Val Loss: 0.001094\n",
      "Epoch: 75/100... Step: 76610... Loss: 0.000068... Val Loss: 0.001251\n",
      "Epoch: 75/100... Step: 76615... Loss: 0.000080... Val Loss: 0.001012\n",
      "Epoch: 75/100... Step: 76620... Loss: 0.000113... Val Loss: 0.001223\n",
      "Epoch: 75/100... Step: 76625... Loss: 0.000078... Val Loss: 0.001170\n",
      "Epoch: 75/100... Step: 76630... Loss: 0.000060... Val Loss: 0.000853\n",
      "Epoch: 75/100... Step: 76635... Loss: 0.000079... Val Loss: 0.001016\n",
      "Epoch: 75/100... Step: 76640... Loss: 0.000054... Val Loss: 0.000837\n",
      "Epoch: 75/100... Step: 76645... Loss: 0.000049... Val Loss: 0.000845\n",
      "Epoch: 75/100... Step: 76650... Loss: 0.000081... Val Loss: 0.000993\n",
      "Epoch: 75/100... Step: 76655... Loss: 0.000114... Val Loss: 0.000867\n",
      "Epoch: 75/100... Step: 76660... Loss: 0.000070... Val Loss: 0.000585\n",
      "Epoch: 75/100... Step: 76665... Loss: 0.000076... Val Loss: 0.000652\n",
      "Epoch: 75/100... Step: 76670... Loss: 0.000105... Val Loss: 0.000603\n",
      "Epoch: 75/100... Step: 76675... Loss: 0.000144... Val Loss: 0.000756\n",
      "Epoch: 75/100... Step: 76680... Loss: 0.000072... Val Loss: 0.000475\n",
      "Epoch: 75/100... Step: 76685... Loss: 0.000069... Val Loss: 0.000668\n",
      "Epoch: 75/100... Step: 76690... Loss: 0.000052... Val Loss: 0.000660\n",
      "Epoch: 75/100... Step: 76695... Loss: 0.000090... Val Loss: 0.000461\n",
      "Epoch: 75/100... Step: 76700... Loss: 0.000051... Val Loss: 0.000508\n",
      "Epoch: 75/100... Step: 76705... Loss: 0.000128... Val Loss: 0.000472\n",
      "Epoch: 75/100... Step: 76710... Loss: 0.000112... Val Loss: 0.000699\n",
      "Epoch: 75/100... Step: 76715... Loss: 0.000170... Val Loss: 0.000397\n",
      "Epoch: 75/100... Step: 76720... Loss: 0.000116... Val Loss: 0.000404\n",
      "Epoch: 75/100... Step: 76725... Loss: 0.000155... Val Loss: 0.000673\n",
      "Epoch: 75/100... Step: 76730... Loss: 0.000118... Val Loss: 0.000726\n",
      "Epoch: 75/100... Step: 76735... Loss: 0.000108... Val Loss: 0.001040\n",
      "Epoch: 75/100... Step: 76740... Loss: 0.000126... Val Loss: 0.000977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75/100... Step: 76745... Loss: 0.000134... Val Loss: 0.000866\n",
      "Epoch: 75/100... Step: 76750... Loss: 0.000122... Val Loss: 0.001109\n",
      "Epoch: 75/100... Step: 76755... Loss: 0.000113... Val Loss: 0.000959\n",
      "Epoch: 75/100... Step: 76760... Loss: 0.000115... Val Loss: 0.000966\n",
      "Epoch: 75/100... Step: 76765... Loss: 0.000048... Val Loss: 0.001064\n",
      "Epoch: 75/100... Step: 76770... Loss: 0.000109... Val Loss: 0.001074\n",
      "Epoch: 75/100... Step: 76775... Loss: 0.000415... Val Loss: 0.000926\n",
      "Epoch: 75/100... Step: 76780... Loss: 0.000399... Val Loss: 0.000263\n",
      "Epoch: 75/100... Step: 76785... Loss: 0.000603... Val Loss: 0.000950\n",
      "Epoch: 75/100... Step: 76790... Loss: 0.000200... Val Loss: 0.001003\n",
      "Epoch: 75/100... Step: 76795... Loss: 0.000216... Val Loss: 0.001303\n",
      "Epoch: 75/100... Step: 76800... Loss: 0.000195... Val Loss: 0.001014\n",
      "Epoch: 75/100... Step: 76805... Loss: 0.000296... Val Loss: 0.001155\n",
      "Epoch: 75/100... Step: 76810... Loss: 0.000076... Val Loss: 0.001226\n",
      "Epoch: 75/100... Step: 76815... Loss: 0.000155... Val Loss: 0.001119\n",
      "Epoch: 75/100... Step: 76820... Loss: 0.000250... Val Loss: 0.001441\n",
      "Epoch: 75/100... Step: 76825... Loss: 0.000163... Val Loss: 0.001275\n",
      "Epoch: 75/100... Step: 76830... Loss: 0.000108... Val Loss: 0.001511\n",
      "Epoch: 75/100... Step: 76835... Loss: 0.000138... Val Loss: 0.001433\n",
      "Epoch: 75/100... Step: 76840... Loss: 0.000097... Val Loss: 0.000972\n",
      "Epoch: 75/100... Step: 76845... Loss: 0.000284... Val Loss: 0.001149\n",
      "Epoch: 75/100... Step: 76850... Loss: 0.000133... Val Loss: 0.001324\n",
      "Epoch: 75/100... Step: 76855... Loss: 0.000123... Val Loss: 0.000783\n",
      "Epoch: 75/100... Step: 76860... Loss: 0.000078... Val Loss: 0.001091\n",
      "Epoch: 75/100... Step: 76865... Loss: 0.000098... Val Loss: 0.000615\n",
      "Epoch: 75/100... Step: 76870... Loss: 0.000173... Val Loss: 0.000770\n",
      "Epoch: 75/100... Step: 76875... Loss: 0.000153... Val Loss: 0.000327\n",
      "Epoch: 75/100... Step: 76880... Loss: 0.000113... Val Loss: 0.000385\n",
      "Epoch: 75/100... Step: 76885... Loss: 0.000078... Val Loss: 0.000458\n",
      "Epoch: 75/100... Step: 76890... Loss: 0.000091... Val Loss: 0.000448\n",
      "Epoch: 75/100... Step: 76895... Loss: 0.000085... Val Loss: 0.000712\n",
      "Epoch: 75/100... Step: 76900... Loss: 0.000078... Val Loss: 0.000744\n",
      "Epoch: 75/100... Step: 76905... Loss: 0.000055... Val Loss: 0.000528\n",
      "Epoch: 75/100... Step: 76910... Loss: 0.000159... Val Loss: 0.000786\n",
      "Epoch: 75/100... Step: 76915... Loss: 0.000064... Val Loss: 0.000983\n",
      "Epoch: 75/100... Step: 76920... Loss: 0.000118... Val Loss: 0.000740\n",
      "Epoch: 75/100... Step: 76925... Loss: 0.000186... Val Loss: 0.000727\n",
      "Epoch: 75/100... Step: 76930... Loss: 0.000052... Val Loss: 0.000799\n",
      "Epoch: 75/100... Step: 76935... Loss: 0.000105... Val Loss: 0.000773\n",
      "Epoch: 75/100... Step: 76940... Loss: 0.000065... Val Loss: 0.000863\n",
      "Epoch: 75/100... Step: 76945... Loss: 0.000153... Val Loss: 0.000917\n",
      "Epoch: 75/100... Step: 76950... Loss: 0.000124... Val Loss: 0.000815\n",
      "Epoch: 75/100... Step: 76955... Loss: 0.000126... Val Loss: 0.000817\n",
      "Epoch: 75/100... Step: 76960... Loss: 0.000102... Val Loss: 0.000866\n",
      "Epoch: 75/100... Step: 76965... Loss: 0.000096... Val Loss: 0.000907\n",
      "Epoch: 75/100... Step: 76970... Loss: 0.000062... Val Loss: 0.001037\n",
      "Epoch: 75/100... Step: 76975... Loss: 0.000045... Val Loss: 0.000953\n",
      "Epoch: 75/100... Step: 76980... Loss: 0.000056... Val Loss: 0.000858\n",
      "Epoch: 75/100... Step: 76985... Loss: 0.000042... Val Loss: 0.000860\n",
      "Epoch: 75/100... Step: 76990... Loss: 0.000051... Val Loss: 0.000871\n",
      "Epoch: 75/100... Step: 76995... Loss: 0.000052... Val Loss: 0.000915\n",
      "Epoch: 75/100... Step: 77000... Loss: 0.000029... Val Loss: 0.000863\n",
      "Epoch: 75/100... Step: 77005... Loss: 0.000036... Val Loss: 0.000900\n",
      "Epoch: 75/100... Step: 77010... Loss: 0.000102... Val Loss: 0.000626\n",
      "Epoch: 75/100... Step: 77015... Loss: 0.000114... Val Loss: 0.000625\n",
      "Epoch: 75/100... Step: 77020... Loss: 0.000252... Val Loss: 0.000725\n",
      "Epoch: 75/100... Step: 77025... Loss: 0.000133... Val Loss: 0.000935\n",
      "Epoch: 75/100... Step: 77030... Loss: 0.000118... Val Loss: 0.000735\n",
      "Epoch: 75/100... Step: 77035... Loss: 0.000164... Val Loss: 0.000533\n",
      "Epoch: 75/100... Step: 77040... Loss: 0.000046... Val Loss: 0.000388\n",
      "Epoch: 75/100... Step: 77045... Loss: 0.000089... Val Loss: 0.000291\n",
      "Epoch: 75/100... Step: 77050... Loss: 0.000171... Val Loss: 0.000331\n",
      "Epoch: 75/100... Step: 77055... Loss: 0.000168... Val Loss: 0.000256\n",
      "Epoch: 75/100... Step: 77060... Loss: 0.000153... Val Loss: 0.000399\n",
      "Epoch: 75/100... Step: 77065... Loss: 0.000084... Val Loss: 0.000309\n",
      "Epoch: 75/100... Step: 77070... Loss: 0.000062... Val Loss: 0.000282\n",
      "Epoch: 75/100... Step: 77075... Loss: 0.000077... Val Loss: 0.000358\n",
      "Epoch: 75/100... Step: 77080... Loss: 0.000047... Val Loss: 0.000396\n",
      "Epoch: 75/100... Step: 77085... Loss: 0.000050... Val Loss: 0.000414\n",
      "Epoch: 75/100... Step: 77090... Loss: 0.000036... Val Loss: 0.000407\n",
      "Epoch: 75/100... Step: 77095... Loss: 0.000041... Val Loss: 0.000328\n",
      "Epoch: 75/100... Step: 77100... Loss: 0.000073... Val Loss: 0.000385\n",
      "Epoch: 75/100... Step: 77105... Loss: 0.000177... Val Loss: 0.000321\n",
      "Epoch: 75/100... Step: 77110... Loss: 0.000144... Val Loss: 0.000344\n",
      "Epoch: 75/100... Step: 77115... Loss: 0.000091... Val Loss: 0.000273\n",
      "Epoch: 75/100... Step: 77120... Loss: 0.000108... Val Loss: 0.000477\n",
      "Epoch: 75/100... Step: 77125... Loss: 0.000077... Val Loss: 0.000430\n",
      "Epoch: 75/100... Step: 77130... Loss: 0.000130... Val Loss: 0.000299\n",
      "Epoch: 75/100... Step: 77135... Loss: 0.000105... Val Loss: 0.000420\n",
      "Epoch: 75/100... Step: 77140... Loss: 0.000041... Val Loss: 0.000402\n",
      "Epoch: 75/100... Step: 77145... Loss: 0.000106... Val Loss: 0.000426\n",
      "Epoch: 75/100... Step: 77150... Loss: 0.000076... Val Loss: 0.000618\n",
      "Epoch: 75/100... Step: 77155... Loss: 0.000036... Val Loss: 0.000549\n",
      "Epoch: 75/100... Step: 77160... Loss: 0.000109... Val Loss: 0.000406\n",
      "Epoch: 75/100... Step: 77165... Loss: 0.000056... Val Loss: 0.000377\n",
      "Epoch: 75/100... Step: 77170... Loss: 0.000133... Val Loss: 0.000450\n",
      "Epoch: 75/100... Step: 77175... Loss: 0.000123... Val Loss: 0.000774\n",
      "Epoch: 75/100... Step: 77180... Loss: 0.000059... Val Loss: 0.000708\n",
      "Epoch: 75/100... Step: 77185... Loss: 0.000056... Val Loss: 0.000613\n",
      "Epoch: 75/100... Step: 77190... Loss: 0.000099... Val Loss: 0.000617\n",
      "Epoch: 75/100... Step: 77195... Loss: 0.000055... Val Loss: 0.000610\n",
      "Epoch: 75/100... Step: 77200... Loss: 0.000051... Val Loss: 0.000656\n",
      "Epoch: 75/100... Step: 77205... Loss: 0.000076... Val Loss: 0.000625\n",
      "Epoch: 75/100... Step: 77210... Loss: 0.000042... Val Loss: 0.000608\n",
      "Epoch: 75/100... Step: 77215... Loss: 0.000044... Val Loss: 0.000619\n",
      "Epoch: 75/100... Step: 77220... Loss: 0.000028... Val Loss: 0.000648\n",
      "Epoch: 75/100... Step: 77225... Loss: 0.000056... Val Loss: 0.000676\n",
      "Epoch: 75/100... Step: 77230... Loss: 0.000080... Val Loss: 0.000766\n",
      "Epoch: 75/100... Step: 77235... Loss: 0.000048... Val Loss: 0.000779\n",
      "Epoch: 75/100... Step: 77240... Loss: 0.000054... Val Loss: 0.000697\n",
      "Epoch: 75/100... Step: 77245... Loss: 0.000058... Val Loss: 0.000781\n",
      "Epoch: 75/100... Step: 77250... Loss: 0.000072... Val Loss: 0.000833\n",
      "Epoch: 75/100... Step: 77255... Loss: 0.000090... Val Loss: 0.000657\n",
      "Epoch: 75/100... Step: 77260... Loss: 0.000058... Val Loss: 0.000758\n",
      "Epoch: 75/100... Step: 77265... Loss: 0.000057... Val Loss: 0.000667\n",
      "Epoch: 75/100... Step: 77270... Loss: 0.000048... Val Loss: 0.000663\n",
      "Epoch: 75/100... Step: 77275... Loss: 0.000052... Val Loss: 0.000537\n",
      "Epoch: 75/100... Step: 77280... Loss: 0.000040... Val Loss: 0.000539\n",
      "Epoch: 75/100... Step: 77285... Loss: 0.000044... Val Loss: 0.000641\n",
      "Epoch: 75/100... Step: 77290... Loss: 0.000054... Val Loss: 0.000704\n",
      "Epoch: 75/100... Step: 77295... Loss: 0.000038... Val Loss: 0.000819\n",
      "Epoch: 75/100... Step: 77300... Loss: 0.000062... Val Loss: 0.000802\n",
      "Epoch: 75/100... Step: 77305... Loss: 0.000064... Val Loss: 0.000842\n",
      "Epoch: 75/100... Step: 77310... Loss: 0.000051... Val Loss: 0.000866\n",
      "Epoch: 75/100... Step: 77315... Loss: 0.000060... Val Loss: 0.000764\n",
      "Epoch: 75/100... Step: 77320... Loss: 0.000090... Val Loss: 0.000872\n",
      "Epoch: 75/100... Step: 77325... Loss: 0.000045... Val Loss: 0.000948\n",
      "Epoch: 75/100... Step: 77330... Loss: 0.000055... Val Loss: 0.000941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75/100... Step: 77335... Loss: 0.000061... Val Loss: 0.000940\n",
      "Epoch: 75/100... Step: 77340... Loss: 0.000064... Val Loss: 0.001016\n",
      "Epoch: 75/100... Step: 77345... Loss: 0.000033... Val Loss: 0.001012\n",
      "Epoch: 75/100... Step: 77350... Loss: 0.000129... Val Loss: 0.000878\n",
      "Epoch: 75/100... Step: 77355... Loss: 0.000068... Val Loss: 0.001121\n",
      "Epoch: 75/100... Step: 77360... Loss: 0.000040... Val Loss: 0.001080\n",
      "Epoch: 75/100... Step: 77365... Loss: 0.000053... Val Loss: 0.001001\n",
      "Epoch: 75/100... Step: 77370... Loss: 0.000048... Val Loss: 0.000957\n",
      "Epoch: 75/100... Step: 77375... Loss: 0.000064... Val Loss: 0.000920\n",
      "Epoch: 75/100... Step: 77380... Loss: 0.000086... Val Loss: 0.000867\n",
      "Epoch: 75/100... Step: 77385... Loss: 0.000060... Val Loss: 0.001029\n",
      "Epoch: 75/100... Step: 77390... Loss: 0.000054... Val Loss: 0.001088\n",
      "Epoch: 75/100... Step: 77395... Loss: 0.000035... Val Loss: 0.000939\n",
      "Epoch: 75/100... Step: 77400... Loss: 0.000080... Val Loss: 0.001041\n",
      "Epoch: 76/100... Step: 77405... Loss: 0.000116... Val Loss: 0.001581\n",
      "Epoch: 76/100... Step: 77410... Loss: 0.000075... Val Loss: 0.001380\n",
      "Epoch: 76/100... Step: 77415... Loss: 0.000099... Val Loss: 0.001705\n",
      "Epoch: 76/100... Step: 77420... Loss: 0.000130... Val Loss: 0.001753\n",
      "Epoch: 76/100... Step: 77425... Loss: 0.000083... Val Loss: 0.001634\n",
      "Epoch: 76/100... Step: 77430... Loss: 0.000061... Val Loss: 0.001911\n",
      "Epoch: 76/100... Step: 77435... Loss: 0.000103... Val Loss: 0.001789\n",
      "Epoch: 76/100... Step: 77440... Loss: 0.000120... Val Loss: 0.001630\n",
      "Epoch: 76/100... Step: 77445... Loss: 0.000215... Val Loss: 0.001666\n",
      "Epoch: 76/100... Step: 77450... Loss: 0.000191... Val Loss: 0.001874\n",
      "Epoch: 76/100... Step: 77455... Loss: 0.000152... Val Loss: 0.001625\n",
      "Epoch: 76/100... Step: 77460... Loss: 0.000150... Val Loss: 0.001762\n",
      "Epoch: 76/100... Step: 77465... Loss: 0.000117... Val Loss: 0.001463\n",
      "Epoch: 76/100... Step: 77470... Loss: 0.000309... Val Loss: 0.001554\n",
      "Epoch: 76/100... Step: 77475... Loss: 0.000177... Val Loss: 0.000902\n",
      "Epoch: 76/100... Step: 77480... Loss: 0.000119... Val Loss: 0.001384\n",
      "Epoch: 76/100... Step: 77485... Loss: 0.000085... Val Loss: 0.001288\n",
      "Epoch: 76/100... Step: 77490... Loss: 0.000131... Val Loss: 0.001370\n",
      "Epoch: 76/100... Step: 77495... Loss: 0.000142... Val Loss: 0.001263\n",
      "Epoch: 76/100... Step: 77500... Loss: 0.000076... Val Loss: 0.000908\n",
      "Epoch: 76/100... Step: 77505... Loss: 0.000150... Val Loss: 0.001212\n",
      "Epoch: 76/100... Step: 77510... Loss: 0.000049... Val Loss: 0.001243\n",
      "Epoch: 76/100... Step: 77515... Loss: 0.000103... Val Loss: 0.001035\n",
      "Epoch: 76/100... Step: 77520... Loss: 0.000086... Val Loss: 0.001193\n",
      "Epoch: 76/100... Step: 77525... Loss: 0.000140... Val Loss: 0.001279\n",
      "Epoch: 76/100... Step: 77530... Loss: 0.000132... Val Loss: 0.001291\n",
      "Epoch: 76/100... Step: 77535... Loss: 0.000036... Val Loss: 0.001473\n",
      "Epoch: 76/100... Step: 77540... Loss: 0.000203... Val Loss: 0.001338\n",
      "Epoch: 76/100... Step: 77545... Loss: 0.000118... Val Loss: 0.001789\n",
      "Epoch: 76/100... Step: 77550... Loss: 0.000051... Val Loss: 0.001771\n",
      "Epoch: 76/100... Step: 77555... Loss: 0.000101... Val Loss: 0.001493\n",
      "Epoch: 76/100... Step: 77560... Loss: 0.000085... Val Loss: 0.001511\n",
      "Epoch: 76/100... Step: 77565... Loss: 0.000056... Val Loss: 0.001453\n",
      "Epoch: 76/100... Step: 77570... Loss: 0.000171... Val Loss: 0.001281\n",
      "Epoch: 76/100... Step: 77575... Loss: 0.000142... Val Loss: 0.001407\n",
      "Epoch: 76/100... Step: 77580... Loss: 0.000033... Val Loss: 0.001389\n",
      "Epoch: 76/100... Step: 77585... Loss: 0.000102... Val Loss: 0.001154\n",
      "Epoch: 76/100... Step: 77590... Loss: 0.000026... Val Loss: 0.001101\n",
      "Epoch: 76/100... Step: 77595... Loss: 0.000067... Val Loss: 0.001187\n",
      "Epoch: 76/100... Step: 77600... Loss: 0.000043... Val Loss: 0.001205\n",
      "Epoch: 76/100... Step: 77605... Loss: 0.000085... Val Loss: 0.001205\n",
      "Epoch: 76/100... Step: 77610... Loss: 0.000086... Val Loss: 0.001161\n",
      "Epoch: 76/100... Step: 77615... Loss: 0.000092... Val Loss: 0.001267\n",
      "Epoch: 76/100... Step: 77620... Loss: 0.000058... Val Loss: 0.001324\n",
      "Epoch: 76/100... Step: 77625... Loss: 0.000122... Val Loss: 0.001112\n",
      "Epoch: 76/100... Step: 77630... Loss: 0.000123... Val Loss: 0.001180\n",
      "Epoch: 76/100... Step: 77635... Loss: 0.000139... Val Loss: 0.000995\n",
      "Epoch: 76/100... Step: 77640... Loss: 0.000092... Val Loss: 0.001194\n",
      "Epoch: 76/100... Step: 77645... Loss: 0.000081... Val Loss: 0.001104\n",
      "Epoch: 76/100... Step: 77650... Loss: 0.000072... Val Loss: 0.001133\n",
      "Epoch: 76/100... Step: 77655... Loss: 0.000066... Val Loss: 0.001122\n",
      "Epoch: 76/100... Step: 77660... Loss: 0.000123... Val Loss: 0.001085\n",
      "Epoch: 76/100... Step: 77665... Loss: 0.000105... Val Loss: 0.000914\n",
      "Epoch: 76/100... Step: 77670... Loss: 0.000071... Val Loss: 0.000860\n",
      "Epoch: 76/100... Step: 77675... Loss: 0.000060... Val Loss: 0.000957\n",
      "Epoch: 76/100... Step: 77680... Loss: 0.000065... Val Loss: 0.000937\n",
      "Epoch: 76/100... Step: 77685... Loss: 0.000052... Val Loss: 0.000860\n",
      "Epoch: 76/100... Step: 77690... Loss: 0.000137... Val Loss: 0.000806\n",
      "Epoch: 76/100... Step: 77695... Loss: 0.000102... Val Loss: 0.000551\n",
      "Epoch: 76/100... Step: 77700... Loss: 0.000042... Val Loss: 0.000534\n",
      "Epoch: 76/100... Step: 77705... Loss: 0.000117... Val Loss: 0.000709\n",
      "Epoch: 76/100... Step: 77710... Loss: 0.000158... Val Loss: 0.000659\n",
      "Epoch: 76/100... Step: 77715... Loss: 0.000136... Val Loss: 0.000532\n",
      "Epoch: 76/100... Step: 77720... Loss: 0.000076... Val Loss: 0.000631\n",
      "Epoch: 76/100... Step: 77725... Loss: 0.000095... Val Loss: 0.000595\n",
      "Epoch: 76/100... Step: 77730... Loss: 0.000074... Val Loss: 0.000371\n",
      "Epoch: 76/100... Step: 77735... Loss: 0.000073... Val Loss: 0.000516\n",
      "Epoch: 76/100... Step: 77740... Loss: 0.000131... Val Loss: 0.000561\n",
      "Epoch: 76/100... Step: 77745... Loss: 0.000218... Val Loss: 0.000637\n",
      "Epoch: 76/100... Step: 77750... Loss: 0.000121... Val Loss: 0.000272\n",
      "Epoch: 76/100... Step: 77755... Loss: 0.000218... Val Loss: 0.000541\n",
      "Epoch: 76/100... Step: 77760... Loss: 0.000146... Val Loss: 0.000771\n",
      "Epoch: 76/100... Step: 77765... Loss: 0.000119... Val Loss: 0.000958\n",
      "Epoch: 76/100... Step: 77770... Loss: 0.000131... Val Loss: 0.000968\n",
      "Epoch: 76/100... Step: 77775... Loss: 0.000094... Val Loss: 0.000994\n",
      "Epoch: 76/100... Step: 77780... Loss: 0.000107... Val Loss: 0.001084\n",
      "Epoch: 76/100... Step: 77785... Loss: 0.000094... Val Loss: 0.000989\n",
      "Epoch: 76/100... Step: 77790... Loss: 0.000098... Val Loss: 0.001005\n",
      "Epoch: 76/100... Step: 77795... Loss: 0.000109... Val Loss: 0.001025\n",
      "Epoch: 76/100... Step: 77800... Loss: 0.000102... Val Loss: 0.001107\n",
      "Epoch: 76/100... Step: 77805... Loss: 0.000305... Val Loss: 0.001006\n",
      "Epoch: 76/100... Step: 77810... Loss: 0.000200... Val Loss: 0.000334\n",
      "Epoch: 76/100... Step: 77815... Loss: 0.000686... Val Loss: 0.000373\n",
      "Epoch: 76/100... Step: 77820... Loss: 0.000330... Val Loss: 0.001775\n",
      "Epoch: 76/100... Step: 77825... Loss: 0.000205... Val Loss: 0.000935\n",
      "Epoch: 76/100... Step: 77830... Loss: 0.000428... Val Loss: 0.000839\n",
      "Epoch: 76/100... Step: 77835... Loss: 0.000342... Val Loss: 0.001529\n",
      "Epoch: 76/100... Step: 77840... Loss: 0.000225... Val Loss: 0.000931\n",
      "Epoch: 76/100... Step: 77845... Loss: 0.000201... Val Loss: 0.001390\n",
      "Epoch: 76/100... Step: 77850... Loss: 0.000196... Val Loss: 0.001095\n",
      "Epoch: 76/100... Step: 77855... Loss: 0.000290... Val Loss: 0.001570\n",
      "Epoch: 76/100... Step: 77860... Loss: 0.000212... Val Loss: 0.001241\n",
      "Epoch: 76/100... Step: 77865... Loss: 0.000182... Val Loss: 0.001539\n",
      "Epoch: 76/100... Step: 77870... Loss: 0.000155... Val Loss: 0.001182\n",
      "Epoch: 76/100... Step: 77875... Loss: 0.000089... Val Loss: 0.001102\n",
      "Epoch: 76/100... Step: 77880... Loss: 0.000088... Val Loss: 0.001256\n",
      "Epoch: 76/100... Step: 77885... Loss: 0.000191... Val Loss: 0.001122\n",
      "Epoch: 76/100... Step: 77890... Loss: 0.000186... Val Loss: 0.000936\n",
      "Epoch: 76/100... Step: 77895... Loss: 0.000128... Val Loss: 0.000865\n",
      "Epoch: 76/100... Step: 77900... Loss: 0.000116... Val Loss: 0.000677\n",
      "Epoch: 76/100... Step: 77905... Loss: 0.000188... Val Loss: 0.000725\n",
      "Epoch: 76/100... Step: 77910... Loss: 0.000116... Val Loss: 0.000300\n",
      "Epoch: 76/100... Step: 77915... Loss: 0.000236... Val Loss: 0.000349\n",
      "Epoch: 76/100... Step: 77920... Loss: 0.000110... Val Loss: 0.000604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76/100... Step: 77925... Loss: 0.000096... Val Loss: 0.000637\n",
      "Epoch: 76/100... Step: 77930... Loss: 0.000090... Val Loss: 0.000700\n",
      "Epoch: 76/100... Step: 77935... Loss: 0.000095... Val Loss: 0.000599\n",
      "Epoch: 76/100... Step: 77940... Loss: 0.000097... Val Loss: 0.000651\n",
      "Epoch: 76/100... Step: 77945... Loss: 0.000145... Val Loss: 0.000878\n",
      "Epoch: 76/100... Step: 77950... Loss: 0.000157... Val Loss: 0.000720\n",
      "Epoch: 76/100... Step: 77955... Loss: 0.000108... Val Loss: 0.000811\n",
      "Epoch: 76/100... Step: 77960... Loss: 0.000126... Val Loss: 0.000863\n",
      "Epoch: 76/100... Step: 77965... Loss: 0.000123... Val Loss: 0.000856\n",
      "Epoch: 76/100... Step: 77970... Loss: 0.000101... Val Loss: 0.000786\n",
      "Epoch: 76/100... Step: 77975... Loss: 0.000112... Val Loss: 0.000958\n",
      "Epoch: 76/100... Step: 77980... Loss: 0.000094... Val Loss: 0.000819\n",
      "Epoch: 76/100... Step: 77985... Loss: 0.000057... Val Loss: 0.000871\n",
      "Epoch: 76/100... Step: 77990... Loss: 0.000112... Val Loss: 0.000872\n",
      "Epoch: 76/100... Step: 77995... Loss: 0.000143... Val Loss: 0.000823\n",
      "Epoch: 76/100... Step: 78000... Loss: 0.000084... Val Loss: 0.000989\n",
      "Epoch: 76/100... Step: 78005... Loss: 0.000061... Val Loss: 0.001001\n",
      "Epoch: 76/100... Step: 78010... Loss: 0.000097... Val Loss: 0.000915\n",
      "Epoch: 76/100... Step: 78015... Loss: 0.000071... Val Loss: 0.000909\n",
      "Epoch: 76/100... Step: 78020... Loss: 0.000075... Val Loss: 0.000812\n",
      "Epoch: 76/100... Step: 78025... Loss: 0.000053... Val Loss: 0.000862\n",
      "Epoch: 76/100... Step: 78030... Loss: 0.000029... Val Loss: 0.000838\n",
      "Epoch: 76/100... Step: 78035... Loss: 0.000081... Val Loss: 0.000818\n",
      "Epoch: 76/100... Step: 78040... Loss: 0.000134... Val Loss: 0.000855\n",
      "Epoch: 76/100... Step: 78045... Loss: 0.000115... Val Loss: 0.000544\n",
      "Epoch: 76/100... Step: 78050... Loss: 0.000166... Val Loss: 0.000830\n",
      "Epoch: 76/100... Step: 78055... Loss: 0.000052... Val Loss: 0.000862\n",
      "Epoch: 76/100... Step: 78060... Loss: 0.000079... Val Loss: 0.000876\n",
      "Epoch: 76/100... Step: 78065... Loss: 0.000125... Val Loss: 0.000558\n",
      "Epoch: 76/100... Step: 78070... Loss: 0.000068... Val Loss: 0.000337\n",
      "Epoch: 76/100... Step: 78075... Loss: 0.000071... Val Loss: 0.000473\n",
      "Epoch: 76/100... Step: 78080... Loss: 0.000161... Val Loss: 0.000378\n",
      "Epoch: 76/100... Step: 78085... Loss: 0.000122... Val Loss: 0.000260\n",
      "Epoch: 76/100... Step: 78090... Loss: 0.000079... Val Loss: 0.000404\n",
      "Epoch: 76/100... Step: 78095... Loss: 0.000153... Val Loss: 0.000322\n",
      "Epoch: 76/100... Step: 78100... Loss: 0.000068... Val Loss: 0.000279\n",
      "Epoch: 76/100... Step: 78105... Loss: 0.000114... Val Loss: 0.000345\n",
      "Epoch: 76/100... Step: 78110... Loss: 0.000087... Val Loss: 0.000373\n",
      "Epoch: 76/100... Step: 78115... Loss: 0.000072... Val Loss: 0.000372\n",
      "Epoch: 76/100... Step: 78120... Loss: 0.000044... Val Loss: 0.000428\n",
      "Epoch: 76/100... Step: 78125... Loss: 0.000047... Val Loss: 0.000406\n",
      "Epoch: 76/100... Step: 78130... Loss: 0.000065... Val Loss: 0.000387\n",
      "Epoch: 76/100... Step: 78135... Loss: 0.000091... Val Loss: 0.000338\n",
      "Epoch: 76/100... Step: 78140... Loss: 0.000225... Val Loss: 0.000262\n",
      "Epoch: 76/100... Step: 78145... Loss: 0.000187... Val Loss: 0.000390\n",
      "Epoch: 76/100... Step: 78150... Loss: 0.000085... Val Loss: 0.000349\n",
      "Epoch: 76/100... Step: 78155... Loss: 0.000045... Val Loss: 0.000327\n",
      "Epoch: 76/100... Step: 78160... Loss: 0.000080... Val Loss: 0.000386\n",
      "Epoch: 76/100... Step: 78165... Loss: 0.000072... Val Loss: 0.000340\n",
      "Epoch: 76/100... Step: 78170... Loss: 0.000072... Val Loss: 0.000413\n",
      "Epoch: 76/100... Step: 78175... Loss: 0.000033... Val Loss: 0.000389\n",
      "Epoch: 76/100... Step: 78180... Loss: 0.000132... Val Loss: 0.000526\n",
      "Epoch: 76/100... Step: 78185... Loss: 0.000044... Val Loss: 0.000508\n",
      "Epoch: 76/100... Step: 78190... Loss: 0.000092... Val Loss: 0.000498\n",
      "Epoch: 76/100... Step: 78195... Loss: 0.000048... Val Loss: 0.000366\n",
      "Epoch: 76/100... Step: 78200... Loss: 0.000068... Val Loss: 0.000386\n",
      "Epoch: 76/100... Step: 78205... Loss: 0.000144... Val Loss: 0.000609\n",
      "Epoch: 76/100... Step: 78210... Loss: 0.000105... Val Loss: 0.000777\n",
      "Epoch: 76/100... Step: 78215... Loss: 0.000054... Val Loss: 0.000683\n",
      "Epoch: 76/100... Step: 78220... Loss: 0.000045... Val Loss: 0.000623\n",
      "Epoch: 76/100... Step: 78225... Loss: 0.000030... Val Loss: 0.000601\n",
      "Epoch: 76/100... Step: 78230... Loss: 0.000025... Val Loss: 0.000622\n",
      "Epoch: 76/100... Step: 78235... Loss: 0.000072... Val Loss: 0.000612\n",
      "Epoch: 76/100... Step: 78240... Loss: 0.000038... Val Loss: 0.000578\n",
      "Epoch: 76/100... Step: 78245... Loss: 0.000038... Val Loss: 0.000592\n",
      "Epoch: 76/100... Step: 78250... Loss: 0.000025... Val Loss: 0.000617\n",
      "Epoch: 76/100... Step: 78255... Loss: 0.000079... Val Loss: 0.000633\n",
      "Epoch: 76/100... Step: 78260... Loss: 0.000058... Val Loss: 0.000765\n",
      "Epoch: 76/100... Step: 78265... Loss: 0.000029... Val Loss: 0.000782\n",
      "Epoch: 76/100... Step: 78270... Loss: 0.000021... Val Loss: 0.000786\n",
      "Epoch: 76/100... Step: 78275... Loss: 0.000056... Val Loss: 0.000767\n",
      "Epoch: 76/100... Step: 78280... Loss: 0.000047... Val Loss: 0.000874\n",
      "Epoch: 76/100... Step: 78285... Loss: 0.000062... Val Loss: 0.000718\n",
      "Epoch: 76/100... Step: 78290... Loss: 0.000057... Val Loss: 0.000709\n",
      "Epoch: 76/100... Step: 78295... Loss: 0.000046... Val Loss: 0.000722\n",
      "Epoch: 76/100... Step: 78300... Loss: 0.000050... Val Loss: 0.000686\n",
      "Epoch: 76/100... Step: 78305... Loss: 0.000086... Val Loss: 0.000617\n",
      "Epoch: 76/100... Step: 78310... Loss: 0.000052... Val Loss: 0.000570\n",
      "Epoch: 76/100... Step: 78315... Loss: 0.000060... Val Loss: 0.000588\n",
      "Epoch: 76/100... Step: 78320... Loss: 0.000109... Val Loss: 0.000666\n",
      "Epoch: 76/100... Step: 78325... Loss: 0.000047... Val Loss: 0.000770\n",
      "Epoch: 76/100... Step: 78330... Loss: 0.000057... Val Loss: 0.000814\n",
      "Epoch: 76/100... Step: 78335... Loss: 0.000036... Val Loss: 0.000842\n",
      "Epoch: 76/100... Step: 78340... Loss: 0.000040... Val Loss: 0.000928\n",
      "Epoch: 76/100... Step: 78345... Loss: 0.000092... Val Loss: 0.000783\n",
      "Epoch: 76/100... Step: 78350... Loss: 0.000102... Val Loss: 0.000779\n",
      "Epoch: 76/100... Step: 78355... Loss: 0.000061... Val Loss: 0.000992\n",
      "Epoch: 76/100... Step: 78360... Loss: 0.000024... Val Loss: 0.001030\n",
      "Epoch: 76/100... Step: 78365... Loss: 0.000030... Val Loss: 0.000980\n",
      "Epoch: 76/100... Step: 78370... Loss: 0.000056... Val Loss: 0.000986\n",
      "Epoch: 76/100... Step: 78375... Loss: 0.000060... Val Loss: 0.000942\n",
      "Epoch: 76/100... Step: 78380... Loss: 0.000049... Val Loss: 0.000846\n",
      "Epoch: 76/100... Step: 78385... Loss: 0.000086... Val Loss: 0.001101\n",
      "Epoch: 76/100... Step: 78390... Loss: 0.000072... Val Loss: 0.000993\n",
      "Epoch: 76/100... Step: 78395... Loss: 0.000030... Val Loss: 0.001003\n",
      "Epoch: 76/100... Step: 78400... Loss: 0.000083... Val Loss: 0.001072\n",
      "Epoch: 76/100... Step: 78405... Loss: 0.000063... Val Loss: 0.000894\n",
      "Epoch: 76/100... Step: 78410... Loss: 0.000050... Val Loss: 0.000839\n",
      "Epoch: 76/100... Step: 78415... Loss: 0.000045... Val Loss: 0.001016\n",
      "Epoch: 76/100... Step: 78420... Loss: 0.000056... Val Loss: 0.001039\n",
      "Epoch: 76/100... Step: 78425... Loss: 0.000053... Val Loss: 0.001062\n",
      "Epoch: 76/100... Step: 78430... Loss: 0.000049... Val Loss: 0.001027\n",
      "Epoch: 77/100... Step: 78435... Loss: 0.000214... Val Loss: 0.001229\n",
      "Epoch: 77/100... Step: 78440... Loss: 0.000181... Val Loss: 0.001623\n",
      "Epoch: 77/100... Step: 78445... Loss: 0.000159... Val Loss: 0.001430\n",
      "Epoch: 77/100... Step: 78450... Loss: 0.000181... Val Loss: 0.001902\n",
      "Epoch: 77/100... Step: 78455... Loss: 0.000192... Val Loss: 0.001770\n",
      "Epoch: 77/100... Step: 78460... Loss: 0.000229... Val Loss: 0.001585\n",
      "Epoch: 77/100... Step: 78465... Loss: 0.000144... Val Loss: 0.002038\n",
      "Epoch: 77/100... Step: 78470... Loss: 0.000145... Val Loss: 0.001584\n",
      "Epoch: 77/100... Step: 78475... Loss: 0.000202... Val Loss: 0.001644\n",
      "Epoch: 77/100... Step: 78480... Loss: 0.000154... Val Loss: 0.001901\n",
      "Epoch: 77/100... Step: 78485... Loss: 0.000187... Val Loss: 0.001592\n",
      "Epoch: 77/100... Step: 78490... Loss: 0.000173... Val Loss: 0.001814\n",
      "Epoch: 77/100... Step: 78495... Loss: 0.000106... Val Loss: 0.001486\n",
      "Epoch: 77/100... Step: 78500... Loss: 0.000136... Val Loss: 0.001504\n",
      "Epoch: 77/100... Step: 78505... Loss: 0.000126... Val Loss: 0.001057\n",
      "Epoch: 77/100... Step: 78510... Loss: 0.000138... Val Loss: 0.001267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77/100... Step: 78515... Loss: 0.000060... Val Loss: 0.001323\n",
      "Epoch: 77/100... Step: 78520... Loss: 0.000064... Val Loss: 0.001405\n",
      "Epoch: 77/100... Step: 78525... Loss: 0.000078... Val Loss: 0.001272\n",
      "Epoch: 77/100... Step: 78530... Loss: 0.000168... Val Loss: 0.001087\n",
      "Epoch: 77/100... Step: 78535... Loss: 0.000128... Val Loss: 0.001092\n",
      "Epoch: 77/100... Step: 78540... Loss: 0.000042... Val Loss: 0.001098\n",
      "Epoch: 77/100... Step: 78545... Loss: 0.000074... Val Loss: 0.001228\n",
      "Epoch: 77/100... Step: 78550... Loss: 0.000065... Val Loss: 0.001095\n",
      "Epoch: 77/100... Step: 78555... Loss: 0.000079... Val Loss: 0.001210\n",
      "Epoch: 77/100... Step: 78560... Loss: 0.000063... Val Loss: 0.001342\n",
      "Epoch: 77/100... Step: 78565... Loss: 0.000052... Val Loss: 0.001394\n",
      "Epoch: 77/100... Step: 78570... Loss: 0.000040... Val Loss: 0.001466\n",
      "Epoch: 77/100... Step: 78575... Loss: 0.000152... Val Loss: 0.001473\n",
      "Epoch: 77/100... Step: 78580... Loss: 0.000159... Val Loss: 0.001800\n",
      "Epoch: 77/100... Step: 78585... Loss: 0.000074... Val Loss: 0.001585\n",
      "Epoch: 77/100... Step: 78590... Loss: 0.000080... Val Loss: 0.001509\n",
      "Epoch: 77/100... Step: 78595... Loss: 0.000054... Val Loss: 0.001454\n",
      "Epoch: 77/100... Step: 78600... Loss: 0.000043... Val Loss: 0.001276\n",
      "Epoch: 77/100... Step: 78605... Loss: 0.000081... Val Loss: 0.001461\n",
      "Epoch: 77/100... Step: 78610... Loss: 0.000056... Val Loss: 0.001260\n",
      "Epoch: 77/100... Step: 78615... Loss: 0.000105... Val Loss: 0.001357\n",
      "Epoch: 77/100... Step: 78620... Loss: 0.000087... Val Loss: 0.001085\n",
      "Epoch: 77/100... Step: 78625... Loss: 0.000030... Val Loss: 0.001123\n",
      "Epoch: 77/100... Step: 78630... Loss: 0.000049... Val Loss: 0.001187\n",
      "Epoch: 77/100... Step: 78635... Loss: 0.000050... Val Loss: 0.001211\n",
      "Epoch: 77/100... Step: 78640... Loss: 0.000042... Val Loss: 0.001200\n",
      "Epoch: 77/100... Step: 78645... Loss: 0.000029... Val Loss: 0.001241\n",
      "Epoch: 77/100... Step: 78650... Loss: 0.000050... Val Loss: 0.001288\n",
      "Epoch: 77/100... Step: 78655... Loss: 0.000046... Val Loss: 0.001157\n",
      "Epoch: 77/100... Step: 78660... Loss: 0.000108... Val Loss: 0.001209\n",
      "Epoch: 77/100... Step: 78665... Loss: 0.000062... Val Loss: 0.000990\n",
      "Epoch: 77/100... Step: 78670... Loss: 0.000081... Val Loss: 0.001173\n",
      "Epoch: 77/100... Step: 78675... Loss: 0.000067... Val Loss: 0.001161\n",
      "Epoch: 77/100... Step: 78680... Loss: 0.000069... Val Loss: 0.001078\n",
      "Epoch: 77/100... Step: 78685... Loss: 0.000049... Val Loss: 0.001220\n",
      "Epoch: 77/100... Step: 78690... Loss: 0.000088... Val Loss: 0.001077\n",
      "Epoch: 77/100... Step: 78695... Loss: 0.000065... Val Loss: 0.000869\n",
      "Epoch: 77/100... Step: 78700... Loss: 0.000064... Val Loss: 0.000963\n",
      "Epoch: 77/100... Step: 78705... Loss: 0.000042... Val Loss: 0.000868\n",
      "Epoch: 77/100... Step: 78710... Loss: 0.000042... Val Loss: 0.000897\n",
      "Epoch: 77/100... Step: 78715... Loss: 0.000036... Val Loss: 0.000959\n",
      "Epoch: 77/100... Step: 78720... Loss: 0.000128... Val Loss: 0.000864\n",
      "Epoch: 77/100... Step: 78725... Loss: 0.000041... Val Loss: 0.000553\n",
      "Epoch: 77/100... Step: 78730... Loss: 0.000060... Val Loss: 0.000620\n",
      "Epoch: 77/100... Step: 78735... Loss: 0.000120... Val Loss: 0.000577\n",
      "Epoch: 77/100... Step: 78740... Loss: 0.000144... Val Loss: 0.000805\n",
      "Epoch: 77/100... Step: 78745... Loss: 0.000049... Val Loss: 0.000465\n",
      "Epoch: 77/100... Step: 78750... Loss: 0.000042... Val Loss: 0.000691\n",
      "Epoch: 77/100... Step: 78755... Loss: 0.000082... Val Loss: 0.000587\n",
      "Epoch: 77/100... Step: 78760... Loss: 0.000072... Val Loss: 0.000418\n",
      "Epoch: 77/100... Step: 78765... Loss: 0.000032... Val Loss: 0.000451\n",
      "Epoch: 77/100... Step: 78770... Loss: 0.000103... Val Loss: 0.000525\n",
      "Epoch: 77/100... Step: 78775... Loss: 0.000156... Val Loss: 0.000695\n",
      "Epoch: 77/100... Step: 78780... Loss: 0.000160... Val Loss: 0.000376\n",
      "Epoch: 77/100... Step: 78785... Loss: 0.000287... Val Loss: 0.000306\n",
      "Epoch: 77/100... Step: 78790... Loss: 0.000079... Val Loss: 0.000847\n",
      "Epoch: 77/100... Step: 78795... Loss: 0.000116... Val Loss: 0.000747\n",
      "Epoch: 77/100... Step: 78800... Loss: 0.000082... Val Loss: 0.001100\n",
      "Epoch: 77/100... Step: 78805... Loss: 0.000077... Val Loss: 0.000873\n",
      "Epoch: 77/100... Step: 78810... Loss: 0.000079... Val Loss: 0.001113\n",
      "Epoch: 77/100... Step: 78815... Loss: 0.000096... Val Loss: 0.001061\n",
      "Epoch: 77/100... Step: 78820... Loss: 0.000059... Val Loss: 0.000929\n",
      "Epoch: 77/100... Step: 78825... Loss: 0.000050... Val Loss: 0.001062\n",
      "Epoch: 77/100... Step: 78830... Loss: 0.000080... Val Loss: 0.001115\n",
      "Epoch: 77/100... Step: 78835... Loss: 0.000142... Val Loss: 0.001034\n",
      "Epoch: 77/100... Step: 78840... Loss: 0.000428... Val Loss: 0.000829\n",
      "Epoch: 77/100... Step: 78845... Loss: 0.000548... Val Loss: 0.000277\n",
      "Epoch: 77/100... Step: 78850... Loss: 0.000265... Val Loss: 0.001322\n",
      "Epoch: 77/100... Step: 78855... Loss: 0.000265... Val Loss: 0.001037\n",
      "Epoch: 77/100... Step: 78860... Loss: 0.000345... Val Loss: 0.000861\n",
      "Epoch: 77/100... Step: 78865... Loss: 0.000399... Val Loss: 0.001321\n",
      "Epoch: 77/100... Step: 78870... Loss: 0.000235... Val Loss: 0.001121\n",
      "Epoch: 77/100... Step: 78875... Loss: 0.000143... Val Loss: 0.001229\n",
      "Epoch: 77/100... Step: 78880... Loss: 0.000148... Val Loss: 0.001255\n",
      "Epoch: 77/100... Step: 78885... Loss: 0.000103... Val Loss: 0.001357\n",
      "Epoch: 77/100... Step: 78890... Loss: 0.000103... Val Loss: 0.001381\n",
      "Epoch: 77/100... Step: 78895... Loss: 0.000122... Val Loss: 0.001492\n",
      "Epoch: 77/100... Step: 78900... Loss: 0.000137... Val Loss: 0.001442\n",
      "Epoch: 77/100... Step: 78905... Loss: 0.000094... Val Loss: 0.001032\n",
      "Epoch: 77/100... Step: 78910... Loss: 0.000085... Val Loss: 0.001258\n",
      "Epoch: 77/100... Step: 78915... Loss: 0.000115... Val Loss: 0.001128\n",
      "Epoch: 77/100... Step: 78920... Loss: 0.000123... Val Loss: 0.000943\n",
      "Epoch: 77/100... Step: 78925... Loss: 0.000113... Val Loss: 0.000960\n",
      "Epoch: 77/100... Step: 78930... Loss: 0.000117... Val Loss: 0.000706\n",
      "Epoch: 77/100... Step: 78935... Loss: 0.000126... Val Loss: 0.000724\n",
      "Epoch: 77/100... Step: 78940... Loss: 0.000246... Val Loss: 0.000549\n",
      "Epoch: 77/100... Step: 78945... Loss: 0.000313... Val Loss: 0.000257\n",
      "Epoch: 77/100... Step: 78950... Loss: 0.000166... Val Loss: 0.000619\n",
      "Epoch: 77/100... Step: 78955... Loss: 0.000141... Val Loss: 0.000428\n",
      "Epoch: 77/100... Step: 78960... Loss: 0.000142... Val Loss: 0.000816\n",
      "Epoch: 77/100... Step: 78965... Loss: 0.000081... Val Loss: 0.000610\n",
      "Epoch: 77/100... Step: 78970... Loss: 0.000072... Val Loss: 0.000642\n",
      "Epoch: 77/100... Step: 78975... Loss: 0.000167... Val Loss: 0.000752\n",
      "Epoch: 77/100... Step: 78980... Loss: 0.000090... Val Loss: 0.000886\n",
      "Epoch: 77/100... Step: 78985... Loss: 0.000076... Val Loss: 0.000907\n",
      "Epoch: 77/100... Step: 78990... Loss: 0.000122... Val Loss: 0.000646\n",
      "Epoch: 77/100... Step: 78995... Loss: 0.000090... Val Loss: 0.000811\n",
      "Epoch: 77/100... Step: 79000... Loss: 0.000070... Val Loss: 0.000830\n",
      "Epoch: 77/100... Step: 79005... Loss: 0.000079... Val Loss: 0.000873\n",
      "Epoch: 77/100... Step: 79010... Loss: 0.000111... Val Loss: 0.000915\n",
      "Epoch: 77/100... Step: 79015... Loss: 0.000070... Val Loss: 0.000828\n",
      "Epoch: 77/100... Step: 79020... Loss: 0.000027... Val Loss: 0.000821\n",
      "Epoch: 77/100... Step: 79025... Loss: 0.000074... Val Loss: 0.000945\n",
      "Epoch: 77/100... Step: 79030... Loss: 0.000081... Val Loss: 0.000856\n",
      "Epoch: 77/100... Step: 79035... Loss: 0.000056... Val Loss: 0.000977\n",
      "Epoch: 77/100... Step: 79040... Loss: 0.000043... Val Loss: 0.000955\n",
      "Epoch: 77/100... Step: 79045... Loss: 0.000048... Val Loss: 0.000856\n",
      "Epoch: 77/100... Step: 79050... Loss: 0.000056... Val Loss: 0.000887\n",
      "Epoch: 77/100... Step: 79055... Loss: 0.000041... Val Loss: 0.000920\n",
      "Epoch: 77/100... Step: 79060... Loss: 0.000028... Val Loss: 0.000916\n",
      "Epoch: 77/100... Step: 79065... Loss: 0.000046... Val Loss: 0.000870\n",
      "Epoch: 77/100... Step: 79070... Loss: 0.000096... Val Loss: 0.000742\n",
      "Epoch: 77/100... Step: 79075... Loss: 0.000109... Val Loss: 0.000733\n",
      "Epoch: 77/100... Step: 79080... Loss: 0.000173... Val Loss: 0.000519\n",
      "Epoch: 77/100... Step: 79085... Loss: 0.000086... Val Loss: 0.000963\n",
      "Epoch: 77/100... Step: 79090... Loss: 0.000091... Val Loss: 0.000838\n",
      "Epoch: 77/100... Step: 79095... Loss: 0.000089... Val Loss: 0.000694\n",
      "Epoch: 77/100... Step: 79100... Loss: 0.000179... Val Loss: 0.000524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77/100... Step: 79105... Loss: 0.000139... Val Loss: 0.000312\n",
      "Epoch: 77/100... Step: 79110... Loss: 0.000132... Val Loss: 0.000398\n",
      "Epoch: 77/100... Step: 79115... Loss: 0.000105... Val Loss: 0.000262\n",
      "Epoch: 77/100... Step: 79120... Loss: 0.000101... Val Loss: 0.000292\n",
      "Epoch: 77/100... Step: 79125... Loss: 0.000064... Val Loss: 0.000315\n",
      "Epoch: 77/100... Step: 79130... Loss: 0.000044... Val Loss: 0.000334\n",
      "Epoch: 77/100... Step: 79135... Loss: 0.000055... Val Loss: 0.000288\n",
      "Epoch: 77/100... Step: 79140... Loss: 0.000041... Val Loss: 0.000316\n",
      "Epoch: 77/100... Step: 79145... Loss: 0.000037... Val Loss: 0.000418\n",
      "Epoch: 77/100... Step: 79150... Loss: 0.000059... Val Loss: 0.000386\n",
      "Epoch: 77/100... Step: 79155... Loss: 0.000079... Val Loss: 0.000443\n",
      "Epoch: 77/100... Step: 79160... Loss: 0.000081... Val Loss: 0.000342\n",
      "Epoch: 77/100... Step: 79165... Loss: 0.000085... Val Loss: 0.000403\n",
      "Epoch: 77/100... Step: 79170... Loss: 0.000177... Val Loss: 0.000275\n",
      "Epoch: 77/100... Step: 79175... Loss: 0.000136... Val Loss: 0.000333\n",
      "Epoch: 77/100... Step: 79180... Loss: 0.000183... Val Loss: 0.000492\n",
      "Epoch: 77/100... Step: 79185... Loss: 0.000092... Val Loss: 0.000340\n",
      "Epoch: 77/100... Step: 79190... Loss: 0.000078... Val Loss: 0.000380\n",
      "Epoch: 77/100... Step: 79195... Loss: 0.000055... Val Loss: 0.000353\n",
      "Epoch: 77/100... Step: 79200... Loss: 0.000053... Val Loss: 0.000414\n",
      "Epoch: 77/100... Step: 79205... Loss: 0.000032... Val Loss: 0.000425\n",
      "Epoch: 77/100... Step: 79210... Loss: 0.000089... Val Loss: 0.000446\n",
      "Epoch: 77/100... Step: 79215... Loss: 0.000041... Val Loss: 0.000610\n",
      "Epoch: 77/100... Step: 79220... Loss: 0.000077... Val Loss: 0.000510\n",
      "Epoch: 77/100... Step: 79225... Loss: 0.000126... Val Loss: 0.000457\n",
      "Epoch: 77/100... Step: 79230... Loss: 0.000081... Val Loss: 0.000352\n",
      "Epoch: 77/100... Step: 79235... Loss: 0.000116... Val Loss: 0.000525\n",
      "Epoch: 77/100... Step: 79240... Loss: 0.000090... Val Loss: 0.000735\n",
      "Epoch: 77/100... Step: 79245... Loss: 0.000057... Val Loss: 0.000652\n",
      "Epoch: 77/100... Step: 79250... Loss: 0.000059... Val Loss: 0.000704\n",
      "Epoch: 77/100... Step: 79255... Loss: 0.000063... Val Loss: 0.000593\n",
      "Epoch: 77/100... Step: 79260... Loss: 0.000030... Val Loss: 0.000593\n",
      "Epoch: 77/100... Step: 79265... Loss: 0.000052... Val Loss: 0.000633\n",
      "Epoch: 77/100... Step: 79270... Loss: 0.000057... Val Loss: 0.000609\n",
      "Epoch: 77/100... Step: 79275... Loss: 0.000030... Val Loss: 0.000585\n",
      "Epoch: 77/100... Step: 79280... Loss: 0.000028... Val Loss: 0.000617\n",
      "Epoch: 77/100... Step: 79285... Loss: 0.000039... Val Loss: 0.000647\n",
      "Epoch: 77/100... Step: 79290... Loss: 0.000035... Val Loss: 0.000732\n",
      "Epoch: 77/100... Step: 79295... Loss: 0.000054... Val Loss: 0.000755\n",
      "Epoch: 77/100... Step: 79300... Loss: 0.000018... Val Loss: 0.000719\n",
      "Epoch: 77/100... Step: 79305... Loss: 0.000020... Val Loss: 0.000745\n",
      "Epoch: 77/100... Step: 79310... Loss: 0.000038... Val Loss: 0.000788\n",
      "Epoch: 77/100... Step: 79315... Loss: 0.000067... Val Loss: 0.000830\n",
      "Epoch: 77/100... Step: 79320... Loss: 0.000060... Val Loss: 0.000630\n",
      "Epoch: 77/100... Step: 79325... Loss: 0.000074... Val Loss: 0.000771\n",
      "Epoch: 77/100... Step: 79330... Loss: 0.000055... Val Loss: 0.000660\n",
      "Epoch: 77/100... Step: 79335... Loss: 0.000044... Val Loss: 0.000653\n",
      "Epoch: 77/100... Step: 79340... Loss: 0.000029... Val Loss: 0.000577\n",
      "Epoch: 77/100... Step: 79345... Loss: 0.000041... Val Loss: 0.000571\n",
      "Epoch: 77/100... Step: 79350... Loss: 0.000050... Val Loss: 0.000665\n",
      "Epoch: 77/100... Step: 79355... Loss: 0.000045... Val Loss: 0.000680\n",
      "Epoch: 77/100... Step: 79360... Loss: 0.000041... Val Loss: 0.000773\n",
      "Epoch: 77/100... Step: 79365... Loss: 0.000051... Val Loss: 0.000847\n",
      "Epoch: 77/100... Step: 79370... Loss: 0.000041... Val Loss: 0.000869\n",
      "Epoch: 77/100... Step: 79375... Loss: 0.000051... Val Loss: 0.000810\n",
      "Epoch: 77/100... Step: 79380... Loss: 0.000043... Val Loss: 0.000778\n",
      "Epoch: 77/100... Step: 79385... Loss: 0.000079... Val Loss: 0.000897\n",
      "Epoch: 77/100... Step: 79390... Loss: 0.000047... Val Loss: 0.000978\n",
      "Epoch: 77/100... Step: 79395... Loss: 0.000042... Val Loss: 0.000989\n",
      "Epoch: 77/100... Step: 79400... Loss: 0.000028... Val Loss: 0.000978\n",
      "Epoch: 77/100... Step: 79405... Loss: 0.000034... Val Loss: 0.000978\n",
      "Epoch: 77/100... Step: 79410... Loss: 0.000034... Val Loss: 0.000937\n",
      "Epoch: 77/100... Step: 79415... Loss: 0.000092... Val Loss: 0.000909\n",
      "Epoch: 77/100... Step: 79420... Loss: 0.000038... Val Loss: 0.001143\n",
      "Epoch: 77/100... Step: 79425... Loss: 0.000073... Val Loss: 0.000983\n",
      "Epoch: 77/100... Step: 79430... Loss: 0.000066... Val Loss: 0.001051\n",
      "Epoch: 77/100... Step: 79435... Loss: 0.000047... Val Loss: 0.000978\n",
      "Epoch: 77/100... Step: 79440... Loss: 0.000049... Val Loss: 0.000823\n",
      "Epoch: 77/100... Step: 79445... Loss: 0.000079... Val Loss: 0.000936\n",
      "Epoch: 77/100... Step: 79450... Loss: 0.000050... Val Loss: 0.000995\n",
      "Epoch: 77/100... Step: 79455... Loss: 0.000055... Val Loss: 0.001081\n",
      "Epoch: 77/100... Step: 79460... Loss: 0.000020... Val Loss: 0.000936\n",
      "Epoch: 78/100... Step: 79465... Loss: 0.000320... Val Loss: 0.001009\n",
      "Epoch: 78/100... Step: 79470... Loss: 0.000103... Val Loss: 0.001601\n",
      "Epoch: 78/100... Step: 79475... Loss: 0.000034... Val Loss: 0.001407\n",
      "Epoch: 78/100... Step: 79480... Loss: 0.000156... Val Loss: 0.001682\n",
      "Epoch: 78/100... Step: 79485... Loss: 0.000239... Val Loss: 0.001931\n",
      "Epoch: 78/100... Step: 79490... Loss: 0.000092... Val Loss: 0.001601\n",
      "Epoch: 78/100... Step: 79495... Loss: 0.000180... Val Loss: 0.001793\n",
      "Epoch: 78/100... Step: 79500... Loss: 0.000283... Val Loss: 0.001999\n",
      "Epoch: 78/100... Step: 79505... Loss: 0.000173... Val Loss: 0.001409\n",
      "Epoch: 78/100... Step: 79510... Loss: 0.000088... Val Loss: 0.001851\n",
      "Epoch: 78/100... Step: 79515... Loss: 0.000171... Val Loss: 0.001787\n",
      "Epoch: 78/100... Step: 79520... Loss: 0.000140... Val Loss: 0.001653\n",
      "Epoch: 78/100... Step: 79525... Loss: 0.000160... Val Loss: 0.001756\n",
      "Epoch: 78/100... Step: 79530... Loss: 0.000182... Val Loss: 0.001428\n",
      "Epoch: 78/100... Step: 79535... Loss: 0.000290... Val Loss: 0.001502\n",
      "Epoch: 78/100... Step: 79540... Loss: 0.000214... Val Loss: 0.000899\n",
      "Epoch: 78/100... Step: 79545... Loss: 0.000142... Val Loss: 0.001373\n",
      "Epoch: 78/100... Step: 79550... Loss: 0.000141... Val Loss: 0.001405\n",
      "Epoch: 78/100... Step: 79555... Loss: 0.000095... Val Loss: 0.001287\n",
      "Epoch: 78/100... Step: 79560... Loss: 0.000183... Val Loss: 0.001203\n",
      "Epoch: 78/100... Step: 79565... Loss: 0.000210... Val Loss: 0.000911\n",
      "Epoch: 78/100... Step: 79570... Loss: 0.000186... Val Loss: 0.001297\n",
      "Epoch: 78/100... Step: 79575... Loss: 0.000119... Val Loss: 0.001080\n",
      "Epoch: 78/100... Step: 79580... Loss: 0.000125... Val Loss: 0.001158\n",
      "Epoch: 78/100... Step: 79585... Loss: 0.000116... Val Loss: 0.001128\n",
      "Epoch: 78/100... Step: 79590... Loss: 0.000059... Val Loss: 0.001244\n",
      "Epoch: 78/100... Step: 79595... Loss: 0.000037... Val Loss: 0.001446\n",
      "Epoch: 78/100... Step: 79600... Loss: 0.000058... Val Loss: 0.001426\n",
      "Epoch: 78/100... Step: 79605... Loss: 0.000129... Val Loss: 0.001413\n",
      "Epoch: 78/100... Step: 79610... Loss: 0.000048... Val Loss: 0.001827\n",
      "Epoch: 78/100... Step: 79615... Loss: 0.000067... Val Loss: 0.001620\n",
      "Epoch: 78/100... Step: 79620... Loss: 0.000086... Val Loss: 0.001673\n",
      "Epoch: 78/100... Step: 79625... Loss: 0.000068... Val Loss: 0.001381\n",
      "Epoch: 78/100... Step: 79630... Loss: 0.000103... Val Loss: 0.001448\n",
      "Epoch: 78/100... Step: 79635... Loss: 0.000094... Val Loss: 0.001313\n",
      "Epoch: 78/100... Step: 79640... Loss: 0.000049... Val Loss: 0.001371\n",
      "Epoch: 78/100... Step: 79645... Loss: 0.000069... Val Loss: 0.001323\n",
      "Epoch: 78/100... Step: 79650... Loss: 0.000093... Val Loss: 0.001238\n",
      "Epoch: 78/100... Step: 79655... Loss: 0.000052... Val Loss: 0.001136\n",
      "Epoch: 78/100... Step: 79660... Loss: 0.000063... Val Loss: 0.001127\n",
      "Epoch: 78/100... Step: 79665... Loss: 0.000030... Val Loss: 0.001112\n",
      "Epoch: 78/100... Step: 79670... Loss: 0.000071... Val Loss: 0.001271\n",
      "Epoch: 78/100... Step: 79675... Loss: 0.000042... Val Loss: 0.001293\n",
      "Epoch: 78/100... Step: 79680... Loss: 0.000059... Val Loss: 0.001202\n",
      "Epoch: 78/100... Step: 79685... Loss: 0.000042... Val Loss: 0.001199\n",
      "Epoch: 78/100... Step: 79690... Loss: 0.000056... Val Loss: 0.001234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78/100... Step: 79695... Loss: 0.000132... Val Loss: 0.001090\n",
      "Epoch: 78/100... Step: 79700... Loss: 0.000065... Val Loss: 0.001071\n",
      "Epoch: 78/100... Step: 79705... Loss: 0.000063... Val Loss: 0.001221\n",
      "Epoch: 78/100... Step: 79710... Loss: 0.000077... Val Loss: 0.001054\n",
      "Epoch: 78/100... Step: 79715... Loss: 0.000053... Val Loss: 0.001184\n",
      "Epoch: 78/100... Step: 79720... Loss: 0.000068... Val Loss: 0.001104\n",
      "Epoch: 78/100... Step: 79725... Loss: 0.000062... Val Loss: 0.000953\n",
      "Epoch: 78/100... Step: 79730... Loss: 0.000037... Val Loss: 0.000917\n",
      "Epoch: 78/100... Step: 79735... Loss: 0.000044... Val Loss: 0.000886\n",
      "Epoch: 78/100... Step: 79740... Loss: 0.000033... Val Loss: 0.000908\n",
      "Epoch: 78/100... Step: 79745... Loss: 0.000058... Val Loss: 0.000934\n",
      "Epoch: 78/100... Step: 79750... Loss: 0.000081... Val Loss: 0.000901\n",
      "Epoch: 78/100... Step: 79755... Loss: 0.000087... Val Loss: 0.000676\n",
      "Epoch: 78/100... Step: 79760... Loss: 0.000047... Val Loss: 0.000626\n",
      "Epoch: 78/100... Step: 79765... Loss: 0.000076... Val Loss: 0.000539\n",
      "Epoch: 78/100... Step: 79770... Loss: 0.000124... Val Loss: 0.000679\n",
      "Epoch: 78/100... Step: 79775... Loss: 0.000194... Val Loss: 0.000725\n",
      "Epoch: 78/100... Step: 79780... Loss: 0.000173... Val Loss: 0.000444\n",
      "Epoch: 78/100... Step: 79785... Loss: 0.000098... Val Loss: 0.000770\n",
      "Epoch: 78/100... Step: 79790... Loss: 0.000103... Val Loss: 0.000502\n",
      "Epoch: 78/100... Step: 79795... Loss: 0.000072... Val Loss: 0.000443\n",
      "Epoch: 78/100... Step: 79800... Loss: 0.000076... Val Loss: 0.000441\n",
      "Epoch: 78/100... Step: 79805... Loss: 0.000059... Val Loss: 0.000708\n",
      "Epoch: 78/100... Step: 79810... Loss: 0.000223... Val Loss: 0.000540\n",
      "Epoch: 78/100... Step: 79815... Loss: 0.000192... Val Loss: 0.000270\n",
      "Epoch: 78/100... Step: 79820... Loss: 0.000241... Val Loss: 0.000550\n",
      "Epoch: 78/100... Step: 79825... Loss: 0.000127... Val Loss: 0.000863\n",
      "Epoch: 78/100... Step: 79830... Loss: 0.000127... Val Loss: 0.000900\n",
      "Epoch: 78/100... Step: 79835... Loss: 0.000103... Val Loss: 0.001052\n",
      "Epoch: 78/100... Step: 79840... Loss: 0.000102... Val Loss: 0.001006\n",
      "Epoch: 78/100... Step: 79845... Loss: 0.000094... Val Loss: 0.001040\n",
      "Epoch: 78/100... Step: 79850... Loss: 0.000097... Val Loss: 0.001020\n",
      "Epoch: 78/100... Step: 79855... Loss: 0.000059... Val Loss: 0.000964\n",
      "Epoch: 78/100... Step: 79860... Loss: 0.000062... Val Loss: 0.001042\n",
      "Epoch: 78/100... Step: 79865... Loss: 0.000038... Val Loss: 0.001120\n",
      "Epoch: 78/100... Step: 79870... Loss: 0.000384... Val Loss: 0.001002\n",
      "Epoch: 78/100... Step: 79875... Loss: 0.000311... Val Loss: 0.000437\n",
      "Epoch: 78/100... Step: 79880... Loss: 0.000663... Val Loss: 0.000714\n",
      "Epoch: 78/100... Step: 79885... Loss: 0.000680... Val Loss: 0.001693\n",
      "Epoch: 78/100... Step: 79890... Loss: 0.000273... Val Loss: 0.000626\n",
      "Epoch: 78/100... Step: 79895... Loss: 0.000184... Val Loss: 0.001360\n",
      "Epoch: 78/100... Step: 79900... Loss: 0.000199... Val Loss: 0.001089\n",
      "Epoch: 78/100... Step: 79905... Loss: 0.000181... Val Loss: 0.001245\n",
      "Epoch: 78/100... Step: 79910... Loss: 0.000095... Val Loss: 0.001121\n",
      "Epoch: 78/100... Step: 79915... Loss: 0.000133... Val Loss: 0.001406\n",
      "Epoch: 78/100... Step: 79920... Loss: 0.000113... Val Loss: 0.001281\n",
      "Epoch: 78/100... Step: 79925... Loss: 0.000106... Val Loss: 0.001427\n",
      "Epoch: 78/100... Step: 79930... Loss: 0.000162... Val Loss: 0.001487\n",
      "Epoch: 78/100... Step: 79935... Loss: 0.000215... Val Loss: 0.001170\n",
      "Epoch: 78/100... Step: 79940... Loss: 0.000120... Val Loss: 0.001128\n",
      "Epoch: 78/100... Step: 79945... Loss: 0.000118... Val Loss: 0.001170\n",
      "Epoch: 78/100... Step: 79950... Loss: 0.000189... Val Loss: 0.001036\n",
      "Epoch: 78/100... Step: 79955... Loss: 0.000154... Val Loss: 0.000857\n",
      "Epoch: 78/100... Step: 79960... Loss: 0.000129... Val Loss: 0.000978\n",
      "Epoch: 78/100... Step: 79965... Loss: 0.000091... Val Loss: 0.000659\n",
      "Epoch: 78/100... Step: 79970... Loss: 0.000206... Val Loss: 0.000451\n",
      "Epoch: 78/100... Step: 79975... Loss: 0.000209... Val Loss: 0.000270\n",
      "Epoch: 78/100... Step: 79980... Loss: 0.000139... Val Loss: 0.000523\n",
      "Epoch: 78/100... Step: 79985... Loss: 0.000120... Val Loss: 0.000400\n",
      "Epoch: 78/100... Step: 79990... Loss: 0.000106... Val Loss: 0.000701\n",
      "Epoch: 78/100... Step: 79995... Loss: 0.000076... Val Loss: 0.000710\n",
      "Epoch: 78/100... Step: 80000... Loss: 0.000068... Val Loss: 0.000587\n",
      "Epoch: 78/100... Step: 80005... Loss: 0.000079... Val Loss: 0.000699\n",
      "Epoch: 78/100... Step: 80010... Loss: 0.000095... Val Loss: 0.000907\n",
      "Epoch: 78/100... Step: 80015... Loss: 0.000081... Val Loss: 0.000929\n",
      "Epoch: 78/100... Step: 80020... Loss: 0.000082... Val Loss: 0.000674\n",
      "Epoch: 78/100... Step: 80025... Loss: 0.000082... Val Loss: 0.000761\n",
      "Epoch: 78/100... Step: 80030... Loss: 0.000050... Val Loss: 0.000827\n",
      "Epoch: 78/100... Step: 80035... Loss: 0.000067... Val Loss: 0.000787\n",
      "Epoch: 78/100... Step: 80040... Loss: 0.000163... Val Loss: 0.000976\n",
      "Epoch: 78/100... Step: 80045... Loss: 0.000129... Val Loss: 0.000765\n",
      "Epoch: 78/100... Step: 80050... Loss: 0.000083... Val Loss: 0.000877\n",
      "Epoch: 78/100... Step: 80055... Loss: 0.000072... Val Loss: 0.000922\n",
      "Epoch: 78/100... Step: 80060... Loss: 0.000118... Val Loss: 0.000949\n",
      "Epoch: 78/100... Step: 80065... Loss: 0.000060... Val Loss: 0.000986\n",
      "Epoch: 78/100... Step: 80070... Loss: 0.000047... Val Loss: 0.000931\n",
      "Epoch: 78/100... Step: 80075... Loss: 0.000078... Val Loss: 0.000856\n",
      "Epoch: 78/100... Step: 80080... Loss: 0.000040... Val Loss: 0.000924\n",
      "Epoch: 78/100... Step: 80085... Loss: 0.000064... Val Loss: 0.000866\n",
      "Epoch: 78/100... Step: 80090... Loss: 0.000049... Val Loss: 0.000922\n",
      "Epoch: 78/100... Step: 80095... Loss: 0.000043... Val Loss: 0.000968\n",
      "Epoch: 78/100... Step: 80100... Loss: 0.000059... Val Loss: 0.000931\n",
      "Epoch: 78/100... Step: 80105... Loss: 0.000088... Val Loss: 0.000697\n",
      "Epoch: 78/100... Step: 80110... Loss: 0.000087... Val Loss: 0.000613\n",
      "Epoch: 78/100... Step: 80115... Loss: 0.000186... Val Loss: 0.000617\n",
      "Epoch: 78/100... Step: 80120... Loss: 0.000105... Val Loss: 0.001025\n",
      "Epoch: 78/100... Step: 80125... Loss: 0.000073... Val Loss: 0.000717\n",
      "Epoch: 78/100... Step: 80130... Loss: 0.000130... Val Loss: 0.000596\n",
      "Epoch: 78/100... Step: 80135... Loss: 0.000066... Val Loss: 0.000359\n",
      "Epoch: 78/100... Step: 80140... Loss: 0.000057... Val Loss: 0.000403\n",
      "Epoch: 78/100... Step: 80145... Loss: 0.000170... Val Loss: 0.000312\n",
      "Epoch: 78/100... Step: 80150... Loss: 0.000170... Val Loss: 0.000263\n",
      "Epoch: 78/100... Step: 80155... Loss: 0.000118... Val Loss: 0.000474\n",
      "Epoch: 78/100... Step: 80160... Loss: 0.000039... Val Loss: 0.000281\n",
      "Epoch: 78/100... Step: 80165... Loss: 0.000052... Val Loss: 0.000356\n",
      "Epoch: 78/100... Step: 80170... Loss: 0.000060... Val Loss: 0.000276\n",
      "Epoch: 78/100... Step: 80175... Loss: 0.000079... Val Loss: 0.000424\n",
      "Epoch: 78/100... Step: 80180... Loss: 0.000057... Val Loss: 0.000406\n",
      "Epoch: 78/100... Step: 80185... Loss: 0.000060... Val Loss: 0.000426\n",
      "Epoch: 78/100... Step: 80190... Loss: 0.000062... Val Loss: 0.000379\n",
      "Epoch: 78/100... Step: 80195... Loss: 0.000088... Val Loss: 0.000384\n",
      "Epoch: 78/100... Step: 80200... Loss: 0.000187... Val Loss: 0.000417\n",
      "Epoch: 78/100... Step: 80205... Loss: 0.000230... Val Loss: 0.000273\n",
      "Epoch: 78/100... Step: 80210... Loss: 0.000136... Val Loss: 0.000698\n",
      "Epoch: 78/100... Step: 80215... Loss: 0.000097... Val Loss: 0.000632\n",
      "Epoch: 78/100... Step: 80220... Loss: 0.000121... Val Loss: 0.000287\n",
      "Epoch: 78/100... Step: 80225... Loss: 0.000078... Val Loss: 0.000410\n",
      "Epoch: 78/100... Step: 80230... Loss: 0.000125... Val Loss: 0.000368\n",
      "Epoch: 78/100... Step: 80235... Loss: 0.000069... Val Loss: 0.000416\n",
      "Epoch: 78/100... Step: 80240... Loss: 0.000058... Val Loss: 0.000449\n",
      "Epoch: 78/100... Step: 80245... Loss: 0.000132... Val Loss: 0.000476\n",
      "Epoch: 78/100... Step: 80250... Loss: 0.000120... Val Loss: 0.000624\n",
      "Epoch: 78/100... Step: 80255... Loss: 0.000100... Val Loss: 0.000407\n",
      "Epoch: 78/100... Step: 80260... Loss: 0.000041... Val Loss: 0.000377\n",
      "Epoch: 78/100... Step: 80265... Loss: 0.000147... Val Loss: 0.000430\n",
      "Epoch: 78/100... Step: 80270... Loss: 0.000126... Val Loss: 0.000702\n",
      "Epoch: 78/100... Step: 80275... Loss: 0.000087... Val Loss: 0.000754\n",
      "Epoch: 78/100... Step: 80280... Loss: 0.000048... Val Loss: 0.000613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78/100... Step: 80285... Loss: 0.000046... Val Loss: 0.000622\n",
      "Epoch: 78/100... Step: 80290... Loss: 0.000028... Val Loss: 0.000666\n",
      "Epoch: 78/100... Step: 80295... Loss: 0.000053... Val Loss: 0.000587\n",
      "Epoch: 78/100... Step: 80300... Loss: 0.000088... Val Loss: 0.000618\n",
      "Epoch: 78/100... Step: 80305... Loss: 0.000043... Val Loss: 0.000615\n",
      "Epoch: 78/100... Step: 80310... Loss: 0.000059... Val Loss: 0.000593\n",
      "Epoch: 78/100... Step: 80315... Loss: 0.000029... Val Loss: 0.000582\n",
      "Epoch: 78/100... Step: 80320... Loss: 0.000054... Val Loss: 0.000723\n",
      "Epoch: 78/100... Step: 80325... Loss: 0.000073... Val Loss: 0.000763\n",
      "Epoch: 78/100... Step: 80330... Loss: 0.000032... Val Loss: 0.000740\n",
      "Epoch: 78/100... Step: 80335... Loss: 0.000021... Val Loss: 0.000745\n",
      "Epoch: 78/100... Step: 80340... Loss: 0.000031... Val Loss: 0.000763\n",
      "Epoch: 78/100... Step: 80345... Loss: 0.000042... Val Loss: 0.000821\n",
      "Epoch: 78/100... Step: 80350... Loss: 0.000071... Val Loss: 0.000713\n",
      "Epoch: 78/100... Step: 80355... Loss: 0.000060... Val Loss: 0.000687\n",
      "Epoch: 78/100... Step: 80360... Loss: 0.000062... Val Loss: 0.000714\n",
      "Epoch: 78/100... Step: 80365... Loss: 0.000029... Val Loss: 0.000705\n",
      "Epoch: 78/100... Step: 80370... Loss: 0.000047... Val Loss: 0.000576\n",
      "Epoch: 78/100... Step: 80375... Loss: 0.000043... Val Loss: 0.000593\n",
      "Epoch: 78/100... Step: 80380... Loss: 0.000060... Val Loss: 0.000596\n",
      "Epoch: 78/100... Step: 80385... Loss: 0.000037... Val Loss: 0.000669\n",
      "Epoch: 78/100... Step: 80390... Loss: 0.000047... Val Loss: 0.000769\n",
      "Epoch: 78/100... Step: 80395... Loss: 0.000049... Val Loss: 0.000791\n",
      "Epoch: 78/100... Step: 80400... Loss: 0.000040... Val Loss: 0.000897\n",
      "Epoch: 78/100... Step: 80405... Loss: 0.000056... Val Loss: 0.000851\n",
      "Epoch: 78/100... Step: 80410... Loss: 0.000048... Val Loss: 0.000759\n",
      "Epoch: 78/100... Step: 80415... Loss: 0.000082... Val Loss: 0.000833\n",
      "Epoch: 78/100... Step: 80420... Loss: 0.000051... Val Loss: 0.001001\n",
      "Epoch: 78/100... Step: 80425... Loss: 0.000026... Val Loss: 0.000993\n",
      "Epoch: 78/100... Step: 80430... Loss: 0.000025... Val Loss: 0.000965\n",
      "Epoch: 78/100... Step: 80435... Loss: 0.000024... Val Loss: 0.001003\n",
      "Epoch: 78/100... Step: 80440... Loss: 0.000040... Val Loss: 0.000943\n",
      "Epoch: 78/100... Step: 80445... Loss: 0.000052... Val Loss: 0.000884\n",
      "Epoch: 78/100... Step: 80450... Loss: 0.000108... Val Loss: 0.001002\n",
      "Epoch: 78/100... Step: 80455... Loss: 0.000095... Val Loss: 0.001103\n",
      "Epoch: 78/100... Step: 80460... Loss: 0.000079... Val Loss: 0.001020\n",
      "Epoch: 78/100... Step: 80465... Loss: 0.000088... Val Loss: 0.001035\n",
      "Epoch: 78/100... Step: 80470... Loss: 0.000067... Val Loss: 0.000862\n",
      "Epoch: 78/100... Step: 80475... Loss: 0.000064... Val Loss: 0.000892\n",
      "Epoch: 78/100... Step: 80480... Loss: 0.000061... Val Loss: 0.000998\n",
      "Epoch: 78/100... Step: 80485... Loss: 0.000070... Val Loss: 0.001080\n",
      "Epoch: 78/100... Step: 80490... Loss: 0.000039... Val Loss: 0.000985\n",
      "Epoch: 78/100... Step: 80495... Loss: 0.000055... Val Loss: 0.000980\n",
      "Epoch: 79/100... Step: 80500... Loss: 0.000209... Val Loss: 0.001264\n",
      "Epoch: 79/100... Step: 80505... Loss: 0.000193... Val Loss: 0.001681\n",
      "Epoch: 79/100... Step: 80510... Loss: 0.000159... Val Loss: 0.001418\n",
      "Epoch: 79/100... Step: 80515... Loss: 0.000130... Val Loss: 0.001831\n",
      "Epoch: 79/100... Step: 80520... Loss: 0.000281... Val Loss: 0.001946\n",
      "Epoch: 79/100... Step: 80525... Loss: 0.000230... Val Loss: 0.001570\n",
      "Epoch: 79/100... Step: 80530... Loss: 0.000077... Val Loss: 0.001988\n",
      "Epoch: 79/100... Step: 80535... Loss: 0.000273... Val Loss: 0.001819\n",
      "Epoch: 79/100... Step: 80540... Loss: 0.000354... Val Loss: 0.001390\n",
      "Epoch: 79/100... Step: 80545... Loss: 0.000059... Val Loss: 0.001933\n",
      "Epoch: 79/100... Step: 80550... Loss: 0.000099... Val Loss: 0.001710\n",
      "Epoch: 79/100... Step: 80555... Loss: 0.000132... Val Loss: 0.001653\n",
      "Epoch: 79/100... Step: 80560... Loss: 0.000104... Val Loss: 0.001562\n",
      "Epoch: 79/100... Step: 80565... Loss: 0.000175... Val Loss: 0.001436\n",
      "Epoch: 79/100... Step: 80570... Loss: 0.000246... Val Loss: 0.001275\n",
      "Epoch: 79/100... Step: 80575... Loss: 0.000318... Val Loss: 0.000990\n",
      "Epoch: 79/100... Step: 80580... Loss: 0.000145... Val Loss: 0.001562\n",
      "Epoch: 79/100... Step: 80585... Loss: 0.000088... Val Loss: 0.001186\n",
      "Epoch: 79/100... Step: 80590... Loss: 0.000112... Val Loss: 0.001345\n",
      "Epoch: 79/100... Step: 80595... Loss: 0.000182... Val Loss: 0.001049\n",
      "Epoch: 79/100... Step: 80600... Loss: 0.000154... Val Loss: 0.001023\n",
      "Epoch: 79/100... Step: 80605... Loss: 0.000124... Val Loss: 0.001224\n",
      "Epoch: 79/100... Step: 80610... Loss: 0.000168... Val Loss: 0.001097\n",
      "Epoch: 79/100... Step: 80615... Loss: 0.000073... Val Loss: 0.001155\n",
      "Epoch: 79/100... Step: 80620... Loss: 0.000146... Val Loss: 0.001216\n",
      "Epoch: 79/100... Step: 80625... Loss: 0.000071... Val Loss: 0.001350\n",
      "Epoch: 79/100... Step: 80630... Loss: 0.000030... Val Loss: 0.001458\n",
      "Epoch: 79/100... Step: 80635... Loss: 0.000072... Val Loss: 0.001539\n",
      "Epoch: 79/100... Step: 80640... Loss: 0.000150... Val Loss: 0.001554\n",
      "Epoch: 79/100... Step: 80645... Loss: 0.000091... Val Loss: 0.001669\n",
      "Epoch: 79/100... Step: 80650... Loss: 0.000056... Val Loss: 0.001670\n",
      "Epoch: 79/100... Step: 80655... Loss: 0.000127... Val Loss: 0.001493\n",
      "Epoch: 79/100... Step: 80660... Loss: 0.000070... Val Loss: 0.001454\n",
      "Epoch: 79/100... Step: 80665... Loss: 0.000065... Val Loss: 0.001308\n",
      "Epoch: 79/100... Step: 80670... Loss: 0.000122... Val Loss: 0.001428\n",
      "Epoch: 79/100... Step: 80675... Loss: 0.000060... Val Loss: 0.001320\n",
      "Epoch: 79/100... Step: 80680... Loss: 0.000075... Val Loss: 0.001277\n",
      "Epoch: 79/100... Step: 80685... Loss: 0.000069... Val Loss: 0.001108\n",
      "Epoch: 79/100... Step: 80690... Loss: 0.000020... Val Loss: 0.001156\n",
      "Epoch: 79/100... Step: 80695... Loss: 0.000079... Val Loss: 0.001187\n",
      "Epoch: 79/100... Step: 80700... Loss: 0.000061... Val Loss: 0.001181\n",
      "Epoch: 79/100... Step: 80705... Loss: 0.000043... Val Loss: 0.001242\n",
      "Epoch: 79/100... Step: 80710... Loss: 0.000024... Val Loss: 0.001326\n",
      "Epoch: 79/100... Step: 80715... Loss: 0.000050... Val Loss: 0.001247\n",
      "Epoch: 79/100... Step: 80720... Loss: 0.000061... Val Loss: 0.001157\n",
      "Epoch: 79/100... Step: 80725... Loss: 0.000113... Val Loss: 0.001237\n",
      "Epoch: 79/100... Step: 80730... Loss: 0.000035... Val Loss: 0.000991\n",
      "Epoch: 79/100... Step: 80735... Loss: 0.000123... Val Loss: 0.001163\n",
      "Epoch: 79/100... Step: 80740... Loss: 0.000117... Val Loss: 0.001216\n",
      "Epoch: 79/100... Step: 80745... Loss: 0.000116... Val Loss: 0.001028\n",
      "Epoch: 79/100... Step: 80750... Loss: 0.000088... Val Loss: 0.001267\n",
      "Epoch: 79/100... Step: 80755... Loss: 0.000095... Val Loss: 0.001006\n",
      "Epoch: 79/100... Step: 80760... Loss: 0.000081... Val Loss: 0.000887\n",
      "Epoch: 79/100... Step: 80765... Loss: 0.000046... Val Loss: 0.000911\n",
      "Epoch: 79/100... Step: 80770... Loss: 0.000027... Val Loss: 0.000873\n",
      "Epoch: 79/100... Step: 80775... Loss: 0.000031... Val Loss: 0.000910\n",
      "Epoch: 79/100... Step: 80780... Loss: 0.000051... Val Loss: 0.000955\n",
      "Epoch: 79/100... Step: 80785... Loss: 0.000128... Val Loss: 0.000805\n",
      "Epoch: 79/100... Step: 80790... Loss: 0.000073... Val Loss: 0.000566\n",
      "Epoch: 79/100... Step: 80795... Loss: 0.000044... Val Loss: 0.000570\n",
      "Epoch: 79/100... Step: 80800... Loss: 0.000124... Val Loss: 0.000610\n",
      "Epoch: 79/100... Step: 80805... Loss: 0.000162... Val Loss: 0.000782\n",
      "Epoch: 79/100... Step: 80810... Loss: 0.000058... Val Loss: 0.000459\n",
      "Epoch: 79/100... Step: 80815... Loss: 0.000101... Val Loss: 0.000626\n",
      "Epoch: 79/100... Step: 80820... Loss: 0.000195... Val Loss: 0.000696\n",
      "Epoch: 79/100... Step: 80825... Loss: 0.000101... Val Loss: 0.000360\n",
      "Epoch: 79/100... Step: 80830... Loss: 0.000060... Val Loss: 0.000529\n",
      "Epoch: 79/100... Step: 80835... Loss: 0.000134... Val Loss: 0.000494\n",
      "Epoch: 79/100... Step: 80840... Loss: 0.000206... Val Loss: 0.000749\n",
      "Epoch: 79/100... Step: 80845... Loss: 0.000200... Val Loss: 0.000399\n",
      "Epoch: 79/100... Step: 80850... Loss: 0.000421... Val Loss: 0.000278\n",
      "Epoch: 79/100... Step: 80855... Loss: 0.000073... Val Loss: 0.000941\n",
      "Epoch: 79/100... Step: 80860... Loss: 0.000137... Val Loss: 0.000824\n",
      "Epoch: 79/100... Step: 80865... Loss: 0.000093... Val Loss: 0.001084\n",
      "Epoch: 79/100... Step: 80870... Loss: 0.000072... Val Loss: 0.000862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79/100... Step: 80875... Loss: 0.000079... Val Loss: 0.001130\n",
      "Epoch: 79/100... Step: 80880... Loss: 0.000098... Val Loss: 0.000993\n",
      "Epoch: 79/100... Step: 80885... Loss: 0.000063... Val Loss: 0.000927\n",
      "Epoch: 79/100... Step: 80890... Loss: 0.000052... Val Loss: 0.001030\n",
      "Epoch: 79/100... Step: 80895... Loss: 0.000065... Val Loss: 0.001086\n",
      "Epoch: 79/100... Step: 80900... Loss: 0.000227... Val Loss: 0.001062\n",
      "Epoch: 79/100... Step: 80905... Loss: 0.000353... Val Loss: 0.000664\n",
      "Epoch: 79/100... Step: 80910... Loss: 0.000666... Val Loss: 0.000304\n",
      "Epoch: 79/100... Step: 80915... Loss: 0.000150... Val Loss: 0.001443\n",
      "Epoch: 79/100... Step: 80920... Loss: 0.000372... Val Loss: 0.000978\n",
      "Epoch: 79/100... Step: 80925... Loss: 0.000283... Val Loss: 0.000979\n",
      "Epoch: 79/100... Step: 80930... Loss: 0.000277... Val Loss: 0.001381\n",
      "Epoch: 79/100... Step: 80935... Loss: 0.000239... Val Loss: 0.001046\n",
      "Epoch: 79/100... Step: 80940... Loss: 0.000157... Val Loss: 0.001190\n",
      "Epoch: 79/100... Step: 80945... Loss: 0.000292... Val Loss: 0.001076\n",
      "Epoch: 79/100... Step: 80950... Loss: 0.000205... Val Loss: 0.001697\n",
      "Epoch: 79/100... Step: 80955... Loss: 0.000238... Val Loss: 0.001214\n",
      "Epoch: 79/100... Step: 80960... Loss: 0.000168... Val Loss: 0.001486\n",
      "Epoch: 79/100... Step: 80965... Loss: 0.000245... Val Loss: 0.001394\n",
      "Epoch: 79/100... Step: 80970... Loss: 0.000264... Val Loss: 0.000937\n",
      "Epoch: 79/100... Step: 80975... Loss: 0.000086... Val Loss: 0.001304\n",
      "Epoch: 79/100... Step: 80980... Loss: 0.000195... Val Loss: 0.001124\n",
      "Epoch: 79/100... Step: 80985... Loss: 0.000131... Val Loss: 0.000730\n",
      "Epoch: 79/100... Step: 80990... Loss: 0.000119... Val Loss: 0.000995\n",
      "Epoch: 79/100... Step: 80995... Loss: 0.000134... Val Loss: 0.000787\n",
      "Epoch: 79/100... Step: 81000... Loss: 0.000152... Val Loss: 0.000631\n",
      "Epoch: 79/100... Step: 81005... Loss: 0.000263... Val Loss: 0.000415\n",
      "Epoch: 79/100... Step: 81010... Loss: 0.000230... Val Loss: 0.000262\n",
      "Epoch: 79/100... Step: 81015... Loss: 0.000058... Val Loss: 0.000583\n",
      "Epoch: 79/100... Step: 81020... Loss: 0.000078... Val Loss: 0.000539\n",
      "Epoch: 79/100... Step: 81025... Loss: 0.000088... Val Loss: 0.000805\n",
      "Epoch: 79/100... Step: 81030... Loss: 0.000082... Val Loss: 0.000609\n",
      "Epoch: 79/100... Step: 81035... Loss: 0.000038... Val Loss: 0.000690\n",
      "Epoch: 79/100... Step: 81040... Loss: 0.000137... Val Loss: 0.000744\n",
      "Epoch: 79/100... Step: 81045... Loss: 0.000105... Val Loss: 0.000984\n",
      "Epoch: 79/100... Step: 81050... Loss: 0.000102... Val Loss: 0.000804\n",
      "Epoch: 79/100... Step: 81055... Loss: 0.000143... Val Loss: 0.000794\n",
      "Epoch: 79/100... Step: 81060... Loss: 0.000084... Val Loss: 0.000735\n",
      "Epoch: 79/100... Step: 81065... Loss: 0.000118... Val Loss: 0.000743\n",
      "Epoch: 79/100... Step: 81070... Loss: 0.000074... Val Loss: 0.000977\n",
      "Epoch: 79/100... Step: 81075... Loss: 0.000090... Val Loss: 0.000841\n",
      "Epoch: 79/100... Step: 81080... Loss: 0.000048... Val Loss: 0.000878\n",
      "Epoch: 79/100... Step: 81085... Loss: 0.000102... Val Loss: 0.000827\n",
      "Epoch: 79/100... Step: 81090... Loss: 0.000070... Val Loss: 0.000931\n",
      "Epoch: 79/100... Step: 81095... Loss: 0.000051... Val Loss: 0.000927\n",
      "Epoch: 79/100... Step: 81100... Loss: 0.000043... Val Loss: 0.000966\n",
      "Epoch: 79/100... Step: 81105... Loss: 0.000044... Val Loss: 0.000903\n",
      "Epoch: 79/100... Step: 81110... Loss: 0.000038... Val Loss: 0.000848\n",
      "Epoch: 79/100... Step: 81115... Loss: 0.000064... Val Loss: 0.000918\n",
      "Epoch: 79/100... Step: 81120... Loss: 0.000031... Val Loss: 0.000921\n",
      "Epoch: 79/100... Step: 81125... Loss: 0.000065... Val Loss: 0.000871\n",
      "Epoch: 79/100... Step: 81130... Loss: 0.000051... Val Loss: 0.000929\n",
      "Epoch: 79/100... Step: 81135... Loss: 0.000093... Val Loss: 0.000854\n",
      "Epoch: 79/100... Step: 81140... Loss: 0.000091... Val Loss: 0.000563\n",
      "Epoch: 79/100... Step: 81145... Loss: 0.000134... Val Loss: 0.000641\n",
      "Epoch: 79/100... Step: 81150... Loss: 0.000038... Val Loss: 0.000979\n",
      "Epoch: 79/100... Step: 81155... Loss: 0.000111... Val Loss: 0.000841\n",
      "Epoch: 79/100... Step: 81160... Loss: 0.000111... Val Loss: 0.000616\n",
      "Epoch: 79/100... Step: 81165... Loss: 0.000170... Val Loss: 0.000506\n",
      "Epoch: 79/100... Step: 81170... Loss: 0.000161... Val Loss: 0.000293\n",
      "Epoch: 79/100... Step: 81175... Loss: 0.000142... Val Loss: 0.000440\n",
      "Epoch: 79/100... Step: 81180... Loss: 0.000097... Val Loss: 0.000259\n",
      "Epoch: 79/100... Step: 81185... Loss: 0.000204... Val Loss: 0.000262\n",
      "Epoch: 79/100... Step: 81190... Loss: 0.000161... Val Loss: 0.000448\n",
      "Epoch: 79/100... Step: 81195... Loss: 0.000099... Val Loss: 0.000263\n",
      "Epoch: 79/100... Step: 81200... Loss: 0.000053... Val Loss: 0.000352\n",
      "Epoch: 79/100... Step: 81205... Loss: 0.000071... Val Loss: 0.000300\n",
      "Epoch: 79/100... Step: 81210... Loss: 0.000053... Val Loss: 0.000442\n",
      "Epoch: 79/100... Step: 81215... Loss: 0.000058... Val Loss: 0.000402\n",
      "Epoch: 79/100... Step: 81220... Loss: 0.000058... Val Loss: 0.000412\n",
      "Epoch: 79/100... Step: 81225... Loss: 0.000060... Val Loss: 0.000359\n",
      "Epoch: 79/100... Step: 81230... Loss: 0.000096... Val Loss: 0.000419\n",
      "Epoch: 79/100... Step: 81235... Loss: 0.000209... Val Loss: 0.000262\n",
      "Epoch: 79/100... Step: 81240... Loss: 0.000061... Val Loss: 0.000462\n",
      "Epoch: 79/100... Step: 81245... Loss: 0.000051... Val Loss: 0.000318\n",
      "Epoch: 79/100... Step: 81250... Loss: 0.000051... Val Loss: 0.000347\n",
      "Epoch: 79/100... Step: 81255... Loss: 0.000056... Val Loss: 0.000355\n",
      "Epoch: 79/100... Step: 81260... Loss: 0.000060... Val Loss: 0.000354\n",
      "Epoch: 79/100... Step: 81265... Loss: 0.000041... Val Loss: 0.000409\n",
      "Epoch: 79/100... Step: 81270... Loss: 0.000040... Val Loss: 0.000417\n",
      "Epoch: 79/100... Step: 81275... Loss: 0.000103... Val Loss: 0.000458\n",
      "Epoch: 79/100... Step: 81280... Loss: 0.000062... Val Loss: 0.000608\n",
      "Epoch: 79/100... Step: 81285... Loss: 0.000071... Val Loss: 0.000474\n",
      "Epoch: 79/100... Step: 81290... Loss: 0.000067... Val Loss: 0.000404\n",
      "Epoch: 79/100... Step: 81295... Loss: 0.000060... Val Loss: 0.000366\n",
      "Epoch: 79/100... Step: 81300... Loss: 0.000152... Val Loss: 0.000529\n",
      "Epoch: 79/100... Step: 81305... Loss: 0.000070... Val Loss: 0.000821\n",
      "Epoch: 79/100... Step: 81310... Loss: 0.000059... Val Loss: 0.000645\n",
      "Epoch: 79/100... Step: 81315... Loss: 0.000040... Val Loss: 0.000681\n",
      "Epoch: 79/100... Step: 81320... Loss: 0.000046... Val Loss: 0.000562\n",
      "Epoch: 79/100... Step: 81325... Loss: 0.000052... Val Loss: 0.000651\n",
      "Epoch: 79/100... Step: 81330... Loss: 0.000052... Val Loss: 0.000632\n",
      "Epoch: 79/100... Step: 81335... Loss: 0.000046... Val Loss: 0.000577\n",
      "Epoch: 79/100... Step: 81340... Loss: 0.000039... Val Loss: 0.000627\n",
      "Epoch: 79/100... Step: 81345... Loss: 0.000035... Val Loss: 0.000598\n",
      "Epoch: 79/100... Step: 81350... Loss: 0.000031... Val Loss: 0.000619\n",
      "Epoch: 79/100... Step: 81355... Loss: 0.000040... Val Loss: 0.000751\n",
      "Epoch: 79/100... Step: 81360... Loss: 0.000030... Val Loss: 0.000759\n",
      "Epoch: 79/100... Step: 81365... Loss: 0.000023... Val Loss: 0.000731\n",
      "Epoch: 79/100... Step: 81370... Loss: 0.000031... Val Loss: 0.000763\n",
      "Epoch: 79/100... Step: 81375... Loss: 0.000040... Val Loss: 0.000837\n",
      "Epoch: 79/100... Step: 81380... Loss: 0.000071... Val Loss: 0.000765\n",
      "Epoch: 79/100... Step: 81385... Loss: 0.000049... Val Loss: 0.000655\n",
      "Epoch: 79/100... Step: 81390... Loss: 0.000067... Val Loss: 0.000747\n",
      "Epoch: 79/100... Step: 81395... Loss: 0.000036... Val Loss: 0.000663\n",
      "Epoch: 79/100... Step: 81400... Loss: 0.000059... Val Loss: 0.000656\n",
      "Epoch: 79/100... Step: 81405... Loss: 0.000030... Val Loss: 0.000535\n",
      "Epoch: 79/100... Step: 81410... Loss: 0.000044... Val Loss: 0.000627\n",
      "Epoch: 79/100... Step: 81415... Loss: 0.000039... Val Loss: 0.000652\n",
      "Epoch: 79/100... Step: 81420... Loss: 0.000037... Val Loss: 0.000686\n",
      "Epoch: 79/100... Step: 81425... Loss: 0.000033... Val Loss: 0.000801\n",
      "Epoch: 79/100... Step: 81430... Loss: 0.000049... Val Loss: 0.000834\n",
      "Epoch: 79/100... Step: 81435... Loss: 0.000030... Val Loss: 0.000872\n",
      "Epoch: 79/100... Step: 81440... Loss: 0.000045... Val Loss: 0.000843\n",
      "Epoch: 79/100... Step: 81445... Loss: 0.000050... Val Loss: 0.000738\n",
      "Epoch: 79/100... Step: 81450... Loss: 0.000084... Val Loss: 0.000928\n",
      "Epoch: 79/100... Step: 81455... Loss: 0.000055... Val Loss: 0.000958\n",
      "Epoch: 79/100... Step: 81460... Loss: 0.000032... Val Loss: 0.001004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79/100... Step: 81465... Loss: 0.000045... Val Loss: 0.000929\n",
      "Epoch: 79/100... Step: 81470... Loss: 0.000056... Val Loss: 0.000992\n",
      "Epoch: 79/100... Step: 81475... Loss: 0.000034... Val Loss: 0.000919\n",
      "Epoch: 79/100... Step: 81480... Loss: 0.000100... Val Loss: 0.000930\n",
      "Epoch: 79/100... Step: 81485... Loss: 0.000046... Val Loss: 0.001142\n",
      "Epoch: 79/100... Step: 81490... Loss: 0.000041... Val Loss: 0.000987\n",
      "Epoch: 79/100... Step: 81495... Loss: 0.000061... Val Loss: 0.001098\n",
      "Epoch: 79/100... Step: 81500... Loss: 0.000047... Val Loss: 0.000902\n",
      "Epoch: 79/100... Step: 81505... Loss: 0.000050... Val Loss: 0.000840\n",
      "Epoch: 79/100... Step: 81510... Loss: 0.000088... Val Loss: 0.000911\n",
      "Epoch: 79/100... Step: 81515... Loss: 0.000064... Val Loss: 0.001110\n",
      "Epoch: 79/100... Step: 81520... Loss: 0.000059... Val Loss: 0.001014\n",
      "Epoch: 79/100... Step: 81525... Loss: 0.000037... Val Loss: 0.001023\n",
      "Epoch: 80/100... Step: 81530... Loss: 0.000323... Val Loss: 0.001003\n",
      "Epoch: 80/100... Step: 81535... Loss: 0.000115... Val Loss: 0.001474\n",
      "Epoch: 80/100... Step: 81540... Loss: 0.000195... Val Loss: 0.001684\n",
      "Epoch: 80/100... Step: 81545... Loss: 0.000241... Val Loss: 0.001512\n",
      "Epoch: 80/100... Step: 81550... Loss: 0.000178... Val Loss: 0.001854\n",
      "Epoch: 80/100... Step: 81555... Loss: 0.000134... Val Loss: 0.001791\n",
      "Epoch: 80/100... Step: 81560... Loss: 0.000161... Val Loss: 0.001760\n",
      "Epoch: 80/100... Step: 81565... Loss: 0.000264... Val Loss: 0.001970\n",
      "Epoch: 80/100... Step: 81570... Loss: 0.000141... Val Loss: 0.001615\n",
      "Epoch: 80/100... Step: 81575... Loss: 0.000266... Val Loss: 0.001584\n",
      "Epoch: 80/100... Step: 81580... Loss: 0.000178... Val Loss: 0.001936\n",
      "Epoch: 80/100... Step: 81585... Loss: 0.000057... Val Loss: 0.001605\n",
      "Epoch: 80/100... Step: 81590... Loss: 0.000056... Val Loss: 0.001631\n",
      "Epoch: 80/100... Step: 81595... Loss: 0.000078... Val Loss: 0.001475\n",
      "Epoch: 80/100... Step: 81600... Loss: 0.000220... Val Loss: 0.001336\n",
      "Epoch: 80/100... Step: 81605... Loss: 0.000202... Val Loss: 0.000957\n",
      "Epoch: 80/100... Step: 81610... Loss: 0.000112... Val Loss: 0.001337\n",
      "Epoch: 80/100... Step: 81615... Loss: 0.000219... Val Loss: 0.001525\n",
      "Epoch: 80/100... Step: 81620... Loss: 0.000177... Val Loss: 0.001117\n",
      "Epoch: 80/100... Step: 81625... Loss: 0.000174... Val Loss: 0.001249\n",
      "Epoch: 80/100... Step: 81630... Loss: 0.000103... Val Loss: 0.000934\n",
      "Epoch: 80/100... Step: 81635... Loss: 0.000042... Val Loss: 0.001253\n",
      "Epoch: 80/100... Step: 81640... Loss: 0.000042... Val Loss: 0.001112\n",
      "Epoch: 80/100... Step: 81645... Loss: 0.000047... Val Loss: 0.001209\n",
      "Epoch: 80/100... Step: 81650... Loss: 0.000059... Val Loss: 0.001172\n",
      "Epoch: 80/100... Step: 81655... Loss: 0.000082... Val Loss: 0.001297\n",
      "Epoch: 80/100... Step: 81660... Loss: 0.000062... Val Loss: 0.001424\n",
      "Epoch: 80/100... Step: 81665... Loss: 0.000032... Val Loss: 0.001446\n",
      "Epoch: 80/100... Step: 81670... Loss: 0.000124... Val Loss: 0.001418\n",
      "Epoch: 80/100... Step: 81675... Loss: 0.000108... Val Loss: 0.001753\n",
      "Epoch: 80/100... Step: 81680... Loss: 0.000141... Val Loss: 0.001772\n",
      "Epoch: 80/100... Step: 81685... Loss: 0.000100... Val Loss: 0.001407\n",
      "Epoch: 80/100... Step: 81690... Loss: 0.000081... Val Loss: 0.001523\n",
      "Epoch: 80/100... Step: 81695... Loss: 0.000046... Val Loss: 0.001309\n",
      "Epoch: 80/100... Step: 81700... Loss: 0.000063... Val Loss: 0.001395\n",
      "Epoch: 80/100... Step: 81705... Loss: 0.000059... Val Loss: 0.001323\n",
      "Epoch: 80/100... Step: 81710... Loss: 0.000035... Val Loss: 0.001347\n",
      "Epoch: 80/100... Step: 81715... Loss: 0.000092... Val Loss: 0.001189\n",
      "Epoch: 80/100... Step: 81720... Loss: 0.000041... Val Loss: 0.001162\n",
      "Epoch: 80/100... Step: 81725... Loss: 0.000036... Val Loss: 0.001122\n",
      "Epoch: 80/100... Step: 81730... Loss: 0.000051... Val Loss: 0.001163\n",
      "Epoch: 80/100... Step: 81735... Loss: 0.000048... Val Loss: 0.001226\n",
      "Epoch: 80/100... Step: 81740... Loss: 0.000034... Val Loss: 0.001281\n",
      "Epoch: 80/100... Step: 81745... Loss: 0.000056... Val Loss: 0.001268\n",
      "Epoch: 80/100... Step: 81750... Loss: 0.000048... Val Loss: 0.001193\n",
      "Epoch: 80/100... Step: 81755... Loss: 0.000075... Val Loss: 0.001208\n",
      "Epoch: 80/100... Step: 81760... Loss: 0.000129... Val Loss: 0.001128\n",
      "Epoch: 80/100... Step: 81765... Loss: 0.000113... Val Loss: 0.001005\n",
      "Epoch: 80/100... Step: 81770... Loss: 0.000089... Val Loss: 0.001276\n",
      "Epoch: 80/100... Step: 81775... Loss: 0.000052... Val Loss: 0.001073\n",
      "Epoch: 80/100... Step: 81780... Loss: 0.000092... Val Loss: 0.001149\n",
      "Epoch: 80/100... Step: 81785... Loss: 0.000151... Val Loss: 0.001220\n",
      "Epoch: 80/100... Step: 81790... Loss: 0.000069... Val Loss: 0.000863\n",
      "Epoch: 80/100... Step: 81795... Loss: 0.000054... Val Loss: 0.000960\n",
      "Epoch: 80/100... Step: 81800... Loss: 0.000055... Val Loss: 0.000858\n",
      "Epoch: 80/100... Step: 81805... Loss: 0.000042... Val Loss: 0.000926\n",
      "Epoch: 80/100... Step: 81810... Loss: 0.000039... Val Loss: 0.000917\n",
      "Epoch: 80/100... Step: 81815... Loss: 0.000132... Val Loss: 0.000913\n",
      "Epoch: 80/100... Step: 81820... Loss: 0.000082... Val Loss: 0.000648\n",
      "Epoch: 80/100... Step: 81825... Loss: 0.000060... Val Loss: 0.000582\n",
      "Epoch: 80/100... Step: 81830... Loss: 0.000073... Val Loss: 0.000535\n",
      "Epoch: 80/100... Step: 81835... Loss: 0.000138... Val Loss: 0.000661\n",
      "Epoch: 80/100... Step: 81840... Loss: 0.000217... Val Loss: 0.000735\n",
      "Epoch: 80/100... Step: 81845... Loss: 0.000169... Val Loss: 0.000442\n",
      "Epoch: 80/100... Step: 81850... Loss: 0.000061... Val Loss: 0.000667\n",
      "Epoch: 80/100... Step: 81855... Loss: 0.000262... Val Loss: 0.000680\n",
      "Epoch: 80/100... Step: 81860... Loss: 0.000166... Val Loss: 0.000306\n",
      "Epoch: 80/100... Step: 81865... Loss: 0.000149... Val Loss: 0.000528\n",
      "Epoch: 80/100... Step: 81870... Loss: 0.000190... Val Loss: 0.000726\n",
      "Epoch: 80/100... Step: 81875... Loss: 0.000168... Val Loss: 0.000372\n",
      "Epoch: 80/100... Step: 81880... Loss: 0.000184... Val Loss: 0.000362\n",
      "Epoch: 80/100... Step: 81885... Loss: 0.000150... Val Loss: 0.000704\n",
      "Epoch: 80/100... Step: 81890... Loss: 0.000115... Val Loss: 0.000797\n",
      "Epoch: 80/100... Step: 81895... Loss: 0.000091... Val Loss: 0.001000\n",
      "Epoch: 80/100... Step: 81900... Loss: 0.000042... Val Loss: 0.000930\n",
      "Epoch: 80/100... Step: 81905... Loss: 0.000075... Val Loss: 0.001085\n",
      "Epoch: 80/100... Step: 81910... Loss: 0.000073... Val Loss: 0.001005\n",
      "Epoch: 80/100... Step: 81915... Loss: 0.000091... Val Loss: 0.000997\n",
      "Epoch: 80/100... Step: 81920... Loss: 0.000147... Val Loss: 0.000908\n",
      "Epoch: 80/100... Step: 81925... Loss: 0.000104... Val Loss: 0.001113\n",
      "Epoch: 80/100... Step: 81930... Loss: 0.000114... Val Loss: 0.001082\n",
      "Epoch: 80/100... Step: 81935... Loss: 0.000453... Val Loss: 0.001010\n",
      "Epoch: 80/100... Step: 81940... Loss: 0.000461... Val Loss: 0.000397\n",
      "Epoch: 80/100... Step: 81945... Loss: 0.000623... Val Loss: 0.000838\n",
      "Epoch: 80/100... Step: 81950... Loss: 0.000745... Val Loss: 0.001813\n",
      "Epoch: 80/100... Step: 81955... Loss: 0.000226... Val Loss: 0.000661\n",
      "Epoch: 80/100... Step: 81960... Loss: 0.000397... Val Loss: 0.000963\n",
      "Epoch: 80/100... Step: 81965... Loss: 0.000403... Val Loss: 0.001522\n",
      "Epoch: 80/100... Step: 81970... Loss: 0.000212... Val Loss: 0.000918\n",
      "Epoch: 80/100... Step: 81975... Loss: 0.000151... Val Loss: 0.001312\n",
      "Epoch: 80/100... Step: 81980... Loss: 0.000153... Val Loss: 0.001322\n",
      "Epoch: 80/100... Step: 81985... Loss: 0.000196... Val Loss: 0.001350\n",
      "Epoch: 80/100... Step: 81990... Loss: 0.000086... Val Loss: 0.001433\n",
      "Epoch: 80/100... Step: 81995... Loss: 0.000147... Val Loss: 0.001477\n",
      "Epoch: 80/100... Step: 82000... Loss: 0.000150... Val Loss: 0.001139\n",
      "Epoch: 80/100... Step: 82005... Loss: 0.000191... Val Loss: 0.001019\n",
      "Epoch: 80/100... Step: 82010... Loss: 0.000209... Val Loss: 0.001386\n",
      "Epoch: 80/100... Step: 82015... Loss: 0.000209... Val Loss: 0.000982\n",
      "Epoch: 80/100... Step: 82020... Loss: 0.000219... Val Loss: 0.000733\n",
      "Epoch: 80/100... Step: 82025... Loss: 0.000252... Val Loss: 0.001031\n",
      "Epoch: 80/100... Step: 82030... Loss: 0.000117... Val Loss: 0.000567\n",
      "Epoch: 80/100... Step: 82035... Loss: 0.000127... Val Loss: 0.000332\n",
      "Epoch: 80/100... Step: 82040... Loss: 0.000116... Val Loss: 0.000400\n",
      "Epoch: 80/100... Step: 82045... Loss: 0.000077... Val Loss: 0.000426\n",
      "Epoch: 80/100... Step: 82050... Loss: 0.000077... Val Loss: 0.000564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80/100... Step: 82055... Loss: 0.000044... Val Loss: 0.000678\n",
      "Epoch: 80/100... Step: 82060... Loss: 0.000094... Val Loss: 0.000704\n",
      "Epoch: 80/100... Step: 82065... Loss: 0.000086... Val Loss: 0.000550\n",
      "Epoch: 80/100... Step: 82070... Loss: 0.000102... Val Loss: 0.000742\n",
      "Epoch: 80/100... Step: 82075... Loss: 0.000058... Val Loss: 0.000911\n",
      "Epoch: 80/100... Step: 82080... Loss: 0.000077... Val Loss: 0.000792\n",
      "Epoch: 80/100... Step: 82085... Loss: 0.000105... Val Loss: 0.000765\n",
      "Epoch: 80/100... Step: 82090... Loss: 0.000070... Val Loss: 0.000743\n",
      "Epoch: 80/100... Step: 82095... Loss: 0.000073... Val Loss: 0.000793\n",
      "Epoch: 80/100... Step: 82100... Loss: 0.000105... Val Loss: 0.000885\n",
      "Epoch: 80/100... Step: 82105... Loss: 0.000128... Val Loss: 0.000913\n",
      "Epoch: 80/100... Step: 82110... Loss: 0.000075... Val Loss: 0.000802\n",
      "Epoch: 80/100... Step: 82115... Loss: 0.000032... Val Loss: 0.000912\n",
      "Epoch: 80/100... Step: 82120... Loss: 0.000060... Val Loss: 0.000895\n",
      "Epoch: 80/100... Step: 82125... Loss: 0.000080... Val Loss: 0.000932\n",
      "Epoch: 80/100... Step: 82130... Loss: 0.000076... Val Loss: 0.000904\n",
      "Epoch: 80/100... Step: 82135... Loss: 0.000084... Val Loss: 0.001001\n",
      "Epoch: 80/100... Step: 82140... Loss: 0.000077... Val Loss: 0.000838\n",
      "Epoch: 80/100... Step: 82145... Loss: 0.000038... Val Loss: 0.000851\n",
      "Epoch: 80/100... Step: 82150... Loss: 0.000063... Val Loss: 0.000931\n",
      "Epoch: 80/100... Step: 82155... Loss: 0.000042... Val Loss: 0.000914\n",
      "Epoch: 80/100... Step: 82160... Loss: 0.000027... Val Loss: 0.000880\n",
      "Epoch: 80/100... Step: 82165... Loss: 0.000041... Val Loss: 0.000877\n",
      "Epoch: 80/100... Step: 82170... Loss: 0.000126... Val Loss: 0.000786\n",
      "Epoch: 80/100... Step: 82175... Loss: 0.000109... Val Loss: 0.000483\n",
      "Epoch: 80/100... Step: 82180... Loss: 0.000174... Val Loss: 0.000769\n",
      "Epoch: 80/100... Step: 82185... Loss: 0.000154... Val Loss: 0.001037\n",
      "Epoch: 80/100... Step: 82190... Loss: 0.000075... Val Loss: 0.000667\n",
      "Epoch: 80/100... Step: 82195... Loss: 0.000128... Val Loss: 0.000472\n",
      "Epoch: 80/100... Step: 82200... Loss: 0.000031... Val Loss: 0.000326\n",
      "Epoch: 80/100... Step: 82205... Loss: 0.000060... Val Loss: 0.000442\n",
      "Epoch: 80/100... Step: 82210... Loss: 0.000128... Val Loss: 0.000283\n",
      "Epoch: 80/100... Step: 82215... Loss: 0.000161... Val Loss: 0.000255\n",
      "Epoch: 80/100... Step: 82220... Loss: 0.000112... Val Loss: 0.000418\n",
      "Epoch: 80/100... Step: 82225... Loss: 0.000056... Val Loss: 0.000286\n",
      "Epoch: 80/100... Step: 82230... Loss: 0.000097... Val Loss: 0.000293\n",
      "Epoch: 80/100... Step: 82235... Loss: 0.000077... Val Loss: 0.000356\n",
      "Epoch: 80/100... Step: 82240... Loss: 0.000077... Val Loss: 0.000328\n",
      "Epoch: 80/100... Step: 82245... Loss: 0.000068... Val Loss: 0.000478\n",
      "Epoch: 80/100... Step: 82250... Loss: 0.000036... Val Loss: 0.000401\n",
      "Epoch: 80/100... Step: 82255... Loss: 0.000046... Val Loss: 0.000385\n",
      "Epoch: 80/100... Step: 82260... Loss: 0.000072... Val Loss: 0.000372\n",
      "Epoch: 80/100... Step: 82265... Loss: 0.000250... Val Loss: 0.000393\n",
      "Epoch: 80/100... Step: 82270... Loss: 0.000248... Val Loss: 0.000275\n",
      "Epoch: 80/100... Step: 82275... Loss: 0.000284... Val Loss: 0.000539\n",
      "Epoch: 80/100... Step: 82280... Loss: 0.000059... Val Loss: 0.000278\n",
      "Epoch: 80/100... Step: 82285... Loss: 0.000141... Val Loss: 0.000349\n",
      "Epoch: 80/100... Step: 82290... Loss: 0.000164... Val Loss: 0.000435\n",
      "Epoch: 80/100... Step: 82295... Loss: 0.000164... Val Loss: 0.000285\n",
      "Epoch: 80/100... Step: 82300... Loss: 0.000037... Val Loss: 0.000493\n",
      "Epoch: 80/100... Step: 82305... Loss: 0.000093... Val Loss: 0.000428\n",
      "Epoch: 80/100... Step: 82310... Loss: 0.000178... Val Loss: 0.000431\n",
      "Epoch: 80/100... Step: 82315... Loss: 0.000125... Val Loss: 0.000638\n",
      "Epoch: 80/100... Step: 82320... Loss: 0.000106... Val Loss: 0.000408\n",
      "Epoch: 80/100... Step: 82325... Loss: 0.000102... Val Loss: 0.000340\n",
      "Epoch: 80/100... Step: 82330... Loss: 0.000112... Val Loss: 0.000480\n",
      "Epoch: 80/100... Step: 82335... Loss: 0.000113... Val Loss: 0.000662\n",
      "Epoch: 80/100... Step: 82340... Loss: 0.000131... Val Loss: 0.000797\n",
      "Epoch: 80/100... Step: 82345... Loss: 0.000066... Val Loss: 0.000605\n",
      "Epoch: 80/100... Step: 82350... Loss: 0.000068... Val Loss: 0.000664\n",
      "Epoch: 80/100... Step: 82355... Loss: 0.000076... Val Loss: 0.000587\n",
      "Epoch: 80/100... Step: 82360... Loss: 0.000045... Val Loss: 0.000636\n",
      "Epoch: 80/100... Step: 82365... Loss: 0.000054... Val Loss: 0.000602\n",
      "Epoch: 80/100... Step: 82370... Loss: 0.000034... Val Loss: 0.000583\n",
      "Epoch: 80/100... Step: 82375... Loss: 0.000042... Val Loss: 0.000602\n",
      "Epoch: 80/100... Step: 82380... Loss: 0.000020... Val Loss: 0.000633\n",
      "Epoch: 80/100... Step: 82385... Loss: 0.000073... Val Loss: 0.000689\n",
      "Epoch: 80/100... Step: 82390... Loss: 0.000059... Val Loss: 0.000763\n",
      "Epoch: 80/100... Step: 82395... Loss: 0.000021... Val Loss: 0.000764\n",
      "Epoch: 80/100... Step: 82400... Loss: 0.000022... Val Loss: 0.000747\n",
      "Epoch: 80/100... Step: 82405... Loss: 0.000056... Val Loss: 0.000775\n",
      "Epoch: 80/100... Step: 82410... Loss: 0.000061... Val Loss: 0.000828\n",
      "Epoch: 80/100... Step: 82415... Loss: 0.000064... Val Loss: 0.000675\n",
      "Epoch: 80/100... Step: 82420... Loss: 0.000063... Val Loss: 0.000706\n",
      "Epoch: 80/100... Step: 82425... Loss: 0.000063... Val Loss: 0.000722\n",
      "Epoch: 80/100... Step: 82430... Loss: 0.000059... Val Loss: 0.000649\n",
      "Epoch: 80/100... Step: 82435... Loss: 0.000117... Val Loss: 0.000658\n",
      "Epoch: 80/100... Step: 82440... Loss: 0.000098... Val Loss: 0.000475\n",
      "Epoch: 80/100... Step: 82445... Loss: 0.000050... Val Loss: 0.000699\n",
      "Epoch: 80/100... Step: 82450... Loss: 0.000052... Val Loss: 0.000632\n",
      "Epoch: 80/100... Step: 82455... Loss: 0.000034... Val Loss: 0.000796\n",
      "Epoch: 80/100... Step: 82460... Loss: 0.000045... Val Loss: 0.000784\n",
      "Epoch: 80/100... Step: 82465... Loss: 0.000043... Val Loss: 0.000917\n",
      "Epoch: 80/100... Step: 82470... Loss: 0.000047... Val Loss: 0.000799\n",
      "Epoch: 80/100... Step: 82475... Loss: 0.000042... Val Loss: 0.000790\n",
      "Epoch: 80/100... Step: 82480... Loss: 0.000103... Val Loss: 0.000815\n",
      "Epoch: 80/100... Step: 82485... Loss: 0.000053... Val Loss: 0.001056\n",
      "Epoch: 80/100... Step: 82490... Loss: 0.000041... Val Loss: 0.000919\n",
      "Epoch: 80/100... Step: 82495... Loss: 0.000034... Val Loss: 0.001018\n",
      "Epoch: 80/100... Step: 82500... Loss: 0.000031... Val Loss: 0.000954\n",
      "Epoch: 80/100... Step: 82505... Loss: 0.000042... Val Loss: 0.000959\n",
      "Epoch: 80/100... Step: 82510... Loss: 0.000094... Val Loss: 0.000895\n",
      "Epoch: 80/100... Step: 82515... Loss: 0.000074... Val Loss: 0.001052\n",
      "Epoch: 80/100... Step: 82520... Loss: 0.000066... Val Loss: 0.001076\n",
      "Epoch: 80/100... Step: 82525... Loss: 0.000074... Val Loss: 0.001011\n",
      "Epoch: 80/100... Step: 82530... Loss: 0.000093... Val Loss: 0.001048\n",
      "Epoch: 80/100... Step: 82535... Loss: 0.000063... Val Loss: 0.000811\n",
      "Epoch: 80/100... Step: 82540... Loss: 0.000077... Val Loss: 0.000909\n",
      "Epoch: 80/100... Step: 82545... Loss: 0.000048... Val Loss: 0.001017\n",
      "Epoch: 80/100... Step: 82550... Loss: 0.000057... Val Loss: 0.001061\n",
      "Epoch: 80/100... Step: 82555... Loss: 0.000068... Val Loss: 0.001028\n",
      "Epoch: 80/100... Step: 82560... Loss: 0.000072... Val Loss: 0.000914\n",
      "Epoch: 81/100... Step: 82565... Loss: 0.000235... Val Loss: 0.001257\n",
      "Epoch: 81/100... Step: 82570... Loss: 0.000197... Val Loss: 0.001712\n",
      "Epoch: 81/100... Step: 82575... Loss: 0.000091... Val Loss: 0.001517\n",
      "Epoch: 81/100... Step: 82580... Loss: 0.000160... Val Loss: 0.001713\n",
      "Epoch: 81/100... Step: 82585... Loss: 0.000207... Val Loss: 0.001887\n",
      "Epoch: 81/100... Step: 82590... Loss: 0.000136... Val Loss: 0.001717\n",
      "Epoch: 81/100... Step: 82595... Loss: 0.000116... Val Loss: 0.001960\n",
      "Epoch: 81/100... Step: 82600... Loss: 0.000228... Val Loss: 0.001798\n",
      "Epoch: 81/100... Step: 82605... Loss: 0.000239... Val Loss: 0.001496\n",
      "Epoch: 81/100... Step: 82610... Loss: 0.000060... Val Loss: 0.001804\n",
      "Epoch: 81/100... Step: 82615... Loss: 0.000168... Val Loss: 0.001823\n",
      "Epoch: 81/100... Step: 82620... Loss: 0.000122... Val Loss: 0.001505\n",
      "Epoch: 81/100... Step: 82625... Loss: 0.000071... Val Loss: 0.001627\n",
      "Epoch: 81/100... Step: 82630... Loss: 0.000195... Val Loss: 0.001396\n",
      "Epoch: 81/100... Step: 82635... Loss: 0.000085... Val Loss: 0.001011\n",
      "Epoch: 81/100... Step: 82640... Loss: 0.000212... Val Loss: 0.001197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81/100... Step: 82645... Loss: 0.000177... Val Loss: 0.001533\n",
      "Epoch: 81/100... Step: 82650... Loss: 0.000080... Val Loss: 0.001183\n",
      "Epoch: 81/100... Step: 82655... Loss: 0.000138... Val Loss: 0.001239\n",
      "Epoch: 81/100... Step: 82660... Loss: 0.000179... Val Loss: 0.001160\n",
      "Epoch: 81/100... Step: 82665... Loss: 0.000186... Val Loss: 0.000988\n",
      "Epoch: 81/100... Step: 82670... Loss: 0.000138... Val Loss: 0.001267\n",
      "Epoch: 81/100... Step: 82675... Loss: 0.000092... Val Loss: 0.001078\n",
      "Epoch: 81/100... Step: 82680... Loss: 0.000080... Val Loss: 0.001263\n",
      "Epoch: 81/100... Step: 82685... Loss: 0.000081... Val Loss: 0.001188\n",
      "Epoch: 81/100... Step: 82690... Loss: 0.000043... Val Loss: 0.001393\n",
      "Epoch: 81/100... Step: 82695... Loss: 0.000065... Val Loss: 0.001385\n",
      "Epoch: 81/100... Step: 82700... Loss: 0.000086... Val Loss: 0.001467\n",
      "Epoch: 81/100... Step: 82705... Loss: 0.000164... Val Loss: 0.001579\n",
      "Epoch: 81/100... Step: 82710... Loss: 0.000130... Val Loss: 0.001822\n",
      "Epoch: 81/100... Step: 82715... Loss: 0.000128... Val Loss: 0.001495\n",
      "Epoch: 81/100... Step: 82720... Loss: 0.000047... Val Loss: 0.001497\n",
      "Epoch: 81/100... Step: 82725... Loss: 0.000068... Val Loss: 0.001415\n",
      "Epoch: 81/100... Step: 82730... Loss: 0.000075... Val Loss: 0.001301\n",
      "Epoch: 81/100... Step: 82735... Loss: 0.000062... Val Loss: 0.001404\n",
      "Epoch: 81/100... Step: 82740... Loss: 0.000032... Val Loss: 0.001307\n",
      "Epoch: 81/100... Step: 82745... Loss: 0.000085... Val Loss: 0.001304\n",
      "Epoch: 81/100... Step: 82750... Loss: 0.000028... Val Loss: 0.001084\n",
      "Epoch: 81/100... Step: 82755... Loss: 0.000032... Val Loss: 0.001205\n",
      "Epoch: 81/100... Step: 82760... Loss: 0.000039... Val Loss: 0.001120\n",
      "Epoch: 81/100... Step: 82765... Loss: 0.000066... Val Loss: 0.001228\n",
      "Epoch: 81/100... Step: 82770... Loss: 0.000040... Val Loss: 0.001236\n",
      "Epoch: 81/100... Step: 82775... Loss: 0.000036... Val Loss: 0.001297\n",
      "Epoch: 81/100... Step: 82780... Loss: 0.000056... Val Loss: 0.001257\n",
      "Epoch: 81/100... Step: 82785... Loss: 0.000075... Val Loss: 0.001170\n",
      "Epoch: 81/100... Step: 82790... Loss: 0.000103... Val Loss: 0.001147\n",
      "Epoch: 81/100... Step: 82795... Loss: 0.000071... Val Loss: 0.000984\n",
      "Epoch: 81/100... Step: 82800... Loss: 0.000061... Val Loss: 0.001175\n",
      "Epoch: 81/100... Step: 82805... Loss: 0.000129... Val Loss: 0.001231\n",
      "Epoch: 81/100... Step: 82810... Loss: 0.000109... Val Loss: 0.001015\n",
      "Epoch: 81/100... Step: 82815... Loss: 0.000050... Val Loss: 0.001247\n",
      "Epoch: 81/100... Step: 82820... Loss: 0.000187... Val Loss: 0.001133\n",
      "Epoch: 81/100... Step: 82825... Loss: 0.000118... Val Loss: 0.000778\n",
      "Epoch: 81/100... Step: 82830... Loss: 0.000050... Val Loss: 0.000957\n",
      "Epoch: 81/100... Step: 82835... Loss: 0.000071... Val Loss: 0.000883\n",
      "Epoch: 81/100... Step: 82840... Loss: 0.000138... Val Loss: 0.000828\n",
      "Epoch: 81/100... Step: 82845... Loss: 0.000153... Val Loss: 0.001068\n",
      "Epoch: 81/100... Step: 82850... Loss: 0.000155... Val Loss: 0.000791\n",
      "Epoch: 81/100... Step: 82855... Loss: 0.000163... Val Loss: 0.000490\n",
      "Epoch: 81/100... Step: 82860... Loss: 0.000074... Val Loss: 0.000649\n",
      "Epoch: 81/100... Step: 82865... Loss: 0.000129... Val Loss: 0.000586\n",
      "Epoch: 81/100... Step: 82870... Loss: 0.000114... Val Loss: 0.000637\n",
      "Epoch: 81/100... Step: 82875... Loss: 0.000072... Val Loss: 0.000516\n",
      "Epoch: 81/100... Step: 82880... Loss: 0.000072... Val Loss: 0.000717\n",
      "Epoch: 81/100... Step: 82885... Loss: 0.000106... Val Loss: 0.000531\n",
      "Epoch: 81/100... Step: 82890... Loss: 0.000067... Val Loss: 0.000416\n",
      "Epoch: 81/100... Step: 82895... Loss: 0.000052... Val Loss: 0.000489\n",
      "Epoch: 81/100... Step: 82900... Loss: 0.000128... Val Loss: 0.000534\n",
      "Epoch: 81/100... Step: 82905... Loss: 0.000246... Val Loss: 0.000727\n",
      "Epoch: 81/100... Step: 82910... Loss: 0.000225... Val Loss: 0.000442\n",
      "Epoch: 81/100... Step: 82915... Loss: 0.000454... Val Loss: 0.000265\n",
      "Epoch: 81/100... Step: 82920... Loss: 0.000376... Val Loss: 0.000547\n",
      "Epoch: 81/100... Step: 82925... Loss: 0.000485... Val Loss: 0.001334\n",
      "Epoch: 81/100... Step: 82930... Loss: 0.000153... Val Loss: 0.000812\n",
      "Epoch: 81/100... Step: 82935... Loss: 0.000081... Val Loss: 0.001122\n",
      "Epoch: 81/100... Step: 82940... Loss: 0.000086... Val Loss: 0.001071\n",
      "Epoch: 81/100... Step: 82945... Loss: 0.000124... Val Loss: 0.001048\n",
      "Epoch: 81/100... Step: 82950... Loss: 0.000135... Val Loss: 0.000867\n",
      "Epoch: 81/100... Step: 82955... Loss: 0.000059... Val Loss: 0.001120\n",
      "Epoch: 81/100... Step: 82960... Loss: 0.000040... Val Loss: 0.001088\n",
      "Epoch: 81/100... Step: 82965... Loss: 0.000312... Val Loss: 0.001086\n",
      "Epoch: 81/100... Step: 82970... Loss: 0.000441... Val Loss: 0.000733\n",
      "Epoch: 81/100... Step: 82975... Loss: 0.000745... Val Loss: 0.000270\n",
      "Epoch: 81/100... Step: 82980... Loss: 0.000262... Val Loss: 0.001562\n",
      "Epoch: 81/100... Step: 82985... Loss: 0.000433... Val Loss: 0.001174\n",
      "Epoch: 81/100... Step: 82990... Loss: 0.000415... Val Loss: 0.000678\n",
      "Epoch: 81/100... Step: 82995... Loss: 0.000177... Val Loss: 0.001496\n",
      "Epoch: 81/100... Step: 83000... Loss: 0.000345... Val Loss: 0.001302\n",
      "Epoch: 81/100... Step: 83005... Loss: 0.000341... Val Loss: 0.000771\n",
      "Epoch: 81/100... Step: 83010... Loss: 0.000270... Val Loss: 0.001327\n",
      "Epoch: 81/100... Step: 83015... Loss: 0.000316... Val Loss: 0.001562\n",
      "Epoch: 81/100... Step: 83020... Loss: 0.000239... Val Loss: 0.001161\n",
      "Epoch: 81/100... Step: 83025... Loss: 0.000174... Val Loss: 0.001517\n",
      "Epoch: 81/100... Step: 83030... Loss: 0.000309... Val Loss: 0.001505\n",
      "Epoch: 81/100... Step: 83035... Loss: 0.000202... Val Loss: 0.000962\n",
      "Epoch: 81/100... Step: 83040... Loss: 0.000203... Val Loss: 0.001096\n",
      "Epoch: 81/100... Step: 83045... Loss: 0.000388... Val Loss: 0.001338\n",
      "Epoch: 81/100... Step: 83050... Loss: 0.000081... Val Loss: 0.000837\n",
      "Epoch: 81/100... Step: 83055... Loss: 0.000203... Val Loss: 0.000699\n",
      "Epoch: 81/100... Step: 83060... Loss: 0.000201... Val Loss: 0.000900\n",
      "Epoch: 81/100... Step: 83065... Loss: 0.000179... Val Loss: 0.000431\n",
      "Epoch: 81/100... Step: 83070... Loss: 0.000106... Val Loss: 0.000292\n",
      "Epoch: 81/100... Step: 83075... Loss: 0.000099... Val Loss: 0.000429\n",
      "Epoch: 81/100... Step: 83080... Loss: 0.000103... Val Loss: 0.000447\n",
      "Epoch: 81/100... Step: 83085... Loss: 0.000059... Val Loss: 0.000677\n",
      "Epoch: 81/100... Step: 83090... Loss: 0.000053... Val Loss: 0.000689\n",
      "Epoch: 81/100... Step: 83095... Loss: 0.000105... Val Loss: 0.000639\n",
      "Epoch: 81/100... Step: 83100... Loss: 0.000151... Val Loss: 0.000560\n",
      "Epoch: 81/100... Step: 83105... Loss: 0.000086... Val Loss: 0.000925\n",
      "Epoch: 81/100... Step: 83110... Loss: 0.000073... Val Loss: 0.000849\n",
      "Epoch: 81/100... Step: 83115... Loss: 0.000078... Val Loss: 0.000834\n",
      "Epoch: 81/100... Step: 83120... Loss: 0.000045... Val Loss: 0.000634\n",
      "Epoch: 81/100... Step: 83125... Loss: 0.000034... Val Loss: 0.000868\n",
      "Epoch: 81/100... Step: 83130... Loss: 0.000081... Val Loss: 0.000801\n",
      "Epoch: 81/100... Step: 83135... Loss: 0.000099... Val Loss: 0.000948\n",
      "Epoch: 81/100... Step: 83140... Loss: 0.000156... Val Loss: 0.000892\n",
      "Epoch: 81/100... Step: 83145... Loss: 0.000143... Val Loss: 0.000744\n",
      "Epoch: 81/100... Step: 83150... Loss: 0.000091... Val Loss: 0.000971\n",
      "Epoch: 81/100... Step: 83155... Loss: 0.000053... Val Loss: 0.000885\n",
      "Epoch: 81/100... Step: 83160... Loss: 0.000057... Val Loss: 0.000952\n",
      "Epoch: 81/100... Step: 83165... Loss: 0.000031... Val Loss: 0.000975\n",
      "Epoch: 81/100... Step: 83170... Loss: 0.000043... Val Loss: 0.000904\n",
      "Epoch: 81/100... Step: 83175... Loss: 0.000045... Val Loss: 0.000843\n",
      "Epoch: 81/100... Step: 83180... Loss: 0.000062... Val Loss: 0.000907\n",
      "Epoch: 81/100... Step: 83185... Loss: 0.000040... Val Loss: 0.000933\n",
      "Epoch: 81/100... Step: 83190... Loss: 0.000032... Val Loss: 0.000872\n",
      "Epoch: 81/100... Step: 83195... Loss: 0.000034... Val Loss: 0.000887\n",
      "Epoch: 81/100... Step: 83200... Loss: 0.000089... Val Loss: 0.000807\n",
      "Epoch: 81/100... Step: 83205... Loss: 0.000049... Val Loss: 0.000593\n",
      "Epoch: 81/100... Step: 83210... Loss: 0.000202... Val Loss: 0.000596\n",
      "Epoch: 81/100... Step: 83215... Loss: 0.000036... Val Loss: 0.000995\n",
      "Epoch: 81/100... Step: 83220... Loss: 0.000201... Val Loss: 0.000961\n",
      "Epoch: 81/100... Step: 83225... Loss: 0.000138... Val Loss: 0.000496\n",
      "Epoch: 81/100... Step: 83230... Loss: 0.000083... Val Loss: 0.000348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81/100... Step: 83235... Loss: 0.000040... Val Loss: 0.000363\n",
      "Epoch: 81/100... Step: 83240... Loss: 0.000105... Val Loss: 0.000354\n",
      "Epoch: 81/100... Step: 83245... Loss: 0.000088... Val Loss: 0.000264\n",
      "Epoch: 81/100... Step: 83250... Loss: 0.000151... Val Loss: 0.000271\n",
      "Epoch: 81/100... Step: 83255... Loss: 0.000159... Val Loss: 0.000451\n",
      "Epoch: 81/100... Step: 83260... Loss: 0.000067... Val Loss: 0.000266\n",
      "Epoch: 81/100... Step: 83265... Loss: 0.000110... Val Loss: 0.000292\n",
      "Epoch: 81/100... Step: 83270... Loss: 0.000098... Val Loss: 0.000402\n",
      "Epoch: 81/100... Step: 83275... Loss: 0.000110... Val Loss: 0.000329\n",
      "Epoch: 81/100... Step: 83280... Loss: 0.000058... Val Loss: 0.000522\n",
      "Epoch: 81/100... Step: 83285... Loss: 0.000051... Val Loss: 0.000361\n",
      "Epoch: 81/100... Step: 83290... Loss: 0.000087... Val Loss: 0.000353\n",
      "Epoch: 81/100... Step: 83295... Loss: 0.000134... Val Loss: 0.000454\n",
      "Epoch: 81/100... Step: 83300... Loss: 0.000268... Val Loss: 0.000265\n",
      "Epoch: 81/100... Step: 83305... Loss: 0.000062... Val Loss: 0.000502\n",
      "Epoch: 81/100... Step: 83310... Loss: 0.000207... Val Loss: 0.000437\n",
      "Epoch: 81/100... Step: 83315... Loss: 0.000154... Val Loss: 0.000267\n",
      "Epoch: 81/100... Step: 83320... Loss: 0.000066... Val Loss: 0.000444\n",
      "Epoch: 81/100... Step: 83325... Loss: 0.000091... Val Loss: 0.000363\n",
      "Epoch: 81/100... Step: 83330... Loss: 0.000182... Val Loss: 0.000322\n",
      "Epoch: 81/100... Step: 83335... Loss: 0.000102... Val Loss: 0.000499\n",
      "Epoch: 81/100... Step: 83340... Loss: 0.000113... Val Loss: 0.000421\n",
      "Epoch: 81/100... Step: 83345... Loss: 0.000037... Val Loss: 0.000609\n",
      "Epoch: 81/100... Step: 83350... Loss: 0.000196... Val Loss: 0.000604\n",
      "Epoch: 81/100... Step: 83355... Loss: 0.000086... Val Loss: 0.000317\n",
      "Epoch: 81/100... Step: 83360... Loss: 0.000101... Val Loss: 0.000378\n",
      "Epoch: 81/100... Step: 83365... Loss: 0.000123... Val Loss: 0.000641\n",
      "Epoch: 81/100... Step: 83370... Loss: 0.000084... Val Loss: 0.000788\n",
      "Epoch: 81/100... Step: 83375... Loss: 0.000057... Val Loss: 0.000643\n",
      "Epoch: 81/100... Step: 83380... Loss: 0.000053... Val Loss: 0.000656\n",
      "Epoch: 81/100... Step: 83385... Loss: 0.000044... Val Loss: 0.000592\n",
      "Epoch: 81/100... Step: 83390... Loss: 0.000035... Val Loss: 0.000631\n",
      "Epoch: 81/100... Step: 83395... Loss: 0.000045... Val Loss: 0.000614\n",
      "Epoch: 81/100... Step: 83400... Loss: 0.000041... Val Loss: 0.000591\n",
      "Epoch: 81/100... Step: 83405... Loss: 0.000037... Val Loss: 0.000614\n",
      "Epoch: 81/100... Step: 83410... Loss: 0.000034... Val Loss: 0.000595\n",
      "Epoch: 81/100... Step: 83415... Loss: 0.000064... Val Loss: 0.000705\n",
      "Epoch: 81/100... Step: 83420... Loss: 0.000038... Val Loss: 0.000708\n",
      "Epoch: 81/100... Step: 83425... Loss: 0.000061... Val Loss: 0.000773\n",
      "Epoch: 81/100... Step: 83430... Loss: 0.000031... Val Loss: 0.000751\n",
      "Epoch: 81/100... Step: 83435... Loss: 0.000029... Val Loss: 0.000755\n",
      "Epoch: 81/100... Step: 83440... Loss: 0.000042... Val Loss: 0.000831\n",
      "Epoch: 81/100... Step: 83445... Loss: 0.000063... Val Loss: 0.000749\n",
      "Epoch: 81/100... Step: 83450... Loss: 0.000076... Val Loss: 0.000633\n",
      "Epoch: 81/100... Step: 83455... Loss: 0.000052... Val Loss: 0.000753\n",
      "Epoch: 81/100... Step: 83460... Loss: 0.000026... Val Loss: 0.000661\n",
      "Epoch: 81/100... Step: 83465... Loss: 0.000052... Val Loss: 0.000613\n",
      "Epoch: 81/100... Step: 83470... Loss: 0.000021... Val Loss: 0.000552\n",
      "Epoch: 81/100... Step: 83475... Loss: 0.000046... Val Loss: 0.000607\n",
      "Epoch: 81/100... Step: 83480... Loss: 0.000033... Val Loss: 0.000683\n",
      "Epoch: 81/100... Step: 83485... Loss: 0.000046... Val Loss: 0.000699\n",
      "Epoch: 81/100... Step: 83490... Loss: 0.000048... Val Loss: 0.000813\n",
      "Epoch: 81/100... Step: 83495... Loss: 0.000030... Val Loss: 0.000847\n",
      "Epoch: 81/100... Step: 83500... Loss: 0.000031... Val Loss: 0.000870\n",
      "Epoch: 81/100... Step: 83505... Loss: 0.000061... Val Loss: 0.000819\n",
      "Epoch: 81/100... Step: 83510... Loss: 0.000080... Val Loss: 0.000739\n",
      "Epoch: 81/100... Step: 83515... Loss: 0.000060... Val Loss: 0.000960\n",
      "Epoch: 81/100... Step: 83520... Loss: 0.000077... Val Loss: 0.001033\n",
      "Epoch: 81/100... Step: 83525... Loss: 0.000063... Val Loss: 0.000921\n",
      "Epoch: 81/100... Step: 83530... Loss: 0.000038... Val Loss: 0.001001\n",
      "Epoch: 81/100... Step: 83535... Loss: 0.000041... Val Loss: 0.000937\n",
      "Epoch: 81/100... Step: 83540... Loss: 0.000042... Val Loss: 0.000932\n",
      "Epoch: 81/100... Step: 83545... Loss: 0.000125... Val Loss: 0.000902\n",
      "Epoch: 81/100... Step: 83550... Loss: 0.000027... Val Loss: 0.001139\n",
      "Epoch: 81/100... Step: 83555... Loss: 0.000071... Val Loss: 0.001062\n",
      "Epoch: 81/100... Step: 83560... Loss: 0.000057... Val Loss: 0.000992\n",
      "Epoch: 81/100... Step: 83565... Loss: 0.000114... Val Loss: 0.001012\n",
      "Epoch: 81/100... Step: 83570... Loss: 0.000083... Val Loss: 0.000768\n",
      "Epoch: 81/100... Step: 83575... Loss: 0.000147... Val Loss: 0.000885\n",
      "Epoch: 81/100... Step: 83580... Loss: 0.000120... Val Loss: 0.001207\n",
      "Epoch: 81/100... Step: 83585... Loss: 0.000070... Val Loss: 0.001040\n",
      "Epoch: 81/100... Step: 83590... Loss: 0.000073... Val Loss: 0.000927\n",
      "Epoch: 82/100... Step: 83595... Loss: 0.000246... Val Loss: 0.001173\n",
      "Epoch: 82/100... Step: 83600... Loss: 0.000131... Val Loss: 0.001647\n",
      "Epoch: 82/100... Step: 83605... Loss: 0.000090... Val Loss: 0.001514\n",
      "Epoch: 82/100... Step: 83610... Loss: 0.000246... Val Loss: 0.001570\n",
      "Epoch: 82/100... Step: 83615... Loss: 0.000199... Val Loss: 0.001915\n",
      "Epoch: 82/100... Step: 83620... Loss: 0.000123... Val Loss: 0.001813\n",
      "Epoch: 82/100... Step: 83625... Loss: 0.000093... Val Loss: 0.001798\n",
      "Epoch: 82/100... Step: 83630... Loss: 0.000271... Val Loss: 0.001941\n",
      "Epoch: 82/100... Step: 83635... Loss: 0.000136... Val Loss: 0.001674\n",
      "Epoch: 82/100... Step: 83640... Loss: 0.000172... Val Loss: 0.001677\n",
      "Epoch: 82/100... Step: 83645... Loss: 0.000116... Val Loss: 0.001861\n",
      "Epoch: 82/100... Step: 83650... Loss: 0.000070... Val Loss: 0.001642\n",
      "Epoch: 82/100... Step: 83655... Loss: 0.000048... Val Loss: 0.001540\n",
      "Epoch: 82/100... Step: 83660... Loss: 0.000147... Val Loss: 0.001583\n",
      "Epoch: 82/100... Step: 83665... Loss: 0.000248... Val Loss: 0.001324\n",
      "Epoch: 82/100... Step: 83670... Loss: 0.000275... Val Loss: 0.000943\n",
      "Epoch: 82/100... Step: 83675... Loss: 0.000225... Val Loss: 0.001225\n",
      "Epoch: 82/100... Step: 83680... Loss: 0.000226... Val Loss: 0.001572\n",
      "Epoch: 82/100... Step: 83685... Loss: 0.000091... Val Loss: 0.001209\n",
      "Epoch: 82/100... Step: 83690... Loss: 0.000177... Val Loss: 0.000947\n",
      "Epoch: 82/100... Step: 83695... Loss: 0.000091... Val Loss: 0.001194\n",
      "Epoch: 82/100... Step: 83700... Loss: 0.000043... Val Loss: 0.001140\n",
      "Epoch: 82/100... Step: 83705... Loss: 0.000032... Val Loss: 0.001156\n",
      "Epoch: 82/100... Step: 83710... Loss: 0.000063... Val Loss: 0.001140\n",
      "Epoch: 82/100... Step: 83715... Loss: 0.000073... Val Loss: 0.001194\n",
      "Epoch: 82/100... Step: 83720... Loss: 0.000060... Val Loss: 0.001344\n",
      "Epoch: 82/100... Step: 83725... Loss: 0.000032... Val Loss: 0.001366\n",
      "Epoch: 82/100... Step: 83730... Loss: 0.000031... Val Loss: 0.001466\n",
      "Epoch: 82/100... Step: 83735... Loss: 0.000158... Val Loss: 0.001504\n",
      "Epoch: 82/100... Step: 83740... Loss: 0.000056... Val Loss: 0.001817\n",
      "Epoch: 82/100... Step: 83745... Loss: 0.000092... Val Loss: 0.001649\n",
      "Epoch: 82/100... Step: 83750... Loss: 0.000095... Val Loss: 0.001421\n",
      "Epoch: 82/100... Step: 83755... Loss: 0.000079... Val Loss: 0.001504\n",
      "Epoch: 82/100... Step: 83760... Loss: 0.000053... Val Loss: 0.001311\n",
      "Epoch: 82/100... Step: 83765... Loss: 0.000075... Val Loss: 0.001354\n",
      "Epoch: 82/100... Step: 83770... Loss: 0.000069... Val Loss: 0.001368\n",
      "Epoch: 82/100... Step: 83775... Loss: 0.000056... Val Loss: 0.001274\n",
      "Epoch: 82/100... Step: 83780... Loss: 0.000100... Val Loss: 0.001242\n",
      "Epoch: 82/100... Step: 83785... Loss: 0.000088... Val Loss: 0.001060\n",
      "Epoch: 82/100... Step: 83790... Loss: 0.000066... Val Loss: 0.001238\n",
      "Epoch: 82/100... Step: 83795... Loss: 0.000070... Val Loss: 0.001080\n",
      "Epoch: 82/100... Step: 83800... Loss: 0.000063... Val Loss: 0.001236\n",
      "Epoch: 82/100... Step: 83805... Loss: 0.000077... Val Loss: 0.001337\n",
      "Epoch: 82/100... Step: 83810... Loss: 0.000066... Val Loss: 0.001231\n",
      "Epoch: 82/100... Step: 83815... Loss: 0.000076... Val Loss: 0.001228\n",
      "Epoch: 82/100... Step: 83820... Loss: 0.000086... Val Loss: 0.001149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82/100... Step: 83825... Loss: 0.000060... Val Loss: 0.001047\n",
      "Epoch: 82/100... Step: 83830... Loss: 0.000089... Val Loss: 0.001068\n",
      "Epoch: 82/100... Step: 83835... Loss: 0.000084... Val Loss: 0.001261\n",
      "Epoch: 82/100... Step: 83840... Loss: 0.000064... Val Loss: 0.001124\n",
      "Epoch: 82/100... Step: 83845... Loss: 0.000145... Val Loss: 0.001056\n",
      "Epoch: 82/100... Step: 83850... Loss: 0.000155... Val Loss: 0.001242\n",
      "Epoch: 82/100... Step: 83855... Loss: 0.000137... Val Loss: 0.000998\n",
      "Epoch: 82/100... Step: 83860... Loss: 0.000172... Val Loss: 0.000760\n",
      "Epoch: 82/100... Step: 83865... Loss: 0.000050... Val Loss: 0.000972\n",
      "Epoch: 82/100... Step: 83870... Loss: 0.000035... Val Loss: 0.000864\n",
      "Epoch: 82/100... Step: 83875... Loss: 0.000059... Val Loss: 0.000941\n",
      "Epoch: 82/100... Step: 83880... Loss: 0.000207... Val Loss: 0.000970\n",
      "Epoch: 82/100... Step: 83885... Loss: 0.000057... Val Loss: 0.000600\n",
      "Epoch: 82/100... Step: 83890... Loss: 0.000123... Val Loss: 0.000506\n",
      "Epoch: 82/100... Step: 83895... Loss: 0.000128... Val Loss: 0.000663\n",
      "Epoch: 82/100... Step: 83900... Loss: 0.000119... Val Loss: 0.000738\n",
      "Epoch: 82/100... Step: 83905... Loss: 0.000113... Val Loss: 0.000574\n",
      "Epoch: 82/100... Step: 83910... Loss: 0.000174... Val Loss: 0.000498\n",
      "Epoch: 82/100... Step: 83915... Loss: 0.000119... Val Loss: 0.000740\n",
      "Epoch: 82/100... Step: 83920... Loss: 0.000184... Val Loss: 0.000557\n",
      "Epoch: 82/100... Step: 83925... Loss: 0.000173... Val Loss: 0.000325\n",
      "Epoch: 82/100... Step: 83930... Loss: 0.000130... Val Loss: 0.000521\n",
      "Epoch: 82/100... Step: 83935... Loss: 0.000230... Val Loss: 0.000772\n",
      "Epoch: 82/100... Step: 83940... Loss: 0.000244... Val Loss: 0.000471\n",
      "Epoch: 82/100... Step: 83945... Loss: 0.000407... Val Loss: 0.000262\n",
      "Epoch: 82/100... Step: 83950... Loss: 0.000336... Val Loss: 0.000519\n",
      "Epoch: 82/100... Step: 83955... Loss: 0.000244... Val Loss: 0.001099\n",
      "Epoch: 82/100... Step: 83960... Loss: 0.000105... Val Loss: 0.000847\n",
      "Epoch: 82/100... Step: 83965... Loss: 0.000084... Val Loss: 0.001062\n",
      "Epoch: 82/100... Step: 83970... Loss: 0.000123... Val Loss: 0.000959\n",
      "Epoch: 82/100... Step: 83975... Loss: 0.000179... Val Loss: 0.001176\n",
      "Epoch: 82/100... Step: 83980... Loss: 0.000059... Val Loss: 0.000864\n",
      "Epoch: 82/100... Step: 83985... Loss: 0.000100... Val Loss: 0.000970\n",
      "Epoch: 82/100... Step: 83990... Loss: 0.000177... Val Loss: 0.001124\n",
      "Epoch: 82/100... Step: 83995... Loss: 0.000163... Val Loss: 0.001019\n",
      "Epoch: 82/100... Step: 84000... Loss: 0.000528... Val Loss: 0.000983\n",
      "Epoch: 82/100... Step: 84005... Loss: 0.000695... Val Loss: 0.000445\n",
      "Epoch: 82/100... Step: 84010... Loss: 0.000617... Val Loss: 0.000793\n",
      "Epoch: 82/100... Step: 84015... Loss: 0.000787... Val Loss: 0.001854\n",
      "Epoch: 82/100... Step: 84020... Loss: 0.000121... Val Loss: 0.000850\n",
      "Epoch: 82/100... Step: 84025... Loss: 0.000578... Val Loss: 0.000639\n",
      "Epoch: 82/100... Step: 84030... Loss: 0.000222... Val Loss: 0.001501\n",
      "Epoch: 82/100... Step: 84035... Loss: 0.000350... Val Loss: 0.001371\n",
      "Epoch: 82/100... Step: 84040... Loss: 0.000344... Val Loss: 0.000865\n",
      "Epoch: 82/100... Step: 84045... Loss: 0.000194... Val Loss: 0.001283\n",
      "Epoch: 82/100... Step: 84050... Loss: 0.000301... Val Loss: 0.001657\n",
      "Epoch: 82/100... Step: 84055... Loss: 0.000130... Val Loss: 0.001235\n",
      "Epoch: 82/100... Step: 84060... Loss: 0.000202... Val Loss: 0.001379\n",
      "Epoch: 82/100... Step: 84065... Loss: 0.000296... Val Loss: 0.001327\n",
      "Epoch: 82/100... Step: 84070... Loss: 0.000272... Val Loss: 0.000896\n",
      "Epoch: 82/100... Step: 84075... Loss: 0.000167... Val Loss: 0.001132\n",
      "Epoch: 82/100... Step: 84080... Loss: 0.000328... Val Loss: 0.001228\n",
      "Epoch: 82/100... Step: 84085... Loss: 0.000105... Val Loss: 0.000855\n",
      "Epoch: 82/100... Step: 84090... Loss: 0.000078... Val Loss: 0.000703\n",
      "Epoch: 82/100... Step: 84095... Loss: 0.000196... Val Loss: 0.000609\n",
      "Epoch: 82/100... Step: 84100... Loss: 0.000110... Val Loss: 0.000421\n",
      "Epoch: 82/100... Step: 84105... Loss: 0.000203... Val Loss: 0.000314\n",
      "Epoch: 82/100... Step: 84110... Loss: 0.000069... Val Loss: 0.000458\n",
      "Epoch: 82/100... Step: 84115... Loss: 0.000115... Val Loss: 0.000612\n",
      "Epoch: 82/100... Step: 84120... Loss: 0.000079... Val Loss: 0.000666\n",
      "Epoch: 82/100... Step: 84125... Loss: 0.000150... Val Loss: 0.000775\n",
      "Epoch: 82/100... Step: 84130... Loss: 0.000056... Val Loss: 0.000540\n",
      "Epoch: 82/100... Step: 84135... Loss: 0.000134... Val Loss: 0.000682\n",
      "Epoch: 82/100... Step: 84140... Loss: 0.000114... Val Loss: 0.001002\n",
      "Epoch: 82/100... Step: 84145... Loss: 0.000079... Val Loss: 0.000834\n",
      "Epoch: 82/100... Step: 84150... Loss: 0.000112... Val Loss: 0.000716\n",
      "Epoch: 82/100... Step: 84155... Loss: 0.000049... Val Loss: 0.000742\n",
      "Epoch: 82/100... Step: 84160... Loss: 0.000056... Val Loss: 0.000819\n",
      "Epoch: 82/100... Step: 84165... Loss: 0.000066... Val Loss: 0.000933\n",
      "Epoch: 82/100... Step: 84170... Loss: 0.000105... Val Loss: 0.000904\n",
      "Epoch: 82/100... Step: 84175... Loss: 0.000046... Val Loss: 0.000793\n",
      "Epoch: 82/100... Step: 84180... Loss: 0.000038... Val Loss: 0.000891\n",
      "Epoch: 82/100... Step: 84185... Loss: 0.000087... Val Loss: 0.000852\n",
      "Epoch: 82/100... Step: 84190... Loss: 0.000089... Val Loss: 0.000989\n",
      "Epoch: 82/100... Step: 84195... Loss: 0.000061... Val Loss: 0.000892\n",
      "Epoch: 82/100... Step: 84200... Loss: 0.000068... Val Loss: 0.000996\n",
      "Epoch: 82/100... Step: 84205... Loss: 0.000051... Val Loss: 0.000824\n",
      "Epoch: 82/100... Step: 84210... Loss: 0.000048... Val Loss: 0.000860\n",
      "Epoch: 82/100... Step: 84215... Loss: 0.000046... Val Loss: 0.000944\n",
      "Epoch: 82/100... Step: 84220... Loss: 0.000033... Val Loss: 0.000889\n",
      "Epoch: 82/100... Step: 84225... Loss: 0.000035... Val Loss: 0.000877\n",
      "Epoch: 82/100... Step: 84230... Loss: 0.000047... Val Loss: 0.000859\n",
      "Epoch: 82/100... Step: 84235... Loss: 0.000111... Val Loss: 0.000689\n",
      "Epoch: 82/100... Step: 84240... Loss: 0.000186... Val Loss: 0.000455\n",
      "Epoch: 82/100... Step: 84245... Loss: 0.000232... Val Loss: 0.000721\n",
      "Epoch: 82/100... Step: 84250... Loss: 0.000204... Val Loss: 0.001065\n",
      "Epoch: 82/100... Step: 84255... Loss: 0.000241... Val Loss: 0.000860\n",
      "Epoch: 82/100... Step: 84260... Loss: 0.000140... Val Loss: 0.000320\n",
      "Epoch: 82/100... Step: 84265... Loss: 0.000101... Val Loss: 0.000360\n",
      "Epoch: 82/100... Step: 84270... Loss: 0.000117... Val Loss: 0.000435\n",
      "Epoch: 82/100... Step: 84275... Loss: 0.000093... Val Loss: 0.000262\n",
      "Epoch: 82/100... Step: 84280... Loss: 0.000163... Val Loss: 0.000257\n",
      "Epoch: 82/100... Step: 84285... Loss: 0.000124... Val Loss: 0.000424\n",
      "Epoch: 82/100... Step: 84290... Loss: 0.000060... Val Loss: 0.000295\n",
      "Epoch: 82/100... Step: 84295... Loss: 0.000099... Val Loss: 0.000273\n",
      "Epoch: 82/100... Step: 84300... Loss: 0.000078... Val Loss: 0.000382\n",
      "Epoch: 82/100... Step: 84305... Loss: 0.000054... Val Loss: 0.000352\n",
      "Epoch: 82/100... Step: 84310... Loss: 0.000041... Val Loss: 0.000465\n",
      "Epoch: 82/100... Step: 84315... Loss: 0.000038... Val Loss: 0.000397\n",
      "Epoch: 82/100... Step: 84320... Loss: 0.000050... Val Loss: 0.000349\n",
      "Epoch: 82/100... Step: 84325... Loss: 0.000072... Val Loss: 0.000417\n",
      "Epoch: 82/100... Step: 84330... Loss: 0.000287... Val Loss: 0.000364\n",
      "Epoch: 82/100... Step: 84335... Loss: 0.000227... Val Loss: 0.000305\n",
      "Epoch: 82/100... Step: 84340... Loss: 0.000284... Val Loss: 0.000666\n",
      "Epoch: 82/100... Step: 84345... Loss: 0.000069... Val Loss: 0.000358\n",
      "Epoch: 82/100... Step: 84350... Loss: 0.000138... Val Loss: 0.000312\n",
      "Epoch: 82/100... Step: 84355... Loss: 0.000136... Val Loss: 0.000455\n",
      "Epoch: 82/100... Step: 84360... Loss: 0.000090... Val Loss: 0.000325\n",
      "Epoch: 82/100... Step: 84365... Loss: 0.000035... Val Loss: 0.000464\n",
      "Epoch: 82/100... Step: 84370... Loss: 0.000100... Val Loss: 0.000458\n",
      "Epoch: 82/100... Step: 84375... Loss: 0.000120... Val Loss: 0.000477\n",
      "Epoch: 82/100... Step: 84380... Loss: 0.000172... Val Loss: 0.000694\n",
      "Epoch: 82/100... Step: 84385... Loss: 0.000124... Val Loss: 0.000442\n",
      "Epoch: 82/100... Step: 84390... Loss: 0.000157... Val Loss: 0.000291\n",
      "Epoch: 82/100... Step: 84395... Loss: 0.000155... Val Loss: 0.000467\n",
      "Epoch: 82/100... Step: 84400... Loss: 0.000108... Val Loss: 0.000787\n",
      "Epoch: 82/100... Step: 84405... Loss: 0.000076... Val Loss: 0.000729\n",
      "Epoch: 82/100... Step: 84410... Loss: 0.000090... Val Loss: 0.000647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82/100... Step: 84415... Loss: 0.000065... Val Loss: 0.000606\n",
      "Epoch: 82/100... Step: 84420... Loss: 0.000061... Val Loss: 0.000614\n",
      "Epoch: 82/100... Step: 84425... Loss: 0.000049... Val Loss: 0.000606\n",
      "Epoch: 82/100... Step: 84430... Loss: 0.000062... Val Loss: 0.000603\n",
      "Epoch: 82/100... Step: 84435... Loss: 0.000035... Val Loss: 0.000567\n",
      "Epoch: 82/100... Step: 84440... Loss: 0.000033... Val Loss: 0.000625\n",
      "Epoch: 82/100... Step: 84445... Loss: 0.000034... Val Loss: 0.000639\n",
      "Epoch: 82/100... Step: 84450... Loss: 0.000045... Val Loss: 0.000723\n",
      "Epoch: 82/100... Step: 84455... Loss: 0.000058... Val Loss: 0.000764\n",
      "Epoch: 82/100... Step: 84460... Loss: 0.000036... Val Loss: 0.000726\n",
      "Epoch: 82/100... Step: 84465... Loss: 0.000021... Val Loss: 0.000775\n",
      "Epoch: 82/100... Step: 84470... Loss: 0.000057... Val Loss: 0.000800\n",
      "Epoch: 82/100... Step: 84475... Loss: 0.000059... Val Loss: 0.000819\n",
      "Epoch: 82/100... Step: 84480... Loss: 0.000069... Val Loss: 0.000637\n",
      "Epoch: 82/100... Step: 84485... Loss: 0.000082... Val Loss: 0.000681\n",
      "Epoch: 82/100... Step: 84490... Loss: 0.000107... Val Loss: 0.000768\n",
      "Epoch: 82/100... Step: 84495... Loss: 0.000059... Val Loss: 0.000595\n",
      "Epoch: 82/100... Step: 84500... Loss: 0.000028... Val Loss: 0.000608\n",
      "Epoch: 82/100... Step: 84505... Loss: 0.000044... Val Loss: 0.000533\n",
      "Epoch: 82/100... Step: 84510... Loss: 0.000036... Val Loss: 0.000662\n",
      "Epoch: 82/100... Step: 84515... Loss: 0.000040... Val Loss: 0.000695\n",
      "Epoch: 82/100... Step: 84520... Loss: 0.000079... Val Loss: 0.000721\n",
      "Epoch: 82/100... Step: 84525... Loss: 0.000053... Val Loss: 0.000884\n",
      "Epoch: 82/100... Step: 84530... Loss: 0.000033... Val Loss: 0.000833\n",
      "Epoch: 82/100... Step: 84535... Loss: 0.000067... Val Loss: 0.000869\n",
      "Epoch: 82/100... Step: 84540... Loss: 0.000040... Val Loss: 0.000737\n",
      "Epoch: 82/100... Step: 84545... Loss: 0.000112... Val Loss: 0.000829\n",
      "Epoch: 82/100... Step: 84550... Loss: 0.000071... Val Loss: 0.001079\n",
      "Epoch: 82/100... Step: 84555... Loss: 0.000055... Val Loss: 0.000962\n",
      "Epoch: 82/100... Step: 84560... Loss: 0.000071... Val Loss: 0.000914\n",
      "Epoch: 82/100... Step: 84565... Loss: 0.000078... Val Loss: 0.001044\n",
      "Epoch: 82/100... Step: 84570... Loss: 0.000038... Val Loss: 0.000890\n",
      "Epoch: 82/100... Step: 84575... Loss: 0.000091... Val Loss: 0.000919\n",
      "Epoch: 82/100... Step: 84580... Loss: 0.000036... Val Loss: 0.001095\n",
      "Epoch: 82/100... Step: 84585... Loss: 0.000075... Val Loss: 0.001083\n",
      "Epoch: 82/100... Step: 84590... Loss: 0.000059... Val Loss: 0.000999\n",
      "Epoch: 82/100... Step: 84595... Loss: 0.000080... Val Loss: 0.001043\n",
      "Epoch: 82/100... Step: 84600... Loss: 0.000053... Val Loss: 0.000845\n",
      "Epoch: 82/100... Step: 84605... Loss: 0.000154... Val Loss: 0.000800\n",
      "Epoch: 82/100... Step: 84610... Loss: 0.000052... Val Loss: 0.001097\n",
      "Epoch: 82/100... Step: 84615... Loss: 0.000161... Val Loss: 0.001193\n",
      "Epoch: 82/100... Step: 84620... Loss: 0.000032... Val Loss: 0.000929\n",
      "Epoch: 83/100... Step: 84625... Loss: 0.000365... Val Loss: 0.000942\n",
      "Epoch: 83/100... Step: 84630... Loss: 0.000199... Val Loss: 0.001329\n",
      "Epoch: 83/100... Step: 84635... Loss: 0.000184... Val Loss: 0.001722\n",
      "Epoch: 83/100... Step: 84640... Loss: 0.000153... Val Loss: 0.001556\n",
      "Epoch: 83/100... Step: 84645... Loss: 0.000150... Val Loss: 0.001753\n",
      "Epoch: 83/100... Step: 84650... Loss: 0.000149... Val Loss: 0.001816\n",
      "Epoch: 83/100... Step: 84655... Loss: 0.000153... Val Loss: 0.001766\n",
      "Epoch: 83/100... Step: 84660... Loss: 0.000189... Val Loss: 0.001969\n",
      "Epoch: 83/100... Step: 84665... Loss: 0.000198... Val Loss: 0.001741\n",
      "Epoch: 83/100... Step: 84670... Loss: 0.000205... Val Loss: 0.001547\n",
      "Epoch: 83/100... Step: 84675... Loss: 0.000059... Val Loss: 0.001826\n",
      "Epoch: 83/100... Step: 84680... Loss: 0.000147... Val Loss: 0.001822\n",
      "Epoch: 83/100... Step: 84685... Loss: 0.000081... Val Loss: 0.001539\n",
      "Epoch: 83/100... Step: 84690... Loss: 0.000064... Val Loss: 0.001533\n",
      "Epoch: 83/100... Step: 84695... Loss: 0.000289... Val Loss: 0.001525\n",
      "Epoch: 83/100... Step: 84700... Loss: 0.000101... Val Loss: 0.001140\n",
      "Epoch: 83/100... Step: 84705... Loss: 0.000368... Val Loss: 0.000960\n",
      "Epoch: 83/100... Step: 84710... Loss: 0.000159... Val Loss: 0.001319\n",
      "Epoch: 83/100... Step: 84715... Loss: 0.000263... Val Loss: 0.001556\n",
      "Epoch: 83/100... Step: 84720... Loss: 0.000159... Val Loss: 0.001167\n",
      "Epoch: 83/100... Step: 84725... Loss: 0.000257... Val Loss: 0.000834\n",
      "Epoch: 83/100... Step: 84730... Loss: 0.000050... Val Loss: 0.001224\n",
      "Epoch: 83/100... Step: 84735... Loss: 0.000115... Val Loss: 0.001231\n",
      "Epoch: 83/100... Step: 84740... Loss: 0.000094... Val Loss: 0.001071\n",
      "Epoch: 83/100... Step: 84745... Loss: 0.000081... Val Loss: 0.001238\n",
      "Epoch: 83/100... Step: 84750... Loss: 0.000085... Val Loss: 0.001267\n",
      "Epoch: 83/100... Step: 84755... Loss: 0.000038... Val Loss: 0.001430\n",
      "Epoch: 83/100... Step: 84760... Loss: 0.000032... Val Loss: 0.001367\n",
      "Epoch: 83/100... Step: 84765... Loss: 0.000142... Val Loss: 0.001412\n",
      "Epoch: 83/100... Step: 84770... Loss: 0.000070... Val Loss: 0.001707\n",
      "Epoch: 83/100... Step: 84775... Loss: 0.000170... Val Loss: 0.001799\n",
      "Epoch: 83/100... Step: 84780... Loss: 0.000101... Val Loss: 0.001490\n",
      "Epoch: 83/100... Step: 84785... Loss: 0.000048... Val Loss: 0.001463\n",
      "Epoch: 83/100... Step: 84790... Loss: 0.000054... Val Loss: 0.001414\n",
      "Epoch: 83/100... Step: 84795... Loss: 0.000106... Val Loss: 0.001283\n",
      "Epoch: 83/100... Step: 84800... Loss: 0.000057... Val Loss: 0.001417\n",
      "Epoch: 83/100... Step: 84805... Loss: 0.000033... Val Loss: 0.001298\n",
      "Epoch: 83/100... Step: 84810... Loss: 0.000102... Val Loss: 0.001247\n",
      "Epoch: 83/100... Step: 84815... Loss: 0.000031... Val Loss: 0.001092\n",
      "Epoch: 83/100... Step: 84820... Loss: 0.000027... Val Loss: 0.001214\n",
      "Epoch: 83/100... Step: 84825... Loss: 0.000037... Val Loss: 0.001149\n",
      "Epoch: 83/100... Step: 84830... Loss: 0.000074... Val Loss: 0.001144\n",
      "Epoch: 83/100... Step: 84835... Loss: 0.000082... Val Loss: 0.001320\n",
      "Epoch: 83/100... Step: 84840... Loss: 0.000045... Val Loss: 0.001261\n",
      "Epoch: 83/100... Step: 84845... Loss: 0.000049... Val Loss: 0.001258\n",
      "Epoch: 83/100... Step: 84850... Loss: 0.000059... Val Loss: 0.001158\n",
      "Epoch: 83/100... Step: 84855... Loss: 0.000110... Val Loss: 0.001106\n",
      "Epoch: 83/100... Step: 84860... Loss: 0.000071... Val Loss: 0.001008\n",
      "Epoch: 83/100... Step: 84865... Loss: 0.000063... Val Loss: 0.001187\n",
      "Epoch: 83/100... Step: 84870... Loss: 0.000137... Val Loss: 0.001246\n",
      "Epoch: 83/100... Step: 84875... Loss: 0.000091... Val Loss: 0.001051\n",
      "Epoch: 83/100... Step: 84880... Loss: 0.000078... Val Loss: 0.001159\n",
      "Epoch: 83/100... Step: 84885... Loss: 0.000237... Val Loss: 0.001190\n",
      "Epoch: 83/100... Step: 84890... Loss: 0.000048... Val Loss: 0.000869\n",
      "Epoch: 83/100... Step: 84895... Loss: 0.000155... Val Loss: 0.000773\n",
      "Epoch: 83/100... Step: 84900... Loss: 0.000060... Val Loss: 0.000962\n",
      "Epoch: 83/100... Step: 84905... Loss: 0.000040... Val Loss: 0.000911\n",
      "Epoch: 83/100... Step: 84910... Loss: 0.000075... Val Loss: 0.000947\n",
      "Epoch: 83/100... Step: 84915... Loss: 0.000180... Val Loss: 0.000817\n",
      "Epoch: 83/100... Step: 84920... Loss: 0.000102... Val Loss: 0.000490\n",
      "Epoch: 83/100... Step: 84925... Loss: 0.000084... Val Loss: 0.000559\n",
      "Epoch: 83/100... Step: 84930... Loss: 0.000103... Val Loss: 0.000745\n",
      "Epoch: 83/100... Step: 84935... Loss: 0.000211... Val Loss: 0.000739\n",
      "Epoch: 83/100... Step: 84940... Loss: 0.000097... Val Loss: 0.000471\n",
      "Epoch: 83/100... Step: 84945... Loss: 0.000132... Val Loss: 0.000559\n",
      "Epoch: 83/100... Step: 84950... Loss: 0.000241... Val Loss: 0.000727\n",
      "Epoch: 83/100... Step: 84955... Loss: 0.000141... Val Loss: 0.000503\n",
      "Epoch: 83/100... Step: 84960... Loss: 0.000243... Val Loss: 0.000322\n",
      "Epoch: 83/100... Step: 84965... Loss: 0.000208... Val Loss: 0.000527\n",
      "Epoch: 83/100... Step: 84970... Loss: 0.000369... Val Loss: 0.000797\n",
      "Epoch: 83/100... Step: 84975... Loss: 0.000329... Val Loss: 0.000561\n",
      "Epoch: 83/100... Step: 84980... Loss: 0.000385... Val Loss: 0.000283\n",
      "Epoch: 83/100... Step: 84985... Loss: 0.000435... Val Loss: 0.000469\n",
      "Epoch: 83/100... Step: 84990... Loss: 0.000089... Val Loss: 0.001100\n",
      "Epoch: 83/100... Step: 84995... Loss: 0.000241... Val Loss: 0.001158\n",
      "Epoch: 83/100... Step: 85000... Loss: 0.000244... Val Loss: 0.000788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83/100... Step: 85005... Loss: 0.000073... Val Loss: 0.001144\n",
      "Epoch: 83/100... Step: 85010... Loss: 0.000279... Val Loss: 0.001261\n",
      "Epoch: 83/100... Step: 85015... Loss: 0.000082... Val Loss: 0.000846\n",
      "Epoch: 83/100... Step: 85020... Loss: 0.000255... Val Loss: 0.000877\n",
      "Epoch: 83/100... Step: 85025... Loss: 0.000122... Val Loss: 0.001242\n",
      "Epoch: 83/100... Step: 85030... Loss: 0.000466... Val Loss: 0.001095\n",
      "Epoch: 83/100... Step: 85035... Loss: 0.000496... Val Loss: 0.000559\n",
      "Epoch: 83/100... Step: 85040... Loss: 0.000734... Val Loss: 0.000488\n",
      "Epoch: 83/100... Step: 85045... Loss: 0.000465... Val Loss: 0.001756\n",
      "Epoch: 83/100... Step: 85050... Loss: 0.000380... Val Loss: 0.001297\n",
      "Epoch: 83/100... Step: 85055... Loss: 0.000520... Val Loss: 0.000601\n",
      "Epoch: 83/100... Step: 85060... Loss: 0.000258... Val Loss: 0.001055\n",
      "Epoch: 83/100... Step: 85065... Loss: 0.000368... Val Loss: 0.001553\n",
      "Epoch: 83/100... Step: 85070... Loss: 0.000117... Val Loss: 0.001101\n",
      "Epoch: 83/100... Step: 85075... Loss: 0.000393... Val Loss: 0.000972\n",
      "Epoch: 83/100... Step: 85080... Loss: 0.000119... Val Loss: 0.001463\n",
      "Epoch: 83/100... Step: 85085... Loss: 0.000167... Val Loss: 0.001551\n",
      "Epoch: 83/100... Step: 85090... Loss: 0.000149... Val Loss: 0.001291\n",
      "Epoch: 83/100... Step: 85095... Loss: 0.000151... Val Loss: 0.001248\n",
      "Epoch: 83/100... Step: 85100... Loss: 0.000121... Val Loss: 0.001011\n",
      "Epoch: 83/100... Step: 85105... Loss: 0.000105... Val Loss: 0.001180\n",
      "Epoch: 83/100... Step: 85110... Loss: 0.000393... Val Loss: 0.001328\n",
      "Epoch: 83/100... Step: 85115... Loss: 0.000095... Val Loss: 0.000893\n",
      "Epoch: 83/100... Step: 85120... Loss: 0.000148... Val Loss: 0.000692\n",
      "Epoch: 83/100... Step: 85125... Loss: 0.000106... Val Loss: 0.000901\n",
      "Epoch: 83/100... Step: 85130... Loss: 0.000273... Val Loss: 0.000721\n",
      "Epoch: 83/100... Step: 85135... Loss: 0.000146... Val Loss: 0.000285\n",
      "Epoch: 83/100... Step: 85140... Loss: 0.000276... Val Loss: 0.000268\n",
      "Epoch: 83/100... Step: 85145... Loss: 0.000195... Val Loss: 0.000579\n",
      "Epoch: 83/100... Step: 85150... Loss: 0.000084... Val Loss: 0.000639\n",
      "Epoch: 83/100... Step: 85155... Loss: 0.000098... Val Loss: 0.000763\n",
      "Epoch: 83/100... Step: 85160... Loss: 0.000159... Val Loss: 0.000730\n",
      "Epoch: 83/100... Step: 85165... Loss: 0.000207... Val Loss: 0.000486\n",
      "Epoch: 83/100... Step: 85170... Loss: 0.000204... Val Loss: 0.000772\n",
      "Epoch: 83/100... Step: 85175... Loss: 0.000174... Val Loss: 0.001062\n",
      "Epoch: 83/100... Step: 85180... Loss: 0.000145... Val Loss: 0.000833\n",
      "Epoch: 83/100... Step: 85185... Loss: 0.000123... Val Loss: 0.000625\n",
      "Epoch: 83/100... Step: 85190... Loss: 0.000060... Val Loss: 0.000869\n",
      "Epoch: 83/100... Step: 85195... Loss: 0.000069... Val Loss: 0.000865\n",
      "Epoch: 83/100... Step: 85200... Loss: 0.000113... Val Loss: 0.000923\n",
      "Epoch: 83/100... Step: 85205... Loss: 0.000111... Val Loss: 0.000908\n",
      "Epoch: 83/100... Step: 85210... Loss: 0.000085... Val Loss: 0.000775\n",
      "Epoch: 83/100... Step: 85215... Loss: 0.000071... Val Loss: 0.000959\n",
      "Epoch: 83/100... Step: 85220... Loss: 0.000069... Val Loss: 0.000959\n",
      "Epoch: 83/100... Step: 85225... Loss: 0.000105... Val Loss: 0.000915\n",
      "Epoch: 83/100... Step: 85230... Loss: 0.000054... Val Loss: 0.000992\n",
      "Epoch: 83/100... Step: 85235... Loss: 0.000055... Val Loss: 0.000891\n",
      "Epoch: 83/100... Step: 85240... Loss: 0.000092... Val Loss: 0.000805\n",
      "Epoch: 83/100... Step: 85245... Loss: 0.000053... Val Loss: 0.000971\n",
      "Epoch: 83/100... Step: 85250... Loss: 0.000069... Val Loss: 0.000922\n",
      "Epoch: 83/100... Step: 85255... Loss: 0.000050... Val Loss: 0.000860\n",
      "Epoch: 83/100... Step: 85260... Loss: 0.000042... Val Loss: 0.000877\n",
      "Epoch: 83/100... Step: 85265... Loss: 0.000095... Val Loss: 0.000779\n",
      "Epoch: 83/100... Step: 85270... Loss: 0.000051... Val Loss: 0.000556\n",
      "Epoch: 83/100... Step: 85275... Loss: 0.000256... Val Loss: 0.000559\n",
      "Epoch: 83/100... Step: 85280... Loss: 0.000097... Val Loss: 0.000870\n",
      "Epoch: 83/100... Step: 85285... Loss: 0.000267... Val Loss: 0.001049\n",
      "Epoch: 83/100... Step: 85290... Loss: 0.000294... Val Loss: 0.000774\n",
      "Epoch: 83/100... Step: 85295... Loss: 0.000121... Val Loss: 0.000265\n",
      "Epoch: 83/100... Step: 85300... Loss: 0.000196... Val Loss: 0.000291\n",
      "Epoch: 83/100... Step: 85305... Loss: 0.000243... Val Loss: 0.000476\n",
      "Epoch: 83/100... Step: 85310... Loss: 0.000122... Val Loss: 0.000277\n",
      "Epoch: 83/100... Step: 85315... Loss: 0.000283... Val Loss: 0.000261\n",
      "Epoch: 83/100... Step: 85320... Loss: 0.000054... Val Loss: 0.000352\n",
      "Epoch: 83/100... Step: 85325... Loss: 0.000186... Val Loss: 0.000420\n",
      "Epoch: 83/100... Step: 85330... Loss: 0.000096... Val Loss: 0.000270\n",
      "Epoch: 83/100... Step: 85335... Loss: 0.000112... Val Loss: 0.000308\n",
      "Epoch: 83/100... Step: 85340... Loss: 0.000073... Val Loss: 0.000496\n",
      "Epoch: 83/100... Step: 85345... Loss: 0.000039... Val Loss: 0.000400\n",
      "Epoch: 83/100... Step: 85350... Loss: 0.000042... Val Loss: 0.000375\n",
      "Epoch: 83/100... Step: 85355... Loss: 0.000053... Val Loss: 0.000384\n",
      "Epoch: 83/100... Step: 85360... Loss: 0.000163... Val Loss: 0.000406\n",
      "Epoch: 83/100... Step: 85365... Loss: 0.000322... Val Loss: 0.000268\n",
      "Epoch: 83/100... Step: 85370... Loss: 0.000051... Val Loss: 0.000503\n",
      "Epoch: 83/100... Step: 85375... Loss: 0.000265... Val Loss: 0.000551\n",
      "Epoch: 83/100... Step: 85380... Loss: 0.000098... Val Loss: 0.000291\n",
      "Epoch: 83/100... Step: 85385... Loss: 0.000096... Val Loss: 0.000331\n",
      "Epoch: 83/100... Step: 85390... Loss: 0.000109... Val Loss: 0.000434\n",
      "Epoch: 83/100... Step: 85395... Loss: 0.000068... Val Loss: 0.000393\n",
      "Epoch: 83/100... Step: 85400... Loss: 0.000060... Val Loss: 0.000436\n",
      "Epoch: 83/100... Step: 85405... Loss: 0.000156... Val Loss: 0.000419\n",
      "Epoch: 83/100... Step: 85410... Loss: 0.000058... Val Loss: 0.000641\n",
      "Epoch: 83/100... Step: 85415... Loss: 0.000159... Val Loss: 0.000547\n",
      "Epoch: 83/100... Step: 85420... Loss: 0.000056... Val Loss: 0.000323\n",
      "Epoch: 83/100... Step: 85425... Loss: 0.000164... Val Loss: 0.000355\n",
      "Epoch: 83/100... Step: 85430... Loss: 0.000185... Val Loss: 0.000591\n",
      "Epoch: 83/100... Step: 85435... Loss: 0.000135... Val Loss: 0.000860\n",
      "Epoch: 83/100... Step: 85440... Loss: 0.000111... Val Loss: 0.000712\n",
      "Epoch: 83/100... Step: 85445... Loss: 0.000090... Val Loss: 0.000576\n",
      "Epoch: 83/100... Step: 85450... Loss: 0.000094... Val Loss: 0.000651\n",
      "Epoch: 83/100... Step: 85455... Loss: 0.000062... Val Loss: 0.000578\n",
      "Epoch: 83/100... Step: 85460... Loss: 0.000081... Val Loss: 0.000674\n",
      "Epoch: 83/100... Step: 85465... Loss: 0.000065... Val Loss: 0.000560\n",
      "Epoch: 83/100... Step: 85470... Loss: 0.000047... Val Loss: 0.000612\n",
      "Epoch: 83/100... Step: 85475... Loss: 0.000039... Val Loss: 0.000629\n",
      "Epoch: 83/100... Step: 85480... Loss: 0.000061... Val Loss: 0.000649\n",
      "Epoch: 83/100... Step: 85485... Loss: 0.000062... Val Loss: 0.000802\n",
      "Epoch: 83/100... Step: 85490... Loss: 0.000021... Val Loss: 0.000761\n",
      "Epoch: 83/100... Step: 85495... Loss: 0.000022... Val Loss: 0.000766\n",
      "Epoch: 83/100... Step: 85500... Loss: 0.000050... Val Loss: 0.000758\n",
      "Epoch: 83/100... Step: 85505... Loss: 0.000033... Val Loss: 0.000834\n",
      "Epoch: 83/100... Step: 85510... Loss: 0.000087... Val Loss: 0.000738\n",
      "Epoch: 83/100... Step: 85515... Loss: 0.000084... Val Loss: 0.000612\n",
      "Epoch: 83/100... Step: 85520... Loss: 0.000051... Val Loss: 0.000723\n",
      "Epoch: 83/100... Step: 85525... Loss: 0.000093... Val Loss: 0.000739\n",
      "Epoch: 83/100... Step: 85530... Loss: 0.000044... Val Loss: 0.000561\n",
      "Epoch: 83/100... Step: 85535... Loss: 0.000034... Val Loss: 0.000565\n",
      "Epoch: 83/100... Step: 85540... Loss: 0.000039... Val Loss: 0.000623\n",
      "Epoch: 83/100... Step: 85545... Loss: 0.000032... Val Loss: 0.000673\n",
      "Epoch: 83/100... Step: 85550... Loss: 0.000042... Val Loss: 0.000741\n",
      "Epoch: 83/100... Step: 85555... Loss: 0.000040... Val Loss: 0.000801\n",
      "Epoch: 83/100... Step: 85560... Loss: 0.000028... Val Loss: 0.000878\n",
      "Epoch: 83/100... Step: 85565... Loss: 0.000041... Val Loss: 0.000869\n",
      "Epoch: 83/100... Step: 85570... Loss: 0.000048... Val Loss: 0.000755\n",
      "Epoch: 83/100... Step: 85575... Loss: 0.000093... Val Loss: 0.000773\n",
      "Epoch: 83/100... Step: 85580... Loss: 0.000043... Val Loss: 0.000974\n",
      "Epoch: 83/100... Step: 85585... Loss: 0.000107... Val Loss: 0.001058\n",
      "Epoch: 83/100... Step: 85590... Loss: 0.000052... Val Loss: 0.000902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83/100... Step: 85595... Loss: 0.000027... Val Loss: 0.000997\n",
      "Epoch: 83/100... Step: 85600... Loss: 0.000057... Val Loss: 0.000997\n",
      "Epoch: 83/100... Step: 85605... Loss: 0.000075... Val Loss: 0.000848\n",
      "Epoch: 83/100... Step: 85610... Loss: 0.000135... Val Loss: 0.000952\n",
      "Epoch: 83/100... Step: 85615... Loss: 0.000098... Val Loss: 0.001191\n",
      "Epoch: 83/100... Step: 85620... Loss: 0.000087... Val Loss: 0.001095\n",
      "Epoch: 83/100... Step: 85625... Loss: 0.000076... Val Loss: 0.000928\n",
      "Epoch: 83/100... Step: 85630... Loss: 0.000075... Val Loss: 0.000969\n",
      "Epoch: 83/100... Step: 85635... Loss: 0.000059... Val Loss: 0.000835\n",
      "Epoch: 83/100... Step: 85640... Loss: 0.000187... Val Loss: 0.000840\n",
      "Epoch: 83/100... Step: 85645... Loss: 0.000053... Val Loss: 0.001125\n",
      "Epoch: 83/100... Step: 85650... Loss: 0.000189... Val Loss: 0.001191\n",
      "Epoch: 83/100... Step: 85655... Loss: 0.000058... Val Loss: 0.000922\n",
      "Epoch: 84/100... Step: 85660... Loss: 0.000534... Val Loss: 0.000861\n",
      "Epoch: 84/100... Step: 85665... Loss: 0.000413... Val Loss: 0.001177\n",
      "Epoch: 84/100... Step: 85670... Loss: 0.000122... Val Loss: 0.001750\n",
      "Epoch: 84/100... Step: 85675... Loss: 0.000197... Val Loss: 0.001905\n",
      "Epoch: 84/100... Step: 85680... Loss: 0.000125... Val Loss: 0.001652\n",
      "Epoch: 84/100... Step: 85685... Loss: 0.000183... Val Loss: 0.001694\n",
      "Epoch: 84/100... Step: 85690... Loss: 0.000152... Val Loss: 0.002025\n",
      "Epoch: 84/100... Step: 85695... Loss: 0.000277... Val Loss: 0.001857\n",
      "Epoch: 84/100... Step: 85700... Loss: 0.000208... Val Loss: 0.001488\n",
      "Epoch: 84/100... Step: 85705... Loss: 0.000196... Val Loss: 0.001632\n",
      "Epoch: 84/100... Step: 85710... Loss: 0.000164... Val Loss: 0.001905\n",
      "Epoch: 84/100... Step: 85715... Loss: 0.000112... Val Loss: 0.001721\n",
      "Epoch: 84/100... Step: 85720... Loss: 0.000076... Val Loss: 0.001469\n",
      "Epoch: 84/100... Step: 85725... Loss: 0.000158... Val Loss: 0.001526\n",
      "Epoch: 84/100... Step: 85730... Loss: 0.000318... Val Loss: 0.001399\n",
      "Epoch: 84/100... Step: 85735... Loss: 0.000194... Val Loss: 0.001039\n",
      "Epoch: 84/100... Step: 85740... Loss: 0.000321... Val Loss: 0.001118\n",
      "Epoch: 84/100... Step: 85745... Loss: 0.000085... Val Loss: 0.001473\n",
      "Epoch: 84/100... Step: 85750... Loss: 0.000261... Val Loss: 0.001420\n",
      "Epoch: 84/100... Step: 85755... Loss: 0.000102... Val Loss: 0.000995\n",
      "Epoch: 84/100... Step: 85760... Loss: 0.000296... Val Loss: 0.000867\n",
      "Epoch: 84/100... Step: 85765... Loss: 0.000030... Val Loss: 0.001232\n",
      "Epoch: 84/100... Step: 85770... Loss: 0.000146... Val Loss: 0.001263\n",
      "Epoch: 84/100... Step: 85775... Loss: 0.000111... Val Loss: 0.001058\n",
      "Epoch: 84/100... Step: 85780... Loss: 0.000106... Val Loss: 0.001162\n",
      "Epoch: 84/100... Step: 85785... Loss: 0.000117... Val Loss: 0.001381\n",
      "Epoch: 84/100... Step: 85790... Loss: 0.000042... Val Loss: 0.001412\n",
      "Epoch: 84/100... Step: 85795... Loss: 0.000056... Val Loss: 0.001466\n",
      "Epoch: 84/100... Step: 85800... Loss: 0.000180... Val Loss: 0.001448\n",
      "Epoch: 84/100... Step: 85805... Loss: 0.000045... Val Loss: 0.001732\n",
      "Epoch: 84/100... Step: 85810... Loss: 0.000215... Val Loss: 0.001791\n",
      "Epoch: 84/100... Step: 85815... Loss: 0.000072... Val Loss: 0.001448\n",
      "Epoch: 84/100... Step: 85820... Loss: 0.000114... Val Loss: 0.001336\n",
      "Epoch: 84/100... Step: 85825... Loss: 0.000103... Val Loss: 0.001440\n",
      "Epoch: 84/100... Step: 85830... Loss: 0.000057... Val Loss: 0.001321\n",
      "Epoch: 84/100... Step: 85835... Loss: 0.000040... Val Loss: 0.001363\n",
      "Epoch: 84/100... Step: 85840... Loss: 0.000070... Val Loss: 0.001288\n",
      "Epoch: 84/100... Step: 85845... Loss: 0.000046... Val Loss: 0.001124\n",
      "Epoch: 84/100... Step: 85850... Loss: 0.000054... Val Loss: 0.001141\n",
      "Epoch: 84/100... Step: 85855... Loss: 0.000044... Val Loss: 0.001187\n",
      "Epoch: 84/100... Step: 85860... Loss: 0.000076... Val Loss: 0.001126\n",
      "Epoch: 84/100... Step: 85865... Loss: 0.000030... Val Loss: 0.001283\n",
      "Epoch: 84/100... Step: 85870... Loss: 0.000028... Val Loss: 0.001284\n",
      "Epoch: 84/100... Step: 85875... Loss: 0.000043... Val Loss: 0.001249\n",
      "Epoch: 84/100... Step: 85880... Loss: 0.000056... Val Loss: 0.001211\n",
      "Epoch: 84/100... Step: 85885... Loss: 0.000099... Val Loss: 0.001144\n",
      "Epoch: 84/100... Step: 85890... Loss: 0.000030... Val Loss: 0.001024\n",
      "Epoch: 84/100... Step: 85895... Loss: 0.000121... Val Loss: 0.001064\n",
      "Epoch: 84/100... Step: 85900... Loss: 0.000099... Val Loss: 0.001259\n",
      "Epoch: 84/100... Step: 85905... Loss: 0.000115... Val Loss: 0.001196\n",
      "Epoch: 84/100... Step: 85910... Loss: 0.000100... Val Loss: 0.001078\n",
      "Epoch: 84/100... Step: 85915... Loss: 0.000112... Val Loss: 0.001166\n",
      "Epoch: 84/100... Step: 85920... Loss: 0.000163... Val Loss: 0.001055\n",
      "Epoch: 84/100... Step: 85925... Loss: 0.000087... Val Loss: 0.000803\n",
      "Epoch: 84/100... Step: 85930... Loss: 0.000057... Val Loss: 0.000867\n",
      "Epoch: 84/100... Step: 85935... Loss: 0.000047... Val Loss: 0.000952\n",
      "Epoch: 84/100... Step: 85940... Loss: 0.000038... Val Loss: 0.000926\n",
      "Epoch: 84/100... Step: 85945... Loss: 0.000153... Val Loss: 0.000900\n",
      "Epoch: 84/100... Step: 85950... Loss: 0.000115... Val Loss: 0.000679\n",
      "Epoch: 84/100... Step: 85955... Loss: 0.000152... Val Loss: 0.000430\n",
      "Epoch: 84/100... Step: 85960... Loss: 0.000196... Val Loss: 0.000540\n",
      "Epoch: 84/100... Step: 85965... Loss: 0.000136... Val Loss: 0.000788\n",
      "Epoch: 84/100... Step: 85970... Loss: 0.000225... Val Loss: 0.000742\n",
      "Epoch: 84/100... Step: 85975... Loss: 0.000150... Val Loss: 0.000478\n",
      "Epoch: 84/100... Step: 85980... Loss: 0.000092... Val Loss: 0.000586\n",
      "Epoch: 84/100... Step: 85985... Loss: 0.000231... Val Loss: 0.000622\n",
      "Epoch: 84/100... Step: 85990... Loss: 0.000051... Val Loss: 0.000373\n",
      "Epoch: 84/100... Step: 85995... Loss: 0.000310... Val Loss: 0.000367\n",
      "Epoch: 84/100... Step: 86000... Loss: 0.000177... Val Loss: 0.000612\n",
      "Epoch: 84/100... Step: 86005... Loss: 0.000445... Val Loss: 0.000721\n",
      "Epoch: 84/100... Step: 86010... Loss: 0.000197... Val Loss: 0.000527\n",
      "Epoch: 84/100... Step: 86015... Loss: 0.000319... Val Loss: 0.000464\n",
      "Epoch: 84/100... Step: 86020... Loss: 0.000238... Val Loss: 0.000720\n",
      "Epoch: 84/100... Step: 86025... Loss: 0.000050... Val Loss: 0.001136\n",
      "Epoch: 84/100... Step: 86030... Loss: 0.000196... Val Loss: 0.001159\n",
      "Epoch: 84/100... Step: 86035... Loss: 0.000152... Val Loss: 0.000867\n",
      "Epoch: 84/100... Step: 86040... Loss: 0.000101... Val Loss: 0.001066\n",
      "Epoch: 84/100... Step: 86045... Loss: 0.000179... Val Loss: 0.001127\n",
      "Epoch: 84/100... Step: 86050... Loss: 0.000141... Val Loss: 0.000879\n",
      "Epoch: 84/100... Step: 86055... Loss: 0.000117... Val Loss: 0.001047\n",
      "Epoch: 84/100... Step: 86060... Loss: 0.000340... Val Loss: 0.001239\n",
      "Epoch: 84/100... Step: 86065... Loss: 0.000696... Val Loss: 0.001030\n",
      "Epoch: 84/100... Step: 86070... Loss: 0.000979... Val Loss: 0.000545\n",
      "Epoch: 84/100... Step: 86075... Loss: 0.000920... Val Loss: 0.000496\n",
      "Epoch: 84/100... Step: 86080... Loss: 0.000732... Val Loss: 0.001886\n",
      "Epoch: 84/100... Step: 86085... Loss: 0.000613... Val Loss: 0.001638\n",
      "Epoch: 84/100... Step: 86090... Loss: 0.000332... Val Loss: 0.000834\n",
      "Epoch: 84/100... Step: 86095... Loss: 0.000355... Val Loss: 0.000914\n",
      "Epoch: 84/100... Step: 86100... Loss: 0.000188... Val Loss: 0.001371\n",
      "Epoch: 84/100... Step: 86105... Loss: 0.000140... Val Loss: 0.001186\n",
      "Epoch: 84/100... Step: 86110... Loss: 0.000157... Val Loss: 0.001188\n",
      "Epoch: 84/100... Step: 86115... Loss: 0.000183... Val Loss: 0.001461\n",
      "Epoch: 84/100... Step: 86120... Loss: 0.000206... Val Loss: 0.001442\n",
      "Epoch: 84/100... Step: 86125... Loss: 0.000198... Val Loss: 0.001371\n",
      "Epoch: 84/100... Step: 86130... Loss: 0.000101... Val Loss: 0.001063\n",
      "Epoch: 84/100... Step: 86135... Loss: 0.000271... Val Loss: 0.000950\n",
      "Epoch: 84/100... Step: 86140... Loss: 0.000138... Val Loss: 0.001255\n",
      "Epoch: 84/100... Step: 86145... Loss: 0.000383... Val Loss: 0.001275\n",
      "Epoch: 84/100... Step: 86150... Loss: 0.000124... Val Loss: 0.000884\n",
      "Epoch: 84/100... Step: 86155... Loss: 0.000129... Val Loss: 0.000602\n",
      "Epoch: 84/100... Step: 86160... Loss: 0.000164... Val Loss: 0.000642\n",
      "Epoch: 84/100... Step: 86165... Loss: 0.000305... Val Loss: 0.000537\n",
      "Epoch: 84/100... Step: 86170... Loss: 0.000108... Val Loss: 0.000294\n",
      "Epoch: 84/100... Step: 86175... Loss: 0.000335... Val Loss: 0.000271\n",
      "Epoch: 84/100... Step: 86180... Loss: 0.000217... Val Loss: 0.000453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84/100... Step: 86185... Loss: 0.000139... Val Loss: 0.000884\n",
      "Epoch: 84/100... Step: 86190... Loss: 0.000201... Val Loss: 0.000782\n",
      "Epoch: 84/100... Step: 86195... Loss: 0.000129... Val Loss: 0.000463\n",
      "Epoch: 84/100... Step: 86200... Loss: 0.000242... Val Loss: 0.000621\n",
      "Epoch: 84/100... Step: 86205... Loss: 0.000081... Val Loss: 0.001033\n",
      "Epoch: 84/100... Step: 86210... Loss: 0.000248... Val Loss: 0.001038\n",
      "Epoch: 84/100... Step: 86215... Loss: 0.000069... Val Loss: 0.000671\n",
      "Epoch: 84/100... Step: 86220... Loss: 0.000182... Val Loss: 0.000583\n",
      "Epoch: 84/100... Step: 86225... Loss: 0.000045... Val Loss: 0.000906\n",
      "Epoch: 84/100... Step: 86230... Loss: 0.000095... Val Loss: 0.000944\n",
      "Epoch: 84/100... Step: 86235... Loss: 0.000089... Val Loss: 0.000818\n",
      "Epoch: 84/100... Step: 86240... Loss: 0.000056... Val Loss: 0.000822\n",
      "Epoch: 84/100... Step: 86245... Loss: 0.000055... Val Loss: 0.000899\n",
      "Epoch: 84/100... Step: 86250... Loss: 0.000098... Val Loss: 0.000839\n",
      "Epoch: 84/100... Step: 86255... Loss: 0.000067... Val Loss: 0.000992\n",
      "Epoch: 84/100... Step: 86260... Loss: 0.000057... Val Loss: 0.000963\n",
      "Epoch: 84/100... Step: 86265... Loss: 0.000048... Val Loss: 0.000962\n",
      "Epoch: 84/100... Step: 86270... Loss: 0.000033... Val Loss: 0.000820\n",
      "Epoch: 84/100... Step: 86275... Loss: 0.000069... Val Loss: 0.000823\n",
      "Epoch: 84/100... Step: 86280... Loss: 0.000055... Val Loss: 0.001000\n",
      "Epoch: 84/100... Step: 86285... Loss: 0.000047... Val Loss: 0.000911\n",
      "Epoch: 84/100... Step: 86290... Loss: 0.000065... Val Loss: 0.000836\n",
      "Epoch: 84/100... Step: 86295... Loss: 0.000107... Val Loss: 0.000911\n",
      "Epoch: 84/100... Step: 86300... Loss: 0.000134... Val Loss: 0.000715\n",
      "Epoch: 84/100... Step: 86305... Loss: 0.000226... Val Loss: 0.000465\n",
      "Epoch: 84/100... Step: 86310... Loss: 0.000393... Val Loss: 0.000555\n",
      "Epoch: 84/100... Step: 86315... Loss: 0.000077... Val Loss: 0.000901\n",
      "Epoch: 84/100... Step: 86320... Loss: 0.000385... Val Loss: 0.001037\n",
      "Epoch: 84/100... Step: 86325... Loss: 0.000452... Val Loss: 0.000792\n",
      "Epoch: 84/100... Step: 86330... Loss: 0.000059... Val Loss: 0.000292\n",
      "Epoch: 84/100... Step: 86335... Loss: 0.000254... Val Loss: 0.000260\n",
      "Epoch: 84/100... Step: 86340... Loss: 0.000108... Val Loss: 0.000321\n",
      "Epoch: 84/100... Step: 86345... Loss: 0.000081... Val Loss: 0.000317\n",
      "Epoch: 84/100... Step: 86350... Loss: 0.000032... Val Loss: 0.000336\n",
      "Epoch: 84/100... Step: 86355... Loss: 0.000066... Val Loss: 0.000294\n",
      "Epoch: 84/100... Step: 86360... Loss: 0.000086... Val Loss: 0.000292\n",
      "Epoch: 84/100... Step: 86365... Loss: 0.000062... Val Loss: 0.000359\n",
      "Epoch: 84/100... Step: 86370... Loss: 0.000040... Val Loss: 0.000373\n",
      "Epoch: 84/100... Step: 86375... Loss: 0.000045... Val Loss: 0.000458\n",
      "Epoch: 84/100... Step: 86380... Loss: 0.000041... Val Loss: 0.000397\n",
      "Epoch: 84/100... Step: 86385... Loss: 0.000055... Val Loss: 0.000334\n",
      "Epoch: 84/100... Step: 86390... Loss: 0.000089... Val Loss: 0.000428\n",
      "Epoch: 84/100... Step: 86395... Loss: 0.000360... Val Loss: 0.000358\n",
      "Epoch: 84/100... Step: 86400... Loss: 0.000255... Val Loss: 0.000282\n",
      "Epoch: 84/100... Step: 86405... Loss: 0.000153... Val Loss: 0.000619\n",
      "Epoch: 84/100... Step: 86410... Loss: 0.000198... Val Loss: 0.000502\n",
      "Epoch: 84/100... Step: 86415... Loss: 0.000142... Val Loss: 0.000274\n",
      "Epoch: 84/100... Step: 86420... Loss: 0.000060... Val Loss: 0.000366\n",
      "Epoch: 84/100... Step: 86425... Loss: 0.000086... Val Loss: 0.000450\n",
      "Epoch: 84/100... Step: 86430... Loss: 0.000059... Val Loss: 0.000426\n",
      "Epoch: 84/100... Step: 86435... Loss: 0.000117... Val Loss: 0.000438\n",
      "Epoch: 84/100... Step: 86440... Loss: 0.000058... Val Loss: 0.000546\n",
      "Epoch: 84/100... Step: 86445... Loss: 0.000135... Val Loss: 0.000615\n",
      "Epoch: 84/100... Step: 86450... Loss: 0.000093... Val Loss: 0.000414\n",
      "Epoch: 84/100... Step: 86455... Loss: 0.000149... Val Loss: 0.000313\n",
      "Epoch: 84/100... Step: 86460... Loss: 0.000242... Val Loss: 0.000439\n",
      "Epoch: 84/100... Step: 86465... Loss: 0.000121... Val Loss: 0.000740\n",
      "Epoch: 84/100... Step: 86470... Loss: 0.000214... Val Loss: 0.000879\n",
      "Epoch: 84/100... Step: 86475... Loss: 0.000090... Val Loss: 0.000632\n",
      "Epoch: 84/100... Step: 86480... Loss: 0.000135... Val Loss: 0.000504\n",
      "Epoch: 84/100... Step: 86485... Loss: 0.000090... Val Loss: 0.000691\n",
      "Epoch: 84/100... Step: 86490... Loss: 0.000081... Val Loss: 0.000650\n",
      "Epoch: 84/100... Step: 86495... Loss: 0.000085... Val Loss: 0.000553\n",
      "Epoch: 84/100... Step: 86500... Loss: 0.000050... Val Loss: 0.000635\n",
      "Epoch: 84/100... Step: 86505... Loss: 0.000027... Val Loss: 0.000595\n",
      "Epoch: 84/100... Step: 86510... Loss: 0.000032... Val Loss: 0.000662\n",
      "Epoch: 84/100... Step: 86515... Loss: 0.000035... Val Loss: 0.000720\n",
      "Epoch: 84/100... Step: 86520... Loss: 0.000037... Val Loss: 0.000765\n",
      "Epoch: 84/100... Step: 86525... Loss: 0.000030... Val Loss: 0.000747\n",
      "Epoch: 84/100... Step: 86530... Loss: 0.000018... Val Loss: 0.000761\n",
      "Epoch: 84/100... Step: 86535... Loss: 0.000034... Val Loss: 0.000818\n",
      "Epoch: 84/100... Step: 86540... Loss: 0.000059... Val Loss: 0.000800\n",
      "Epoch: 84/100... Step: 86545... Loss: 0.000040... Val Loss: 0.000670\n",
      "Epoch: 84/100... Step: 86550... Loss: 0.000096... Val Loss: 0.000648\n",
      "Epoch: 84/100... Step: 86555... Loss: 0.000083... Val Loss: 0.000754\n",
      "Epoch: 84/100... Step: 86560... Loss: 0.000063... Val Loss: 0.000640\n",
      "Epoch: 84/100... Step: 86565... Loss: 0.000041... Val Loss: 0.000542\n",
      "Epoch: 84/100... Step: 86570... Loss: 0.000040... Val Loss: 0.000602\n",
      "Epoch: 84/100... Step: 86575... Loss: 0.000034... Val Loss: 0.000654\n",
      "Epoch: 84/100... Step: 86580... Loss: 0.000038... Val Loss: 0.000717\n",
      "Epoch: 84/100... Step: 86585... Loss: 0.000032... Val Loss: 0.000779\n",
      "Epoch: 84/100... Step: 86590... Loss: 0.000041... Val Loss: 0.000849\n",
      "Epoch: 84/100... Step: 86595... Loss: 0.000037... Val Loss: 0.000886\n",
      "Epoch: 84/100... Step: 86600... Loss: 0.000051... Val Loss: 0.000829\n",
      "Epoch: 84/100... Step: 86605... Loss: 0.000044... Val Loss: 0.000757\n",
      "Epoch: 84/100... Step: 86610... Loss: 0.000137... Val Loss: 0.000810\n",
      "Epoch: 84/100... Step: 86615... Loss: 0.000059... Val Loss: 0.001069\n",
      "Epoch: 84/100... Step: 86620... Loss: 0.000136... Val Loss: 0.001081\n",
      "Epoch: 84/100... Step: 86625... Loss: 0.000072... Val Loss: 0.000873\n",
      "Epoch: 84/100... Step: 86630... Loss: 0.000090... Val Loss: 0.000913\n",
      "Epoch: 84/100... Step: 86635... Loss: 0.000135... Val Loss: 0.001052\n",
      "Epoch: 84/100... Step: 86640... Loss: 0.000098... Val Loss: 0.000960\n",
      "Epoch: 84/100... Step: 86645... Loss: 0.000118... Val Loss: 0.000968\n",
      "Epoch: 84/100... Step: 86650... Loss: 0.000068... Val Loss: 0.001137\n",
      "Epoch: 84/100... Step: 86655... Loss: 0.000058... Val Loss: 0.001039\n",
      "Epoch: 84/100... Step: 86660... Loss: 0.000062... Val Loss: 0.000896\n",
      "Epoch: 84/100... Step: 86665... Loss: 0.000046... Val Loss: 0.000864\n",
      "Epoch: 84/100... Step: 86670... Loss: 0.000103... Val Loss: 0.000867\n",
      "Epoch: 84/100... Step: 86675... Loss: 0.000066... Val Loss: 0.001042\n",
      "Epoch: 84/100... Step: 86680... Loss: 0.000118... Val Loss: 0.001167\n",
      "Epoch: 84/100... Step: 86685... Loss: 0.000062... Val Loss: 0.000987\n",
      "Epoch: 85/100... Step: 86690... Loss: 0.000449... Val Loss: 0.000858\n",
      "Epoch: 85/100... Step: 86695... Loss: 0.000485... Val Loss: 0.001014\n",
      "Epoch: 85/100... Step: 86700... Loss: 0.000197... Val Loss: 0.001434\n",
      "Epoch: 85/100... Step: 86705... Loss: 0.000219... Val Loss: 0.001919\n",
      "Epoch: 85/100... Step: 86710... Loss: 0.000304... Val Loss: 0.001959\n",
      "Epoch: 85/100... Step: 86715... Loss: 0.000106... Val Loss: 0.001686\n",
      "Epoch: 85/100... Step: 86720... Loss: 0.000265... Val Loss: 0.001657\n",
      "Epoch: 85/100... Step: 86725... Loss: 0.000179... Val Loss: 0.001929\n",
      "Epoch: 85/100... Step: 86730... Loss: 0.000261... Val Loss: 0.001906\n",
      "Epoch: 85/100... Step: 86735... Loss: 0.000066... Val Loss: 0.001686\n",
      "Epoch: 85/100... Step: 86740... Loss: 0.000071... Val Loss: 0.001718\n",
      "Epoch: 85/100... Step: 86745... Loss: 0.000094... Val Loss: 0.001773\n",
      "Epoch: 85/100... Step: 86750... Loss: 0.000120... Val Loss: 0.001658\n",
      "Epoch: 85/100... Step: 86755... Loss: 0.000104... Val Loss: 0.001450\n",
      "Epoch: 85/100... Step: 86760... Loss: 0.000224... Val Loss: 0.001373\n",
      "Epoch: 85/100... Step: 86765... Loss: 0.000090... Val Loss: 0.001116\n",
      "Epoch: 85/100... Step: 86770... Loss: 0.000288... Val Loss: 0.001064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85/100... Step: 86775... Loss: 0.000068... Val Loss: 0.001372\n",
      "Epoch: 85/100... Step: 86780... Loss: 0.000207... Val Loss: 0.001503\n",
      "Epoch: 85/100... Step: 86785... Loss: 0.000237... Val Loss: 0.001272\n",
      "Epoch: 85/100... Step: 86790... Loss: 0.000137... Val Loss: 0.000880\n",
      "Epoch: 85/100... Step: 86795... Loss: 0.000322... Val Loss: 0.000909\n",
      "Epoch: 85/100... Step: 86800... Loss: 0.000064... Val Loss: 0.001263\n",
      "Epoch: 85/100... Step: 86805... Loss: 0.000111... Val Loss: 0.001245\n",
      "Epoch: 85/100... Step: 86810... Loss: 0.000122... Val Loss: 0.001036\n",
      "Epoch: 85/100... Step: 86815... Loss: 0.000167... Val Loss: 0.001167\n",
      "Epoch: 85/100... Step: 86820... Loss: 0.000073... Val Loss: 0.001500\n",
      "Epoch: 85/100... Step: 86825... Loss: 0.000105... Val Loss: 0.001497\n",
      "Epoch: 85/100... Step: 86830... Loss: 0.000216... Val Loss: 0.001323\n",
      "Epoch: 85/100... Step: 86835... Loss: 0.000255... Val Loss: 0.001493\n",
      "Epoch: 85/100... Step: 86840... Loss: 0.000135... Val Loss: 0.001832\n",
      "Epoch: 85/100... Step: 86845... Loss: 0.000323... Val Loss: 0.001810\n",
      "Epoch: 85/100... Step: 86850... Loss: 0.000124... Val Loss: 0.001450\n",
      "Epoch: 85/100... Step: 86855... Loss: 0.000144... Val Loss: 0.001220\n",
      "Epoch: 85/100... Step: 86860... Loss: 0.000068... Val Loss: 0.001412\n",
      "Epoch: 85/100... Step: 86865... Loss: 0.000107... Val Loss: 0.001444\n",
      "Epoch: 85/100... Step: 86870... Loss: 0.000064... Val Loss: 0.001240\n",
      "Epoch: 85/100... Step: 86875... Loss: 0.000085... Val Loss: 0.001206\n",
      "Epoch: 85/100... Step: 86880... Loss: 0.000041... Val Loss: 0.001099\n",
      "Epoch: 85/100... Step: 86885... Loss: 0.000022... Val Loss: 0.001176\n",
      "Epoch: 85/100... Step: 86890... Loss: 0.000040... Val Loss: 0.001157\n",
      "Epoch: 85/100... Step: 86895... Loss: 0.000080... Val Loss: 0.001174\n",
      "Epoch: 85/100... Step: 86900... Loss: 0.000038... Val Loss: 0.001342\n",
      "Epoch: 85/100... Step: 86905... Loss: 0.000049... Val Loss: 0.001245\n",
      "Epoch: 85/100... Step: 86910... Loss: 0.000042... Val Loss: 0.001181\n",
      "Epoch: 85/100... Step: 86915... Loss: 0.000073... Val Loss: 0.001183\n",
      "Epoch: 85/100... Step: 86920... Loss: 0.000126... Val Loss: 0.001164\n",
      "Epoch: 85/100... Step: 86925... Loss: 0.000069... Val Loss: 0.001012\n",
      "Epoch: 85/100... Step: 86930... Loss: 0.000153... Val Loss: 0.001051\n",
      "Epoch: 85/100... Step: 86935... Loss: 0.000097... Val Loss: 0.001216\n",
      "Epoch: 85/100... Step: 86940... Loss: 0.000031... Val Loss: 0.001145\n",
      "Epoch: 85/100... Step: 86945... Loss: 0.000076... Val Loss: 0.001170\n",
      "Epoch: 85/100... Step: 86950... Loss: 0.000163... Val Loss: 0.001083\n",
      "Epoch: 85/100... Step: 86955... Loss: 0.000064... Val Loss: 0.000838\n",
      "Epoch: 85/100... Step: 86960... Loss: 0.000095... Val Loss: 0.000838\n",
      "Epoch: 85/100... Step: 86965... Loss: 0.000057... Val Loss: 0.000936\n",
      "Epoch: 85/100... Step: 86970... Loss: 0.000035... Val Loss: 0.000922\n",
      "Epoch: 85/100... Step: 86975... Loss: 0.000120... Val Loss: 0.000936\n",
      "Epoch: 85/100... Step: 86980... Loss: 0.000180... Val Loss: 0.000796\n",
      "Epoch: 85/100... Step: 86985... Loss: 0.000053... Val Loss: 0.000503\n",
      "Epoch: 85/100... Step: 86990... Loss: 0.000210... Val Loss: 0.000446\n",
      "Epoch: 85/100... Step: 86995... Loss: 0.000145... Val Loss: 0.000630\n",
      "Epoch: 85/100... Step: 87000... Loss: 0.000224... Val Loss: 0.000777\n",
      "Epoch: 85/100... Step: 87005... Loss: 0.000102... Val Loss: 0.000641\n",
      "Epoch: 85/100... Step: 87010... Loss: 0.000133... Val Loss: 0.000517\n",
      "Epoch: 85/100... Step: 87015... Loss: 0.000113... Val Loss: 0.000579\n",
      "Epoch: 85/100... Step: 87020... Loss: 0.000082... Val Loss: 0.000458\n",
      "Epoch: 85/100... Step: 87025... Loss: 0.000187... Val Loss: 0.000391\n",
      "Epoch: 85/100... Step: 87030... Loss: 0.000142... Val Loss: 0.000578\n",
      "Epoch: 85/100... Step: 87035... Loss: 0.000384... Val Loss: 0.000757\n",
      "Epoch: 85/100... Step: 87040... Loss: 0.000329... Val Loss: 0.000645\n",
      "Epoch: 85/100... Step: 87045... Loss: 0.000219... Val Loss: 0.000470\n",
      "Epoch: 85/100... Step: 87050... Loss: 0.000315... Val Loss: 0.000565\n",
      "Epoch: 85/100... Step: 87055... Loss: 0.000165... Val Loss: 0.000888\n",
      "Epoch: 85/100... Step: 87060... Loss: 0.000190... Val Loss: 0.001197\n",
      "Epoch: 85/100... Step: 87065... Loss: 0.000120... Val Loss: 0.001058\n",
      "Epoch: 85/100... Step: 87070... Loss: 0.000172... Val Loss: 0.000929\n",
      "Epoch: 85/100... Step: 87075... Loss: 0.000143... Val Loss: 0.001109\n",
      "Epoch: 85/100... Step: 87080... Loss: 0.000042... Val Loss: 0.000964\n",
      "Epoch: 85/100... Step: 87085... Loss: 0.000129... Val Loss: 0.000993\n",
      "Epoch: 85/100... Step: 87090... Loss: 0.000148... Val Loss: 0.001202\n",
      "Epoch: 85/100... Step: 87095... Loss: 0.000513... Val Loss: 0.001092\n",
      "Epoch: 85/100... Step: 87100... Loss: 0.000755... Val Loss: 0.000693\n",
      "Epoch: 85/100... Step: 87105... Loss: 0.001044... Val Loss: 0.000266\n",
      "Epoch: 85/100... Step: 87110... Loss: 0.000320... Val Loss: 0.001547\n",
      "Epoch: 85/100... Step: 87115... Loss: 0.000958... Val Loss: 0.001782\n",
      "Epoch: 85/100... Step: 87120... Loss: 0.000188... Val Loss: 0.000978\n",
      "Epoch: 85/100... Step: 87125... Loss: 0.000419... Val Loss: 0.000751\n",
      "Epoch: 85/100... Step: 87130... Loss: 0.000120... Val Loss: 0.001217\n",
      "Epoch: 85/100... Step: 87135... Loss: 0.000338... Val Loss: 0.001450\n",
      "Epoch: 85/100... Step: 87140... Loss: 0.000155... Val Loss: 0.001202\n",
      "Epoch: 85/100... Step: 87145... Loss: 0.000181... Val Loss: 0.001251\n",
      "Epoch: 85/100... Step: 87150... Loss: 0.000194... Val Loss: 0.001439\n",
      "Epoch: 85/100... Step: 87155... Loss: 0.000153... Val Loss: 0.001444\n",
      "Epoch: 85/100... Step: 87160... Loss: 0.000162... Val Loss: 0.001221\n",
      "Epoch: 85/100... Step: 87165... Loss: 0.000175... Val Loss: 0.000998\n",
      "Epoch: 85/100... Step: 87170... Loss: 0.000234... Val Loss: 0.001111\n",
      "Epoch: 85/100... Step: 87175... Loss: 0.000296... Val Loss: 0.001234\n",
      "Epoch: 85/100... Step: 87180... Loss: 0.000185... Val Loss: 0.000979\n",
      "Epoch: 85/100... Step: 87185... Loss: 0.000095... Val Loss: 0.000701\n",
      "Epoch: 85/100... Step: 87190... Loss: 0.000103... Val Loss: 0.000688\n",
      "Epoch: 85/100... Step: 87195... Loss: 0.000249... Val Loss: 0.000663\n",
      "Epoch: 85/100... Step: 87200... Loss: 0.000203... Val Loss: 0.000412\n",
      "Epoch: 85/100... Step: 87205... Loss: 0.000223... Val Loss: 0.000269\n",
      "Epoch: 85/100... Step: 87210... Loss: 0.000267... Val Loss: 0.000319\n",
      "Epoch: 85/100... Step: 87215... Loss: 0.000124... Val Loss: 0.000640\n",
      "Epoch: 85/100... Step: 87220... Loss: 0.000237... Val Loss: 0.000941\n",
      "Epoch: 85/100... Step: 87225... Loss: 0.000248... Val Loss: 0.000782\n",
      "Epoch: 85/100... Step: 87230... Loss: 0.000245... Val Loss: 0.000455\n",
      "Epoch: 85/100... Step: 87235... Loss: 0.000380... Val Loss: 0.000563\n",
      "Epoch: 85/100... Step: 87240... Loss: 0.000061... Val Loss: 0.000967\n",
      "Epoch: 85/100... Step: 87245... Loss: 0.000351... Val Loss: 0.001095\n",
      "Epoch: 85/100... Step: 87250... Loss: 0.000158... Val Loss: 0.000819\n",
      "Epoch: 85/100... Step: 87255... Loss: 0.000211... Val Loss: 0.000578\n",
      "Epoch: 85/100... Step: 87260... Loss: 0.000179... Val Loss: 0.000786\n",
      "Epoch: 85/100... Step: 87265... Loss: 0.000201... Val Loss: 0.001095\n",
      "Epoch: 85/100... Step: 87270... Loss: 0.000210... Val Loss: 0.000967\n",
      "Epoch: 85/100... Step: 87275... Loss: 0.000151... Val Loss: 0.000695\n",
      "Epoch: 85/100... Step: 87280... Loss: 0.000153... Val Loss: 0.000781\n",
      "Epoch: 85/100... Step: 87285... Loss: 0.000105... Val Loss: 0.001050\n",
      "Epoch: 85/100... Step: 87290... Loss: 0.000122... Val Loss: 0.001022\n",
      "Epoch: 85/100... Step: 87295... Loss: 0.000073... Val Loss: 0.000895\n",
      "Epoch: 85/100... Step: 87300... Loss: 0.000073... Val Loss: 0.000927\n",
      "Epoch: 85/100... Step: 87305... Loss: 0.000067... Val Loss: 0.000841\n",
      "Epoch: 85/100... Step: 87310... Loss: 0.000074... Val Loss: 0.000853\n",
      "Epoch: 85/100... Step: 87315... Loss: 0.000084... Val Loss: 0.000999\n",
      "Epoch: 85/100... Step: 87320... Loss: 0.000061... Val Loss: 0.000902\n",
      "Epoch: 85/100... Step: 87325... Loss: 0.000062... Val Loss: 0.000806\n",
      "Epoch: 85/100... Step: 87330... Loss: 0.000120... Val Loss: 0.000827\n",
      "Epoch: 85/100... Step: 87335... Loss: 0.000104... Val Loss: 0.000619\n",
      "Epoch: 85/100... Step: 87340... Loss: 0.000334... Val Loss: 0.000483\n",
      "Epoch: 85/100... Step: 87345... Loss: 0.000269... Val Loss: 0.000652\n",
      "Epoch: 85/100... Step: 87350... Loss: 0.000165... Val Loss: 0.000944\n",
      "Epoch: 85/100... Step: 87355... Loss: 0.000463... Val Loss: 0.000930\n",
      "Epoch: 85/100... Step: 87360... Loss: 0.000356... Val Loss: 0.000617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85/100... Step: 87365... Loss: 0.000101... Val Loss: 0.000274\n",
      "Epoch: 85/100... Step: 87370... Loss: 0.000155... Val Loss: 0.000266\n",
      "Epoch: 85/100... Step: 87375... Loss: 0.000101... Val Loss: 0.000295\n",
      "Epoch: 85/100... Step: 87380... Loss: 0.000057... Val Loss: 0.000351\n",
      "Epoch: 85/100... Step: 87385... Loss: 0.000059... Val Loss: 0.000298\n",
      "Epoch: 85/100... Step: 87390... Loss: 0.000071... Val Loss: 0.000286\n",
      "Epoch: 85/100... Step: 87395... Loss: 0.000041... Val Loss: 0.000347\n",
      "Epoch: 85/100... Step: 87400... Loss: 0.000045... Val Loss: 0.000359\n",
      "Epoch: 85/100... Step: 87405... Loss: 0.000054... Val Loss: 0.000430\n",
      "Epoch: 85/100... Step: 87410... Loss: 0.000049... Val Loss: 0.000444\n",
      "Epoch: 85/100... Step: 87415... Loss: 0.000061... Val Loss: 0.000336\n",
      "Epoch: 85/100... Step: 87420... Loss: 0.000077... Val Loss: 0.000369\n",
      "Epoch: 85/100... Step: 87425... Loss: 0.000261... Val Loss: 0.000449\n",
      "Epoch: 85/100... Step: 87430... Loss: 0.000422... Val Loss: 0.000290\n",
      "Epoch: 85/100... Step: 87435... Loss: 0.000122... Val Loss: 0.000372\n",
      "Epoch: 85/100... Step: 87440... Loss: 0.000230... Val Loss: 0.000568\n",
      "Epoch: 85/100... Step: 87445... Loss: 0.000139... Val Loss: 0.000393\n",
      "Epoch: 85/100... Step: 87450... Loss: 0.000140... Val Loss: 0.000277\n",
      "Epoch: 85/100... Step: 87455... Loss: 0.000088... Val Loss: 0.000375\n",
      "Epoch: 85/100... Step: 87460... Loss: 0.000100... Val Loss: 0.000505\n",
      "Epoch: 85/100... Step: 87465... Loss: 0.000085... Val Loss: 0.000379\n",
      "Epoch: 85/100... Step: 87470... Loss: 0.000160... Val Loss: 0.000425\n",
      "Epoch: 85/100... Step: 87475... Loss: 0.000103... Val Loss: 0.000640\n",
      "Epoch: 85/100... Step: 87480... Loss: 0.000257... Val Loss: 0.000639\n",
      "Epoch: 85/100... Step: 87485... Loss: 0.000117... Val Loss: 0.000400\n",
      "Epoch: 85/100... Step: 87490... Loss: 0.000284... Val Loss: 0.000282\n",
      "Epoch: 85/100... Step: 87495... Loss: 0.000454... Val Loss: 0.000353\n",
      "Epoch: 85/100... Step: 87500... Loss: 0.000176... Val Loss: 0.000653\n",
      "Epoch: 85/100... Step: 87505... Loss: 0.000291... Val Loss: 0.000926\n",
      "Epoch: 85/100... Step: 87510... Loss: 0.000240... Val Loss: 0.000781\n",
      "Epoch: 85/100... Step: 87515... Loss: 0.000153... Val Loss: 0.000492\n",
      "Epoch: 85/100... Step: 87520... Loss: 0.000131... Val Loss: 0.000563\n",
      "Epoch: 85/100... Step: 87525... Loss: 0.000124... Val Loss: 0.000738\n",
      "Epoch: 85/100... Step: 87530... Loss: 0.000079... Val Loss: 0.000586\n",
      "Epoch: 85/100... Step: 87535... Loss: 0.000113... Val Loss: 0.000517\n",
      "Epoch: 85/100... Step: 87540... Loss: 0.000032... Val Loss: 0.000680\n",
      "Epoch: 85/100... Step: 87545... Loss: 0.000055... Val Loss: 0.000685\n",
      "Epoch: 85/100... Step: 87550... Loss: 0.000061... Val Loss: 0.000744\n",
      "Epoch: 85/100... Step: 87555... Loss: 0.000054... Val Loss: 0.000815\n",
      "Epoch: 85/100... Step: 87560... Loss: 0.000051... Val Loss: 0.000693\n",
      "Epoch: 85/100... Step: 87565... Loss: 0.000045... Val Loss: 0.000785\n",
      "Epoch: 85/100... Step: 87570... Loss: 0.000078... Val Loss: 0.000862\n",
      "Epoch: 85/100... Step: 87575... Loss: 0.000082... Val Loss: 0.000709\n",
      "Epoch: 85/100... Step: 87580... Loss: 0.000109... Val Loss: 0.000609\n",
      "Epoch: 85/100... Step: 87585... Loss: 0.000046... Val Loss: 0.000708\n",
      "Epoch: 85/100... Step: 87590... Loss: 0.000066... Val Loss: 0.000722\n",
      "Epoch: 85/100... Step: 87595... Loss: 0.000051... Val Loss: 0.000583\n",
      "Epoch: 85/100... Step: 87600... Loss: 0.000067... Val Loss: 0.000526\n",
      "Epoch: 85/100... Step: 87605... Loss: 0.000045... Val Loss: 0.000633\n",
      "Epoch: 85/100... Step: 87610... Loss: 0.000049... Val Loss: 0.000715\n",
      "Epoch: 85/100... Step: 87615... Loss: 0.000052... Val Loss: 0.000711\n",
      "Epoch: 85/100... Step: 87620... Loss: 0.000049... Val Loss: 0.000811\n",
      "Epoch: 85/100... Step: 87625... Loss: 0.000052... Val Loss: 0.000925\n",
      "Epoch: 85/100... Step: 87630... Loss: 0.000053... Val Loss: 0.000868\n",
      "Epoch: 85/100... Step: 87635... Loss: 0.000043... Val Loss: 0.000754\n",
      "Epoch: 85/100... Step: 87640... Loss: 0.000128... Val Loss: 0.000762\n",
      "Epoch: 85/100... Step: 87645... Loss: 0.000071... Val Loss: 0.000946\n",
      "Epoch: 85/100... Step: 87650... Loss: 0.000140... Val Loss: 0.001119\n",
      "Epoch: 85/100... Step: 87655... Loss: 0.000093... Val Loss: 0.001018\n",
      "Epoch: 85/100... Step: 87660... Loss: 0.000098... Val Loss: 0.000875\n",
      "Epoch: 85/100... Step: 87665... Loss: 0.000035... Val Loss: 0.000964\n",
      "Epoch: 85/100... Step: 87670... Loss: 0.000094... Val Loss: 0.000981\n",
      "Epoch: 85/100... Step: 87675... Loss: 0.000109... Val Loss: 0.000950\n",
      "Epoch: 85/100... Step: 87680... Loss: 0.000024... Val Loss: 0.001096\n",
      "Epoch: 85/100... Step: 87685... Loss: 0.000064... Val Loss: 0.001101\n",
      "Epoch: 85/100... Step: 87690... Loss: 0.000049... Val Loss: 0.000938\n",
      "Epoch: 85/100... Step: 87695... Loss: 0.000054... Val Loss: 0.000880\n",
      "Epoch: 85/100... Step: 87700... Loss: 0.000080... Val Loss: 0.000845\n",
      "Epoch: 85/100... Step: 87705... Loss: 0.000126... Val Loss: 0.000918\n",
      "Epoch: 85/100... Step: 87710... Loss: 0.000061... Val Loss: 0.001148\n",
      "Epoch: 85/100... Step: 87715... Loss: 0.000172... Val Loss: 0.001168\n",
      "Epoch: 85/100... Step: 87720... Loss: 0.000066... Val Loss: 0.000958\n",
      "Epoch: 86/100... Step: 87725... Loss: 0.000488... Val Loss: 0.000901\n",
      "Epoch: 86/100... Step: 87730... Loss: 0.000451... Val Loss: 0.001084\n",
      "Epoch: 86/100... Step: 87735... Loss: 0.000216... Val Loss: 0.001482\n",
      "Epoch: 86/100... Step: 87740... Loss: 0.000193... Val Loss: 0.001990\n",
      "Epoch: 86/100... Step: 87745... Loss: 0.000399... Val Loss: 0.002082\n",
      "Epoch: 86/100... Step: 87750... Loss: 0.000071... Val Loss: 0.001795\n",
      "Epoch: 86/100... Step: 87755... Loss: 0.000146... Val Loss: 0.001716\n",
      "Epoch: 86/100... Step: 87760... Loss: 0.000148... Val Loss: 0.001759\n",
      "Epoch: 86/100... Step: 87765... Loss: 0.000104... Val Loss: 0.001715\n",
      "Epoch: 86/100... Step: 87770... Loss: 0.000067... Val Loss: 0.001804\n",
      "Epoch: 86/100... Step: 87775... Loss: 0.000065... Val Loss: 0.001720\n",
      "Epoch: 86/100... Step: 87780... Loss: 0.000093... Val Loss: 0.001631\n",
      "Epoch: 86/100... Step: 87785... Loss: 0.000048... Val Loss: 0.001536\n",
      "Epoch: 86/100... Step: 87790... Loss: 0.000206... Val Loss: 0.001469\n",
      "Epoch: 86/100... Step: 87795... Loss: 0.000193... Val Loss: 0.001284\n",
      "Epoch: 86/100... Step: 87800... Loss: 0.000235... Val Loss: 0.001057\n",
      "Epoch: 86/100... Step: 87805... Loss: 0.000201... Val Loss: 0.001249\n",
      "Epoch: 86/100... Step: 87810... Loss: 0.000185... Val Loss: 0.001507\n",
      "Epoch: 86/100... Step: 87815... Loss: 0.000285... Val Loss: 0.001415\n",
      "Epoch: 86/100... Step: 87820... Loss: 0.000128... Val Loss: 0.001092\n",
      "Epoch: 86/100... Step: 87825... Loss: 0.000265... Val Loss: 0.000858\n",
      "Epoch: 86/100... Step: 87830... Loss: 0.000189... Val Loss: 0.001016\n",
      "Epoch: 86/100... Step: 87835... Loss: 0.000122... Val Loss: 0.001317\n",
      "Epoch: 86/100... Step: 87840... Loss: 0.000120... Val Loss: 0.001242\n",
      "Epoch: 86/100... Step: 87845... Loss: 0.000155... Val Loss: 0.001081\n",
      "Epoch: 86/100... Step: 87850... Loss: 0.000191... Val Loss: 0.001217\n",
      "Epoch: 86/100... Step: 87855... Loss: 0.000074... Val Loss: 0.001513\n",
      "Epoch: 86/100... Step: 87860... Loss: 0.000107... Val Loss: 0.001500\n",
      "Epoch: 86/100... Step: 87865... Loss: 0.000215... Val Loss: 0.001447\n",
      "Epoch: 86/100... Step: 87870... Loss: 0.000110... Val Loss: 0.001631\n",
      "Epoch: 86/100... Step: 87875... Loss: 0.000203... Val Loss: 0.001798\n",
      "Epoch: 86/100... Step: 87880... Loss: 0.000219... Val Loss: 0.001645\n",
      "Epoch: 86/100... Step: 87885... Loss: 0.000054... Val Loss: 0.001321\n",
      "Epoch: 86/100... Step: 87890... Loss: 0.000113... Val Loss: 0.001249\n",
      "Epoch: 86/100... Step: 87895... Loss: 0.000049... Val Loss: 0.001396\n",
      "Epoch: 86/100... Step: 87900... Loss: 0.000095... Val Loss: 0.001402\n",
      "Epoch: 86/100... Step: 87905... Loss: 0.000082... Val Loss: 0.001227\n",
      "Epoch: 86/100... Step: 87910... Loss: 0.000038... Val Loss: 0.001095\n",
      "Epoch: 86/100... Step: 87915... Loss: 0.000032... Val Loss: 0.001170\n",
      "Epoch: 86/100... Step: 87920... Loss: 0.000044... Val Loss: 0.001182\n",
      "Epoch: 86/100... Step: 87925... Loss: 0.000066... Val Loss: 0.001129\n",
      "Epoch: 86/100... Step: 87930... Loss: 0.000048... Val Loss: 0.001253\n",
      "Epoch: 86/100... Step: 87935... Loss: 0.000075... Val Loss: 0.001346\n",
      "Epoch: 86/100... Step: 87940... Loss: 0.000043... Val Loss: 0.001243\n",
      "Epoch: 86/100... Step: 87945... Loss: 0.000086... Val Loss: 0.001126\n",
      "Epoch: 86/100... Step: 87950... Loss: 0.000104... Val Loss: 0.001164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86/100... Step: 87955... Loss: 0.000079... Val Loss: 0.001128\n",
      "Epoch: 86/100... Step: 87960... Loss: 0.000117... Val Loss: 0.001041\n",
      "Epoch: 86/100... Step: 87965... Loss: 0.000082... Val Loss: 0.001137\n",
      "Epoch: 86/100... Step: 87970... Loss: 0.000063... Val Loss: 0.001188\n",
      "Epoch: 86/100... Step: 87975... Loss: 0.000041... Val Loss: 0.001161\n",
      "Epoch: 86/100... Step: 87980... Loss: 0.000125... Val Loss: 0.001115\n",
      "Epoch: 86/100... Step: 87985... Loss: 0.000068... Val Loss: 0.000939\n",
      "Epoch: 86/100... Step: 87990... Loss: 0.000095... Val Loss: 0.000819\n",
      "Epoch: 86/100... Step: 87995... Loss: 0.000020... Val Loss: 0.000899\n",
      "Epoch: 86/100... Step: 88000... Loss: 0.000035... Val Loss: 0.000923\n",
      "Epoch: 86/100... Step: 88005... Loss: 0.000048... Val Loss: 0.000947\n",
      "Epoch: 86/100... Step: 88010... Loss: 0.000177... Val Loss: 0.000883\n",
      "Epoch: 86/100... Step: 88015... Loss: 0.000099... Val Loss: 0.000667\n",
      "Epoch: 86/100... Step: 88020... Loss: 0.000117... Val Loss: 0.000448\n",
      "Epoch: 86/100... Step: 88025... Loss: 0.000273... Val Loss: 0.000477\n",
      "Epoch: 86/100... Step: 88030... Loss: 0.000111... Val Loss: 0.000650\n",
      "Epoch: 86/100... Step: 88035... Loss: 0.000124... Val Loss: 0.000674\n",
      "Epoch: 86/100... Step: 88040... Loss: 0.000060... Val Loss: 0.000566\n",
      "Epoch: 86/100... Step: 88045... Loss: 0.000095... Val Loss: 0.000594\n",
      "Epoch: 86/100... Step: 88050... Loss: 0.000134... Val Loss: 0.000535\n",
      "Epoch: 86/100... Step: 88055... Loss: 0.000113... Val Loss: 0.000373\n",
      "Epoch: 86/100... Step: 88060... Loss: 0.000272... Val Loss: 0.000403\n",
      "Epoch: 86/100... Step: 88065... Loss: 0.000188... Val Loss: 0.000568\n",
      "Epoch: 86/100... Step: 88070... Loss: 0.000372... Val Loss: 0.000634\n",
      "Epoch: 86/100... Step: 88075... Loss: 0.000177... Val Loss: 0.000545\n",
      "Epoch: 86/100... Step: 88080... Loss: 0.000217... Val Loss: 0.000569\n",
      "Epoch: 86/100... Step: 88085... Loss: 0.000211... Val Loss: 0.000781\n",
      "Epoch: 86/100... Step: 88090... Loss: 0.000054... Val Loss: 0.001106\n",
      "Epoch: 86/100... Step: 88095... Loss: 0.000204... Val Loss: 0.001188\n",
      "Epoch: 86/100... Step: 88100... Loss: 0.000053... Val Loss: 0.001009\n",
      "Epoch: 86/100... Step: 88105... Loss: 0.000078... Val Loss: 0.001008\n",
      "Epoch: 86/100... Step: 88110... Loss: 0.000074... Val Loss: 0.000999\n",
      "Epoch: 86/100... Step: 88115... Loss: 0.000101... Val Loss: 0.000952\n",
      "Epoch: 86/100... Step: 88120... Loss: 0.000046... Val Loss: 0.001132\n",
      "Epoch: 86/100... Step: 88125... Loss: 0.000365... Val Loss: 0.001153\n",
      "Epoch: 86/100... Step: 88130... Loss: 0.000619... Val Loss: 0.000930\n",
      "Epoch: 86/100... Step: 88135... Loss: 0.001134... Val Loss: 0.000525\n",
      "Epoch: 86/100... Step: 88140... Loss: 0.000990... Val Loss: 0.000351\n",
      "Epoch: 86/100... Step: 88145... Loss: 0.000330... Val Loss: 0.001586\n",
      "Epoch: 86/100... Step: 88150... Loss: 0.000766... Val Loss: 0.001775\n",
      "Epoch: 86/100... Step: 88155... Loss: 0.000237... Val Loss: 0.001122\n",
      "Epoch: 86/100... Step: 88160... Loss: 0.000369... Val Loss: 0.000789\n",
      "Epoch: 86/100... Step: 88165... Loss: 0.000242... Val Loss: 0.001048\n",
      "Epoch: 86/100... Step: 88170... Loss: 0.000154... Val Loss: 0.001417\n",
      "Epoch: 86/100... Step: 88175... Loss: 0.000223... Val Loss: 0.001436\n",
      "Epoch: 86/100... Step: 88180... Loss: 0.000167... Val Loss: 0.001306\n",
      "Epoch: 86/100... Step: 88185... Loss: 0.000101... Val Loss: 0.001385\n",
      "Epoch: 86/100... Step: 88190... Loss: 0.000218... Val Loss: 0.001442\n",
      "Epoch: 86/100... Step: 88195... Loss: 0.000145... Val Loss: 0.001147\n",
      "Epoch: 86/100... Step: 88200... Loss: 0.000309... Val Loss: 0.000926\n",
      "Epoch: 86/100... Step: 88205... Loss: 0.000162... Val Loss: 0.001040\n",
      "Epoch: 86/100... Step: 88210... Loss: 0.000166... Val Loss: 0.001028\n",
      "Epoch: 86/100... Step: 88215... Loss: 0.000086... Val Loss: 0.000820\n",
      "Epoch: 86/100... Step: 88220... Loss: 0.000046... Val Loss: 0.000710\n",
      "Epoch: 86/100... Step: 88225... Loss: 0.000196... Val Loss: 0.000673\n",
      "Epoch: 86/100... Step: 88230... Loss: 0.000236... Val Loss: 0.000485\n",
      "Epoch: 86/100... Step: 88235... Loss: 0.000159... Val Loss: 0.000272\n",
      "Epoch: 86/100... Step: 88240... Loss: 0.000328... Val Loss: 0.000279\n",
      "Epoch: 86/100... Step: 88245... Loss: 0.000201... Val Loss: 0.000479\n",
      "Epoch: 86/100... Step: 88250... Loss: 0.000062... Val Loss: 0.000826\n",
      "Epoch: 86/100... Step: 88255... Loss: 0.000252... Val Loss: 0.000878\n",
      "Epoch: 86/100... Step: 88260... Loss: 0.000063... Val Loss: 0.000589\n",
      "Epoch: 86/100... Step: 88265... Loss: 0.000327... Val Loss: 0.000506\n",
      "Epoch: 86/100... Step: 88270... Loss: 0.000150... Val Loss: 0.000810\n",
      "Epoch: 86/100... Step: 88275... Loss: 0.000236... Val Loss: 0.001083\n",
      "Epoch: 86/100... Step: 88280... Loss: 0.000304... Val Loss: 0.001009\n",
      "Epoch: 86/100... Step: 88285... Loss: 0.000028... Val Loss: 0.000686\n",
      "Epoch: 86/100... Step: 88290... Loss: 0.000257... Val Loss: 0.000594\n",
      "Epoch: 86/100... Step: 88295... Loss: 0.000149... Val Loss: 0.000837\n",
      "Epoch: 86/100... Step: 88300... Loss: 0.000176... Val Loss: 0.001049\n",
      "Epoch: 86/100... Step: 88305... Loss: 0.000115... Val Loss: 0.000939\n",
      "Epoch: 86/100... Step: 88310... Loss: 0.000099... Val Loss: 0.000760\n",
      "Epoch: 86/100... Step: 88315... Loss: 0.000122... Val Loss: 0.000849\n",
      "Epoch: 86/100... Step: 88320... Loss: 0.000087... Val Loss: 0.000975\n",
      "Epoch: 86/100... Step: 88325... Loss: 0.000035... Val Loss: 0.000973\n",
      "Epoch: 86/100... Step: 88330... Loss: 0.000108... Val Loss: 0.000978\n",
      "Epoch: 86/100... Step: 88335... Loss: 0.000074... Val Loss: 0.000887\n",
      "Epoch: 86/100... Step: 88340... Loss: 0.000094... Val Loss: 0.000796\n",
      "Epoch: 86/100... Step: 88345... Loss: 0.000059... Val Loss: 0.000910\n",
      "Epoch: 86/100... Step: 88350... Loss: 0.000106... Val Loss: 0.000963\n",
      "Epoch: 86/100... Step: 88355... Loss: 0.000034... Val Loss: 0.000841\n",
      "Epoch: 86/100... Step: 88360... Loss: 0.000096... Val Loss: 0.000809\n",
      "Epoch: 86/100... Step: 88365... Loss: 0.000146... Val Loss: 0.000729\n",
      "Epoch: 86/100... Step: 88370... Loss: 0.000176... Val Loss: 0.000542\n",
      "Epoch: 86/100... Step: 88375... Loss: 0.000359... Val Loss: 0.000561\n",
      "Epoch: 86/100... Step: 88380... Loss: 0.000111... Val Loss: 0.000777\n",
      "Epoch: 86/100... Step: 88385... Loss: 0.000310... Val Loss: 0.000929\n",
      "Epoch: 86/100... Step: 88390... Loss: 0.000479... Val Loss: 0.000827\n",
      "Epoch: 86/100... Step: 88395... Loss: 0.000236... Val Loss: 0.000532\n",
      "Epoch: 86/100... Step: 88400... Loss: 0.000136... Val Loss: 0.000263\n",
      "Epoch: 86/100... Step: 88405... Loss: 0.000169... Val Loss: 0.000263\n",
      "Epoch: 86/100... Step: 88410... Loss: 0.000125... Val Loss: 0.000278\n",
      "Epoch: 86/100... Step: 88415... Loss: 0.000119... Val Loss: 0.000440\n",
      "Epoch: 86/100... Step: 88420... Loss: 0.000099... Val Loss: 0.000331\n",
      "Epoch: 86/100... Step: 88425... Loss: 0.000117... Val Loss: 0.000264\n",
      "Epoch: 86/100... Step: 88430... Loss: 0.000099... Val Loss: 0.000308\n",
      "Epoch: 86/100... Step: 88435... Loss: 0.000065... Val Loss: 0.000468\n",
      "Epoch: 86/100... Step: 88440... Loss: 0.000039... Val Loss: 0.000446\n",
      "Epoch: 86/100... Step: 88445... Loss: 0.000042... Val Loss: 0.000384\n",
      "Epoch: 86/100... Step: 88450... Loss: 0.000067... Val Loss: 0.000361\n",
      "Epoch: 86/100... Step: 88455... Loss: 0.000091... Val Loss: 0.000378\n",
      "Epoch: 86/100... Step: 88460... Loss: 0.000363... Val Loss: 0.000325\n",
      "Epoch: 86/100... Step: 88465... Loss: 0.000349... Val Loss: 0.000302\n",
      "Epoch: 86/100... Step: 88470... Loss: 0.000053... Val Loss: 0.000550\n",
      "Epoch: 86/100... Step: 88475... Loss: 0.000317... Val Loss: 0.000532\n",
      "Epoch: 86/100... Step: 88480... Loss: 0.000141... Val Loss: 0.000391\n",
      "Epoch: 86/100... Step: 88485... Loss: 0.000111... Val Loss: 0.000284\n",
      "Epoch: 86/100... Step: 88490... Loss: 0.000161... Val Loss: 0.000328\n",
      "Epoch: 86/100... Step: 88495... Loss: 0.000100... Val Loss: 0.000518\n",
      "Epoch: 86/100... Step: 88500... Loss: 0.000091... Val Loss: 0.000556\n",
      "Epoch: 86/100... Step: 88505... Loss: 0.000020... Val Loss: 0.000540\n",
      "Epoch: 86/100... Step: 88510... Loss: 0.000080... Val Loss: 0.000530\n",
      "Epoch: 86/100... Step: 88515... Loss: 0.000121... Val Loss: 0.000439\n",
      "Epoch: 86/100... Step: 88520... Loss: 0.000128... Val Loss: 0.000308\n",
      "Epoch: 86/100... Step: 88525... Loss: 0.000363... Val Loss: 0.000351\n",
      "Epoch: 86/100... Step: 88530... Loss: 0.000258... Val Loss: 0.000555\n",
      "Epoch: 86/100... Step: 88535... Loss: 0.000131... Val Loss: 0.000840\n",
      "Epoch: 86/100... Step: 88540... Loss: 0.000279... Val Loss: 0.000861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86/100... Step: 88545... Loss: 0.000104... Val Loss: 0.000621\n",
      "Epoch: 86/100... Step: 88550... Loss: 0.000154... Val Loss: 0.000497\n",
      "Epoch: 86/100... Step: 88555... Loss: 0.000048... Val Loss: 0.000670\n",
      "Epoch: 86/100... Step: 88560... Loss: 0.000149... Val Loss: 0.000701\n",
      "Epoch: 86/100... Step: 88565... Loss: 0.000059... Val Loss: 0.000529\n",
      "Epoch: 86/100... Step: 88570... Loss: 0.000107... Val Loss: 0.000540\n",
      "Epoch: 86/100... Step: 88575... Loss: 0.000046... Val Loss: 0.000715\n",
      "Epoch: 86/100... Step: 88580... Loss: 0.000051... Val Loss: 0.000734\n",
      "Epoch: 86/100... Step: 88585... Loss: 0.000046... Val Loss: 0.000736\n",
      "Epoch: 86/100... Step: 88590... Loss: 0.000047... Val Loss: 0.000778\n",
      "Epoch: 86/100... Step: 88595... Loss: 0.000036... Val Loss: 0.000745\n",
      "Epoch: 86/100... Step: 88600... Loss: 0.000041... Val Loss: 0.000808\n",
      "Epoch: 86/100... Step: 88605... Loss: 0.000121... Val Loss: 0.000846\n",
      "Epoch: 86/100... Step: 88610... Loss: 0.000076... Val Loss: 0.000710\n",
      "Epoch: 86/100... Step: 88615... Loss: 0.000125... Val Loss: 0.000595\n",
      "Epoch: 86/100... Step: 88620... Loss: 0.000041... Val Loss: 0.000678\n",
      "Epoch: 86/100... Step: 88625... Loss: 0.000121... Val Loss: 0.000739\n",
      "Epoch: 86/100... Step: 88630... Loss: 0.000088... Val Loss: 0.000617\n",
      "Epoch: 86/100... Step: 88635... Loss: 0.000127... Val Loss: 0.000480\n",
      "Epoch: 86/100... Step: 88640... Loss: 0.000171... Val Loss: 0.000537\n",
      "Epoch: 86/100... Step: 88645... Loss: 0.000042... Val Loss: 0.000743\n",
      "Epoch: 86/100... Step: 88650... Loss: 0.000089... Val Loss: 0.000877\n",
      "Epoch: 86/100... Step: 88655... Loss: 0.000032... Val Loss: 0.000830\n",
      "Epoch: 86/100... Step: 88660... Loss: 0.000037... Val Loss: 0.000863\n",
      "Epoch: 86/100... Step: 88665... Loss: 0.000088... Val Loss: 0.000857\n",
      "Epoch: 86/100... Step: 88670... Loss: 0.000073... Val Loss: 0.000733\n",
      "Epoch: 86/100... Step: 88675... Loss: 0.000215... Val Loss: 0.000758\n",
      "Epoch: 86/100... Step: 88680... Loss: 0.000082... Val Loss: 0.000963\n",
      "Epoch: 86/100... Step: 88685... Loss: 0.000141... Val Loss: 0.001120\n",
      "Epoch: 86/100... Step: 88690... Loss: 0.000094... Val Loss: 0.001041\n",
      "Epoch: 86/100... Step: 88695... Loss: 0.000080... Val Loss: 0.000871\n",
      "Epoch: 86/100... Step: 88700... Loss: 0.000038... Val Loss: 0.000897\n",
      "Epoch: 86/100... Step: 88705... Loss: 0.000100... Val Loss: 0.000992\n",
      "Epoch: 86/100... Step: 88710... Loss: 0.000038... Val Loss: 0.001111\n",
      "Epoch: 86/100... Step: 88715... Loss: 0.000034... Val Loss: 0.001053\n",
      "Epoch: 86/100... Step: 88720... Loss: 0.000041... Val Loss: 0.000991\n",
      "Epoch: 86/100... Step: 88725... Loss: 0.000063... Val Loss: 0.000970\n",
      "Epoch: 86/100... Step: 88730... Loss: 0.000053... Val Loss: 0.000855\n",
      "Epoch: 86/100... Step: 88735... Loss: 0.000179... Val Loss: 0.000804\n",
      "Epoch: 86/100... Step: 88740... Loss: 0.000163... Val Loss: 0.000935\n",
      "Epoch: 86/100... Step: 88745... Loss: 0.000083... Val Loss: 0.001145\n",
      "Epoch: 86/100... Step: 88750... Loss: 0.000189... Val Loss: 0.001153\n",
      "Epoch: 87/100... Step: 88755... Loss: 0.000264... Val Loss: 0.001065\n",
      "Epoch: 87/100... Step: 88760... Loss: 0.000335... Val Loss: 0.001176\n",
      "Epoch: 87/100... Step: 88765... Loss: 0.000171... Val Loss: 0.001442\n",
      "Epoch: 87/100... Step: 88770... Loss: 0.000140... Val Loss: 0.001777\n",
      "Epoch: 87/100... Step: 88775... Loss: 0.000244... Val Loss: 0.001978\n",
      "Epoch: 87/100... Step: 88780... Loss: 0.000195... Val Loss: 0.001925\n",
      "Epoch: 87/100... Step: 88785... Loss: 0.000087... Val Loss: 0.001770\n",
      "Epoch: 87/100... Step: 88790... Loss: 0.000162... Val Loss: 0.001789\n",
      "Epoch: 87/100... Step: 88795... Loss: 0.000136... Val Loss: 0.001732\n",
      "Epoch: 87/100... Step: 88800... Loss: 0.000080... Val Loss: 0.001741\n",
      "Epoch: 87/100... Step: 88805... Loss: 0.000078... Val Loss: 0.001782\n",
      "Epoch: 87/100... Step: 88810... Loss: 0.000076... Val Loss: 0.001694\n",
      "Epoch: 87/100... Step: 88815... Loss: 0.000052... Val Loss: 0.001562\n",
      "Epoch: 87/100... Step: 88820... Loss: 0.000127... Val Loss: 0.001486\n",
      "Epoch: 87/100... Step: 88825... Loss: 0.000266... Val Loss: 0.001413\n",
      "Epoch: 87/100... Step: 88830... Loss: 0.000114... Val Loss: 0.001106\n",
      "Epoch: 87/100... Step: 88835... Loss: 0.000296... Val Loss: 0.001104\n",
      "Epoch: 87/100... Step: 88840... Loss: 0.000037... Val Loss: 0.001423\n",
      "Epoch: 87/100... Step: 88845... Loss: 0.000262... Val Loss: 0.001525\n",
      "Epoch: 87/100... Step: 88850... Loss: 0.000331... Val Loss: 0.001364\n",
      "Epoch: 87/100... Step: 88855... Loss: 0.000069... Val Loss: 0.001007\n",
      "Epoch: 87/100... Step: 88860... Loss: 0.000287... Val Loss: 0.000859\n",
      "Epoch: 87/100... Step: 88865... Loss: 0.000171... Val Loss: 0.001023\n",
      "Epoch: 87/100... Step: 88870... Loss: 0.000103... Val Loss: 0.001274\n",
      "Epoch: 87/100... Step: 88875... Loss: 0.000118... Val Loss: 0.001271\n",
      "Epoch: 87/100... Step: 88880... Loss: 0.000140... Val Loss: 0.001187\n",
      "Epoch: 87/100... Step: 88885... Loss: 0.000113... Val Loss: 0.001331\n",
      "Epoch: 87/100... Step: 88890... Loss: 0.000085... Val Loss: 0.001521\n",
      "Epoch: 87/100... Step: 88895... Loss: 0.000119... Val Loss: 0.001553\n",
      "Epoch: 87/100... Step: 88900... Loss: 0.000053... Val Loss: 0.001672\n",
      "Epoch: 87/100... Step: 88905... Loss: 0.000123... Val Loss: 0.001777\n",
      "Epoch: 87/100... Step: 88910... Loss: 0.000166... Val Loss: 0.001636\n",
      "Epoch: 87/100... Step: 88915... Loss: 0.000065... Val Loss: 0.001363\n",
      "Epoch: 87/100... Step: 88920... Loss: 0.000077... Val Loss: 0.001285\n",
      "Epoch: 87/100... Step: 88925... Loss: 0.000076... Val Loss: 0.001398\n",
      "Epoch: 87/100... Step: 88930... Loss: 0.000080... Val Loss: 0.001413\n",
      "Epoch: 87/100... Step: 88935... Loss: 0.000069... Val Loss: 0.001275\n",
      "Epoch: 87/100... Step: 88940... Loss: 0.000071... Val Loss: 0.001161\n",
      "Epoch: 87/100... Step: 88945... Loss: 0.000053... Val Loss: 0.001110\n",
      "Epoch: 87/100... Step: 88950... Loss: 0.000037... Val Loss: 0.001192\n",
      "Epoch: 87/100... Step: 88955... Loss: 0.000050... Val Loss: 0.001143\n",
      "Epoch: 87/100... Step: 88960... Loss: 0.000103... Val Loss: 0.001154\n",
      "Epoch: 87/100... Step: 88965... Loss: 0.000045... Val Loss: 0.001333\n",
      "Epoch: 87/100... Step: 88970... Loss: 0.000108... Val Loss: 0.001380\n",
      "Epoch: 87/100... Step: 88975... Loss: 0.000080... Val Loss: 0.001216\n",
      "Epoch: 87/100... Step: 88980... Loss: 0.000111... Val Loss: 0.001089\n",
      "Epoch: 87/100... Step: 88985... Loss: 0.000044... Val Loss: 0.001045\n",
      "Epoch: 87/100... Step: 88990... Loss: 0.000097... Val Loss: 0.001052\n",
      "Epoch: 87/100... Step: 88995... Loss: 0.000063... Val Loss: 0.001180\n",
      "Epoch: 87/100... Step: 89000... Loss: 0.000117... Val Loss: 0.001241\n",
      "Epoch: 87/100... Step: 89005... Loss: 0.000029... Val Loss: 0.001138\n",
      "Epoch: 87/100... Step: 89010... Loss: 0.000088... Val Loss: 0.001134\n",
      "Epoch: 87/100... Step: 89015... Loss: 0.000161... Val Loss: 0.001082\n",
      "Epoch: 87/100... Step: 89020... Loss: 0.000039... Val Loss: 0.000886\n",
      "Epoch: 87/100... Step: 89025... Loss: 0.000096... Val Loss: 0.000790\n",
      "Epoch: 87/100... Step: 89030... Loss: 0.000048... Val Loss: 0.000886\n",
      "Epoch: 87/100... Step: 89035... Loss: 0.000057... Val Loss: 0.001014\n",
      "Epoch: 87/100... Step: 89040... Loss: 0.000185... Val Loss: 0.000952\n",
      "Epoch: 87/100... Step: 89045... Loss: 0.000178... Val Loss: 0.000757\n",
      "Epoch: 87/100... Step: 89050... Loss: 0.000046... Val Loss: 0.000499\n",
      "Epoch: 87/100... Step: 89055... Loss: 0.000235... Val Loss: 0.000442\n",
      "Epoch: 87/100... Step: 89060... Loss: 0.000172... Val Loss: 0.000566\n",
      "Epoch: 87/100... Step: 89065... Loss: 0.000129... Val Loss: 0.000676\n",
      "Epoch: 87/100... Step: 89070... Loss: 0.000037... Val Loss: 0.000592\n",
      "Epoch: 87/100... Step: 89075... Loss: 0.000069... Val Loss: 0.000588\n",
      "Epoch: 87/100... Step: 89080... Loss: 0.000155... Val Loss: 0.000595\n",
      "Epoch: 87/100... Step: 89085... Loss: 0.000054... Val Loss: 0.000451\n",
      "Epoch: 87/100... Step: 89090... Loss: 0.000227... Val Loss: 0.000376\n",
      "Epoch: 87/100... Step: 89095... Loss: 0.000217... Val Loss: 0.000469\n",
      "Epoch: 87/100... Step: 89100... Loss: 0.000252... Val Loss: 0.000569\n",
      "Epoch: 87/100... Step: 89105... Loss: 0.000214... Val Loss: 0.000534\n",
      "Epoch: 87/100... Step: 89110... Loss: 0.000225... Val Loss: 0.000512\n",
      "Epoch: 87/100... Step: 89115... Loss: 0.000283... Val Loss: 0.000635\n",
      "Epoch: 87/100... Step: 89120... Loss: 0.000160... Val Loss: 0.000903\n",
      "Epoch: 87/100... Step: 89125... Loss: 0.000168... Val Loss: 0.001189\n",
      "Epoch: 87/100... Step: 89130... Loss: 0.000157... Val Loss: 0.001170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87/100... Step: 89135... Loss: 0.000052... Val Loss: 0.001011\n",
      "Epoch: 87/100... Step: 89140... Loss: 0.000042... Val Loss: 0.000944\n",
      "Epoch: 87/100... Step: 89145... Loss: 0.000034... Val Loss: 0.000998\n",
      "Epoch: 87/100... Step: 89150... Loss: 0.000051... Val Loss: 0.001085\n",
      "Epoch: 87/100... Step: 89155... Loss: 0.000154... Val Loss: 0.001123\n",
      "Epoch: 87/100... Step: 89160... Loss: 0.000544... Val Loss: 0.001031\n",
      "Epoch: 87/100... Step: 89165... Loss: 0.001019... Val Loss: 0.000811\n",
      "Epoch: 87/100... Step: 89170... Loss: 0.001557... Val Loss: 0.000474\n",
      "Epoch: 87/100... Step: 89175... Loss: 0.001031... Val Loss: 0.000285\n",
      "Epoch: 87/100... Step: 89180... Loss: 0.000111... Val Loss: 0.001256\n",
      "Epoch: 87/100... Step: 89185... Loss: 0.000587... Val Loss: 0.001759\n",
      "Epoch: 87/100... Step: 89190... Loss: 0.000270... Val Loss: 0.001353\n",
      "Epoch: 87/100... Step: 89195... Loss: 0.000194... Val Loss: 0.000924\n",
      "Epoch: 87/100... Step: 89200... Loss: 0.000269... Val Loss: 0.001012\n",
      "Epoch: 87/100... Step: 89205... Loss: 0.000095... Val Loss: 0.001401\n",
      "Epoch: 87/100... Step: 89210... Loss: 0.000320... Val Loss: 0.001609\n",
      "Epoch: 87/100... Step: 89215... Loss: 0.000115... Val Loss: 0.001410\n",
      "Epoch: 87/100... Step: 89220... Loss: 0.000137... Val Loss: 0.001282\n",
      "Epoch: 87/100... Step: 89225... Loss: 0.000134... Val Loss: 0.001150\n",
      "Epoch: 87/100... Step: 89230... Loss: 0.000143... Val Loss: 0.001040\n",
      "Epoch: 87/100... Step: 89235... Loss: 0.000119... Val Loss: 0.001169\n",
      "Epoch: 87/100... Step: 89240... Loss: 0.000256... Val Loss: 0.001221\n",
      "Epoch: 87/100... Step: 89245... Loss: 0.000206... Val Loss: 0.001020\n",
      "Epoch: 87/100... Step: 89250... Loss: 0.000061... Val Loss: 0.000734\n",
      "Epoch: 87/100... Step: 89255... Loss: 0.000161... Val Loss: 0.000605\n",
      "Epoch: 87/100... Step: 89260... Loss: 0.000193... Val Loss: 0.000547\n",
      "Epoch: 87/100... Step: 89265... Loss: 0.000089... Val Loss: 0.000358\n",
      "Epoch: 87/100... Step: 89270... Loss: 0.000236... Val Loss: 0.000285\n",
      "Epoch: 87/100... Step: 89275... Loss: 0.000266... Val Loss: 0.000358\n",
      "Epoch: 87/100... Step: 89280... Loss: 0.000128... Val Loss: 0.000639\n",
      "Epoch: 87/100... Step: 89285... Loss: 0.000200... Val Loss: 0.000895\n",
      "Epoch: 87/100... Step: 89290... Loss: 0.000255... Val Loss: 0.000830\n",
      "Epoch: 87/100... Step: 89295... Loss: 0.000143... Val Loss: 0.000593\n",
      "Epoch: 87/100... Step: 89300... Loss: 0.000316... Val Loss: 0.000603\n",
      "Epoch: 87/100... Step: 89305... Loss: 0.000085... Val Loss: 0.000850\n",
      "Epoch: 87/100... Step: 89310... Loss: 0.000227... Val Loss: 0.000991\n",
      "Epoch: 87/100... Step: 89315... Loss: 0.000174... Val Loss: 0.000894\n",
      "Epoch: 87/100... Step: 89320... Loss: 0.000128... Val Loss: 0.000683\n",
      "Epoch: 87/100... Step: 89325... Loss: 0.000226... Val Loss: 0.000702\n",
      "Epoch: 87/100... Step: 89330... Loss: 0.000112... Val Loss: 0.000886\n",
      "Epoch: 87/100... Step: 89335... Loss: 0.000136... Val Loss: 0.000964\n",
      "Epoch: 87/100... Step: 89340... Loss: 0.000045... Val Loss: 0.000851\n",
      "Epoch: 87/100... Step: 89345... Loss: 0.000089... Val Loss: 0.000837\n",
      "Epoch: 87/100... Step: 89350... Loss: 0.000086... Val Loss: 0.000932\n",
      "Epoch: 87/100... Step: 89355... Loss: 0.000056... Val Loss: 0.000971\n",
      "Epoch: 87/100... Step: 89360... Loss: 0.000044... Val Loss: 0.000965\n",
      "Epoch: 87/100... Step: 89365... Loss: 0.000052... Val Loss: 0.000910\n",
      "Epoch: 87/100... Step: 89370... Loss: 0.000069... Val Loss: 0.000791\n",
      "Epoch: 87/100... Step: 89375... Loss: 0.000111... Val Loss: 0.000844\n",
      "Epoch: 87/100... Step: 89380... Loss: 0.000078... Val Loss: 0.000991\n",
      "Epoch: 87/100... Step: 89385... Loss: 0.000086... Val Loss: 0.000954\n",
      "Epoch: 87/100... Step: 89390... Loss: 0.000067... Val Loss: 0.000801\n",
      "Epoch: 87/100... Step: 89395... Loss: 0.000097... Val Loss: 0.000698\n",
      "Epoch: 87/100... Step: 89400... Loss: 0.000121... Val Loss: 0.000543\n",
      "Epoch: 87/100... Step: 89405... Loss: 0.000315... Val Loss: 0.000563\n",
      "Epoch: 87/100... Step: 89410... Loss: 0.000160... Val Loss: 0.000752\n",
      "Epoch: 87/100... Step: 89415... Loss: 0.000224... Val Loss: 0.000950\n",
      "Epoch: 87/100... Step: 89420... Loss: 0.000493... Val Loss: 0.000922\n",
      "Epoch: 87/100... Step: 89425... Loss: 0.000428... Val Loss: 0.000725\n",
      "Epoch: 87/100... Step: 89430... Loss: 0.000122... Val Loss: 0.000392\n",
      "Epoch: 87/100... Step: 89435... Loss: 0.000122... Val Loss: 0.000262\n",
      "Epoch: 87/100... Step: 89440... Loss: 0.000281... Val Loss: 0.000264\n",
      "Epoch: 87/100... Step: 89445... Loss: 0.000123... Val Loss: 0.000295\n",
      "Epoch: 87/100... Step: 89450... Loss: 0.000176... Val Loss: 0.000446\n",
      "Epoch: 87/100... Step: 89455... Loss: 0.000095... Val Loss: 0.000335\n",
      "Epoch: 87/100... Step: 89460... Loss: 0.000132... Val Loss: 0.000266\n",
      "Epoch: 87/100... Step: 89465... Loss: 0.000122... Val Loss: 0.000310\n",
      "Epoch: 87/100... Step: 89470... Loss: 0.000067... Val Loss: 0.000502\n",
      "Epoch: 87/100... Step: 89475... Loss: 0.000126... Val Loss: 0.000510\n",
      "Epoch: 87/100... Step: 89480... Loss: 0.000046... Val Loss: 0.000345\n",
      "Epoch: 87/100... Step: 89485... Loss: 0.000126... Val Loss: 0.000302\n",
      "Epoch: 87/100... Step: 89490... Loss: 0.000191... Val Loss: 0.000313\n",
      "Epoch: 87/100... Step: 89495... Loss: 0.000391... Val Loss: 0.000268\n",
      "Epoch: 87/100... Step: 89500... Loss: 0.000218... Val Loss: 0.000306\n",
      "Epoch: 87/100... Step: 89505... Loss: 0.000175... Val Loss: 0.000520\n",
      "Epoch: 87/100... Step: 89510... Loss: 0.000218... Val Loss: 0.000500\n",
      "Epoch: 87/100... Step: 89515... Loss: 0.000097... Val Loss: 0.000353\n",
      "Epoch: 87/100... Step: 89520... Loss: 0.000186... Val Loss: 0.000279\n",
      "Epoch: 87/100... Step: 89525... Loss: 0.000165... Val Loss: 0.000318\n",
      "Epoch: 87/100... Step: 89530... Loss: 0.000093... Val Loss: 0.000500\n",
      "Epoch: 87/100... Step: 89535... Loss: 0.000089... Val Loss: 0.000640\n",
      "Epoch: 87/100... Step: 89540... Loss: 0.000087... Val Loss: 0.000548\n",
      "Epoch: 87/100... Step: 89545... Loss: 0.000085... Val Loss: 0.000416\n",
      "Epoch: 87/100... Step: 89550... Loss: 0.000050... Val Loss: 0.000332\n",
      "Epoch: 87/100... Step: 89555... Loss: 0.000189... Val Loss: 0.000376\n",
      "Epoch: 87/100... Step: 89560... Loss: 0.000238... Val Loss: 0.000551\n",
      "Epoch: 87/100... Step: 89565... Loss: 0.000108... Val Loss: 0.000848\n",
      "Epoch: 87/100... Step: 89570... Loss: 0.000304... Val Loss: 0.000904\n",
      "Epoch: 87/100... Step: 89575... Loss: 0.000183... Val Loss: 0.000718\n",
      "Epoch: 87/100... Step: 89580... Loss: 0.000104... Val Loss: 0.000493\n",
      "Epoch: 87/100... Step: 89585... Loss: 0.000135... Val Loss: 0.000531\n",
      "Epoch: 87/100... Step: 89590... Loss: 0.000096... Val Loss: 0.000702\n",
      "Epoch: 87/100... Step: 89595... Loss: 0.000121... Val Loss: 0.000674\n",
      "Epoch: 87/100... Step: 89600... Loss: 0.000067... Val Loss: 0.000540\n",
      "Epoch: 87/100... Step: 89605... Loss: 0.000074... Val Loss: 0.000597\n",
      "Epoch: 87/100... Step: 89610... Loss: 0.000063... Val Loss: 0.000732\n",
      "Epoch: 87/100... Step: 89615... Loss: 0.000055... Val Loss: 0.000770\n",
      "Epoch: 87/100... Step: 89620... Loss: 0.000047... Val Loss: 0.000781\n",
      "Epoch: 87/100... Step: 89625... Loss: 0.000032... Val Loss: 0.000728\n",
      "Epoch: 87/100... Step: 89630... Loss: 0.000062... Val Loss: 0.000775\n",
      "Epoch: 87/100... Step: 89635... Loss: 0.000067... Val Loss: 0.000848\n",
      "Epoch: 87/100... Step: 89640... Loss: 0.000126... Val Loss: 0.000759\n",
      "Epoch: 87/100... Step: 89645... Loss: 0.000090... Val Loss: 0.000608\n",
      "Epoch: 87/100... Step: 89650... Loss: 0.000087... Val Loss: 0.000632\n",
      "Epoch: 87/100... Step: 89655... Loss: 0.000074... Val Loss: 0.000730\n",
      "Epoch: 87/100... Step: 89660... Loss: 0.000137... Val Loss: 0.000672\n",
      "Epoch: 87/100... Step: 89665... Loss: 0.000066... Val Loss: 0.000509\n",
      "Epoch: 87/100... Step: 89670... Loss: 0.000182... Val Loss: 0.000489\n",
      "Epoch: 87/100... Step: 89675... Loss: 0.000135... Val Loss: 0.000617\n",
      "Epoch: 87/100... Step: 89680... Loss: 0.000057... Val Loss: 0.000806\n",
      "Epoch: 87/100... Step: 89685... Loss: 0.000059... Val Loss: 0.000887\n",
      "Epoch: 87/100... Step: 89690... Loss: 0.000048... Val Loss: 0.000886\n",
      "Epoch: 87/100... Step: 89695... Loss: 0.000067... Val Loss: 0.000862\n",
      "Epoch: 87/100... Step: 89700... Loss: 0.000040... Val Loss: 0.000770\n",
      "Epoch: 87/100... Step: 89705... Loss: 0.000170... Val Loss: 0.000741\n",
      "Epoch: 87/100... Step: 89710... Loss: 0.000151... Val Loss: 0.000872\n",
      "Epoch: 87/100... Step: 89715... Loss: 0.000072... Val Loss: 0.001078\n",
      "Epoch: 87/100... Step: 89720... Loss: 0.000141... Val Loss: 0.001094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87/100... Step: 89725... Loss: 0.000030... Val Loss: 0.000955\n",
      "Epoch: 87/100... Step: 89730... Loss: 0.000063... Val Loss: 0.000881\n",
      "Epoch: 87/100... Step: 89735... Loss: 0.000088... Val Loss: 0.000924\n",
      "Epoch: 87/100... Step: 89740... Loss: 0.000091... Val Loss: 0.001008\n",
      "Epoch: 87/100... Step: 89745... Loss: 0.000072... Val Loss: 0.001135\n",
      "Epoch: 87/100... Step: 89750... Loss: 0.000067... Val Loss: 0.001080\n",
      "Epoch: 87/100... Step: 89755... Loss: 0.000046... Val Loss: 0.000940\n",
      "Epoch: 87/100... Step: 89760... Loss: 0.000049... Val Loss: 0.000857\n",
      "Epoch: 87/100... Step: 89765... Loss: 0.000114... Val Loss: 0.000819\n",
      "Epoch: 87/100... Step: 89770... Loss: 0.000163... Val Loss: 0.000904\n",
      "Epoch: 87/100... Step: 89775... Loss: 0.000054... Val Loss: 0.001107\n",
      "Epoch: 87/100... Step: 89780... Loss: 0.000192... Val Loss: 0.001195\n",
      "Epoch: 88/100... Step: 89785... Loss: 0.000145... Val Loss: 0.001104\n",
      "Epoch: 88/100... Step: 89790... Loss: 0.000306... Val Loss: 0.001128\n",
      "Epoch: 88/100... Step: 89795... Loss: 0.000255... Val Loss: 0.001304\n",
      "Epoch: 88/100... Step: 89800... Loss: 0.000180... Val Loss: 0.001594\n",
      "Epoch: 88/100... Step: 89805... Loss: 0.000191... Val Loss: 0.001890\n",
      "Epoch: 88/100... Step: 89810... Loss: 0.000254... Val Loss: 0.001999\n",
      "Epoch: 88/100... Step: 89815... Loss: 0.000076... Val Loss: 0.001883\n",
      "Epoch: 88/100... Step: 89820... Loss: 0.000133... Val Loss: 0.001766\n",
      "Epoch: 88/100... Step: 89825... Loss: 0.000130... Val Loss: 0.001677\n",
      "Epoch: 88/100... Step: 89830... Loss: 0.000169... Val Loss: 0.001626\n",
      "Epoch: 88/100... Step: 89835... Loss: 0.000049... Val Loss: 0.001768\n",
      "Epoch: 88/100... Step: 89840... Loss: 0.000108... Val Loss: 0.001833\n",
      "Epoch: 88/100... Step: 89845... Loss: 0.000141... Val Loss: 0.001739\n",
      "Epoch: 88/100... Step: 89850... Loss: 0.000044... Val Loss: 0.001529\n",
      "Epoch: 88/100... Step: 89855... Loss: 0.000204... Val Loss: 0.001343\n",
      "Epoch: 88/100... Step: 89860... Loss: 0.000088... Val Loss: 0.001106\n",
      "Epoch: 88/100... Step: 89865... Loss: 0.000305... Val Loss: 0.001027\n",
      "Epoch: 88/100... Step: 89870... Loss: 0.000224... Val Loss: 0.001215\n",
      "Epoch: 88/100... Step: 89875... Loss: 0.000129... Val Loss: 0.001468\n",
      "Epoch: 88/100... Step: 89880... Loss: 0.000341... Val Loss: 0.001460\n",
      "Epoch: 88/100... Step: 89885... Loss: 0.000248... Val Loss: 0.001255\n",
      "Epoch: 88/100... Step: 89890... Loss: 0.000130... Val Loss: 0.000980\n",
      "Epoch: 88/100... Step: 89895... Loss: 0.000191... Val Loss: 0.000975\n",
      "Epoch: 88/100... Step: 89900... Loss: 0.000030... Val Loss: 0.001185\n",
      "Epoch: 88/100... Step: 89905... Loss: 0.000092... Val Loss: 0.001264\n",
      "Epoch: 88/100... Step: 89910... Loss: 0.000073... Val Loss: 0.001192\n",
      "Epoch: 88/100... Step: 89915... Loss: 0.000127... Val Loss: 0.001287\n",
      "Epoch: 88/100... Step: 89920... Loss: 0.000069... Val Loss: 0.001490\n",
      "Epoch: 88/100... Step: 89925... Loss: 0.000108... Val Loss: 0.001486\n",
      "Epoch: 88/100... Step: 89930... Loss: 0.000178... Val Loss: 0.001509\n",
      "Epoch: 88/100... Step: 89935... Loss: 0.000061... Val Loss: 0.001729\n",
      "Epoch: 88/100... Step: 89940... Loss: 0.000211... Val Loss: 0.001767\n",
      "Epoch: 88/100... Step: 89945... Loss: 0.000161... Val Loss: 0.001552\n",
      "Epoch: 88/100... Step: 89950... Loss: 0.000066... Val Loss: 0.001303\n",
      "Epoch: 88/100... Step: 89955... Loss: 0.000121... Val Loss: 0.001268\n",
      "Epoch: 88/100... Step: 89960... Loss: 0.000066... Val Loss: 0.001379\n",
      "Epoch: 88/100... Step: 89965... Loss: 0.000082... Val Loss: 0.001403\n",
      "Epoch: 88/100... Step: 89970... Loss: 0.000103... Val Loss: 0.001262\n",
      "Epoch: 88/100... Step: 89975... Loss: 0.000040... Val Loss: 0.001091\n",
      "Epoch: 88/100... Step: 89980... Loss: 0.000107... Val Loss: 0.001074\n",
      "Epoch: 88/100... Step: 89985... Loss: 0.000063... Val Loss: 0.001187\n",
      "Epoch: 88/100... Step: 89990... Loss: 0.000049... Val Loss: 0.001209\n",
      "Epoch: 88/100... Step: 89995... Loss: 0.000046... Val Loss: 0.001264\n",
      "Epoch: 88/100... Step: 90000... Loss: 0.000063... Val Loss: 0.001365\n",
      "Epoch: 88/100... Step: 90005... Loss: 0.000085... Val Loss: 0.001287\n",
      "Epoch: 88/100... Step: 90010... Loss: 0.000062... Val Loss: 0.001139\n",
      "Epoch: 88/100... Step: 90015... Loss: 0.000102... Val Loss: 0.001085\n",
      "Epoch: 88/100... Step: 90020... Loss: 0.000048... Val Loss: 0.001031\n",
      "Epoch: 88/100... Step: 90025... Loss: 0.000118... Val Loss: 0.001074\n",
      "Epoch: 88/100... Step: 90030... Loss: 0.000077... Val Loss: 0.001202\n",
      "Epoch: 88/100... Step: 90035... Loss: 0.000056... Val Loss: 0.001188\n",
      "Epoch: 88/100... Step: 90040... Loss: 0.000067... Val Loss: 0.001143\n",
      "Epoch: 88/100... Step: 90045... Loss: 0.000157... Val Loss: 0.001122\n",
      "Epoch: 88/100... Step: 90050... Loss: 0.000111... Val Loss: 0.000985\n",
      "Epoch: 88/100... Step: 90055... Loss: 0.000067... Val Loss: 0.000818\n",
      "Epoch: 88/100... Step: 90060... Loss: 0.000082... Val Loss: 0.000817\n",
      "Epoch: 88/100... Step: 90065... Loss: 0.000045... Val Loss: 0.000941\n",
      "Epoch: 88/100... Step: 90070... Loss: 0.000128... Val Loss: 0.001011\n",
      "Epoch: 88/100... Step: 90075... Loss: 0.000252... Val Loss: 0.000921\n",
      "Epoch: 88/100... Step: 90080... Loss: 0.000174... Val Loss: 0.000724\n",
      "Epoch: 88/100... Step: 90085... Loss: 0.000080... Val Loss: 0.000476\n",
      "Epoch: 88/100... Step: 90090... Loss: 0.000320... Val Loss: 0.000408\n",
      "Epoch: 88/100... Step: 90095... Loss: 0.000141... Val Loss: 0.000501\n",
      "Epoch: 88/100... Step: 90100... Loss: 0.000054... Val Loss: 0.000654\n",
      "Epoch: 88/100... Step: 90105... Loss: 0.000043... Val Loss: 0.000664\n",
      "Epoch: 88/100... Step: 90110... Loss: 0.000113... Val Loss: 0.000582\n",
      "Epoch: 88/100... Step: 90115... Loss: 0.000043... Val Loss: 0.000434\n",
      "Epoch: 88/100... Step: 90120... Loss: 0.000192... Val Loss: 0.000367\n",
      "Epoch: 88/100... Step: 90125... Loss: 0.000227... Val Loss: 0.000469\n",
      "Epoch: 88/100... Step: 90130... Loss: 0.000232... Val Loss: 0.000625\n",
      "Epoch: 88/100... Step: 90135... Loss: 0.000354... Val Loss: 0.000663\n",
      "Epoch: 88/100... Step: 90140... Loss: 0.000143... Val Loss: 0.000594\n",
      "Epoch: 88/100... Step: 90145... Loss: 0.000182... Val Loss: 0.000624\n",
      "Epoch: 88/100... Step: 90150... Loss: 0.000204... Val Loss: 0.000789\n",
      "Epoch: 88/100... Step: 90155... Loss: 0.000038... Val Loss: 0.001065\n",
      "Epoch: 88/100... Step: 90160... Loss: 0.000190... Val Loss: 0.001187\n",
      "Epoch: 88/100... Step: 90165... Loss: 0.000058... Val Loss: 0.001095\n",
      "Epoch: 88/100... Step: 90170... Loss: 0.000076... Val Loss: 0.000970\n",
      "Epoch: 88/100... Step: 90175... Loss: 0.000061... Val Loss: 0.000930\n",
      "Epoch: 88/100... Step: 90180... Loss: 0.000044... Val Loss: 0.001040\n",
      "Epoch: 88/100... Step: 90185... Loss: 0.000035... Val Loss: 0.001142\n",
      "Epoch: 88/100... Step: 90190... Loss: 0.000440... Val Loss: 0.001112\n",
      "Epoch: 88/100... Step: 90195... Loss: 0.000753... Val Loss: 0.000946\n",
      "Epoch: 88/100... Step: 90200... Loss: 0.001463... Val Loss: 0.000678\n",
      "Epoch: 88/100... Step: 90205... Loss: 0.001449... Val Loss: 0.000325\n",
      "Epoch: 88/100... Step: 90210... Loss: 0.000760... Val Loss: 0.000498\n",
      "Epoch: 88/100... Step: 90215... Loss: 0.000254... Val Loss: 0.001497\n",
      "Epoch: 88/100... Step: 90220... Loss: 0.000528... Val Loss: 0.001774\n",
      "Epoch: 88/100... Step: 90225... Loss: 0.000317... Val Loss: 0.001341\n",
      "Epoch: 88/100... Step: 90230... Loss: 0.000277... Val Loss: 0.000907\n",
      "Epoch: 88/100... Step: 90235... Loss: 0.000409... Val Loss: 0.000930\n",
      "Epoch: 88/100... Step: 90240... Loss: 0.000224... Val Loss: 0.001272\n",
      "Epoch: 88/100... Step: 90245... Loss: 0.000209... Val Loss: 0.001626\n",
      "Epoch: 88/100... Step: 90250... Loss: 0.000134... Val Loss: 0.001572\n",
      "Epoch: 88/100... Step: 90255... Loss: 0.000200... Val Loss: 0.001313\n",
      "Epoch: 88/100... Step: 90260... Loss: 0.000177... Val Loss: 0.001050\n",
      "Epoch: 88/100... Step: 90265... Loss: 0.000229... Val Loss: 0.001005\n",
      "Epoch: 88/100... Step: 90270... Loss: 0.000182... Val Loss: 0.001102\n",
      "Epoch: 88/100... Step: 90275... Loss: 0.000216... Val Loss: 0.001042\n",
      "Epoch: 88/100... Step: 90280... Loss: 0.000144... Val Loss: 0.000883\n",
      "Epoch: 88/100... Step: 90285... Loss: 0.000060... Val Loss: 0.000703\n",
      "Epoch: 88/100... Step: 90290... Loss: 0.000177... Val Loss: 0.000568\n",
      "Epoch: 88/100... Step: 90295... Loss: 0.000085... Val Loss: 0.000361\n",
      "Epoch: 88/100... Step: 90300... Loss: 0.000165... Val Loss: 0.000279\n",
      "Epoch: 88/100... Step: 90305... Loss: 0.000224... Val Loss: 0.000331\n",
      "Epoch: 88/100... Step: 90310... Loss: 0.000114... Val Loss: 0.000611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88/100... Step: 90315... Loss: 0.000193... Val Loss: 0.000895\n",
      "Epoch: 88/100... Step: 90320... Loss: 0.000239... Val Loss: 0.000819\n",
      "Epoch: 88/100... Step: 90325... Loss: 0.000110... Val Loss: 0.000578\n",
      "Epoch: 88/100... Step: 90330... Loss: 0.000340... Val Loss: 0.000523\n",
      "Epoch: 88/100... Step: 90335... Loss: 0.000240... Val Loss: 0.000683\n",
      "Epoch: 88/100... Step: 90340... Loss: 0.000137... Val Loss: 0.000926\n",
      "Epoch: 88/100... Step: 90345... Loss: 0.000250... Val Loss: 0.000972\n",
      "Epoch: 88/100... Step: 90350... Loss: 0.000095... Val Loss: 0.000820\n",
      "Epoch: 88/100... Step: 90355... Loss: 0.000169... Val Loss: 0.000686\n",
      "Epoch: 88/100... Step: 90360... Loss: 0.000190... Val Loss: 0.000748\n",
      "Epoch: 88/100... Step: 90365... Loss: 0.000089... Val Loss: 0.000889\n",
      "Epoch: 88/100... Step: 90370... Loss: 0.000071... Val Loss: 0.000918\n",
      "Epoch: 88/100... Step: 90375... Loss: 0.000046... Val Loss: 0.000832\n",
      "Epoch: 88/100... Step: 90380... Loss: 0.000076... Val Loss: 0.000861\n",
      "Epoch: 88/100... Step: 90385... Loss: 0.000056... Val Loss: 0.000956\n",
      "Epoch: 88/100... Step: 90390... Loss: 0.000038... Val Loss: 0.001001\n",
      "Epoch: 88/100... Step: 90395... Loss: 0.000052... Val Loss: 0.000929\n",
      "Epoch: 88/100... Step: 90400... Loss: 0.000044... Val Loss: 0.000800\n",
      "Epoch: 88/100... Step: 90405... Loss: 0.000114... Val Loss: 0.000796\n",
      "Epoch: 88/100... Step: 90410... Loss: 0.000037... Val Loss: 0.000938\n",
      "Epoch: 88/100... Step: 90415... Loss: 0.000112... Val Loss: 0.001007\n",
      "Epoch: 88/100... Step: 90420... Loss: 0.000069... Val Loss: 0.000908\n",
      "Epoch: 88/100... Step: 90425... Loss: 0.000088... Val Loss: 0.000758\n",
      "Epoch: 88/100... Step: 90430... Loss: 0.000038... Val Loss: 0.000610\n",
      "Epoch: 88/100... Step: 90435... Loss: 0.000267... Val Loss: 0.000512\n",
      "Epoch: 88/100... Step: 90440... Loss: 0.000309... Val Loss: 0.000590\n",
      "Epoch: 88/100... Step: 90445... Loss: 0.000085... Val Loss: 0.000785\n",
      "Epoch: 88/100... Step: 90450... Loss: 0.000338... Val Loss: 0.000911\n",
      "Epoch: 88/100... Step: 90455... Loss: 0.000505... Val Loss: 0.000845\n",
      "Epoch: 88/100... Step: 90460... Loss: 0.000341... Val Loss: 0.000633\n",
      "Epoch: 88/100... Step: 90465... Loss: 0.000178... Val Loss: 0.000345\n",
      "Epoch: 88/100... Step: 90470... Loss: 0.000127... Val Loss: 0.000263\n",
      "Epoch: 88/100... Step: 90475... Loss: 0.000358... Val Loss: 0.000277\n",
      "Epoch: 88/100... Step: 90480... Loss: 0.000218... Val Loss: 0.000266\n",
      "Epoch: 88/100... Step: 90485... Loss: 0.000081... Val Loss: 0.000380\n",
      "Epoch: 88/100... Step: 90490... Loss: 0.000103... Val Loss: 0.000374\n",
      "Epoch: 88/100... Step: 90495... Loss: 0.000093... Val Loss: 0.000288\n",
      "Epoch: 88/100... Step: 90500... Loss: 0.000139... Val Loss: 0.000318\n",
      "Epoch: 88/100... Step: 90505... Loss: 0.000047... Val Loss: 0.000503\n",
      "Epoch: 88/100... Step: 90510... Loss: 0.000198... Val Loss: 0.000545\n",
      "Epoch: 88/100... Step: 90515... Loss: 0.000054... Val Loss: 0.000386\n",
      "Epoch: 88/100... Step: 90520... Loss: 0.000118... Val Loss: 0.000293\n",
      "Epoch: 88/100... Step: 90525... Loss: 0.000287... Val Loss: 0.000264\n",
      "Epoch: 88/100... Step: 90530... Loss: 0.000230... Val Loss: 0.000286\n",
      "Epoch: 88/100... Step: 90535... Loss: 0.000053... Val Loss: 0.000513\n",
      "Epoch: 88/100... Step: 90540... Loss: 0.000239... Val Loss: 0.000543\n",
      "Epoch: 88/100... Step: 90545... Loss: 0.000101... Val Loss: 0.000384\n",
      "Epoch: 88/100... Step: 90550... Loss: 0.000146... Val Loss: 0.000284\n",
      "Epoch: 88/100... Step: 90555... Loss: 0.000224... Val Loss: 0.000295\n",
      "Epoch: 88/100... Step: 90560... Loss: 0.000082... Val Loss: 0.000413\n",
      "Epoch: 88/100... Step: 90565... Loss: 0.000063... Val Loss: 0.000584\n",
      "Epoch: 88/100... Step: 90570... Loss: 0.000120... Val Loss: 0.000641\n",
      "Epoch: 88/100... Step: 90575... Loss: 0.000120... Val Loss: 0.000530\n",
      "Epoch: 88/100... Step: 90580... Loss: 0.000042... Val Loss: 0.000377\n",
      "Epoch: 88/100... Step: 90585... Loss: 0.000192... Val Loss: 0.000308\n",
      "Epoch: 88/100... Step: 90590... Loss: 0.000404... Val Loss: 0.000353\n",
      "Epoch: 88/100... Step: 90595... Loss: 0.000326... Val Loss: 0.000510\n",
      "Epoch: 88/100... Step: 90600... Loss: 0.000107... Val Loss: 0.000755\n",
      "Epoch: 88/100... Step: 90605... Loss: 0.000233... Val Loss: 0.000837\n",
      "Epoch: 88/100... Step: 90610... Loss: 0.000162... Val Loss: 0.000703\n",
      "Epoch: 88/100... Step: 90615... Loss: 0.000122... Val Loss: 0.000531\n",
      "Epoch: 88/100... Step: 90620... Loss: 0.000093... Val Loss: 0.000560\n",
      "Epoch: 88/100... Step: 90625... Loss: 0.000087... Val Loss: 0.000648\n",
      "Epoch: 88/100... Step: 90630... Loss: 0.000028... Val Loss: 0.000602\n",
      "Epoch: 88/100... Step: 90635... Loss: 0.000062... Val Loss: 0.000582\n",
      "Epoch: 88/100... Step: 90640... Loss: 0.000042... Val Loss: 0.000676\n",
      "Epoch: 88/100... Step: 90645... Loss: 0.000048... Val Loss: 0.000751\n",
      "Epoch: 88/100... Step: 90650... Loss: 0.000021... Val Loss: 0.000774\n",
      "Epoch: 88/100... Step: 90655... Loss: 0.000023... Val Loss: 0.000753\n",
      "Epoch: 88/100... Step: 90660... Loss: 0.000034... Val Loss: 0.000766\n",
      "Epoch: 88/100... Step: 90665... Loss: 0.000032... Val Loss: 0.000819\n",
      "Epoch: 88/100... Step: 90670... Loss: 0.000110... Val Loss: 0.000812\n",
      "Epoch: 88/100... Step: 90675... Loss: 0.000047... Val Loss: 0.000689\n",
      "Epoch: 88/100... Step: 90680... Loss: 0.000095... Val Loss: 0.000619\n",
      "Epoch: 88/100... Step: 90685... Loss: 0.000038... Val Loss: 0.000682\n",
      "Epoch: 88/100... Step: 90690... Loss: 0.000115... Val Loss: 0.000720\n",
      "Epoch: 88/100... Step: 90695... Loss: 0.000104... Val Loss: 0.000632\n",
      "Epoch: 88/100... Step: 90700... Loss: 0.000102... Val Loss: 0.000510\n",
      "Epoch: 88/100... Step: 90705... Loss: 0.000181... Val Loss: 0.000525\n",
      "Epoch: 88/100... Step: 90710... Loss: 0.000125... Val Loss: 0.000652\n",
      "Epoch: 88/100... Step: 90715... Loss: 0.000043... Val Loss: 0.000846\n",
      "Epoch: 88/100... Step: 90720... Loss: 0.000073... Val Loss: 0.000947\n",
      "Epoch: 88/100... Step: 90725... Loss: 0.000065... Val Loss: 0.000909\n",
      "Epoch: 88/100... Step: 90730... Loss: 0.000049... Val Loss: 0.000788\n",
      "Epoch: 88/100... Step: 90735... Loss: 0.000129... Val Loss: 0.000712\n",
      "Epoch: 88/100... Step: 90740... Loss: 0.000214... Val Loss: 0.000782\n",
      "Epoch: 88/100... Step: 90745... Loss: 0.000061... Val Loss: 0.000959\n",
      "Epoch: 88/100... Step: 90750... Loss: 0.000124... Val Loss: 0.001098\n",
      "Epoch: 88/100... Step: 90755... Loss: 0.000099... Val Loss: 0.001063\n",
      "Epoch: 88/100... Step: 90760... Loss: 0.000028... Val Loss: 0.000921\n",
      "Epoch: 88/100... Step: 90765... Loss: 0.000083... Val Loss: 0.000861\n",
      "Epoch: 88/100... Step: 90770... Loss: 0.000132... Val Loss: 0.000927\n",
      "Epoch: 88/100... Step: 90775... Loss: 0.000022... Val Loss: 0.001084\n",
      "Epoch: 88/100... Step: 90780... Loss: 0.000106... Val Loss: 0.001168\n",
      "Epoch: 88/100... Step: 90785... Loss: 0.000108... Val Loss: 0.001094\n",
      "Epoch: 88/100... Step: 90790... Loss: 0.000064... Val Loss: 0.000926\n",
      "Epoch: 88/100... Step: 90795... Loss: 0.000094... Val Loss: 0.000776\n",
      "Epoch: 88/100... Step: 90800... Loss: 0.000244... Val Loss: 0.000777\n",
      "Epoch: 88/100... Step: 90805... Loss: 0.000190... Val Loss: 0.000903\n",
      "Epoch: 88/100... Step: 90810... Loss: 0.000069... Val Loss: 0.001102\n",
      "Epoch: 88/100... Step: 90815... Loss: 0.000176... Val Loss: 0.001147\n",
      "Epoch: 89/100... Step: 90820... Loss: 0.000222... Val Loss: 0.001147\n",
      "Epoch: 89/100... Step: 90825... Loss: 0.000224... Val Loss: 0.001308\n",
      "Epoch: 89/100... Step: 90830... Loss: 0.000096... Val Loss: 0.001595\n",
      "Epoch: 89/100... Step: 90835... Loss: 0.000149... Val Loss: 0.001840\n",
      "Epoch: 89/100... Step: 90840... Loss: 0.000235... Val Loss: 0.001913\n",
      "Epoch: 89/100... Step: 90845... Loss: 0.000077... Val Loss: 0.001815\n",
      "Epoch: 89/100... Step: 90850... Loss: 0.000077... Val Loss: 0.001814\n",
      "Epoch: 89/100... Step: 90855... Loss: 0.000193... Val Loss: 0.001853\n",
      "Epoch: 89/100... Step: 90860... Loss: 0.000113... Val Loss: 0.001775\n",
      "Epoch: 89/100... Step: 90865... Loss: 0.000051... Val Loss: 0.001729\n",
      "Epoch: 89/100... Step: 90870... Loss: 0.000060... Val Loss: 0.001740\n",
      "Epoch: 89/100... Step: 90875... Loss: 0.000094... Val Loss: 0.001710\n",
      "Epoch: 89/100... Step: 90880... Loss: 0.000084... Val Loss: 0.001619\n",
      "Epoch: 89/100... Step: 90885... Loss: 0.000156... Val Loss: 0.001464\n",
      "Epoch: 89/100... Step: 90890... Loss: 0.000216... Val Loss: 0.001309\n",
      "Epoch: 89/100... Step: 90895... Loss: 0.000152... Val Loss: 0.001092\n",
      "Epoch: 89/100... Step: 90900... Loss: 0.000303... Val Loss: 0.001087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89/100... Step: 90905... Loss: 0.000119... Val Loss: 0.001258\n",
      "Epoch: 89/100... Step: 90910... Loss: 0.000184... Val Loss: 0.001421\n",
      "Epoch: 89/100... Step: 90915... Loss: 0.000365... Val Loss: 0.001399\n",
      "Epoch: 89/100... Step: 90920... Loss: 0.000161... Val Loss: 0.001242\n",
      "Epoch: 89/100... Step: 90925... Loss: 0.000078... Val Loss: 0.001032\n",
      "Epoch: 89/100... Step: 90930... Loss: 0.000168... Val Loss: 0.001009\n",
      "Epoch: 89/100... Step: 90935... Loss: 0.000056... Val Loss: 0.001139\n",
      "Epoch: 89/100... Step: 90940... Loss: 0.000056... Val Loss: 0.001271\n",
      "Epoch: 89/100... Step: 90945... Loss: 0.000054... Val Loss: 0.001337\n",
      "Epoch: 89/100... Step: 90950... Loss: 0.000019... Val Loss: 0.001418\n",
      "Epoch: 89/100... Step: 90955... Loss: 0.000048... Val Loss: 0.001424\n",
      "Epoch: 89/100... Step: 90960... Loss: 0.000203... Val Loss: 0.001439\n",
      "Epoch: 89/100... Step: 90965... Loss: 0.000159... Val Loss: 0.001568\n",
      "Epoch: 89/100... Step: 90970... Loss: 0.000114... Val Loss: 0.001757\n",
      "Epoch: 89/100... Step: 90975... Loss: 0.000289... Val Loss: 0.001761\n",
      "Epoch: 89/100... Step: 90980... Loss: 0.000224... Val Loss: 0.001607\n",
      "Epoch: 89/100... Step: 90985... Loss: 0.000070... Val Loss: 0.001367\n",
      "Epoch: 89/100... Step: 90990... Loss: 0.000135... Val Loss: 0.001200\n",
      "Epoch: 89/100... Step: 90995... Loss: 0.000111... Val Loss: 0.001236\n",
      "Epoch: 89/100... Step: 91000... Loss: 0.000100... Val Loss: 0.001379\n",
      "Epoch: 89/100... Step: 91005... Loss: 0.000226... Val Loss: 0.001356\n",
      "Epoch: 89/100... Step: 91010... Loss: 0.000055... Val Loss: 0.001168\n",
      "Epoch: 89/100... Step: 91015... Loss: 0.000107... Val Loss: 0.001030\n",
      "Epoch: 89/100... Step: 91020... Loss: 0.000126... Val Loss: 0.001066\n",
      "Epoch: 89/100... Step: 91025... Loss: 0.000070... Val Loss: 0.001217\n",
      "Epoch: 89/100... Step: 91030... Loss: 0.000075... Val Loss: 0.001356\n",
      "Epoch: 89/100... Step: 91035... Loss: 0.000094... Val Loss: 0.001343\n",
      "Epoch: 89/100... Step: 91040... Loss: 0.000070... Val Loss: 0.001193\n",
      "Epoch: 89/100... Step: 91045... Loss: 0.000102... Val Loss: 0.001093\n",
      "Epoch: 89/100... Step: 91050... Loss: 0.000036... Val Loss: 0.001038\n",
      "Epoch: 89/100... Step: 91055... Loss: 0.000120... Val Loss: 0.001047\n",
      "Epoch: 89/100... Step: 91060... Loss: 0.000074... Val Loss: 0.001154\n",
      "Epoch: 89/100... Step: 91065... Loss: 0.000090... Val Loss: 0.001208\n",
      "Epoch: 89/100... Step: 91070... Loss: 0.000027... Val Loss: 0.001161\n",
      "Epoch: 89/100... Step: 91075... Loss: 0.000111... Val Loss: 0.001135\n",
      "Epoch: 89/100... Step: 91080... Loss: 0.000139... Val Loss: 0.001049\n",
      "Epoch: 89/100... Step: 91085... Loss: 0.000045... Val Loss: 0.000877\n",
      "Epoch: 89/100... Step: 91090... Loss: 0.000083... Val Loss: 0.000792\n",
      "Epoch: 89/100... Step: 91095... Loss: 0.000084... Val Loss: 0.000851\n",
      "Epoch: 89/100... Step: 91100... Loss: 0.000045... Val Loss: 0.000988\n",
      "Epoch: 89/100... Step: 91105... Loss: 0.000266... Val Loss: 0.001025\n",
      "Epoch: 89/100... Step: 91110... Loss: 0.000340... Val Loss: 0.000929\n",
      "Epoch: 89/100... Step: 91115... Loss: 0.000249... Val Loss: 0.000749\n",
      "Epoch: 89/100... Step: 91120... Loss: 0.000127... Val Loss: 0.000532\n",
      "Epoch: 89/100... Step: 91125... Loss: 0.000229... Val Loss: 0.000448\n",
      "Epoch: 89/100... Step: 91130... Loss: 0.000107... Val Loss: 0.000501\n",
      "Epoch: 89/100... Step: 91135... Loss: 0.000034... Val Loss: 0.000652\n",
      "Epoch: 89/100... Step: 91140... Loss: 0.000155... Val Loss: 0.000720\n",
      "Epoch: 89/100... Step: 91145... Loss: 0.000233... Val Loss: 0.000633\n",
      "Epoch: 89/100... Step: 91150... Loss: 0.000053... Val Loss: 0.000459\n",
      "Epoch: 89/100... Step: 91155... Loss: 0.000275... Val Loss: 0.000366\n",
      "Epoch: 89/100... Step: 91160... Loss: 0.000216... Val Loss: 0.000398\n",
      "Epoch: 89/100... Step: 91165... Loss: 0.000107... Val Loss: 0.000425\n",
      "Epoch: 89/100... Step: 91170... Loss: 0.000196... Val Loss: 0.000374\n",
      "Epoch: 89/100... Step: 91175... Loss: 0.000432... Val Loss: 0.000366\n",
      "Epoch: 89/100... Step: 91180... Loss: 0.000505... Val Loss: 0.000470\n",
      "Epoch: 89/100... Step: 91185... Loss: 0.000364... Val Loss: 0.000718\n",
      "Epoch: 89/100... Step: 91190... Loss: 0.000065... Val Loss: 0.001102\n",
      "Epoch: 89/100... Step: 91195... Loss: 0.000231... Val Loss: 0.001270\n",
      "Epoch: 89/100... Step: 91200... Loss: 0.000146... Val Loss: 0.001115\n",
      "Epoch: 89/100... Step: 91205... Loss: 0.000062... Val Loss: 0.000890\n",
      "Epoch: 89/100... Step: 91210... Loss: 0.000183... Val Loss: 0.000860\n",
      "Epoch: 89/100... Step: 91215... Loss: 0.000136... Val Loss: 0.001010\n",
      "Epoch: 89/100... Step: 91220... Loss: 0.000279... Val Loss: 0.001203\n",
      "Epoch: 89/100... Step: 91225... Loss: 0.000802... Val Loss: 0.001211\n",
      "Epoch: 89/100... Step: 91230... Loss: 0.001429... Val Loss: 0.001060\n",
      "Epoch: 89/100... Step: 91235... Loss: 0.002027... Val Loss: 0.000824\n",
      "Epoch: 89/100... Step: 91240... Loss: 0.001589... Val Loss: 0.000526\n",
      "Epoch: 89/100... Step: 91245... Loss: 0.001165... Val Loss: 0.000262\n",
      "Epoch: 89/100... Step: 91250... Loss: 0.000609... Val Loss: 0.000756\n",
      "Epoch: 89/100... Step: 91255... Loss: 0.000318... Val Loss: 0.001686\n",
      "Epoch: 89/100... Step: 91260... Loss: 0.000620... Val Loss: 0.001825\n",
      "Epoch: 89/100... Step: 91265... Loss: 0.000253... Val Loss: 0.001376\n",
      "Epoch: 89/100... Step: 91270... Loss: 0.000317... Val Loss: 0.001005\n",
      "Epoch: 89/100... Step: 91275... Loss: 0.000323... Val Loss: 0.001066\n",
      "Epoch: 89/100... Step: 91280... Loss: 0.000135... Val Loss: 0.001409\n",
      "Epoch: 89/100... Step: 91285... Loss: 0.000375... Val Loss: 0.001711\n",
      "Epoch: 89/100... Step: 91290... Loss: 0.000552... Val Loss: 0.001608\n",
      "Epoch: 89/100... Step: 91295... Loss: 0.000111... Val Loss: 0.001260\n",
      "Epoch: 89/100... Step: 91300... Loss: 0.000162... Val Loss: 0.000976\n",
      "Epoch: 89/100... Step: 91305... Loss: 0.000114... Val Loss: 0.000835\n",
      "Epoch: 89/100... Step: 91310... Loss: 0.000131... Val Loss: 0.000858\n",
      "Epoch: 89/100... Step: 91315... Loss: 0.000110... Val Loss: 0.000888\n",
      "Epoch: 89/100... Step: 91320... Loss: 0.000241... Val Loss: 0.000768\n",
      "Epoch: 89/100... Step: 91325... Loss: 0.000234... Val Loss: 0.000540\n",
      "Epoch: 89/100... Step: 91330... Loss: 0.000121... Val Loss: 0.000307\n",
      "Epoch: 89/100... Step: 91335... Loss: 0.000349... Val Loss: 0.000264\n",
      "Epoch: 89/100... Step: 91340... Loss: 0.000405... Val Loss: 0.000287\n",
      "Epoch: 89/100... Step: 91345... Loss: 0.000327... Val Loss: 0.000430\n",
      "Epoch: 89/100... Step: 91350... Loss: 0.000081... Val Loss: 0.000709\n",
      "Epoch: 89/100... Step: 91355... Loss: 0.000235... Val Loss: 0.000853\n",
      "Epoch: 89/100... Step: 91360... Loss: 0.000102... Val Loss: 0.000809\n",
      "Epoch: 89/100... Step: 91365... Loss: 0.000095... Val Loss: 0.000839\n",
      "Epoch: 89/100... Step: 91370... Loss: 0.000120... Val Loss: 0.000937\n",
      "Epoch: 89/100... Step: 91375... Loss: 0.000161... Val Loss: 0.000885\n",
      "Epoch: 89/100... Step: 91380... Loss: 0.000038... Val Loss: 0.000691\n",
      "Epoch: 89/100... Step: 91385... Loss: 0.000172... Val Loss: 0.000654\n",
      "Epoch: 89/100... Step: 91390... Loss: 0.000154... Val Loss: 0.000810\n",
      "Epoch: 89/100... Step: 91395... Loss: 0.000127... Val Loss: 0.001003\n",
      "Epoch: 89/100... Step: 91400... Loss: 0.000179... Val Loss: 0.001028\n",
      "Epoch: 89/100... Step: 91405... Loss: 0.000069... Val Loss: 0.000903\n",
      "Epoch: 89/100... Step: 91410... Loss: 0.000133... Val Loss: 0.000793\n",
      "Epoch: 89/100... Step: 91415... Loss: 0.000112... Val Loss: 0.000829\n",
      "Epoch: 89/100... Step: 91420... Loss: 0.000059... Val Loss: 0.000958\n",
      "Epoch: 89/100... Step: 91425... Loss: 0.000112... Val Loss: 0.001038\n",
      "Epoch: 89/100... Step: 91430... Loss: 0.000141... Val Loss: 0.000976\n",
      "Epoch: 89/100... Step: 91435... Loss: 0.000052... Val Loss: 0.000841\n",
      "Epoch: 89/100... Step: 91440... Loss: 0.000150... Val Loss: 0.000781\n",
      "Epoch: 89/100... Step: 91445... Loss: 0.000051... Val Loss: 0.000859\n",
      "Epoch: 89/100... Step: 91450... Loss: 0.000070... Val Loss: 0.000952\n",
      "Epoch: 89/100... Step: 91455... Loss: 0.000128... Val Loss: 0.000919\n",
      "Epoch: 89/100... Step: 91460... Loss: 0.000160... Val Loss: 0.000788\n",
      "Epoch: 89/100... Step: 91465... Loss: 0.000134... Val Loss: 0.000572\n",
      "Epoch: 89/100... Step: 91470... Loss: 0.000427... Val Loss: 0.000457\n",
      "Epoch: 89/100... Step: 91475... Loss: 0.000348... Val Loss: 0.000507\n",
      "Epoch: 89/100... Step: 91480... Loss: 0.000107... Val Loss: 0.000658\n",
      "Epoch: 89/100... Step: 91485... Loss: 0.000330... Val Loss: 0.000754\n",
      "Epoch: 89/100... Step: 91490... Loss: 0.000376... Val Loss: 0.000701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89/100... Step: 91495... Loss: 0.000273... Val Loss: 0.000545\n",
      "Epoch: 89/100... Step: 91500... Loss: 0.000209... Val Loss: 0.000338\n",
      "Epoch: 89/100... Step: 91505... Loss: 0.000138... Val Loss: 0.000256\n",
      "Epoch: 89/100... Step: 91510... Loss: 0.000264... Val Loss: 0.000261\n",
      "Epoch: 89/100... Step: 91515... Loss: 0.000154... Val Loss: 0.000261\n",
      "Epoch: 89/100... Step: 91520... Loss: 0.000048... Val Loss: 0.000333\n",
      "Epoch: 89/100... Step: 91525... Loss: 0.000097... Val Loss: 0.000401\n",
      "Epoch: 89/100... Step: 91530... Loss: 0.000042... Val Loss: 0.000360\n",
      "Epoch: 89/100... Step: 91535... Loss: 0.000077... Val Loss: 0.000387\n",
      "Epoch: 89/100... Step: 91540... Loss: 0.000078... Val Loss: 0.000479\n",
      "Epoch: 89/100... Step: 91545... Loss: 0.000078... Val Loss: 0.000432\n",
      "Epoch: 89/100... Step: 91550... Loss: 0.000086... Val Loss: 0.000354\n",
      "Epoch: 89/100... Step: 91555... Loss: 0.000246... Val Loss: 0.000302\n",
      "Epoch: 89/100... Step: 91560... Loss: 0.000337... Val Loss: 0.000258\n",
      "Epoch: 89/100... Step: 91565... Loss: 0.000125... Val Loss: 0.000329\n",
      "Epoch: 89/100... Step: 91570... Loss: 0.000191... Val Loss: 0.000551\n",
      "Epoch: 89/100... Step: 91575... Loss: 0.000176... Val Loss: 0.000502\n",
      "Epoch: 89/100... Step: 91580... Loss: 0.000064... Val Loss: 0.000325\n",
      "Epoch: 89/100... Step: 91585... Loss: 0.000192... Val Loss: 0.000286\n",
      "Epoch: 89/100... Step: 91590... Loss: 0.000173... Val Loss: 0.000324\n",
      "Epoch: 89/100... Step: 91595... Loss: 0.000126... Val Loss: 0.000460\n",
      "Epoch: 89/100... Step: 91600... Loss: 0.000079... Val Loss: 0.000630\n",
      "Epoch: 89/100... Step: 91605... Loss: 0.000209... Val Loss: 0.000678\n",
      "Epoch: 89/100... Step: 91610... Loss: 0.000253... Val Loss: 0.000553\n",
      "Epoch: 89/100... Step: 91615... Loss: 0.000084... Val Loss: 0.000378\n",
      "Epoch: 89/100... Step: 91620... Loss: 0.000311... Val Loss: 0.000303\n",
      "Epoch: 89/100... Step: 91625... Loss: 0.000496... Val Loss: 0.000329\n",
      "Epoch: 89/100... Step: 91630... Loss: 0.000339... Val Loss: 0.000447\n",
      "Epoch: 89/100... Step: 91635... Loss: 0.000052... Val Loss: 0.000660\n",
      "Epoch: 89/100... Step: 91640... Loss: 0.000176... Val Loss: 0.000789\n",
      "Epoch: 89/100... Step: 91645... Loss: 0.000153... Val Loss: 0.000721\n",
      "Epoch: 89/100... Step: 91650... Loss: 0.000079... Val Loss: 0.000577\n",
      "Epoch: 89/100... Step: 91655... Loss: 0.000055... Val Loss: 0.000554\n",
      "Epoch: 89/100... Step: 91660... Loss: 0.000046... Val Loss: 0.000606\n",
      "Epoch: 89/100... Step: 91665... Loss: 0.000043... Val Loss: 0.000609\n",
      "Epoch: 89/100... Step: 91670... Loss: 0.000038... Val Loss: 0.000636\n",
      "Epoch: 89/100... Step: 91675... Loss: 0.000045... Val Loss: 0.000707\n",
      "Epoch: 89/100... Step: 91680... Loss: 0.000029... Val Loss: 0.000788\n",
      "Epoch: 89/100... Step: 91685... Loss: 0.000047... Val Loss: 0.000790\n",
      "Epoch: 89/100... Step: 91690... Loss: 0.000024... Val Loss: 0.000727\n",
      "Epoch: 89/100... Step: 91695... Loss: 0.000076... Val Loss: 0.000753\n",
      "Epoch: 89/100... Step: 91700... Loss: 0.000069... Val Loss: 0.000841\n",
      "Epoch: 89/100... Step: 91705... Loss: 0.000172... Val Loss: 0.000828\n",
      "Epoch: 89/100... Step: 91710... Loss: 0.000060... Val Loss: 0.000720\n",
      "Epoch: 89/100... Step: 91715... Loss: 0.000061... Val Loss: 0.000629\n",
      "Epoch: 89/100... Step: 91720... Loss: 0.000060... Val Loss: 0.000625\n",
      "Epoch: 89/100... Step: 91725... Loss: 0.000045... Val Loss: 0.000603\n",
      "Epoch: 89/100... Step: 91730... Loss: 0.000044... Val Loss: 0.000540\n",
      "Epoch: 89/100... Step: 91735... Loss: 0.000107... Val Loss: 0.000565\n",
      "Epoch: 89/100... Step: 91740... Loss: 0.000068... Val Loss: 0.000677\n",
      "Epoch: 89/100... Step: 91745... Loss: 0.000034... Val Loss: 0.000811\n",
      "Epoch: 89/100... Step: 91750... Loss: 0.000044... Val Loss: 0.000872\n",
      "Epoch: 89/100... Step: 91755... Loss: 0.000032... Val Loss: 0.000863\n",
      "Epoch: 89/100... Step: 91760... Loss: 0.000044... Val Loss: 0.000836\n",
      "Epoch: 89/100... Step: 91765... Loss: 0.000042... Val Loss: 0.000780\n",
      "Epoch: 89/100... Step: 91770... Loss: 0.000167... Val Loss: 0.000761\n",
      "Epoch: 89/100... Step: 91775... Loss: 0.000159... Val Loss: 0.000839\n",
      "Epoch: 89/100... Step: 91780... Loss: 0.000034... Val Loss: 0.000986\n",
      "Epoch: 89/100... Step: 91785... Loss: 0.000096... Val Loss: 0.001075\n",
      "Epoch: 89/100... Step: 91790... Loss: 0.000090... Val Loss: 0.001036\n",
      "Epoch: 89/100... Step: 91795... Loss: 0.000040... Val Loss: 0.000920\n",
      "Epoch: 89/100... Step: 91800... Loss: 0.000133... Val Loss: 0.000843\n",
      "Epoch: 89/100... Step: 91805... Loss: 0.000201... Val Loss: 0.000883\n",
      "Epoch: 89/100... Step: 91810... Loss: 0.000068... Val Loss: 0.001020\n",
      "Epoch: 89/100... Step: 91815... Loss: 0.000103... Val Loss: 0.001151\n",
      "Epoch: 89/100... Step: 91820... Loss: 0.000198... Val Loss: 0.001144\n",
      "Epoch: 89/100... Step: 91825... Loss: 0.000195... Val Loss: 0.001034\n",
      "Epoch: 89/100... Step: 91830... Loss: 0.000076... Val Loss: 0.000872\n",
      "Epoch: 89/100... Step: 91835... Loss: 0.000230... Val Loss: 0.000823\n",
      "Epoch: 89/100... Step: 91840... Loss: 0.000178... Val Loss: 0.000895\n",
      "Epoch: 89/100... Step: 91845... Loss: 0.000047... Val Loss: 0.001043\n",
      "Epoch: 90/100... Step: 91850... Loss: 0.000196... Val Loss: 0.001122\n",
      "Epoch: 90/100... Step: 91855... Loss: 0.000221... Val Loss: 0.001262\n",
      "Epoch: 90/100... Step: 91860... Loss: 0.000087... Val Loss: 0.001484\n",
      "Epoch: 90/100... Step: 91865... Loss: 0.000158... Val Loss: 0.001726\n",
      "Epoch: 90/100... Step: 91870... Loss: 0.000221... Val Loss: 0.001885\n",
      "Epoch: 90/100... Step: 91875... Loss: 0.000172... Val Loss: 0.001883\n",
      "Epoch: 90/100... Step: 91880... Loss: 0.000060... Val Loss: 0.001814\n",
      "Epoch: 90/100... Step: 91885... Loss: 0.000160... Val Loss: 0.001838\n",
      "Epoch: 90/100... Step: 91890... Loss: 0.000172... Val Loss: 0.001802\n",
      "Epoch: 90/100... Step: 91895... Loss: 0.000064... Val Loss: 0.001746\n",
      "Epoch: 90/100... Step: 91900... Loss: 0.000051... Val Loss: 0.001745\n",
      "Epoch: 90/100... Step: 91905... Loss: 0.000065... Val Loss: 0.001725\n",
      "Epoch: 90/100... Step: 91910... Loss: 0.000096... Val Loss: 0.001669\n",
      "Epoch: 90/100... Step: 91915... Loss: 0.000088... Val Loss: 0.001546\n",
      "Epoch: 90/100... Step: 91920... Loss: 0.000249... Val Loss: 0.001396\n",
      "Epoch: 90/100... Step: 91925... Loss: 0.000101... Val Loss: 0.001188\n",
      "Epoch: 90/100... Step: 91930... Loss: 0.000284... Val Loss: 0.001054\n",
      "Epoch: 90/100... Step: 91935... Loss: 0.000268... Val Loss: 0.001120\n",
      "Epoch: 90/100... Step: 91940... Loss: 0.000052... Val Loss: 0.001296\n",
      "Epoch: 90/100... Step: 91945... Loss: 0.000285... Val Loss: 0.001403\n",
      "Epoch: 90/100... Step: 91950... Loss: 0.000302... Val Loss: 0.001362\n",
      "Epoch: 90/100... Step: 91955... Loss: 0.000092... Val Loss: 0.001216\n",
      "Epoch: 90/100... Step: 91960... Loss: 0.000086... Val Loss: 0.001039\n",
      "Epoch: 90/100... Step: 91965... Loss: 0.000153... Val Loss: 0.001020\n",
      "Epoch: 90/100... Step: 91970... Loss: 0.000090... Val Loss: 0.001121\n",
      "Epoch: 90/100... Step: 91975... Loss: 0.000065... Val Loss: 0.001270\n",
      "Epoch: 90/100... Step: 91980... Loss: 0.000027... Val Loss: 0.001432\n",
      "Epoch: 90/100... Step: 91985... Loss: 0.000069... Val Loss: 0.001485\n",
      "Epoch: 90/100... Step: 91990... Loss: 0.000121... Val Loss: 0.001444\n",
      "Epoch: 90/100... Step: 91995... Loss: 0.000251... Val Loss: 0.001458\n",
      "Epoch: 90/100... Step: 92000... Loss: 0.000112... Val Loss: 0.001582\n",
      "Epoch: 90/100... Step: 92005... Loss: 0.000173... Val Loss: 0.001714\n",
      "Epoch: 90/100... Step: 92010... Loss: 0.000268... Val Loss: 0.001705\n",
      "Epoch: 90/100... Step: 92015... Loss: 0.000221... Val Loss: 0.001584\n",
      "Epoch: 90/100... Step: 92020... Loss: 0.000076... Val Loss: 0.001392\n",
      "Epoch: 90/100... Step: 92025... Loss: 0.000112... Val Loss: 0.001234\n",
      "Epoch: 90/100... Step: 92030... Loss: 0.000094... Val Loss: 0.001238\n",
      "Epoch: 90/100... Step: 92035... Loss: 0.000115... Val Loss: 0.001311\n",
      "Epoch: 90/100... Step: 92040... Loss: 0.000160... Val Loss: 0.001285\n",
      "Epoch: 90/100... Step: 92045... Loss: 0.000025... Val Loss: 0.001146\n",
      "Epoch: 90/100... Step: 92050... Loss: 0.000097... Val Loss: 0.001066\n",
      "Epoch: 90/100... Step: 92055... Loss: 0.000119... Val Loss: 0.001125\n",
      "Epoch: 90/100... Step: 92060... Loss: 0.000044... Val Loss: 0.001283\n",
      "Epoch: 90/100... Step: 92065... Loss: 0.000112... Val Loss: 0.001389\n",
      "Epoch: 90/100... Step: 92070... Loss: 0.000155... Val Loss: 0.001345\n",
      "Epoch: 90/100... Step: 92075... Loss: 0.000097... Val Loss: 0.001223\n",
      "Epoch: 90/100... Step: 92080... Loss: 0.000097... Val Loss: 0.001097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90/100... Step: 92085... Loss: 0.000097... Val Loss: 0.000989\n",
      "Epoch: 90/100... Step: 92090... Loss: 0.000191... Val Loss: 0.001001\n",
      "Epoch: 90/100... Step: 92095... Loss: 0.000055... Val Loss: 0.001106\n",
      "Epoch: 90/100... Step: 92100... Loss: 0.000031... Val Loss: 0.001201\n",
      "Epoch: 90/100... Step: 92105... Loss: 0.000113... Val Loss: 0.001211\n",
      "Epoch: 90/100... Step: 92110... Loss: 0.000199... Val Loss: 0.001123\n",
      "Epoch: 90/100... Step: 92115... Loss: 0.000076... Val Loss: 0.000960\n",
      "Epoch: 90/100... Step: 92120... Loss: 0.000064... Val Loss: 0.000810\n",
      "Epoch: 90/100... Step: 92125... Loss: 0.000121... Val Loss: 0.000787\n",
      "Epoch: 90/100... Step: 92130... Loss: 0.000104... Val Loss: 0.000867\n",
      "Epoch: 90/100... Step: 92135... Loss: 0.000139... Val Loss: 0.000991\n",
      "Epoch: 90/100... Step: 92140... Loss: 0.000359... Val Loss: 0.001001\n",
      "Epoch: 90/100... Step: 92145... Loss: 0.000370... Val Loss: 0.000901\n",
      "Epoch: 90/100... Step: 92150... Loss: 0.000190... Val Loss: 0.000736\n",
      "Epoch: 90/100... Step: 92155... Loss: 0.000145... Val Loss: 0.000582\n",
      "Epoch: 90/100... Step: 92160... Loss: 0.000071... Val Loss: 0.000521\n",
      "Epoch: 90/100... Step: 92165... Loss: 0.000088... Val Loss: 0.000539\n",
      "Epoch: 90/100... Step: 92170... Loss: 0.000061... Val Loss: 0.000656\n",
      "Epoch: 90/100... Step: 92175... Loss: 0.000248... Val Loss: 0.000708\n",
      "Epoch: 90/100... Step: 92180... Loss: 0.000252... Val Loss: 0.000625\n",
      "Epoch: 90/100... Step: 92185... Loss: 0.000083... Val Loss: 0.000486\n",
      "Epoch: 90/100... Step: 92190... Loss: 0.000244... Val Loss: 0.000420\n",
      "Epoch: 90/100... Step: 92195... Loss: 0.000133... Val Loss: 0.000410\n",
      "Epoch: 90/100... Step: 92200... Loss: 0.000120... Val Loss: 0.000360\n",
      "Epoch: 90/100... Step: 92205... Loss: 0.000381... Val Loss: 0.000328\n",
      "Epoch: 90/100... Step: 92210... Loss: 0.000533... Val Loss: 0.000369\n",
      "Epoch: 90/100... Step: 92215... Loss: 0.000556... Val Loss: 0.000496\n",
      "Epoch: 90/100... Step: 92220... Loss: 0.000354... Val Loss: 0.000712\n",
      "Epoch: 90/100... Step: 92225... Loss: 0.000088... Val Loss: 0.001037\n",
      "Epoch: 90/100... Step: 92230... Loss: 0.000199... Val Loss: 0.001291\n",
      "Epoch: 90/100... Step: 92235... Loss: 0.000267... Val Loss: 0.001244\n",
      "Epoch: 90/100... Step: 92240... Loss: 0.000115... Val Loss: 0.001046\n",
      "Epoch: 90/100... Step: 92245... Loss: 0.000144... Val Loss: 0.000901\n",
      "Epoch: 90/100... Step: 92250... Loss: 0.000197... Val Loss: 0.000937\n",
      "Epoch: 90/100... Step: 92255... Loss: 0.000452... Val Loss: 0.001013\n",
      "Epoch: 90/100... Step: 92260... Loss: 0.000995... Val Loss: 0.000996\n",
      "Epoch: 90/100... Step: 92265... Loss: 0.001800... Val Loss: 0.000873\n",
      "Epoch: 90/100... Step: 92270... Loss: 0.001807... Val Loss: 0.000682\n",
      "Epoch: 90/100... Step: 92275... Loss: 0.001483... Val Loss: 0.000433\n",
      "Epoch: 90/100... Step: 92280... Loss: 0.001175... Val Loss: 0.000258\n",
      "Epoch: 90/100... Step: 92285... Loss: 0.000637... Val Loss: 0.000660\n",
      "Epoch: 90/100... Step: 92290... Loss: 0.000267... Val Loss: 0.001478\n",
      "Epoch: 90/100... Step: 92295... Loss: 0.000546... Val Loss: 0.001719\n",
      "Epoch: 90/100... Step: 92300... Loss: 0.000157... Val Loss: 0.001418\n",
      "Epoch: 90/100... Step: 92305... Loss: 0.000200... Val Loss: 0.001135\n",
      "Epoch: 90/100... Step: 92310... Loss: 0.000345... Val Loss: 0.001166\n",
      "Epoch: 90/100... Step: 92315... Loss: 0.000156... Val Loss: 0.001454\n",
      "Epoch: 90/100... Step: 92320... Loss: 0.000356... Val Loss: 0.001584\n",
      "Epoch: 90/100... Step: 92325... Loss: 0.000274... Val Loss: 0.001385\n",
      "Epoch: 90/100... Step: 92330... Loss: 0.000136... Val Loss: 0.001084\n",
      "Epoch: 90/100... Step: 92335... Loss: 0.000140... Val Loss: 0.000920\n",
      "Epoch: 90/100... Step: 92340... Loss: 0.000116... Val Loss: 0.000809\n",
      "Epoch: 90/100... Step: 92345... Loss: 0.000114... Val Loss: 0.000798\n",
      "Epoch: 90/100... Step: 92350... Loss: 0.000153... Val Loss: 0.000778\n",
      "Epoch: 90/100... Step: 92355... Loss: 0.000226... Val Loss: 0.000668\n",
      "Epoch: 90/100... Step: 92360... Loss: 0.000136... Val Loss: 0.000435\n",
      "Epoch: 90/100... Step: 92365... Loss: 0.000214... Val Loss: 0.000287\n",
      "Epoch: 90/100... Step: 92370... Loss: 0.000361... Val Loss: 0.000269\n",
      "Epoch: 90/100... Step: 92375... Loss: 0.000429... Val Loss: 0.000316\n",
      "Epoch: 90/100... Step: 92380... Loss: 0.000272... Val Loss: 0.000476\n",
      "Epoch: 90/100... Step: 92385... Loss: 0.000117... Val Loss: 0.000693\n",
      "Epoch: 90/100... Step: 92390... Loss: 0.000118... Val Loss: 0.000783\n",
      "Epoch: 90/100... Step: 92395... Loss: 0.000078... Val Loss: 0.000852\n",
      "Epoch: 90/100... Step: 92400... Loss: 0.000084... Val Loss: 0.000947\n",
      "Epoch: 90/100... Step: 92405... Loss: 0.000176... Val Loss: 0.000924\n",
      "Epoch: 90/100... Step: 92410... Loss: 0.000089... Val Loss: 0.000786\n",
      "Epoch: 90/100... Step: 92415... Loss: 0.000127... Val Loss: 0.000660\n",
      "Epoch: 90/100... Step: 92420... Loss: 0.000211... Val Loss: 0.000700\n",
      "Epoch: 90/100... Step: 92425... Loss: 0.000132... Val Loss: 0.000844\n",
      "Epoch: 90/100... Step: 92430... Loss: 0.000132... Val Loss: 0.000959\n",
      "Epoch: 90/100... Step: 92435... Loss: 0.000088... Val Loss: 0.000937\n",
      "Epoch: 90/100... Step: 92440... Loss: 0.000074... Val Loss: 0.000848\n",
      "Epoch: 90/100... Step: 92445... Loss: 0.000140... Val Loss: 0.000833\n",
      "Epoch: 90/100... Step: 92450... Loss: 0.000071... Val Loss: 0.000905\n",
      "Epoch: 90/100... Step: 92455... Loss: 0.000060... Val Loss: 0.001005\n",
      "Epoch: 90/100... Step: 92460... Loss: 0.000137... Val Loss: 0.001019\n",
      "Epoch: 90/100... Step: 92465... Loss: 0.000112... Val Loss: 0.000916\n",
      "Epoch: 90/100... Step: 92470... Loss: 0.000083... Val Loss: 0.000812\n",
      "Epoch: 90/100... Step: 92475... Loss: 0.000125... Val Loss: 0.000826\n",
      "Epoch: 90/100... Step: 92480... Loss: 0.000021... Val Loss: 0.000917\n",
      "Epoch: 90/100... Step: 92485... Loss: 0.000072... Val Loss: 0.000939\n",
      "Epoch: 90/100... Step: 92490... Loss: 0.000161... Val Loss: 0.000862\n",
      "Epoch: 90/100... Step: 92495... Loss: 0.000135... Val Loss: 0.000719\n",
      "Epoch: 90/100... Step: 92500... Loss: 0.000244... Val Loss: 0.000546\n",
      "Epoch: 90/100... Step: 92505... Loss: 0.000355... Val Loss: 0.000510\n",
      "Epoch: 90/100... Step: 92510... Loss: 0.000200... Val Loss: 0.000580\n",
      "Epoch: 90/100... Step: 92515... Loss: 0.000169... Val Loss: 0.000689\n",
      "Epoch: 90/100... Step: 92520... Loss: 0.000361... Val Loss: 0.000702\n",
      "Epoch: 90/100... Step: 92525... Loss: 0.000289... Val Loss: 0.000613\n",
      "Epoch: 90/100... Step: 92530... Loss: 0.000283... Val Loss: 0.000457\n",
      "Epoch: 90/100... Step: 92535... Loss: 0.000111... Val Loss: 0.000302\n",
      "Epoch: 90/100... Step: 92540... Loss: 0.000164... Val Loss: 0.000259\n",
      "Epoch: 90/100... Step: 92545... Loss: 0.000183... Val Loss: 0.000258\n",
      "Epoch: 90/100... Step: 92550... Loss: 0.000111... Val Loss: 0.000273\n",
      "Epoch: 90/100... Step: 92555... Loss: 0.000022... Val Loss: 0.000339\n",
      "Epoch: 90/100... Step: 92560... Loss: 0.000062... Val Loss: 0.000405\n",
      "Epoch: 90/100... Step: 92565... Loss: 0.000043... Val Loss: 0.000404\n",
      "Epoch: 90/100... Step: 92570... Loss: 0.000029... Val Loss: 0.000421\n",
      "Epoch: 90/100... Step: 92575... Loss: 0.000065... Val Loss: 0.000425\n",
      "Epoch: 90/100... Step: 92580... Loss: 0.000060... Val Loss: 0.000380\n",
      "Epoch: 90/100... Step: 92585... Loss: 0.000190... Val Loss: 0.000356\n",
      "Epoch: 90/100... Step: 92590... Loss: 0.000406... Val Loss: 0.000298\n",
      "Epoch: 90/100... Step: 92595... Loss: 0.000331... Val Loss: 0.000258\n",
      "Epoch: 90/100... Step: 92600... Loss: 0.000073... Val Loss: 0.000341\n",
      "Epoch: 90/100... Step: 92605... Loss: 0.000167... Val Loss: 0.000529\n",
      "Epoch: 90/100... Step: 92610... Loss: 0.000221... Val Loss: 0.000505\n",
      "Epoch: 90/100... Step: 92615... Loss: 0.000080... Val Loss: 0.000380\n",
      "Epoch: 90/100... Step: 92620... Loss: 0.000115... Val Loss: 0.000334\n",
      "Epoch: 90/100... Step: 92625... Loss: 0.000114... Val Loss: 0.000383\n",
      "Epoch: 90/100... Step: 92630... Loss: 0.000097... Val Loss: 0.000491\n",
      "Epoch: 90/100... Step: 92635... Loss: 0.000086... Val Loss: 0.000609\n",
      "Epoch: 90/100... Step: 92640... Loss: 0.000233... Val Loss: 0.000638\n",
      "Epoch: 90/100... Step: 92645... Loss: 0.000255... Val Loss: 0.000539\n",
      "Epoch: 90/100... Step: 92650... Loss: 0.000112... Val Loss: 0.000403\n",
      "Epoch: 90/100... Step: 92655... Loss: 0.000415... Val Loss: 0.000345\n",
      "Epoch: 90/100... Step: 92660... Loss: 0.000421... Val Loss: 0.000374\n",
      "Epoch: 90/100... Step: 92665... Loss: 0.000255... Val Loss: 0.000474\n",
      "Epoch: 90/100... Step: 92670... Loss: 0.000049... Val Loss: 0.000637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90/100... Step: 92675... Loss: 0.000126... Val Loss: 0.000741\n",
      "Epoch: 90/100... Step: 92680... Loss: 0.000119... Val Loss: 0.000716\n",
      "Epoch: 90/100... Step: 92685... Loss: 0.000061... Val Loss: 0.000620\n",
      "Epoch: 90/100... Step: 92690... Loss: 0.000059... Val Loss: 0.000537\n",
      "Epoch: 90/100... Step: 92695... Loss: 0.000075... Val Loss: 0.000546\n",
      "Epoch: 90/100... Step: 92700... Loss: 0.000033... Val Loss: 0.000631\n",
      "Epoch: 90/100... Step: 92705... Loss: 0.000038... Val Loss: 0.000715\n",
      "Epoch: 90/100... Step: 92710... Loss: 0.000045... Val Loss: 0.000743\n",
      "Epoch: 90/100... Step: 92715... Loss: 0.000028... Val Loss: 0.000769\n",
      "Epoch: 90/100... Step: 92720... Loss: 0.000027... Val Loss: 0.000765\n",
      "Epoch: 90/100... Step: 92725... Loss: 0.000049... Val Loss: 0.000744\n",
      "Epoch: 90/100... Step: 92730... Loss: 0.000048... Val Loss: 0.000789\n",
      "Epoch: 90/100... Step: 92735... Loss: 0.000140... Val Loss: 0.000826\n",
      "Epoch: 90/100... Step: 92740... Loss: 0.000092... Val Loss: 0.000773\n",
      "Epoch: 90/100... Step: 92745... Loss: 0.000047... Val Loss: 0.000679\n",
      "Epoch: 90/100... Step: 92750... Loss: 0.000054... Val Loss: 0.000632\n",
      "Epoch: 90/100... Step: 92755... Loss: 0.000043... Val Loss: 0.000612\n",
      "Epoch: 90/100... Step: 92760... Loss: 0.000023... Val Loss: 0.000556\n",
      "Epoch: 90/100... Step: 92765... Loss: 0.000097... Val Loss: 0.000542\n",
      "Epoch: 90/100... Step: 92770... Loss: 0.000103... Val Loss: 0.000611\n",
      "Epoch: 90/100... Step: 92775... Loss: 0.000055... Val Loss: 0.000736\n",
      "Epoch: 90/100... Step: 92780... Loss: 0.000050... Val Loss: 0.000858\n",
      "Epoch: 90/100... Step: 92785... Loss: 0.000052... Val Loss: 0.000908\n",
      "Epoch: 90/100... Step: 92790... Loss: 0.000059... Val Loss: 0.000882\n",
      "Epoch: 90/100... Step: 92795... Loss: 0.000055... Val Loss: 0.000802\n",
      "Epoch: 90/100... Step: 92800... Loss: 0.000140... Val Loss: 0.000723\n",
      "Epoch: 90/100... Step: 92805... Loss: 0.000262... Val Loss: 0.000741\n",
      "Epoch: 90/100... Step: 92810... Loss: 0.000167... Val Loss: 0.000842\n",
      "Epoch: 90/100... Step: 92815... Loss: 0.000026... Val Loss: 0.000989\n",
      "Epoch: 90/100... Step: 92820... Loss: 0.000089... Val Loss: 0.001072\n",
      "Epoch: 90/100... Step: 92825... Loss: 0.000107... Val Loss: 0.001044\n",
      "Epoch: 90/100... Step: 92830... Loss: 0.000077... Val Loss: 0.000934\n",
      "Epoch: 90/100... Step: 92835... Loss: 0.000193... Val Loss: 0.000855\n",
      "Epoch: 90/100... Step: 92840... Loss: 0.000197... Val Loss: 0.000881\n",
      "Epoch: 90/100... Step: 92845... Loss: 0.000083... Val Loss: 0.000990\n",
      "Epoch: 90/100... Step: 92850... Loss: 0.000103... Val Loss: 0.001109\n",
      "Epoch: 90/100... Step: 92855... Loss: 0.000241... Val Loss: 0.001113\n",
      "Epoch: 90/100... Step: 92860... Loss: 0.000160... Val Loss: 0.001029\n",
      "Epoch: 90/100... Step: 92865... Loss: 0.000081... Val Loss: 0.000915\n",
      "Epoch: 90/100... Step: 92870... Loss: 0.000160... Val Loss: 0.000900\n",
      "Epoch: 90/100... Step: 92875... Loss: 0.000053... Val Loss: 0.000970\n",
      "Epoch: 90/100... Step: 92880... Loss: 0.000083... Val Loss: 0.001055\n",
      "Epoch: 91/100... Step: 92885... Loss: 0.000252... Val Loss: 0.001152\n",
      "Epoch: 91/100... Step: 92890... Loss: 0.000215... Val Loss: 0.001309\n",
      "Epoch: 91/100... Step: 92895... Loss: 0.000133... Val Loss: 0.001523\n",
      "Epoch: 91/100... Step: 92900... Loss: 0.000131... Val Loss: 0.001771\n",
      "Epoch: 91/100... Step: 92905... Loss: 0.000232... Val Loss: 0.001944\n",
      "Epoch: 91/100... Step: 92910... Loss: 0.000130... Val Loss: 0.001949\n",
      "Epoch: 91/100... Step: 92915... Loss: 0.000099... Val Loss: 0.001861\n",
      "Epoch: 91/100... Step: 92920... Loss: 0.000154... Val Loss: 0.001760\n",
      "Epoch: 91/100... Step: 92925... Loss: 0.000114... Val Loss: 0.001668\n",
      "Epoch: 91/100... Step: 92930... Loss: 0.000087... Val Loss: 0.001673\n",
      "Epoch: 91/100... Step: 92935... Loss: 0.000057... Val Loss: 0.001737\n",
      "Epoch: 91/100... Step: 92940... Loss: 0.000118... Val Loss: 0.001765\n",
      "Epoch: 91/100... Step: 92945... Loss: 0.000193... Val Loss: 0.001723\n",
      "Epoch: 91/100... Step: 92950... Loss: 0.000297... Val Loss: 0.001599\n",
      "Epoch: 91/100... Step: 92955... Loss: 0.000307... Val Loss: 0.001420\n",
      "Epoch: 91/100... Step: 92960... Loss: 0.000098... Val Loss: 0.001207\n",
      "Epoch: 91/100... Step: 92965... Loss: 0.000245... Val Loss: 0.001113\n",
      "Epoch: 91/100... Step: 92970... Loss: 0.000174... Val Loss: 0.001156\n",
      "Epoch: 91/100... Step: 92975... Loss: 0.000126... Val Loss: 0.001275\n",
      "Epoch: 91/100... Step: 92980... Loss: 0.000287... Val Loss: 0.001330\n",
      "Epoch: 91/100... Step: 92985... Loss: 0.000167... Val Loss: 0.001272\n",
      "Epoch: 91/100... Step: 92990... Loss: 0.000032... Val Loss: 0.001139\n",
      "Epoch: 91/100... Step: 92995... Loss: 0.000096... Val Loss: 0.001060\n",
      "Epoch: 91/100... Step: 93000... Loss: 0.000094... Val Loss: 0.001086\n",
      "Epoch: 91/100... Step: 93005... Loss: 0.000119... Val Loss: 0.001167\n",
      "Epoch: 91/100... Step: 93010... Loss: 0.000089... Val Loss: 0.001298\n",
      "Epoch: 91/100... Step: 93015... Loss: 0.000027... Val Loss: 0.001449\n",
      "Epoch: 91/100... Step: 93020... Loss: 0.000104... Val Loss: 0.001511\n",
      "Epoch: 91/100... Step: 93025... Loss: 0.000146... Val Loss: 0.001545\n",
      "Epoch: 91/100... Step: 93030... Loss: 0.000062... Val Loss: 0.001653\n",
      "Epoch: 91/100... Step: 93035... Loss: 0.000151... Val Loss: 0.001746\n",
      "Epoch: 91/100... Step: 93040... Loss: 0.000241... Val Loss: 0.001710\n",
      "Epoch: 91/100... Step: 93045... Loss: 0.000202... Val Loss: 0.001590\n",
      "Epoch: 91/100... Step: 93050... Loss: 0.000103... Val Loss: 0.001420\n",
      "Epoch: 91/100... Step: 93055... Loss: 0.000073... Val Loss: 0.001267\n",
      "Epoch: 91/100... Step: 93060... Loss: 0.000095... Val Loss: 0.001233\n",
      "Epoch: 91/100... Step: 93065... Loss: 0.000080... Val Loss: 0.001295\n",
      "Epoch: 91/100... Step: 93070... Loss: 0.000188... Val Loss: 0.001319\n",
      "Epoch: 91/100... Step: 93075... Loss: 0.000097... Val Loss: 0.001237\n",
      "Epoch: 91/100... Step: 93080... Loss: 0.000030... Val Loss: 0.001102\n",
      "Epoch: 91/100... Step: 93085... Loss: 0.000123... Val Loss: 0.001070\n",
      "Epoch: 91/100... Step: 93090... Loss: 0.000134... Val Loss: 0.001140\n",
      "Epoch: 91/100... Step: 93095... Loss: 0.000061... Val Loss: 0.001272\n",
      "Epoch: 91/100... Step: 93100... Loss: 0.000113... Val Loss: 0.001366\n",
      "Epoch: 91/100... Step: 93105... Loss: 0.000146... Val Loss: 0.001356\n",
      "Epoch: 91/100... Step: 93110... Loss: 0.000156... Val Loss: 0.001274\n",
      "Epoch: 91/100... Step: 93115... Loss: 0.000111... Val Loss: 0.001152\n",
      "Epoch: 91/100... Step: 93120... Loss: 0.000128... Val Loss: 0.001020\n",
      "Epoch: 91/100... Step: 93125... Loss: 0.000145... Val Loss: 0.000997\n",
      "Epoch: 91/100... Step: 93130... Loss: 0.000088... Val Loss: 0.001066\n",
      "Epoch: 91/100... Step: 93135... Loss: 0.000040... Val Loss: 0.001187\n",
      "Epoch: 91/100... Step: 93140... Loss: 0.000225... Val Loss: 0.001246\n",
      "Epoch: 91/100... Step: 93145... Loss: 0.000300... Val Loss: 0.001208\n",
      "Epoch: 91/100... Step: 93150... Loss: 0.000221... Val Loss: 0.001100\n",
      "Epoch: 91/100... Step: 93155... Loss: 0.000101... Val Loss: 0.000941\n",
      "Epoch: 91/100... Step: 93160... Loss: 0.000111... Val Loss: 0.000801\n",
      "Epoch: 91/100... Step: 93165... Loss: 0.000161... Val Loss: 0.000774\n",
      "Epoch: 91/100... Step: 93170... Loss: 0.000116... Val Loss: 0.000808\n",
      "Epoch: 91/100... Step: 93175... Loss: 0.000194... Val Loss: 0.000794\n",
      "Epoch: 91/100... Step: 93180... Loss: 0.000184... Val Loss: 0.000703\n",
      "Epoch: 91/100... Step: 93185... Loss: 0.000126... Val Loss: 0.000582\n",
      "Epoch: 91/100... Step: 93190... Loss: 0.000116... Val Loss: 0.000518\n",
      "Epoch: 91/100... Step: 93195... Loss: 0.000083... Val Loss: 0.000516\n",
      "Epoch: 91/100... Step: 93200... Loss: 0.000072... Val Loss: 0.000587\n",
      "Epoch: 91/100... Step: 93205... Loss: 0.000132... Val Loss: 0.000680\n",
      "Epoch: 91/100... Step: 93210... Loss: 0.000293... Val Loss: 0.000692\n",
      "Epoch: 91/100... Step: 93215... Loss: 0.000197... Val Loss: 0.000632\n",
      "Epoch: 91/100... Step: 93220... Loss: 0.000096... Val Loss: 0.000544\n",
      "Epoch: 91/100... Step: 93225... Loss: 0.000188... Val Loss: 0.000509\n",
      "Epoch: 91/100... Step: 93230... Loss: 0.000171... Val Loss: 0.000467\n",
      "Epoch: 91/100... Step: 93235... Loss: 0.000224... Val Loss: 0.000399\n",
      "Epoch: 91/100... Step: 93240... Loss: 0.000436... Val Loss: 0.000380\n",
      "Epoch: 91/100... Step: 93245... Loss: 0.000580... Val Loss: 0.000427\n",
      "Epoch: 91/100... Step: 93250... Loss: 0.000540... Val Loss: 0.000526\n",
      "Epoch: 91/100... Step: 93255... Loss: 0.000376... Val Loss: 0.000672\n",
      "Epoch: 91/100... Step: 93260... Loss: 0.000268... Val Loss: 0.000856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91/100... Step: 93265... Loss: 0.000072... Val Loss: 0.001085\n",
      "Epoch: 91/100... Step: 93270... Loss: 0.000242... Val Loss: 0.001225\n",
      "Epoch: 91/100... Step: 93275... Loss: 0.000193... Val Loss: 0.001219\n",
      "Epoch: 91/100... Step: 93280... Loss: 0.000054... Val Loss: 0.001116\n",
      "Epoch: 91/100... Step: 93285... Loss: 0.000305... Val Loss: 0.001012\n",
      "Epoch: 91/100... Step: 93290... Loss: 0.000553... Val Loss: 0.000905\n",
      "Epoch: 91/100... Step: 93295... Loss: 0.001279... Val Loss: 0.000721\n",
      "Epoch: 91/100... Step: 93300... Loss: 0.001569... Val Loss: 0.000445\n",
      "Epoch: 91/100... Step: 93305... Loss: 0.001035... Val Loss: 0.000262\n",
      "Epoch: 91/100... Step: 93310... Loss: 0.000335... Val Loss: 0.000863\n",
      "Epoch: 91/100... Step: 93315... Loss: 0.000314... Val Loss: 0.001645\n",
      "Epoch: 91/100... Step: 93320... Loss: 0.000749... Val Loss: 0.001742\n",
      "Epoch: 91/100... Step: 93325... Loss: 0.000387... Val Loss: 0.001346\n",
      "Epoch: 91/100... Step: 93330... Loss: 0.000293... Val Loss: 0.000944\n",
      "Epoch: 91/100... Step: 93335... Loss: 0.000467... Val Loss: 0.000907\n",
      "Epoch: 91/100... Step: 93340... Loss: 0.000262... Val Loss: 0.001125\n",
      "Epoch: 91/100... Step: 93345... Loss: 0.000186... Val Loss: 0.001515\n",
      "Epoch: 91/100... Step: 93350... Loss: 0.000473... Val Loss: 0.001764\n",
      "Epoch: 91/100... Step: 93355... Loss: 0.000586... Val Loss: 0.001659\n",
      "Epoch: 91/100... Step: 93360... Loss: 0.000204... Val Loss: 0.001351\n",
      "Epoch: 91/100... Step: 93365... Loss: 0.000207... Val Loss: 0.001041\n",
      "Epoch: 91/100... Step: 93370... Loss: 0.000126... Val Loss: 0.000830\n",
      "Epoch: 91/100... Step: 93375... Loss: 0.000155... Val Loss: 0.000727\n",
      "Epoch: 91/100... Step: 93380... Loss: 0.000070... Val Loss: 0.000717\n",
      "Epoch: 91/100... Step: 93385... Loss: 0.000281... Val Loss: 0.000691\n",
      "Epoch: 91/100... Step: 93390... Loss: 0.000322... Val Loss: 0.000573\n",
      "Epoch: 91/100... Step: 93395... Loss: 0.000107... Val Loss: 0.000373\n",
      "Epoch: 91/100... Step: 93400... Loss: 0.000276... Val Loss: 0.000291\n",
      "Epoch: 91/100... Step: 93405... Loss: 0.000363... Val Loss: 0.000301\n",
      "Epoch: 91/100... Step: 93410... Loss: 0.000316... Val Loss: 0.000393\n",
      "Epoch: 91/100... Step: 93415... Loss: 0.000110... Val Loss: 0.000595\n",
      "Epoch: 91/100... Step: 93420... Loss: 0.000103... Val Loss: 0.000764\n",
      "Epoch: 91/100... Step: 93425... Loss: 0.000084... Val Loss: 0.000845\n",
      "Epoch: 91/100... Step: 93430... Loss: 0.000085... Val Loss: 0.000911\n",
      "Epoch: 91/100... Step: 93435... Loss: 0.000190... Val Loss: 0.000928\n",
      "Epoch: 91/100... Step: 93440... Loss: 0.000189... Val Loss: 0.000840\n",
      "Epoch: 91/100... Step: 93445... Loss: 0.000085... Val Loss: 0.000695\n",
      "Epoch: 91/100... Step: 93450... Loss: 0.000128... Val Loss: 0.000671\n",
      "Epoch: 91/100... Step: 93455... Loss: 0.000150... Val Loss: 0.000760\n",
      "Epoch: 91/100... Step: 93460... Loss: 0.000130... Val Loss: 0.000897\n",
      "Epoch: 91/100... Step: 93465... Loss: 0.000086... Val Loss: 0.000952\n",
      "Epoch: 91/100... Step: 93470... Loss: 0.000062... Val Loss: 0.000906\n",
      "Epoch: 91/100... Step: 93475... Loss: 0.000115... Val Loss: 0.000858\n",
      "Epoch: 91/100... Step: 93480... Loss: 0.000103... Val Loss: 0.000863\n",
      "Epoch: 91/100... Step: 93485... Loss: 0.000059... Val Loss: 0.000941\n",
      "Epoch: 91/100... Step: 93490... Loss: 0.000104... Val Loss: 0.001010\n",
      "Epoch: 91/100... Step: 93495... Loss: 0.000145... Val Loss: 0.000977\n",
      "Epoch: 91/100... Step: 93500... Loss: 0.000063... Val Loss: 0.000874\n",
      "Epoch: 91/100... Step: 93505... Loss: 0.000138... Val Loss: 0.000823\n",
      "Epoch: 91/100... Step: 93510... Loss: 0.000063... Val Loss: 0.000875\n",
      "Epoch: 91/100... Step: 93515... Loss: 0.000092... Val Loss: 0.000919\n",
      "Epoch: 91/100... Step: 93520... Loss: 0.000148... Val Loss: 0.000899\n",
      "Epoch: 91/100... Step: 93525... Loss: 0.000196... Val Loss: 0.000794\n",
      "Epoch: 91/100... Step: 93530... Loss: 0.000162... Val Loss: 0.000636\n",
      "Epoch: 91/100... Step: 93535... Loss: 0.000345... Val Loss: 0.000544\n",
      "Epoch: 91/100... Step: 93540... Loss: 0.000268... Val Loss: 0.000577\n",
      "Epoch: 91/100... Step: 93545... Loss: 0.000124... Val Loss: 0.000686\n",
      "Epoch: 91/100... Step: 93550... Loss: 0.000289... Val Loss: 0.000728\n",
      "Epoch: 91/100... Step: 93555... Loss: 0.000306... Val Loss: 0.000651\n",
      "Epoch: 91/100... Step: 93560... Loss: 0.000267... Val Loss: 0.000495\n",
      "Epoch: 91/100... Step: 93565... Loss: 0.000167... Val Loss: 0.000327\n",
      "Epoch: 91/100... Step: 93570... Loss: 0.000153... Val Loss: 0.000259\n",
      "Epoch: 91/100... Step: 93575... Loss: 0.000202... Val Loss: 0.000257\n",
      "Epoch: 91/100... Step: 93580... Loss: 0.000187... Val Loss: 0.000259\n",
      "Epoch: 91/100... Step: 93585... Loss: 0.000072... Val Loss: 0.000287\n",
      "Epoch: 91/100... Step: 93590... Loss: 0.000051... Val Loss: 0.000363\n",
      "Epoch: 91/100... Step: 93595... Loss: 0.000050... Val Loss: 0.000435\n",
      "Epoch: 91/100... Step: 93600... Loss: 0.000042... Val Loss: 0.000465\n",
      "Epoch: 91/100... Step: 93605... Loss: 0.000072... Val Loss: 0.000454\n",
      "Epoch: 91/100... Step: 93610... Loss: 0.000054... Val Loss: 0.000392\n",
      "Epoch: 91/100... Step: 93615... Loss: 0.000094... Val Loss: 0.000350\n",
      "Epoch: 91/100... Step: 93620... Loss: 0.000315... Val Loss: 0.000310\n",
      "Epoch: 91/100... Step: 93625... Loss: 0.000400... Val Loss: 0.000264\n",
      "Epoch: 91/100... Step: 93630... Loss: 0.000258... Val Loss: 0.000271\n",
      "Epoch: 91/100... Step: 93635... Loss: 0.000041... Val Loss: 0.000391\n",
      "Epoch: 91/100... Step: 93640... Loss: 0.000175... Val Loss: 0.000513\n",
      "Epoch: 91/100... Step: 93645... Loss: 0.000232... Val Loss: 0.000509\n",
      "Epoch: 91/100... Step: 93650... Loss: 0.000033... Val Loss: 0.000425\n",
      "Epoch: 91/100... Step: 93655... Loss: 0.000047... Val Loss: 0.000375\n",
      "Epoch: 91/100... Step: 93660... Loss: 0.000145... Val Loss: 0.000396\n",
      "Epoch: 91/100... Step: 93665... Loss: 0.000124... Val Loss: 0.000471\n",
      "Epoch: 91/100... Step: 93670... Loss: 0.000090... Val Loss: 0.000581\n",
      "Epoch: 91/100... Step: 93675... Loss: 0.000292... Val Loss: 0.000620\n",
      "Epoch: 91/100... Step: 93680... Loss: 0.000232... Val Loss: 0.000575\n",
      "Epoch: 91/100... Step: 93685... Loss: 0.000151... Val Loss: 0.000506\n",
      "Epoch: 91/100... Step: 93690... Loss: 0.000279... Val Loss: 0.000500\n",
      "Epoch: 91/100... Step: 93695... Loss: 0.000157... Val Loss: 0.000559\n",
      "Epoch: 91/100... Step: 93700... Loss: 0.000038... Val Loss: 0.000665\n",
      "Epoch: 91/100... Step: 93705... Loss: 0.000114... Val Loss: 0.000717\n",
      "Epoch: 91/100... Step: 93710... Loss: 0.000090... Val Loss: 0.000686\n",
      "Epoch: 91/100... Step: 93715... Loss: 0.000049... Val Loss: 0.000613\n",
      "Epoch: 91/100... Step: 93720... Loss: 0.000037... Val Loss: 0.000553\n",
      "Epoch: 91/100... Step: 93725... Loss: 0.000062... Val Loss: 0.000547\n",
      "Epoch: 91/100... Step: 93730... Loss: 0.000044... Val Loss: 0.000599\n",
      "Epoch: 91/100... Step: 93735... Loss: 0.000040... Val Loss: 0.000674\n",
      "Epoch: 91/100... Step: 93740... Loss: 0.000035... Val Loss: 0.000731\n",
      "Epoch: 91/100... Step: 93745... Loss: 0.000023... Val Loss: 0.000770\n",
      "Epoch: 91/100... Step: 93750... Loss: 0.000033... Val Loss: 0.000781\n",
      "Epoch: 91/100... Step: 93755... Loss: 0.000029... Val Loss: 0.000750\n",
      "Epoch: 91/100... Step: 93760... Loss: 0.000069... Val Loss: 0.000750\n",
      "Epoch: 91/100... Step: 93765... Loss: 0.000066... Val Loss: 0.000796\n",
      "Epoch: 91/100... Step: 93770... Loss: 0.000126... Val Loss: 0.000797\n",
      "Epoch: 91/100... Step: 93775... Loss: 0.000060... Val Loss: 0.000739\n",
      "Epoch: 91/100... Step: 93780... Loss: 0.000021... Val Loss: 0.000666\n",
      "Epoch: 91/100... Step: 93785... Loss: 0.000054... Val Loss: 0.000620\n",
      "Epoch: 91/100... Step: 93790... Loss: 0.000025... Val Loss: 0.000571\n",
      "Epoch: 91/100... Step: 93795... Loss: 0.000057... Val Loss: 0.000538\n",
      "Epoch: 91/100... Step: 93800... Loss: 0.000128... Val Loss: 0.000568\n",
      "Epoch: 91/100... Step: 93805... Loss: 0.000096... Val Loss: 0.000654\n",
      "Epoch: 91/100... Step: 93810... Loss: 0.000045... Val Loss: 0.000773\n",
      "Epoch: 91/100... Step: 93815... Loss: 0.000033... Val Loss: 0.000884\n",
      "Epoch: 91/100... Step: 93820... Loss: 0.000068... Val Loss: 0.000938\n",
      "Epoch: 91/100... Step: 93825... Loss: 0.000145... Val Loss: 0.000924\n",
      "Epoch: 91/100... Step: 93830... Loss: 0.000090... Val Loss: 0.000848\n",
      "Epoch: 91/100... Step: 93835... Loss: 0.000155... Val Loss: 0.000782\n",
      "Epoch: 91/100... Step: 93840... Loss: 0.000199... Val Loss: 0.000796\n",
      "Epoch: 91/100... Step: 93845... Loss: 0.000117... Val Loss: 0.000873\n",
      "Epoch: 91/100... Step: 93850... Loss: 0.000024... Val Loss: 0.000994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91/100... Step: 93855... Loss: 0.000100... Val Loss: 0.001076\n",
      "Epoch: 91/100... Step: 93860... Loss: 0.000159... Val Loss: 0.001066\n",
      "Epoch: 91/100... Step: 93865... Loss: 0.000086... Val Loss: 0.001012\n",
      "Epoch: 91/100... Step: 93870... Loss: 0.000059... Val Loss: 0.001002\n",
      "Epoch: 91/100... Step: 93875... Loss: 0.000027... Val Loss: 0.001054\n",
      "Epoch: 91/100... Step: 93880... Loss: 0.000077... Val Loss: 0.001086\n",
      "Epoch: 91/100... Step: 93885... Loss: 0.000136... Val Loss: 0.001045\n",
      "Epoch: 91/100... Step: 93890... Loss: 0.000101... Val Loss: 0.000948\n",
      "Epoch: 91/100... Step: 93895... Loss: 0.000101... Val Loss: 0.000855\n",
      "Epoch: 91/100... Step: 93900... Loss: 0.000223... Val Loss: 0.000842\n",
      "Epoch: 91/100... Step: 93905... Loss: 0.000162... Val Loss: 0.000899\n",
      "Epoch: 91/100... Step: 93910... Loss: 0.000048... Val Loss: 0.001003\n",
      "Epoch: 92/100... Step: 93915... Loss: 0.000244... Val Loss: 0.001106\n",
      "Epoch: 92/100... Step: 93920... Loss: 0.000250... Val Loss: 0.001250\n",
      "Epoch: 92/100... Step: 93925... Loss: 0.000153... Val Loss: 0.001435\n",
      "Epoch: 92/100... Step: 93930... Loss: 0.000171... Val Loss: 0.001654\n",
      "Epoch: 92/100... Step: 93935... Loss: 0.000185... Val Loss: 0.001862\n",
      "Epoch: 92/100... Step: 93940... Loss: 0.000188... Val Loss: 0.001942\n",
      "Epoch: 92/100... Step: 93945... Loss: 0.000050... Val Loss: 0.001903\n",
      "Epoch: 92/100... Step: 93950... Loss: 0.000184... Val Loss: 0.001826\n",
      "Epoch: 92/100... Step: 93955... Loss: 0.000136... Val Loss: 0.001735\n",
      "Epoch: 92/100... Step: 93960... Loss: 0.000104... Val Loss: 0.001684\n",
      "Epoch: 92/100... Step: 93965... Loss: 0.000053... Val Loss: 0.001703\n",
      "Epoch: 92/100... Step: 93970... Loss: 0.000081... Val Loss: 0.001730\n",
      "Epoch: 92/100... Step: 93975... Loss: 0.000141... Val Loss: 0.001714\n",
      "Epoch: 92/100... Step: 93980... Loss: 0.000210... Val Loss: 0.001633\n",
      "Epoch: 92/100... Step: 93985... Loss: 0.000353... Val Loss: 0.001498\n",
      "Epoch: 92/100... Step: 93990... Loss: 0.000133... Val Loss: 0.001317\n",
      "Epoch: 92/100... Step: 93995... Loss: 0.000181... Val Loss: 0.001169\n",
      "Epoch: 92/100... Step: 94000... Loss: 0.000194... Val Loss: 0.001151\n",
      "Epoch: 92/100... Step: 94005... Loss: 0.000093... Val Loss: 0.001226\n",
      "Epoch: 92/100... Step: 94010... Loss: 0.000218... Val Loss: 0.001301\n",
      "Epoch: 92/100... Step: 94015... Loss: 0.000203... Val Loss: 0.001294\n",
      "Epoch: 92/100... Step: 94020... Loss: 0.000095... Val Loss: 0.001210\n",
      "Epoch: 92/100... Step: 94025... Loss: 0.000050... Val Loss: 0.001097\n",
      "Epoch: 92/100... Step: 94030... Loss: 0.000076... Val Loss: 0.001074\n",
      "Epoch: 92/100... Step: 94035... Loss: 0.000092... Val Loss: 0.001117\n",
      "Epoch: 92/100... Step: 94040... Loss: 0.000130... Val Loss: 0.001206\n",
      "Epoch: 92/100... Step: 94045... Loss: 0.000078... Val Loss: 0.001340\n",
      "Epoch: 92/100... Step: 94050... Loss: 0.000049... Val Loss: 0.001488\n",
      "Epoch: 92/100... Step: 94055... Loss: 0.000123... Val Loss: 0.001574\n",
      "Epoch: 92/100... Step: 94060... Loss: 0.000058... Val Loss: 0.001668\n",
      "Epoch: 92/100... Step: 94065... Loss: 0.000102... Val Loss: 0.001754\n",
      "Epoch: 92/100... Step: 94070... Loss: 0.000243... Val Loss: 0.001743\n",
      "Epoch: 92/100... Step: 94075... Loss: 0.000237... Val Loss: 0.001657\n",
      "Epoch: 92/100... Step: 94080... Loss: 0.000197... Val Loss: 0.001524\n",
      "Epoch: 92/100... Step: 94085... Loss: 0.000057... Val Loss: 0.001360\n",
      "Epoch: 92/100... Step: 94090... Loss: 0.000088... Val Loss: 0.001243\n",
      "Epoch: 92/100... Step: 94095... Loss: 0.000089... Val Loss: 0.001241\n",
      "Epoch: 92/100... Step: 94100... Loss: 0.000107... Val Loss: 0.001281\n",
      "Epoch: 92/100... Step: 94105... Loss: 0.000111... Val Loss: 0.001247\n",
      "Epoch: 92/100... Step: 94110... Loss: 0.000018... Val Loss: 0.001141\n",
      "Epoch: 92/100... Step: 94115... Loss: 0.000085... Val Loss: 0.001094\n",
      "Epoch: 92/100... Step: 94120... Loss: 0.000112... Val Loss: 0.001144\n",
      "Epoch: 92/100... Step: 94125... Loss: 0.000062... Val Loss: 0.001258\n",
      "Epoch: 92/100... Step: 94130... Loss: 0.000099... Val Loss: 0.001367\n",
      "Epoch: 92/100... Step: 94135... Loss: 0.000172... Val Loss: 0.001362\n",
      "Epoch: 92/100... Step: 94140... Loss: 0.000133... Val Loss: 0.001278\n",
      "Epoch: 92/100... Step: 94145... Loss: 0.000137... Val Loss: 0.001166\n",
      "Epoch: 92/100... Step: 94150... Loss: 0.000077... Val Loss: 0.001046\n",
      "Epoch: 92/100... Step: 94155... Loss: 0.000169... Val Loss: 0.000998\n",
      "Epoch: 92/100... Step: 94160... Loss: 0.000111... Val Loss: 0.001035\n",
      "Epoch: 92/100... Step: 94165... Loss: 0.000081... Val Loss: 0.001126\n",
      "Epoch: 92/100... Step: 94170... Loss: 0.000126... Val Loss: 0.001229\n",
      "Epoch: 92/100... Step: 94175... Loss: 0.000301... Val Loss: 0.001243\n",
      "Epoch: 92/100... Step: 94180... Loss: 0.000293... Val Loss: 0.001182\n",
      "Epoch: 92/100... Step: 94185... Loss: 0.000208... Val Loss: 0.001070\n",
      "Epoch: 92/100... Step: 94190... Loss: 0.000061... Val Loss: 0.000925\n",
      "Epoch: 92/100... Step: 94195... Loss: 0.000115... Val Loss: 0.000821\n",
      "Epoch: 92/100... Step: 94200... Loss: 0.000129... Val Loss: 0.000805\n",
      "Epoch: 92/100... Step: 94205... Loss: 0.000169... Val Loss: 0.000792\n",
      "Epoch: 92/100... Step: 94210... Loss: 0.000176... Val Loss: 0.000718\n",
      "Epoch: 92/100... Step: 94215... Loss: 0.000114... Val Loss: 0.000595\n",
      "Epoch: 92/100... Step: 94220... Loss: 0.000177... Val Loss: 0.000506\n",
      "Epoch: 92/100... Step: 94225... Loss: 0.000071... Val Loss: 0.000498\n",
      "Epoch: 92/100... Step: 94230... Loss: 0.000080... Val Loss: 0.000560\n",
      "Epoch: 92/100... Step: 94235... Loss: 0.000058... Val Loss: 0.000660\n",
      "Epoch: 92/100... Step: 94240... Loss: 0.000261... Val Loss: 0.000705\n",
      "Epoch: 92/100... Step: 94245... Loss: 0.000279... Val Loss: 0.000675\n",
      "Epoch: 92/100... Step: 94250... Loss: 0.000104... Val Loss: 0.000596\n",
      "Epoch: 92/100... Step: 94255... Loss: 0.000150... Val Loss: 0.000539\n",
      "Epoch: 92/100... Step: 94260... Loss: 0.000203... Val Loss: 0.000508\n",
      "Epoch: 92/100... Step: 94265... Loss: 0.000185... Val Loss: 0.000445\n",
      "Epoch: 92/100... Step: 94270... Loss: 0.000348... Val Loss: 0.000401\n",
      "Epoch: 92/100... Step: 94275... Loss: 0.000497... Val Loss: 0.000420\n",
      "Epoch: 92/100... Step: 94280... Loss: 0.000578... Val Loss: 0.000491\n",
      "Epoch: 92/100... Step: 94285... Loss: 0.000442... Val Loss: 0.000602\n",
      "Epoch: 92/100... Step: 94290... Loss: 0.000339... Val Loss: 0.000741\n",
      "Epoch: 92/100... Step: 94295... Loss: 0.000201... Val Loss: 0.000907\n",
      "Epoch: 92/100... Step: 94300... Loss: 0.000087... Val Loss: 0.001087\n",
      "Epoch: 92/100... Step: 94305... Loss: 0.000151... Val Loss: 0.001158\n",
      "Epoch: 92/100... Step: 94310... Loss: 0.000073... Val Loss: 0.001126\n",
      "Epoch: 92/100... Step: 94315... Loss: 0.000142... Val Loss: 0.001062\n",
      "Epoch: 92/100... Step: 94320... Loss: 0.000523... Val Loss: 0.001012\n",
      "Epoch: 92/100... Step: 94325... Loss: 0.001080... Val Loss: 0.000903\n",
      "Epoch: 92/100... Step: 94330... Loss: 0.001802... Val Loss: 0.000724\n",
      "Epoch: 92/100... Step: 94335... Loss: 0.001567... Val Loss: 0.000463\n",
      "Epoch: 92/100... Step: 94340... Loss: 0.001091... Val Loss: 0.000258\n",
      "Epoch: 92/100... Step: 94345... Loss: 0.000570... Val Loss: 0.000706\n",
      "Epoch: 92/100... Step: 94350... Loss: 0.000228... Val Loss: 0.001524\n",
      "Epoch: 92/100... Step: 94355... Loss: 0.000743... Val Loss: 0.001811\n",
      "Epoch: 92/100... Step: 94360... Loss: 0.000381... Val Loss: 0.001575\n",
      "Epoch: 92/100... Step: 94365... Loss: 0.000225... Val Loss: 0.001160\n",
      "Epoch: 92/100... Step: 94370... Loss: 0.000355... Val Loss: 0.001024\n",
      "Epoch: 92/100... Step: 94375... Loss: 0.000293... Val Loss: 0.001165\n",
      "Epoch: 92/100... Step: 94380... Loss: 0.000171... Val Loss: 0.001479\n",
      "Epoch: 92/100... Step: 94385... Loss: 0.000389... Val Loss: 0.001602\n",
      "Epoch: 92/100... Step: 94390... Loss: 0.000294... Val Loss: 0.001431\n",
      "Epoch: 92/100... Step: 94395... Loss: 0.000144... Val Loss: 0.001131\n",
      "Epoch: 92/100... Step: 94400... Loss: 0.000166... Val Loss: 0.000868\n",
      "Epoch: 92/100... Step: 94405... Loss: 0.000144... Val Loss: 0.000739\n",
      "Epoch: 92/100... Step: 94410... Loss: 0.000134... Val Loss: 0.000728\n",
      "Epoch: 92/100... Step: 94415... Loss: 0.000152... Val Loss: 0.000694\n",
      "Epoch: 92/100... Step: 94420... Loss: 0.000325... Val Loss: 0.000580\n",
      "Epoch: 92/100... Step: 94425... Loss: 0.000212... Val Loss: 0.000422\n",
      "Epoch: 92/100... Step: 94430... Loss: 0.000139... Val Loss: 0.000299\n",
      "Epoch: 92/100... Step: 94435... Loss: 0.000354... Val Loss: 0.000281\n",
      "Epoch: 92/100... Step: 94440... Loss: 0.000441... Val Loss: 0.000315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92/100... Step: 94445... Loss: 0.000249... Val Loss: 0.000445\n",
      "Epoch: 92/100... Step: 94450... Loss: 0.000092... Val Loss: 0.000668\n",
      "Epoch: 92/100... Step: 94455... Loss: 0.000109... Val Loss: 0.000810\n",
      "Epoch: 92/100... Step: 94460... Loss: 0.000094... Val Loss: 0.000914\n",
      "Epoch: 92/100... Step: 94465... Loss: 0.000166... Val Loss: 0.000961\n",
      "Epoch: 92/100... Step: 94470... Loss: 0.000176... Val Loss: 0.000890\n",
      "Epoch: 92/100... Step: 94475... Loss: 0.000082... Val Loss: 0.000727\n",
      "Epoch: 92/100... Step: 94480... Loss: 0.000209... Val Loss: 0.000607\n",
      "Epoch: 92/100... Step: 94485... Loss: 0.000307... Val Loss: 0.000631\n",
      "Epoch: 92/100... Step: 94490... Loss: 0.000174... Val Loss: 0.000750\n",
      "Epoch: 92/100... Step: 94495... Loss: 0.000085... Val Loss: 0.000888\n",
      "Epoch: 92/100... Step: 94500... Loss: 0.000149... Val Loss: 0.000936\n",
      "Epoch: 92/100... Step: 94505... Loss: 0.000082... Val Loss: 0.000892\n",
      "Epoch: 92/100... Step: 94510... Loss: 0.000104... Val Loss: 0.000854\n",
      "Epoch: 92/100... Step: 94515... Loss: 0.000116... Val Loss: 0.000873\n",
      "Epoch: 92/100... Step: 94520... Loss: 0.000093... Val Loss: 0.000966\n",
      "Epoch: 92/100... Step: 94525... Loss: 0.000170... Val Loss: 0.001014\n",
      "Epoch: 92/100... Step: 94530... Loss: 0.000121... Val Loss: 0.000965\n",
      "Epoch: 92/100... Step: 94535... Loss: 0.000081... Val Loss: 0.000861\n",
      "Epoch: 92/100... Step: 94540... Loss: 0.000121... Val Loss: 0.000817\n",
      "Epoch: 92/100... Step: 94545... Loss: 0.000088... Val Loss: 0.000854\n",
      "Epoch: 92/100... Step: 94550... Loss: 0.000067... Val Loss: 0.000902\n",
      "Epoch: 92/100... Step: 94555... Loss: 0.000168... Val Loss: 0.000870\n",
      "Epoch: 92/100... Step: 94560... Loss: 0.000188... Val Loss: 0.000722\n",
      "Epoch: 92/100... Step: 94565... Loss: 0.000230... Val Loss: 0.000644\n",
      "Epoch: 92/100... Step: 94570... Loss: 0.000246... Val Loss: 0.000648\n",
      "Epoch: 92/100... Step: 94575... Loss: 0.000093... Val Loss: 0.000727\n",
      "Epoch: 92/100... Step: 94580... Loss: 0.000276... Val Loss: 0.000750\n",
      "Epoch: 92/100... Step: 94585... Loss: 0.000319... Val Loss: 0.000671\n",
      "Epoch: 92/100... Step: 94590... Loss: 0.000210... Val Loss: 0.000519\n",
      "Epoch: 92/100... Step: 94595... Loss: 0.000202... Val Loss: 0.000344\n",
      "Epoch: 92/100... Step: 94600... Loss: 0.000102... Val Loss: 0.000263\n",
      "Epoch: 92/100... Step: 94605... Loss: 0.000225... Val Loss: 0.000259\n",
      "Epoch: 92/100... Step: 94610... Loss: 0.000233... Val Loss: 0.000259\n",
      "Epoch: 92/100... Step: 94615... Loss: 0.000146... Val Loss: 0.000263\n",
      "Epoch: 92/100... Step: 94620... Loss: 0.000063... Val Loss: 0.000307\n",
      "Epoch: 92/100... Step: 94625... Loss: 0.000049... Val Loss: 0.000400\n",
      "Epoch: 92/100... Step: 94630... Loss: 0.000070... Val Loss: 0.000467\n",
      "Epoch: 92/100... Step: 94635... Loss: 0.000102... Val Loss: 0.000471\n",
      "Epoch: 92/100... Step: 94640... Loss: 0.000064... Val Loss: 0.000411\n",
      "Epoch: 92/100... Step: 94645... Loss: 0.000078... Val Loss: 0.000348\n",
      "Epoch: 92/100... Step: 94650... Loss: 0.000219... Val Loss: 0.000314\n",
      "Epoch: 92/100... Step: 94655... Loss: 0.000407... Val Loss: 0.000278\n",
      "Epoch: 92/100... Step: 94660... Loss: 0.000320... Val Loss: 0.000256\n",
      "Epoch: 92/100... Step: 94665... Loss: 0.000138... Val Loss: 0.000289\n",
      "Epoch: 92/100... Step: 94670... Loss: 0.000077... Val Loss: 0.000430\n",
      "Epoch: 92/100... Step: 94675... Loss: 0.000206... Val Loss: 0.000509\n",
      "Epoch: 92/100... Step: 94680... Loss: 0.000142... Val Loss: 0.000487\n",
      "Epoch: 92/100... Step: 94685... Loss: 0.000035... Val Loss: 0.000413\n",
      "Epoch: 92/100... Step: 94690... Loss: 0.000138... Val Loss: 0.000366\n",
      "Epoch: 92/100... Step: 94695... Loss: 0.000224... Val Loss: 0.000379\n",
      "Epoch: 92/100... Step: 94700... Loss: 0.000125... Val Loss: 0.000445\n",
      "Epoch: 92/100... Step: 94705... Loss: 0.000119... Val Loss: 0.000516\n",
      "Epoch: 92/100... Step: 94710... Loss: 0.000202... Val Loss: 0.000529\n",
      "Epoch: 92/100... Step: 94715... Loss: 0.000115... Val Loss: 0.000495\n",
      "Epoch: 92/100... Step: 94720... Loss: 0.000247... Val Loss: 0.000486\n",
      "Epoch: 92/100... Step: 94725... Loss: 0.000199... Val Loss: 0.000532\n",
      "Epoch: 92/100... Step: 94730... Loss: 0.000060... Val Loss: 0.000627\n",
      "Epoch: 92/100... Step: 94735... Loss: 0.000083... Val Loss: 0.000711\n",
      "Epoch: 92/100... Step: 94740... Loss: 0.000122... Val Loss: 0.000718\n",
      "Epoch: 92/100... Step: 94745... Loss: 0.000075... Val Loss: 0.000666\n",
      "Epoch: 92/100... Step: 94750... Loss: 0.000058... Val Loss: 0.000596\n",
      "Epoch: 92/100... Step: 94755... Loss: 0.000053... Val Loss: 0.000546\n",
      "Epoch: 92/100... Step: 94760... Loss: 0.000083... Val Loss: 0.000549\n",
      "Epoch: 92/100... Step: 94765... Loss: 0.000067... Val Loss: 0.000605\n",
      "Epoch: 92/100... Step: 94770... Loss: 0.000047... Val Loss: 0.000691\n",
      "Epoch: 92/100... Step: 94775... Loss: 0.000036... Val Loss: 0.000776\n",
      "Epoch: 92/100... Step: 94780... Loss: 0.000063... Val Loss: 0.000822\n",
      "Epoch: 92/100... Step: 94785... Loss: 0.000052... Val Loss: 0.000797\n",
      "Epoch: 92/100... Step: 94790... Loss: 0.000041... Val Loss: 0.000746\n",
      "Epoch: 92/100... Step: 94795... Loss: 0.000065... Val Loss: 0.000741\n",
      "Epoch: 92/100... Step: 94800... Loss: 0.000066... Val Loss: 0.000749\n",
      "Epoch: 92/100... Step: 94805... Loss: 0.000060... Val Loss: 0.000721\n",
      "Epoch: 92/100... Step: 94810... Loss: 0.000042... Val Loss: 0.000689\n",
      "Epoch: 92/100... Step: 94815... Loss: 0.000044... Val Loss: 0.000668\n",
      "Epoch: 92/100... Step: 94820... Loss: 0.000083... Val Loss: 0.000643\n",
      "Epoch: 92/100... Step: 94825... Loss: 0.000035... Val Loss: 0.000580\n",
      "Epoch: 92/100... Step: 94830... Loss: 0.000118... Val Loss: 0.000530\n",
      "Epoch: 92/100... Step: 94835... Loss: 0.000181... Val Loss: 0.000542\n",
      "Epoch: 92/100... Step: 94840... Loss: 0.000193... Val Loss: 0.000604\n",
      "Epoch: 92/100... Step: 94845... Loss: 0.000164... Val Loss: 0.000710\n",
      "Epoch: 92/100... Step: 94850... Loss: 0.000082... Val Loss: 0.000853\n",
      "Epoch: 92/100... Step: 94855... Loss: 0.000104... Val Loss: 0.000966\n",
      "Epoch: 92/100... Step: 94860... Loss: 0.000216... Val Loss: 0.000997\n",
      "Epoch: 92/100... Step: 94865... Loss: 0.000092... Val Loss: 0.000966\n",
      "Epoch: 92/100... Step: 94870... Loss: 0.000050... Val Loss: 0.000926\n",
      "Epoch: 92/100... Step: 94875... Loss: 0.000048... Val Loss: 0.000944\n",
      "Epoch: 92/100... Step: 94880... Loss: 0.000033... Val Loss: 0.000984\n",
      "Epoch: 92/100... Step: 94885... Loss: 0.000033... Val Loss: 0.000991\n",
      "Epoch: 92/100... Step: 94890... Loss: 0.000040... Val Loss: 0.000968\n",
      "Epoch: 92/100... Step: 94895... Loss: 0.000087... Val Loss: 0.000921\n",
      "Epoch: 92/100... Step: 94900... Loss: 0.000172... Val Loss: 0.000904\n",
      "Epoch: 92/100... Step: 94905... Loss: 0.000123... Val Loss: 0.000950\n",
      "Epoch: 92/100... Step: 94910... Loss: 0.000046... Val Loss: 0.001034\n",
      "Epoch: 92/100... Step: 94915... Loss: 0.000112... Val Loss: 0.001092\n",
      "Epoch: 92/100... Step: 94920... Loss: 0.000216... Val Loss: 0.001085\n",
      "Epoch: 92/100... Step: 94925... Loss: 0.000125... Val Loss: 0.001024\n",
      "Epoch: 92/100... Step: 94930... Loss: 0.000082... Val Loss: 0.000943\n",
      "Epoch: 92/100... Step: 94935... Loss: 0.000123... Val Loss: 0.000922\n",
      "Epoch: 92/100... Step: 94940... Loss: 0.000053... Val Loss: 0.000962\n",
      "Epoch: 93/100... Step: 94945... Loss: 0.000251... Val Loss: 0.001026\n",
      "Epoch: 93/100... Step: 94950... Loss: 0.000310... Val Loss: 0.001116\n",
      "Epoch: 93/100... Step: 94955... Loss: 0.000284... Val Loss: 0.001237\n",
      "Epoch: 93/100... Step: 94960... Loss: 0.000325... Val Loss: 0.001393\n",
      "Epoch: 93/100... Step: 94965... Loss: 0.000237... Val Loss: 0.001574\n",
      "Epoch: 93/100... Step: 94970... Loss: 0.000114... Val Loss: 0.001747\n",
      "Epoch: 93/100... Step: 94975... Loss: 0.000054... Val Loss: 0.001896\n",
      "Epoch: 93/100... Step: 94980... Loss: 0.000191... Val Loss: 0.001976\n",
      "Epoch: 93/100... Step: 94985... Loss: 0.000292... Val Loss: 0.001956\n",
      "Epoch: 93/100... Step: 94990... Loss: 0.000108... Val Loss: 0.001869\n",
      "Epoch: 93/100... Step: 94995... Loss: 0.000072... Val Loss: 0.001767\n",
      "Epoch: 93/100... Step: 95000... Loss: 0.000045... Val Loss: 0.001674\n",
      "Epoch: 93/100... Step: 95005... Loss: 0.000089... Val Loss: 0.001619\n",
      "Epoch: 93/100... Step: 95010... Loss: 0.000041... Val Loss: 0.001557\n",
      "Epoch: 93/100... Step: 95015... Loss: 0.000252... Val Loss: 0.001485\n",
      "Epoch: 93/100... Step: 95020... Loss: 0.000232... Val Loss: 0.001379\n",
      "Epoch: 93/100... Step: 95025... Loss: 0.000083... Val Loss: 0.001250\n",
      "Epoch: 93/100... Step: 95030... Loss: 0.000166... Val Loss: 0.001207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93/100... Step: 95035... Loss: 0.000080... Val Loss: 0.001250\n",
      "Epoch: 93/100... Step: 95040... Loss: 0.000171... Val Loss: 0.001310\n",
      "Epoch: 93/100... Step: 95045... Loss: 0.000265... Val Loss: 0.001314\n",
      "Epoch: 93/100... Step: 95050... Loss: 0.000137... Val Loss: 0.001255\n",
      "Epoch: 93/100... Step: 95055... Loss: 0.000033... Val Loss: 0.001149\n",
      "Epoch: 93/100... Step: 95060... Loss: 0.000074... Val Loss: 0.001078\n",
      "Epoch: 93/100... Step: 95065... Loss: 0.000104... Val Loss: 0.001086\n",
      "Epoch: 93/100... Step: 95070... Loss: 0.000151... Val Loss: 0.001142\n",
      "Epoch: 93/100... Step: 95075... Loss: 0.000155... Val Loss: 0.001241\n",
      "Epoch: 93/100... Step: 95080... Loss: 0.000086... Val Loss: 0.001367\n",
      "Epoch: 93/100... Step: 95085... Loss: 0.000104... Val Loss: 0.001487\n",
      "Epoch: 93/100... Step: 95090... Loss: 0.000124... Val Loss: 0.001586\n",
      "Epoch: 93/100... Step: 95095... Loss: 0.000048... Val Loss: 0.001701\n",
      "Epoch: 93/100... Step: 95100... Loss: 0.000207... Val Loss: 0.001770\n",
      "Epoch: 93/100... Step: 95105... Loss: 0.000317... Val Loss: 0.001764\n",
      "Epoch: 93/100... Step: 95110... Loss: 0.000310... Val Loss: 0.001699\n",
      "Epoch: 93/100... Step: 95115... Loss: 0.000240... Val Loss: 0.001588\n",
      "Epoch: 93/100... Step: 95120... Loss: 0.000109... Val Loss: 0.001437\n",
      "Epoch: 93/100... Step: 95125... Loss: 0.000051... Val Loss: 0.001265\n",
      "Epoch: 93/100... Step: 95130... Loss: 0.000093... Val Loss: 0.001170\n",
      "Epoch: 93/100... Step: 95135... Loss: 0.000027... Val Loss: 0.001115\n",
      "Epoch: 93/100... Step: 95140... Loss: 0.000037... Val Loss: 0.001141\n",
      "Epoch: 93/100... Step: 95145... Loss: 0.000041... Val Loss: 0.001195\n",
      "Epoch: 93/100... Step: 95150... Loss: 0.000042... Val Loss: 0.001198\n",
      "Epoch: 93/100... Step: 95155... Loss: 0.000051... Val Loss: 0.001231\n",
      "Epoch: 93/100... Step: 95160... Loss: 0.000032... Val Loss: 0.001313\n",
      "Epoch: 93/100... Step: 95165... Loss: 0.000105... Val Loss: 0.001336\n",
      "Epoch: 93/100... Step: 95170... Loss: 0.000090... Val Loss: 0.001288\n",
      "Epoch: 93/100... Step: 95175... Loss: 0.000140... Val Loss: 0.001213\n",
      "Epoch: 93/100... Step: 95180... Loss: 0.000065... Val Loss: 0.001110\n",
      "Epoch: 93/100... Step: 95185... Loss: 0.000142... Val Loss: 0.001023\n",
      "Epoch: 93/100... Step: 95190... Loss: 0.000105... Val Loss: 0.001030\n",
      "Epoch: 93/100... Step: 95195... Loss: 0.000085... Val Loss: 0.001097\n",
      "Epoch: 93/100... Step: 95200... Loss: 0.000062... Val Loss: 0.001192\n",
      "Epoch: 93/100... Step: 95205... Loss: 0.000250... Val Loss: 0.001234\n",
      "Epoch: 93/100... Step: 95210... Loss: 0.000311... Val Loss: 0.001207\n",
      "Epoch: 93/100... Step: 95215... Loss: 0.000254... Val Loss: 0.001130\n",
      "Epoch: 93/100... Step: 95220... Loss: 0.000163... Val Loss: 0.001013\n",
      "Epoch: 93/100... Step: 95225... Loss: 0.000048... Val Loss: 0.000878\n",
      "Epoch: 93/100... Step: 95230... Loss: 0.000109... Val Loss: 0.000813\n",
      "Epoch: 93/100... Step: 95235... Loss: 0.000123... Val Loss: 0.000791\n",
      "Epoch: 93/100... Step: 95240... Loss: 0.000147... Val Loss: 0.000731\n",
      "Epoch: 93/100... Step: 95245... Loss: 0.000096... Val Loss: 0.000619\n",
      "Epoch: 93/100... Step: 95250... Loss: 0.000182... Val Loss: 0.000508\n",
      "Epoch: 93/100... Step: 95255... Loss: 0.000130... Val Loss: 0.000479\n",
      "Epoch: 93/100... Step: 95260... Loss: 0.000113... Val Loss: 0.000526\n",
      "Epoch: 93/100... Step: 95265... Loss: 0.000038... Val Loss: 0.000626\n",
      "Epoch: 93/100... Step: 95270... Loss: 0.000188... Val Loss: 0.000699\n",
      "Epoch: 93/100... Step: 95275... Loss: 0.000312... Val Loss: 0.000699\n",
      "Epoch: 93/100... Step: 95280... Loss: 0.000179... Val Loss: 0.000644\n",
      "Epoch: 93/100... Step: 95285... Loss: 0.000084... Val Loss: 0.000568\n",
      "Epoch: 93/100... Step: 95290... Loss: 0.000199... Val Loss: 0.000534\n",
      "Epoch: 93/100... Step: 95295... Loss: 0.000174... Val Loss: 0.000480\n",
      "Epoch: 93/100... Step: 95300... Loss: 0.000242... Val Loss: 0.000417\n",
      "Epoch: 93/100... Step: 95305... Loss: 0.000426... Val Loss: 0.000409\n",
      "Epoch: 93/100... Step: 95310... Loss: 0.000555... Val Loss: 0.000458\n",
      "Epoch: 93/100... Step: 95315... Loss: 0.000500... Val Loss: 0.000551\n",
      "Epoch: 93/100... Step: 95320... Loss: 0.000371... Val Loss: 0.000676\n",
      "Epoch: 93/100... Step: 95325... Loss: 0.000300... Val Loss: 0.000817\n",
      "Epoch: 93/100... Step: 95330... Loss: 0.000094... Val Loss: 0.000968\n",
      "Epoch: 93/100... Step: 95335... Loss: 0.000094... Val Loss: 0.001085\n",
      "Epoch: 93/100... Step: 95340... Loss: 0.000068... Val Loss: 0.001123\n",
      "Epoch: 93/100... Step: 95345... Loss: 0.000025... Val Loss: 0.001110\n",
      "Epoch: 93/100... Step: 95350... Loss: 0.000396... Val Loss: 0.001079\n",
      "Epoch: 93/100... Step: 95355... Loss: 0.000787... Val Loss: 0.001023\n",
      "Epoch: 93/100... Step: 95360... Loss: 0.001673... Val Loss: 0.000928\n",
      "Epoch: 93/100... Step: 95365... Loss: 0.001933... Val Loss: 0.000803\n",
      "Epoch: 93/100... Step: 95370... Loss: 0.001691... Val Loss: 0.000644\n",
      "Epoch: 93/100... Step: 95375... Loss: 0.001499... Val Loss: 0.000448\n",
      "Epoch: 93/100... Step: 95380... Loss: 0.001304... Val Loss: 0.000275\n",
      "Epoch: 93/100... Step: 95385... Loss: 0.000899... Val Loss: 0.000345\n",
      "Epoch: 93/100... Step: 95390... Loss: 0.000429... Val Loss: 0.000847\n",
      "Epoch: 93/100... Step: 95395... Loss: 0.000132... Val Loss: 0.001504\n",
      "Epoch: 93/100... Step: 95400... Loss: 0.000394... Val Loss: 0.001856\n",
      "Epoch: 93/100... Step: 95405... Loss: 0.000454... Val Loss: 0.001785\n",
      "Epoch: 93/100... Step: 95410... Loss: 0.000149... Val Loss: 0.001493\n",
      "Epoch: 93/100... Step: 95415... Loss: 0.000142... Val Loss: 0.001205\n",
      "Epoch: 93/100... Step: 95420... Loss: 0.000114... Val Loss: 0.000992\n",
      "Epoch: 93/100... Step: 95425... Loss: 0.000300... Val Loss: 0.000958\n",
      "Epoch: 93/100... Step: 95430... Loss: 0.000178... Val Loss: 0.001046\n",
      "Epoch: 93/100... Step: 95435... Loss: 0.000184... Val Loss: 0.001065\n",
      "Epoch: 93/100... Step: 95440... Loss: 0.000196... Val Loss: 0.000960\n",
      "Epoch: 93/100... Step: 95445... Loss: 0.000051... Val Loss: 0.000797\n",
      "Epoch: 93/100... Step: 95450... Loss: 0.000192... Val Loss: 0.000622\n",
      "Epoch: 93/100... Step: 95455... Loss: 0.000108... Val Loss: 0.000426\n",
      "Epoch: 93/100... Step: 95460... Loss: 0.000139... Val Loss: 0.000303\n",
      "Epoch: 93/100... Step: 95465... Loss: 0.000267... Val Loss: 0.000280\n",
      "Epoch: 93/100... Step: 95470... Loss: 0.000406... Val Loss: 0.000298\n",
      "Epoch: 93/100... Step: 95475... Loss: 0.000381... Val Loss: 0.000367\n",
      "Epoch: 93/100... Step: 95480... Loss: 0.000169... Val Loss: 0.000516\n",
      "Epoch: 93/100... Step: 95485... Loss: 0.000102... Val Loss: 0.000694\n",
      "Epoch: 93/100... Step: 95490... Loss: 0.000076... Val Loss: 0.000846\n",
      "Epoch: 93/100... Step: 95495... Loss: 0.000084... Val Loss: 0.000973\n",
      "Epoch: 93/100... Step: 95500... Loss: 0.000236... Val Loss: 0.000993\n",
      "Epoch: 93/100... Step: 95505... Loss: 0.000235... Val Loss: 0.000916\n",
      "Epoch: 93/100... Step: 95510... Loss: 0.000099... Val Loss: 0.000778\n",
      "Epoch: 93/100... Step: 95515... Loss: 0.000190... Val Loss: 0.000703\n",
      "Epoch: 93/100... Step: 95520... Loss: 0.000195... Val Loss: 0.000721\n",
      "Epoch: 93/100... Step: 95525... Loss: 0.000105... Val Loss: 0.000796\n",
      "Epoch: 93/100... Step: 95530... Loss: 0.000063... Val Loss: 0.000882\n",
      "Epoch: 93/100... Step: 95535... Loss: 0.000077... Val Loss: 0.000922\n",
      "Epoch: 93/100... Step: 95540... Loss: 0.000081... Val Loss: 0.000923\n",
      "Epoch: 93/100... Step: 95545... Loss: 0.000075... Val Loss: 0.000924\n",
      "Epoch: 93/100... Step: 95550... Loss: 0.000064... Val Loss: 0.000939\n",
      "Epoch: 93/100... Step: 95555... Loss: 0.000050... Val Loss: 0.000971\n",
      "Epoch: 93/100... Step: 95560... Loss: 0.000107... Val Loss: 0.000934\n",
      "Epoch: 93/100... Step: 95565... Loss: 0.000069... Val Loss: 0.000855\n",
      "Epoch: 93/100... Step: 95570... Loss: 0.000079... Val Loss: 0.000837\n",
      "Epoch: 93/100... Step: 95575... Loss: 0.000057... Val Loss: 0.000879\n",
      "Epoch: 93/100... Step: 95580... Loss: 0.000027... Val Loss: 0.000912\n",
      "Epoch: 93/100... Step: 95585... Loss: 0.000133... Val Loss: 0.000893\n",
      "Epoch: 93/100... Step: 95590... Loss: 0.000219... Val Loss: 0.000822\n",
      "Epoch: 93/100... Step: 95595... Loss: 0.000173... Val Loss: 0.000680\n",
      "Epoch: 93/100... Step: 95600... Loss: 0.000252... Val Loss: 0.000624\n",
      "Epoch: 93/100... Step: 95605... Loss: 0.000160... Val Loss: 0.000658\n",
      "Epoch: 93/100... Step: 95610... Loss: 0.000180... Val Loss: 0.000720\n",
      "Epoch: 93/100... Step: 95615... Loss: 0.000366... Val Loss: 0.000711\n",
      "Epoch: 93/100... Step: 95620... Loss: 0.000281... Val Loss: 0.000625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93/100... Step: 95625... Loss: 0.000283... Val Loss: 0.000494\n",
      "Epoch: 93/100... Step: 95630... Loss: 0.000206... Val Loss: 0.000355\n",
      "Epoch: 93/100... Step: 95635... Loss: 0.000072... Val Loss: 0.000275\n",
      "Epoch: 93/100... Step: 95640... Loss: 0.000146... Val Loss: 0.000262\n",
      "Epoch: 93/100... Step: 95645... Loss: 0.000134... Val Loss: 0.000263\n",
      "Epoch: 93/100... Step: 95650... Loss: 0.000089... Val Loss: 0.000278\n",
      "Epoch: 93/100... Step: 95655... Loss: 0.000072... Val Loss: 0.000325\n",
      "Epoch: 93/100... Step: 95660... Loss: 0.000045... Val Loss: 0.000408\n",
      "Epoch: 93/100... Step: 95665... Loss: 0.000065... Val Loss: 0.000496\n",
      "Epoch: 93/100... Step: 95670... Loss: 0.000167... Val Loss: 0.000522\n",
      "Epoch: 93/100... Step: 95675... Loss: 0.000119... Val Loss: 0.000484\n",
      "Epoch: 93/100... Step: 95680... Loss: 0.000180... Val Loss: 0.000412\n",
      "Epoch: 93/100... Step: 95685... Loss: 0.000380... Val Loss: 0.000332\n",
      "Epoch: 93/100... Step: 95690... Loss: 0.000424... Val Loss: 0.000269\n",
      "Epoch: 93/100... Step: 95695... Loss: 0.000301... Val Loss: 0.000263\n",
      "Epoch: 93/100... Step: 95700... Loss: 0.000138... Val Loss: 0.000307\n",
      "Epoch: 93/100... Step: 95705... Loss: 0.000064... Val Loss: 0.000422\n",
      "Epoch: 93/100... Step: 95710... Loss: 0.000170... Val Loss: 0.000495\n",
      "Epoch: 93/100... Step: 95715... Loss: 0.000107... Val Loss: 0.000498\n",
      "Epoch: 93/100... Step: 95720... Loss: 0.000075... Val Loss: 0.000454\n",
      "Epoch: 93/100... Step: 95725... Loss: 0.000140... Val Loss: 0.000408\n",
      "Epoch: 93/100... Step: 95730... Loss: 0.000190... Val Loss: 0.000405\n",
      "Epoch: 93/100... Step: 95735... Loss: 0.000082... Val Loss: 0.000434\n",
      "Epoch: 93/100... Step: 95740... Loss: 0.000096... Val Loss: 0.000453\n",
      "Epoch: 93/100... Step: 95745... Loss: 0.000107... Val Loss: 0.000436\n",
      "Epoch: 93/100... Step: 95750... Loss: 0.000260... Val Loss: 0.000424\n",
      "Epoch: 93/100... Step: 95755... Loss: 0.000349... Val Loss: 0.000448\n",
      "Epoch: 93/100... Step: 95760... Loss: 0.000206... Val Loss: 0.000505\n",
      "Epoch: 93/100... Step: 95765... Loss: 0.000065... Val Loss: 0.000592\n",
      "Epoch: 93/100... Step: 95770... Loss: 0.000057... Val Loss: 0.000672\n",
      "Epoch: 93/100... Step: 95775... Loss: 0.000078... Val Loss: 0.000694\n",
      "Epoch: 93/100... Step: 95780... Loss: 0.000088... Val Loss: 0.000675\n",
      "Epoch: 93/100... Step: 95785... Loss: 0.000057... Val Loss: 0.000627\n",
      "Epoch: 93/100... Step: 95790... Loss: 0.000035... Val Loss: 0.000568\n",
      "Epoch: 93/100... Step: 95795... Loss: 0.000091... Val Loss: 0.000551\n",
      "Epoch: 93/100... Step: 95800... Loss: 0.000131... Val Loss: 0.000576\n",
      "Epoch: 93/100... Step: 95805... Loss: 0.000144... Val Loss: 0.000631\n",
      "Epoch: 93/100... Step: 95810... Loss: 0.000094... Val Loss: 0.000710\n",
      "Epoch: 93/100... Step: 95815... Loss: 0.000056... Val Loss: 0.000797\n",
      "Epoch: 93/100... Step: 95820... Loss: 0.000066... Val Loss: 0.000843\n",
      "Epoch: 93/100... Step: 95825... Loss: 0.000045... Val Loss: 0.000843\n",
      "Epoch: 93/100... Step: 95830... Loss: 0.000110... Val Loss: 0.000806\n",
      "Epoch: 93/100... Step: 95835... Loss: 0.000082... Val Loss: 0.000746\n",
      "Epoch: 93/100... Step: 95840... Loss: 0.000056... Val Loss: 0.000685\n",
      "Epoch: 93/100... Step: 95845... Loss: 0.000045... Val Loss: 0.000650\n",
      "Epoch: 93/100... Step: 95850... Loss: 0.000057... Val Loss: 0.000635\n",
      "Epoch: 93/100... Step: 95855... Loss: 0.000044... Val Loss: 0.000597\n",
      "Epoch: 93/100... Step: 95860... Loss: 0.000066... Val Loss: 0.000552\n",
      "Epoch: 93/100... Step: 95865... Loss: 0.000137... Val Loss: 0.000546\n",
      "Epoch: 93/100... Step: 95870... Loss: 0.000180... Val Loss: 0.000581\n",
      "Epoch: 93/100... Step: 95875... Loss: 0.000186... Val Loss: 0.000644\n",
      "Epoch: 93/100... Step: 95880... Loss: 0.000175... Val Loss: 0.000729\n",
      "Epoch: 93/100... Step: 95885... Loss: 0.000051... Val Loss: 0.000837\n",
      "Epoch: 93/100... Step: 95890... Loss: 0.000133... Val Loss: 0.000925\n",
      "Epoch: 93/100... Step: 95895... Loss: 0.000127... Val Loss: 0.000949\n",
      "Epoch: 93/100... Step: 95900... Loss: 0.000033... Val Loss: 0.000943\n",
      "Epoch: 93/100... Step: 95905... Loss: 0.000030... Val Loss: 0.000960\n",
      "Epoch: 93/100... Step: 95910... Loss: 0.000026... Val Loss: 0.000986\n",
      "Epoch: 93/100... Step: 95915... Loss: 0.000023... Val Loss: 0.000986\n",
      "Epoch: 93/100... Step: 95920... Loss: 0.000029... Val Loss: 0.000974\n",
      "Epoch: 93/100... Step: 95925... Loss: 0.000058... Val Loss: 0.000941\n",
      "Epoch: 93/100... Step: 95930... Loss: 0.000130... Val Loss: 0.000912\n",
      "Epoch: 93/100... Step: 95935... Loss: 0.000138... Val Loss: 0.000923\n",
      "Epoch: 93/100... Step: 95940... Loss: 0.000089... Val Loss: 0.000976\n",
      "Epoch: 93/100... Step: 95945... Loss: 0.000049... Val Loss: 0.001049\n",
      "Epoch: 93/100... Step: 95950... Loss: 0.000178... Val Loss: 0.001083\n",
      "Epoch: 93/100... Step: 95955... Loss: 0.000200... Val Loss: 0.001064\n",
      "Epoch: 93/100... Step: 95960... Loss: 0.000044... Val Loss: 0.001008\n",
      "Epoch: 93/100... Step: 95965... Loss: 0.000084... Val Loss: 0.000959\n",
      "Epoch: 93/100... Step: 95970... Loss: 0.000071... Val Loss: 0.000959\n",
      "Epoch: 93/100... Step: 95975... Loss: 0.000055... Val Loss: 0.000996\n",
      "Epoch: 94/100... Step: 95980... Loss: 0.000316... Val Loss: 0.001046\n",
      "Epoch: 94/100... Step: 95985... Loss: 0.000378... Val Loss: 0.001126\n",
      "Epoch: 94/100... Step: 95990... Loss: 0.000376... Val Loss: 0.001230\n",
      "Epoch: 94/100... Step: 95995... Loss: 0.000464... Val Loss: 0.001352\n",
      "Epoch: 94/100... Step: 96000... Loss: 0.000241... Val Loss: 0.001502\n",
      "Epoch: 94/100... Step: 96005... Loss: 0.000164... Val Loss: 0.001663\n",
      "Epoch: 94/100... Step: 96010... Loss: 0.000085... Val Loss: 0.001843\n",
      "Epoch: 94/100... Step: 96015... Loss: 0.000320... Val Loss: 0.001984\n",
      "Epoch: 94/100... Step: 96020... Loss: 0.000306... Val Loss: 0.002026\n",
      "Epoch: 94/100... Step: 96025... Loss: 0.000235... Val Loss: 0.001991\n",
      "Epoch: 94/100... Step: 96030... Loss: 0.000190... Val Loss: 0.001906\n",
      "Epoch: 94/100... Step: 96035... Loss: 0.000140... Val Loss: 0.001795\n",
      "Epoch: 94/100... Step: 96040... Loss: 0.000137... Val Loss: 0.001677\n",
      "Epoch: 94/100... Step: 96045... Loss: 0.000204... Val Loss: 0.001552\n",
      "Epoch: 94/100... Step: 96050... Loss: 0.000319... Val Loss: 0.001427\n",
      "Epoch: 94/100... Step: 96055... Loss: 0.000094... Val Loss: 0.001296\n",
      "Epoch: 94/100... Step: 96060... Loss: 0.000150... Val Loss: 0.001208\n",
      "Epoch: 94/100... Step: 96065... Loss: 0.000143... Val Loss: 0.001188\n",
      "Epoch: 94/100... Step: 96070... Loss: 0.000112... Val Loss: 0.001216\n",
      "Epoch: 94/100... Step: 96075... Loss: 0.000198... Val Loss: 0.001241\n",
      "Epoch: 94/100... Step: 96080... Loss: 0.000118... Val Loss: 0.001223\n",
      "Epoch: 94/100... Step: 96085... Loss: 0.000036... Val Loss: 0.001166\n",
      "Epoch: 94/100... Step: 96090... Loss: 0.000051... Val Loss: 0.001109\n",
      "Epoch: 94/100... Step: 96095... Loss: 0.000059... Val Loss: 0.001104\n",
      "Epoch: 94/100... Step: 96100... Loss: 0.000108... Val Loss: 0.001128\n",
      "Epoch: 94/100... Step: 96105... Loss: 0.000162... Val Loss: 0.001179\n",
      "Epoch: 94/100... Step: 96110... Loss: 0.000157... Val Loss: 0.001258\n",
      "Epoch: 94/100... Step: 96115... Loss: 0.000092... Val Loss: 0.001360\n",
      "Epoch: 94/100... Step: 96120... Loss: 0.000182... Val Loss: 0.001479\n",
      "Epoch: 94/100... Step: 96125... Loss: 0.000108... Val Loss: 0.001614\n",
      "Epoch: 94/100... Step: 96130... Loss: 0.000123... Val Loss: 0.001750\n",
      "Epoch: 94/100... Step: 96135... Loss: 0.000288... Val Loss: 0.001797\n",
      "Epoch: 94/100... Step: 96140... Loss: 0.000342... Val Loss: 0.001772\n",
      "Epoch: 94/100... Step: 96145... Loss: 0.000357... Val Loss: 0.001698\n",
      "Epoch: 94/100... Step: 96150... Loss: 0.000265... Val Loss: 0.001599\n",
      "Epoch: 94/100... Step: 96155... Loss: 0.000174... Val Loss: 0.001486\n",
      "Epoch: 94/100... Step: 96160... Loss: 0.000122... Val Loss: 0.001363\n",
      "Epoch: 94/100... Step: 96165... Loss: 0.000115... Val Loss: 0.001236\n",
      "Epoch: 94/100... Step: 96170... Loss: 0.000033... Val Loss: 0.001106\n",
      "Epoch: 94/100... Step: 96175... Loss: 0.000108... Val Loss: 0.001043\n",
      "Epoch: 94/100... Step: 96180... Loss: 0.000139... Val Loss: 0.001048\n",
      "Epoch: 94/100... Step: 96185... Loss: 0.000159... Val Loss: 0.001102\n",
      "Epoch: 94/100... Step: 96190... Loss: 0.000115... Val Loss: 0.001183\n",
      "Epoch: 94/100... Step: 96195... Loss: 0.000044... Val Loss: 0.001289\n",
      "Epoch: 94/100... Step: 96200... Loss: 0.000132... Val Loss: 0.001348\n",
      "Epoch: 94/100... Step: 96205... Loss: 0.000185... Val Loss: 0.001337\n",
      "Epoch: 94/100... Step: 96210... Loss: 0.000244... Val Loss: 0.001276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94/100... Step: 96215... Loss: 0.000094... Val Loss: 0.001190\n",
      "Epoch: 94/100... Step: 96220... Loss: 0.000075... Val Loss: 0.001114\n",
      "Epoch: 94/100... Step: 96225... Loss: 0.000061... Val Loss: 0.001062\n",
      "Epoch: 94/100... Step: 96230... Loss: 0.000124... Val Loss: 0.001048\n",
      "Epoch: 94/100... Step: 96235... Loss: 0.000089... Val Loss: 0.001064\n",
      "Epoch: 94/100... Step: 96240... Loss: 0.000123... Val Loss: 0.001057\n",
      "Epoch: 94/100... Step: 96245... Loss: 0.000119... Val Loss: 0.001012\n",
      "Epoch: 94/100... Step: 96250... Loss: 0.000076... Val Loss: 0.000935\n",
      "Epoch: 94/100... Step: 96255... Loss: 0.000054... Val Loss: 0.000853\n",
      "Epoch: 94/100... Step: 96260... Loss: 0.000110... Val Loss: 0.000825\n",
      "Epoch: 94/100... Step: 96265... Loss: 0.000122... Val Loss: 0.000836\n",
      "Epoch: 94/100... Step: 96270... Loss: 0.000220... Val Loss: 0.000830\n",
      "Epoch: 94/100... Step: 96275... Loss: 0.000257... Val Loss: 0.000785\n",
      "Epoch: 94/100... Step: 96280... Loss: 0.000119... Val Loss: 0.000714\n",
      "Epoch: 94/100... Step: 96285... Loss: 0.000126... Val Loss: 0.000658\n",
      "Epoch: 94/100... Step: 96290... Loss: 0.000062... Val Loss: 0.000599\n",
      "Epoch: 94/100... Step: 96295... Loss: 0.000091... Val Loss: 0.000543\n",
      "Epoch: 94/100... Step: 96300... Loss: 0.000088... Val Loss: 0.000546\n",
      "Epoch: 94/100... Step: 96305... Loss: 0.000122... Val Loss: 0.000549\n",
      "Epoch: 94/100... Step: 96310... Loss: 0.000076... Val Loss: 0.000507\n",
      "Epoch: 94/100... Step: 96315... Loss: 0.000150... Val Loss: 0.000452\n",
      "Epoch: 94/100... Step: 96320... Loss: 0.000194... Val Loss: 0.000446\n",
      "Epoch: 94/100... Step: 96325... Loss: 0.000128... Val Loss: 0.000440\n",
      "Epoch: 94/100... Step: 96330... Loss: 0.000196... Val Loss: 0.000402\n",
      "Epoch: 94/100... Step: 96335... Loss: 0.000387... Val Loss: 0.000389\n",
      "Epoch: 94/100... Step: 96340... Loss: 0.000523... Val Loss: 0.000424\n",
      "Epoch: 94/100... Step: 96345... Loss: 0.000577... Val Loss: 0.000503\n",
      "Epoch: 94/100... Step: 96350... Loss: 0.000434... Val Loss: 0.000603\n",
      "Epoch: 94/100... Step: 96355... Loss: 0.000381... Val Loss: 0.000714\n",
      "Epoch: 94/100... Step: 96360... Loss: 0.000250... Val Loss: 0.000826\n",
      "Epoch: 94/100... Step: 96365... Loss: 0.000070... Val Loss: 0.000939\n",
      "Epoch: 94/100... Step: 96370... Loss: 0.000042... Val Loss: 0.001042\n",
      "Epoch: 94/100... Step: 96375... Loss: 0.000046... Val Loss: 0.001111\n",
      "Epoch: 94/100... Step: 96380... Loss: 0.000244... Val Loss: 0.001149\n",
      "Epoch: 94/100... Step: 96385... Loss: 0.000736... Val Loss: 0.001144\n",
      "Epoch: 94/100... Step: 96390... Loss: 0.001451... Val Loss: 0.001099\n",
      "Epoch: 94/100... Step: 96395... Loss: 0.002215... Val Loss: 0.001022\n",
      "Epoch: 94/100... Step: 96400... Loss: 0.002010... Val Loss: 0.000918\n",
      "Epoch: 94/100... Step: 96405... Loss: 0.001879... Val Loss: 0.000792\n",
      "Epoch: 94/100... Step: 96410... Loss: 0.001828... Val Loss: 0.000635\n",
      "Epoch: 94/100... Step: 96415... Loss: 0.001640... Val Loss: 0.000450\n",
      "Epoch: 94/100... Step: 96420... Loss: 0.001313... Val Loss: 0.000282\n",
      "Epoch: 94/100... Step: 96425... Loss: 0.001067... Val Loss: 0.000304\n",
      "Epoch: 94/100... Step: 96430... Loss: 0.000771... Val Loss: 0.000696\n",
      "Epoch: 94/100... Step: 96435... Loss: 0.000205... Val Loss: 0.001243\n",
      "Epoch: 94/100... Step: 96440... Loss: 0.000248... Val Loss: 0.001761\n",
      "Epoch: 94/100... Step: 96445... Loss: 0.000568... Val Loss: 0.001917\n",
      "Epoch: 94/100... Step: 96450... Loss: 0.000766... Val Loss: 0.001786\n",
      "Epoch: 94/100... Step: 96455... Loss: 0.000382... Val Loss: 0.001493\n",
      "Epoch: 94/100... Step: 96460... Loss: 0.000154... Val Loss: 0.001144\n",
      "Epoch: 94/100... Step: 96465... Loss: 0.000101... Val Loss: 0.000844\n",
      "Epoch: 94/100... Step: 96470... Loss: 0.000214... Val Loss: 0.000673\n",
      "Epoch: 94/100... Step: 96475... Loss: 0.000102... Val Loss: 0.000646\n",
      "Epoch: 94/100... Step: 96480... Loss: 0.000152... Val Loss: 0.000683\n",
      "Epoch: 94/100... Step: 96485... Loss: 0.000386... Val Loss: 0.000676\n",
      "Epoch: 94/100... Step: 96490... Loss: 0.000269... Val Loss: 0.000577\n",
      "Epoch: 94/100... Step: 96495... Loss: 0.000069... Val Loss: 0.000432\n",
      "Epoch: 94/100... Step: 96500... Loss: 0.000234... Val Loss: 0.000357\n",
      "Epoch: 94/100... Step: 96505... Loss: 0.000378... Val Loss: 0.000360\n",
      "Epoch: 94/100... Step: 96510... Loss: 0.000286... Val Loss: 0.000423\n",
      "Epoch: 94/100... Step: 96515... Loss: 0.000149... Val Loss: 0.000552\n",
      "Epoch: 94/100... Step: 96520... Loss: 0.000141... Val Loss: 0.000714\n",
      "Epoch: 94/100... Step: 96525... Loss: 0.000076... Val Loss: 0.000891\n",
      "Epoch: 94/100... Step: 96530... Loss: 0.000175... Val Loss: 0.001022\n",
      "Epoch: 94/100... Step: 96535... Loss: 0.000296... Val Loss: 0.001043\n",
      "Epoch: 94/100... Step: 96540... Loss: 0.000195... Val Loss: 0.000964\n",
      "Epoch: 94/100... Step: 96545... Loss: 0.000080... Val Loss: 0.000823\n",
      "Epoch: 94/100... Step: 96550... Loss: 0.000171... Val Loss: 0.000718\n",
      "Epoch: 94/100... Step: 96555... Loss: 0.000156... Val Loss: 0.000704\n",
      "Epoch: 94/100... Step: 96560... Loss: 0.000099... Val Loss: 0.000746\n",
      "Epoch: 94/100... Step: 96565... Loss: 0.000057... Val Loss: 0.000839\n",
      "Epoch: 94/100... Step: 96570... Loss: 0.000074... Val Loss: 0.000949\n",
      "Epoch: 94/100... Step: 96575... Loss: 0.000126... Val Loss: 0.001011\n",
      "Epoch: 94/100... Step: 96580... Loss: 0.000081... Val Loss: 0.001016\n",
      "Epoch: 94/100... Step: 96585... Loss: 0.000081... Val Loss: 0.000981\n",
      "Epoch: 94/100... Step: 96590... Loss: 0.000099... Val Loss: 0.000912\n",
      "Epoch: 94/100... Step: 96595... Loss: 0.000085... Val Loss: 0.000818\n",
      "Epoch: 94/100... Step: 96600... Loss: 0.000155... Val Loss: 0.000782\n",
      "Epoch: 94/100... Step: 96605... Loss: 0.000083... Val Loss: 0.000824\n",
      "Epoch: 94/100... Step: 96610... Loss: 0.000107... Val Loss: 0.000897\n",
      "Epoch: 94/100... Step: 96615... Loss: 0.000168... Val Loss: 0.000934\n",
      "Epoch: 94/100... Step: 96620... Loss: 0.000226... Val Loss: 0.000903\n",
      "Epoch: 94/100... Step: 96625... Loss: 0.000181... Val Loss: 0.000806\n",
      "Epoch: 94/100... Step: 96630... Loss: 0.000193... Val Loss: 0.000713\n",
      "Epoch: 94/100... Step: 96635... Loss: 0.000122... Val Loss: 0.000718\n",
      "Epoch: 94/100... Step: 96640... Loss: 0.000136... Val Loss: 0.000759\n",
      "Epoch: 94/100... Step: 96645... Loss: 0.000306... Val Loss: 0.000749\n",
      "Epoch: 94/100... Step: 96650... Loss: 0.000370... Val Loss: 0.000673\n",
      "Epoch: 94/100... Step: 96655... Loss: 0.000295... Val Loss: 0.000555\n",
      "Epoch: 94/100... Step: 96660... Loss: 0.000291... Val Loss: 0.000416\n",
      "Epoch: 94/100... Step: 96665... Loss: 0.000063... Val Loss: 0.000308\n",
      "Epoch: 94/100... Step: 96670... Loss: 0.000131... Val Loss: 0.000268\n",
      "Epoch: 94/100... Step: 96675... Loss: 0.000129... Val Loss: 0.000263\n",
      "Epoch: 94/100... Step: 96680... Loss: 0.000091... Val Loss: 0.000267\n",
      "Epoch: 94/100... Step: 96685... Loss: 0.000101... Val Loss: 0.000288\n",
      "Epoch: 94/100... Step: 96690... Loss: 0.000093... Val Loss: 0.000338\n",
      "Epoch: 94/100... Step: 96695... Loss: 0.000056... Val Loss: 0.000420\n",
      "Epoch: 94/100... Step: 96700... Loss: 0.000094... Val Loss: 0.000497\n",
      "Epoch: 94/100... Step: 96705... Loss: 0.000181... Val Loss: 0.000510\n",
      "Epoch: 94/100... Step: 96710... Loss: 0.000117... Val Loss: 0.000472\n",
      "Epoch: 94/100... Step: 96715... Loss: 0.000418... Val Loss: 0.000407\n",
      "Epoch: 94/100... Step: 96720... Loss: 0.000545... Val Loss: 0.000341\n",
      "Epoch: 94/100... Step: 96725... Loss: 0.000495... Val Loss: 0.000281\n",
      "Epoch: 94/100... Step: 96730... Loss: 0.000299... Val Loss: 0.000256\n",
      "Epoch: 94/100... Step: 96735... Loss: 0.000185... Val Loss: 0.000285\n",
      "Epoch: 94/100... Step: 96740... Loss: 0.000060... Val Loss: 0.000366\n",
      "Epoch: 94/100... Step: 96745... Loss: 0.000047... Val Loss: 0.000440\n",
      "Epoch: 94/100... Step: 96750... Loss: 0.000090... Val Loss: 0.000483\n",
      "Epoch: 94/100... Step: 96755... Loss: 0.000100... Val Loss: 0.000492\n",
      "Epoch: 94/100... Step: 96760... Loss: 0.000047... Val Loss: 0.000509\n",
      "Epoch: 94/100... Step: 96765... Loss: 0.000069... Val Loss: 0.000539\n",
      "Epoch: 94/100... Step: 96770... Loss: 0.000186... Val Loss: 0.000553\n",
      "Epoch: 94/100... Step: 96775... Loss: 0.000202... Val Loss: 0.000536\n",
      "Epoch: 94/100... Step: 96780... Loss: 0.000145... Val Loss: 0.000503\n",
      "Epoch: 94/100... Step: 96785... Loss: 0.000269... Val Loss: 0.000492\n",
      "Epoch: 94/100... Step: 96790... Loss: 0.000220... Val Loss: 0.000513\n",
      "Epoch: 94/100... Step: 96795... Loss: 0.000087... Val Loss: 0.000559\n",
      "Epoch: 94/100... Step: 96800... Loss: 0.000036... Val Loss: 0.000622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94/100... Step: 96805... Loss: 0.000041... Val Loss: 0.000659\n",
      "Epoch: 94/100... Step: 96810... Loss: 0.000062... Val Loss: 0.000665\n",
      "Epoch: 94/100... Step: 96815... Loss: 0.000078... Val Loss: 0.000648\n",
      "Epoch: 94/100... Step: 96820... Loss: 0.000056... Val Loss: 0.000611\n",
      "Epoch: 94/100... Step: 96825... Loss: 0.000040... Val Loss: 0.000576\n",
      "Epoch: 94/100... Step: 96830... Loss: 0.000085... Val Loss: 0.000572\n",
      "Epoch: 94/100... Step: 96835... Loss: 0.000143... Val Loss: 0.000598\n",
      "Epoch: 94/100... Step: 96840... Loss: 0.000141... Val Loss: 0.000642\n",
      "Epoch: 94/100... Step: 96845... Loss: 0.000065... Val Loss: 0.000702\n",
      "Epoch: 94/100... Step: 96850... Loss: 0.000048... Val Loss: 0.000774\n",
      "Epoch: 94/100... Step: 96855... Loss: 0.000045... Val Loss: 0.000823\n",
      "Epoch: 94/100... Step: 96860... Loss: 0.000083... Val Loss: 0.000844\n",
      "Epoch: 94/100... Step: 96865... Loss: 0.000166... Val Loss: 0.000821\n",
      "Epoch: 94/100... Step: 96870... Loss: 0.000101... Val Loss: 0.000772\n",
      "Epoch: 94/100... Step: 96875... Loss: 0.000071... Val Loss: 0.000713\n",
      "Epoch: 94/100... Step: 96880... Loss: 0.000083... Val Loss: 0.000655\n",
      "Epoch: 94/100... Step: 96885... Loss: 0.000063... Val Loss: 0.000603\n",
      "Epoch: 94/100... Step: 96890... Loss: 0.000056... Val Loss: 0.000551\n",
      "Epoch: 94/100... Step: 96895... Loss: 0.000126... Val Loss: 0.000529\n",
      "Epoch: 94/100... Step: 96900... Loss: 0.000190... Val Loss: 0.000542\n",
      "Epoch: 94/100... Step: 96905... Loss: 0.000229... Val Loss: 0.000578\n",
      "Epoch: 94/100... Step: 96910... Loss: 0.000249... Val Loss: 0.000627\n",
      "Epoch: 94/100... Step: 96915... Loss: 0.000209... Val Loss: 0.000690\n",
      "Epoch: 94/100... Step: 96920... Loss: 0.000073... Val Loss: 0.000776\n",
      "Epoch: 94/100... Step: 96925... Loss: 0.000083... Val Loss: 0.000849\n",
      "Epoch: 94/100... Step: 96930... Loss: 0.000069... Val Loss: 0.000893\n",
      "Epoch: 94/100... Step: 96935... Loss: 0.000042... Val Loss: 0.000941\n",
      "Epoch: 94/100... Step: 96940... Loss: 0.000033... Val Loss: 0.000997\n",
      "Epoch: 94/100... Step: 96945... Loss: 0.000042... Val Loss: 0.001018\n",
      "Epoch: 94/100... Step: 96950... Loss: 0.000052... Val Loss: 0.001006\n",
      "Epoch: 94/100... Step: 96955... Loss: 0.000060... Val Loss: 0.000978\n",
      "Epoch: 94/100... Step: 96960... Loss: 0.000098... Val Loss: 0.000936\n",
      "Epoch: 94/100... Step: 96965... Loss: 0.000145... Val Loss: 0.000922\n",
      "Epoch: 94/100... Step: 96970... Loss: 0.000120... Val Loss: 0.000942\n",
      "Epoch: 94/100... Step: 96975... Loss: 0.000080... Val Loss: 0.000976\n",
      "Epoch: 94/100... Step: 96980... Loss: 0.000067... Val Loss: 0.001011\n",
      "Epoch: 94/100... Step: 96985... Loss: 0.000162... Val Loss: 0.001020\n",
      "Epoch: 94/100... Step: 96990... Loss: 0.000069... Val Loss: 0.000999\n",
      "Epoch: 94/100... Step: 96995... Loss: 0.000076... Val Loss: 0.000973\n",
      "Epoch: 94/100... Step: 97000... Loss: 0.000074... Val Loss: 0.000975\n",
      "Epoch: 94/100... Step: 97005... Loss: 0.000038... Val Loss: 0.000997\n",
      "Epoch: 95/100... Step: 97010... Loss: 0.000286... Val Loss: 0.001019\n",
      "Epoch: 95/100... Step: 97015... Loss: 0.000394... Val Loss: 0.001060\n",
      "Epoch: 95/100... Step: 97020... Loss: 0.000409... Val Loss: 0.001123\n",
      "Epoch: 95/100... Step: 97025... Loss: 0.000568... Val Loss: 0.001201\n",
      "Epoch: 95/100... Step: 97030... Loss: 0.000476... Val Loss: 0.001292\n",
      "Epoch: 95/100... Step: 97035... Loss: 0.000352... Val Loss: 0.001393\n",
      "Epoch: 95/100... Step: 97040... Loss: 0.000390... Val Loss: 0.001505\n",
      "Epoch: 95/100... Step: 97045... Loss: 0.000170... Val Loss: 0.001624\n",
      "Epoch: 95/100... Step: 97050... Loss: 0.000144... Val Loss: 0.001724\n",
      "Epoch: 95/100... Step: 97055... Loss: 0.000054... Val Loss: 0.001800\n",
      "Epoch: 95/100... Step: 97060... Loss: 0.000116... Val Loss: 0.001845\n",
      "Epoch: 95/100... Step: 97065... Loss: 0.000137... Val Loss: 0.001842\n",
      "Epoch: 95/100... Step: 97070... Loss: 0.000213... Val Loss: 0.001803\n",
      "Epoch: 95/100... Step: 97075... Loss: 0.000252... Val Loss: 0.001739\n",
      "Epoch: 95/100... Step: 97080... Loss: 0.000456... Val Loss: 0.001659\n",
      "Epoch: 95/100... Step: 97085... Loss: 0.000389... Val Loss: 0.001568\n",
      "Epoch: 95/100... Step: 97090... Loss: 0.000144... Val Loss: 0.001473\n",
      "Epoch: 95/100... Step: 97095... Loss: 0.000045... Val Loss: 0.001374\n",
      "Epoch: 95/100... Step: 97100... Loss: 0.000041... Val Loss: 0.001285\n",
      "Epoch: 95/100... Step: 97105... Loss: 0.000164... Val Loss: 0.001219\n",
      "Epoch: 95/100... Step: 97110... Loss: 0.000094... Val Loss: 0.001153\n",
      "Epoch: 95/100... Step: 97115... Loss: 0.000046... Val Loss: 0.001098\n",
      "Epoch: 95/100... Step: 97120... Loss: 0.000078... Val Loss: 0.001077\n",
      "Epoch: 95/100... Step: 97125... Loss: 0.000078... Val Loss: 0.001088\n",
      "Epoch: 95/100... Step: 97130... Loss: 0.000079... Val Loss: 0.001119\n",
      "Epoch: 95/100... Step: 97135... Loss: 0.000137... Val Loss: 0.001162\n",
      "Epoch: 95/100... Step: 97140... Loss: 0.000172... Val Loss: 0.001223\n",
      "Epoch: 95/100... Step: 97145... Loss: 0.000118... Val Loss: 0.001297\n",
      "Epoch: 95/100... Step: 97150... Loss: 0.000164... Val Loss: 0.001384\n",
      "Epoch: 95/100... Step: 97155... Loss: 0.000207... Val Loss: 0.001486\n",
      "Epoch: 95/100... Step: 97160... Loss: 0.000074... Val Loss: 0.001608\n",
      "Epoch: 95/100... Step: 97165... Loss: 0.000161... Val Loss: 0.001715\n",
      "Epoch: 95/100... Step: 97170... Loss: 0.000288... Val Loss: 0.001746\n",
      "Epoch: 95/100... Step: 97175... Loss: 0.000349... Val Loss: 0.001728\n",
      "Epoch: 95/100... Step: 97180... Loss: 0.000319... Val Loss: 0.001676\n",
      "Epoch: 95/100... Step: 97185... Loss: 0.000251... Val Loss: 0.001604\n",
      "Epoch: 95/100... Step: 97190... Loss: 0.000223... Val Loss: 0.001521\n",
      "Epoch: 95/100... Step: 97195... Loss: 0.000250... Val Loss: 0.001432\n",
      "Epoch: 95/100... Step: 97200... Loss: 0.000218... Val Loss: 0.001340\n",
      "Epoch: 95/100... Step: 97205... Loss: 0.000107... Val Loss: 0.001245\n",
      "Epoch: 95/100... Step: 97210... Loss: 0.000037... Val Loss: 0.001150\n",
      "Epoch: 95/100... Step: 97215... Loss: 0.000119... Val Loss: 0.001087\n",
      "Epoch: 95/100... Step: 97220... Loss: 0.000203... Val Loss: 0.001069\n",
      "Epoch: 95/100... Step: 97225... Loss: 0.000196... Val Loss: 0.001083\n",
      "Epoch: 95/100... Step: 97230... Loss: 0.000105... Val Loss: 0.001115\n",
      "Epoch: 95/100... Step: 97235... Loss: 0.000074... Val Loss: 0.001170\n",
      "Epoch: 95/100... Step: 97240... Loss: 0.000139... Val Loss: 0.001216\n",
      "Epoch: 95/100... Step: 97245... Loss: 0.000134... Val Loss: 0.001230\n",
      "Epoch: 95/100... Step: 97250... Loss: 0.000066... Val Loss: 0.001219\n",
      "Epoch: 95/100... Step: 97255... Loss: 0.000088... Val Loss: 0.001197\n",
      "Epoch: 95/100... Step: 97260... Loss: 0.000030... Val Loss: 0.001162\n",
      "Epoch: 95/100... Step: 97265... Loss: 0.000075... Val Loss: 0.001142\n",
      "Epoch: 95/100... Step: 97270... Loss: 0.000189... Val Loss: 0.001124\n",
      "Epoch: 95/100... Step: 97275... Loss: 0.000173... Val Loss: 0.001085\n",
      "Epoch: 95/100... Step: 97280... Loss: 0.000158... Val Loss: 0.001028\n",
      "Epoch: 95/100... Step: 97285... Loss: 0.000084... Val Loss: 0.000959\n",
      "Epoch: 95/100... Step: 97290... Loss: 0.000043... Val Loss: 0.000894\n",
      "Epoch: 95/100... Step: 97295... Loss: 0.000111... Val Loss: 0.000859\n",
      "Epoch: 95/100... Step: 97300... Loss: 0.000180... Val Loss: 0.000826\n",
      "Epoch: 95/100... Step: 97305... Loss: 0.000222... Val Loss: 0.000779\n",
      "Epoch: 95/100... Step: 97310... Loss: 0.000166... Val Loss: 0.000719\n",
      "Epoch: 95/100... Step: 97315... Loss: 0.000119... Val Loss: 0.000667\n",
      "Epoch: 95/100... Step: 97320... Loss: 0.000100... Val Loss: 0.000626\n",
      "Epoch: 95/100... Step: 97325... Loss: 0.000034... Val Loss: 0.000579\n",
      "Epoch: 95/100... Step: 97330... Loss: 0.000084... Val Loss: 0.000555\n",
      "Epoch: 95/100... Step: 97335... Loss: 0.000101... Val Loss: 0.000549\n",
      "Epoch: 95/100... Step: 97340... Loss: 0.000110... Val Loss: 0.000526\n",
      "Epoch: 95/100... Step: 97345... Loss: 0.000071... Val Loss: 0.000486\n",
      "Epoch: 95/100... Step: 97350... Loss: 0.000203... Val Loss: 0.000462\n",
      "Epoch: 95/100... Step: 97355... Loss: 0.000161... Val Loss: 0.000453\n",
      "Epoch: 95/100... Step: 97360... Loss: 0.000162... Val Loss: 0.000430\n",
      "Epoch: 95/100... Step: 97365... Loss: 0.000300... Val Loss: 0.000409\n",
      "Epoch: 95/100... Step: 97370... Loss: 0.000479... Val Loss: 0.000413\n",
      "Epoch: 95/100... Step: 97375... Loss: 0.000603... Val Loss: 0.000442\n",
      "Epoch: 95/100... Step: 97380... Loss: 0.000551... Val Loss: 0.000490\n",
      "Epoch: 95/100... Step: 97385... Loss: 0.000518... Val Loss: 0.000553\n",
      "Epoch: 95/100... Step: 97390... Loss: 0.000500... Val Loss: 0.000626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95/100... Step: 97395... Loss: 0.000318... Val Loss: 0.000704\n",
      "Epoch: 95/100... Step: 97400... Loss: 0.000236... Val Loss: 0.000786\n",
      "Epoch: 95/100... Step: 97405... Loss: 0.000198... Val Loss: 0.000872\n",
      "Epoch: 95/100... Step: 97410... Loss: 0.000184... Val Loss: 0.000963\n",
      "Epoch: 95/100... Step: 97415... Loss: 0.000446... Val Loss: 0.001047\n",
      "Epoch: 95/100... Step: 97420... Loss: 0.001051... Val Loss: 0.001094\n",
      "Epoch: 95/100... Step: 97425... Loss: 0.001990... Val Loss: 0.001103\n",
      "Epoch: 95/100... Step: 97430... Loss: 0.002169... Val Loss: 0.001084\n",
      "Epoch: 95/100... Step: 97435... Loss: 0.002091... Val Loss: 0.001044\n",
      "Epoch: 95/100... Step: 97440... Loss: 0.002127... Val Loss: 0.000987\n",
      "Epoch: 95/100... Step: 97445... Loss: 0.002104... Val Loss: 0.000921\n",
      "Epoch: 95/100... Step: 97450... Loss: 0.001995... Val Loss: 0.000848\n",
      "Epoch: 95/100... Step: 97455... Loss: 0.001955... Val Loss: 0.000767\n",
      "Epoch: 95/100... Step: 97460... Loss: 0.002027... Val Loss: 0.000679\n",
      "Epoch: 95/100... Step: 97465... Loss: 0.001912... Val Loss: 0.000581\n",
      "Epoch: 95/100... Step: 97470... Loss: 0.001849... Val Loss: 0.000477\n",
      "Epoch: 95/100... Step: 97475... Loss: 0.001700... Val Loss: 0.000373\n",
      "Epoch: 95/100... Step: 97480... Loss: 0.001300... Val Loss: 0.000287\n",
      "Epoch: 95/100... Step: 97485... Loss: 0.001146... Val Loss: 0.000257\n",
      "Epoch: 95/100... Step: 97490... Loss: 0.000956... Val Loss: 0.000318\n",
      "Epoch: 95/100... Step: 97495... Loss: 0.000468... Val Loss: 0.000527\n",
      "Epoch: 95/100... Step: 97500... Loss: 0.000160... Val Loss: 0.000815\n",
      "Epoch: 95/100... Step: 97505... Loss: 0.000192... Val Loss: 0.001087\n",
      "Epoch: 95/100... Step: 97510... Loss: 0.000475... Val Loss: 0.001194\n",
      "Epoch: 95/100... Step: 97515... Loss: 0.000664... Val Loss: 0.001165\n",
      "Epoch: 95/100... Step: 97520... Loss: 0.000715... Val Loss: 0.001025\n",
      "Epoch: 95/100... Step: 97525... Loss: 0.000496... Val Loss: 0.000838\n",
      "Epoch: 95/100... Step: 97530... Loss: 0.000150... Val Loss: 0.000639\n",
      "Epoch: 95/100... Step: 97535... Loss: 0.000181... Val Loss: 0.000486\n",
      "Epoch: 95/100... Step: 97540... Loss: 0.000255... Val Loss: 0.000429\n",
      "Epoch: 95/100... Step: 97545... Loss: 0.000193... Val Loss: 0.000432\n",
      "Epoch: 95/100... Step: 97550... Loss: 0.000224... Val Loss: 0.000481\n",
      "Epoch: 95/100... Step: 97555... Loss: 0.000336... Val Loss: 0.000572\n",
      "Epoch: 95/100... Step: 97560... Loss: 0.000212... Val Loss: 0.000696\n",
      "Epoch: 95/100... Step: 97565... Loss: 0.000100... Val Loss: 0.000839\n",
      "Epoch: 95/100... Step: 97570... Loss: 0.000202... Val Loss: 0.000910\n",
      "Epoch: 95/100... Step: 97575... Loss: 0.000102... Val Loss: 0.000905\n",
      "Epoch: 95/100... Step: 97580... Loss: 0.000086... Val Loss: 0.000866\n",
      "Epoch: 95/100... Step: 97585... Loss: 0.000112... Val Loss: 0.000842\n",
      "Epoch: 95/100... Step: 97590... Loss: 0.000070... Val Loss: 0.000831\n",
      "Epoch: 95/100... Step: 97595... Loss: 0.000072... Val Loss: 0.000838\n",
      "Epoch: 95/100... Step: 97600... Loss: 0.000063... Val Loss: 0.000865\n",
      "Epoch: 95/100... Step: 97605... Loss: 0.000075... Val Loss: 0.000908\n",
      "Epoch: 95/100... Step: 97610... Loss: 0.000064... Val Loss: 0.000952\n",
      "Epoch: 95/100... Step: 97615... Loss: 0.000041... Val Loss: 0.000988\n",
      "Epoch: 95/100... Step: 97620... Loss: 0.000115... Val Loss: 0.000988\n",
      "Epoch: 95/100... Step: 97625... Loss: 0.000096... Val Loss: 0.000945\n",
      "Epoch: 95/100... Step: 97630... Loss: 0.000062... Val Loss: 0.000878\n",
      "Epoch: 95/100... Step: 97635... Loss: 0.000105... Val Loss: 0.000839\n",
      "Epoch: 95/100... Step: 97640... Loss: 0.000037... Val Loss: 0.000843\n",
      "Epoch: 95/100... Step: 97645... Loss: 0.000062... Val Loss: 0.000861\n",
      "Epoch: 95/100... Step: 97650... Loss: 0.000174... Val Loss: 0.000855\n",
      "Epoch: 95/100... Step: 97655... Loss: 0.000224... Val Loss: 0.000785\n",
      "Epoch: 95/100... Step: 97660... Loss: 0.000139... Val Loss: 0.000765\n",
      "Epoch: 95/100... Step: 97665... Loss: 0.000137... Val Loss: 0.000750\n",
      "Epoch: 95/100... Step: 97670... Loss: 0.000078... Val Loss: 0.000773\n",
      "Epoch: 95/100... Step: 97675... Loss: 0.000258... Val Loss: 0.000777\n",
      "Epoch: 95/100... Step: 97680... Loss: 0.000391... Val Loss: 0.000737\n",
      "Epoch: 95/100... Step: 97685... Loss: 0.000300... Val Loss: 0.000661\n",
      "Epoch: 95/100... Step: 97690... Loss: 0.000390... Val Loss: 0.000559\n",
      "Epoch: 95/100... Step: 97695... Loss: 0.000291... Val Loss: 0.000444\n",
      "Epoch: 95/100... Step: 97700... Loss: 0.000059... Val Loss: 0.000343\n",
      "Epoch: 95/100... Step: 97705... Loss: 0.000050... Val Loss: 0.000285\n",
      "Epoch: 95/100... Step: 97710... Loss: 0.000104... Val Loss: 0.000266\n",
      "Epoch: 95/100... Step: 97715... Loss: 0.000160... Val Loss: 0.000263\n",
      "Epoch: 95/100... Step: 97720... Loss: 0.000193... Val Loss: 0.000266\n",
      "Epoch: 95/100... Step: 97725... Loss: 0.000208... Val Loss: 0.000279\n",
      "Epoch: 95/100... Step: 97730... Loss: 0.000167... Val Loss: 0.000308\n",
      "Epoch: 95/100... Step: 97735... Loss: 0.000043... Val Loss: 0.000357\n",
      "Epoch: 95/100... Step: 97740... Loss: 0.000067... Val Loss: 0.000413\n",
      "Epoch: 95/100... Step: 97745... Loss: 0.000256... Val Loss: 0.000455\n",
      "Epoch: 95/100... Step: 97750... Loss: 0.000600... Val Loss: 0.000472\n",
      "Epoch: 95/100... Step: 97755... Loss: 0.000693... Val Loss: 0.000472\n",
      "Epoch: 95/100... Step: 97760... Loss: 0.000653... Val Loss: 0.000447\n",
      "Epoch: 95/100... Step: 97765... Loss: 0.000578... Val Loss: 0.000363\n",
      "Epoch: 95/100... Step: 97770... Loss: 0.000463... Val Loss: 0.000290\n",
      "Epoch: 95/100... Step: 97775... Loss: 0.000412... Val Loss: 0.000265\n",
      "Epoch: 95/100... Step: 97780... Loss: 0.000381... Val Loss: 0.000264\n",
      "Epoch: 95/100... Step: 97785... Loss: 0.000325... Val Loss: 0.000281\n",
      "Epoch: 95/100... Step: 97790... Loss: 0.000322... Val Loss: 0.000324\n",
      "Epoch: 95/100... Step: 97795... Loss: 0.000219... Val Loss: 0.000384\n",
      "Epoch: 95/100... Step: 97800... Loss: 0.000091... Val Loss: 0.000461\n",
      "Epoch: 95/100... Step: 97805... Loss: 0.000151... Val Loss: 0.000511\n",
      "Epoch: 95/100... Step: 97810... Loss: 0.000127... Val Loss: 0.000532\n",
      "Epoch: 95/100... Step: 97815... Loss: 0.000150... Val Loss: 0.000548\n",
      "Epoch: 95/100... Step: 97820... Loss: 0.000171... Val Loss: 0.000580\n",
      "Epoch: 95/100... Step: 97825... Loss: 0.000047... Val Loss: 0.000628\n",
      "Epoch: 95/100... Step: 97830... Loss: 0.000055... Val Loss: 0.000672\n",
      "Epoch: 95/100... Step: 97835... Loss: 0.000075... Val Loss: 0.000689\n",
      "Epoch: 95/100... Step: 97840... Loss: 0.000067... Val Loss: 0.000681\n",
      "Epoch: 95/100... Step: 97845... Loss: 0.000067... Val Loss: 0.000659\n",
      "Epoch: 95/100... Step: 97850... Loss: 0.000055... Val Loss: 0.000626\n",
      "Epoch: 95/100... Step: 97855... Loss: 0.000026... Val Loss: 0.000589\n",
      "Epoch: 95/100... Step: 97860... Loss: 0.000065... Val Loss: 0.000571\n",
      "Epoch: 95/100... Step: 97865... Loss: 0.000123... Val Loss: 0.000576\n",
      "Epoch: 95/100... Step: 97870... Loss: 0.000166... Val Loss: 0.000600\n",
      "Epoch: 95/100... Step: 97875... Loss: 0.000158... Val Loss: 0.000637\n",
      "Epoch: 95/100... Step: 97880... Loss: 0.000089... Val Loss: 0.000684\n",
      "Epoch: 95/100... Step: 97885... Loss: 0.000058... Val Loss: 0.000742\n",
      "Epoch: 95/100... Step: 97890... Loss: 0.000039... Val Loss: 0.000804\n",
      "Epoch: 95/100... Step: 97895... Loss: 0.000163... Val Loss: 0.000853\n",
      "Epoch: 95/100... Step: 97900... Loss: 0.000180... Val Loss: 0.000867\n",
      "Epoch: 95/100... Step: 97905... Loss: 0.000163... Val Loss: 0.000855\n",
      "Epoch: 95/100... Step: 97910... Loss: 0.000179... Val Loss: 0.000826\n",
      "Epoch: 95/100... Step: 97915... Loss: 0.000219... Val Loss: 0.000783\n",
      "Epoch: 95/100... Step: 97920... Loss: 0.000185... Val Loss: 0.000731\n",
      "Epoch: 95/100... Step: 97925... Loss: 0.000069... Val Loss: 0.000673\n",
      "Epoch: 95/100... Step: 97930... Loss: 0.000065... Val Loss: 0.000623\n",
      "Epoch: 95/100... Step: 97935... Loss: 0.000153... Val Loss: 0.000602\n",
      "Epoch: 95/100... Step: 97940... Loss: 0.000238... Val Loss: 0.000606\n",
      "Epoch: 95/100... Step: 97945... Loss: 0.000283... Val Loss: 0.000627\n",
      "Epoch: 95/100... Step: 97950... Loss: 0.000226... Val Loss: 0.000660\n",
      "Epoch: 95/100... Step: 97955... Loss: 0.000103... Val Loss: 0.000702\n",
      "Epoch: 95/100... Step: 97960... Loss: 0.000132... Val Loss: 0.000752\n",
      "Epoch: 95/100... Step: 97965... Loss: 0.000187... Val Loss: 0.000809\n",
      "Epoch: 95/100... Step: 97970... Loss: 0.000117... Val Loss: 0.000871\n",
      "Epoch: 95/100... Step: 97975... Loss: 0.000044... Val Loss: 0.000939\n",
      "Epoch: 95/100... Step: 97980... Loss: 0.000027... Val Loss: 0.001004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95/100... Step: 97985... Loss: 0.000093... Val Loss: 0.001044\n",
      "Epoch: 95/100... Step: 97990... Loss: 0.000122... Val Loss: 0.001055\n",
      "Epoch: 95/100... Step: 97995... Loss: 0.000043... Val Loss: 0.001055\n",
      "Epoch: 95/100... Step: 98000... Loss: 0.000019... Val Loss: 0.001056\n",
      "Epoch: 95/100... Step: 98005... Loss: 0.000022... Val Loss: 0.001055\n",
      "Epoch: 95/100... Step: 98010... Loss: 0.000060... Val Loss: 0.001045\n",
      "Epoch: 95/100... Step: 98015... Loss: 0.000149... Val Loss: 0.001024\n",
      "Epoch: 95/100... Step: 98020... Loss: 0.000108... Val Loss: 0.000988\n",
      "Epoch: 95/100... Step: 98025... Loss: 0.000056... Val Loss: 0.000953\n",
      "Epoch: 95/100... Step: 98030... Loss: 0.000112... Val Loss: 0.000942\n",
      "Epoch: 95/100... Step: 98035... Loss: 0.000059... Val Loss: 0.000953\n",
      "Epoch: 95/100... Step: 98040... Loss: 0.000056... Val Loss: 0.000976\n",
      "Epoch: 96/100... Step: 98045... Loss: 0.000381... Val Loss: 0.001008\n",
      "Epoch: 96/100... Step: 98050... Loss: 0.000451... Val Loss: 0.001053\n",
      "Epoch: 96/100... Step: 98055... Loss: 0.000514... Val Loss: 0.001108\n",
      "Epoch: 96/100... Step: 98060... Loss: 0.000629... Val Loss: 0.001169\n",
      "Epoch: 96/100... Step: 98065... Loss: 0.000464... Val Loss: 0.001239\n",
      "Epoch: 96/100... Step: 98070... Loss: 0.000534... Val Loss: 0.001313\n",
      "Epoch: 96/100... Step: 98075... Loss: 0.000437... Val Loss: 0.001392\n",
      "Epoch: 96/100... Step: 98080... Loss: 0.000199... Val Loss: 0.001477\n",
      "Epoch: 96/100... Step: 98085... Loss: 0.000215... Val Loss: 0.001565\n",
      "Epoch: 96/100... Step: 98090... Loss: 0.000115... Val Loss: 0.001659\n",
      "Epoch: 96/100... Step: 98095... Loss: 0.000057... Val Loss: 0.001751\n",
      "Epoch: 96/100... Step: 98100... Loss: 0.000144... Val Loss: 0.001813\n",
      "Epoch: 96/100... Step: 98105... Loss: 0.000280... Val Loss: 0.001835\n",
      "Epoch: 96/100... Step: 98110... Loss: 0.000482... Val Loss: 0.001821\n",
      "Epoch: 96/100... Step: 98115... Loss: 0.000645... Val Loss: 0.001782\n",
      "Epoch: 96/100... Step: 98120... Loss: 0.000444... Val Loss: 0.001727\n",
      "Epoch: 96/100... Step: 98125... Loss: 0.000309... Val Loss: 0.001662\n",
      "Epoch: 96/100... Step: 98130... Loss: 0.000295... Val Loss: 0.001592\n",
      "Epoch: 96/100... Step: 98135... Loss: 0.000367... Val Loss: 0.001520\n",
      "Epoch: 96/100... Step: 98140... Loss: 0.000424... Val Loss: 0.001446\n",
      "Epoch: 96/100... Step: 98145... Loss: 0.000272... Val Loss: 0.001373\n",
      "Epoch: 96/100... Step: 98150... Loss: 0.000172... Val Loss: 0.001300\n",
      "Epoch: 96/100... Step: 98155... Loss: 0.000076... Val Loss: 0.001229\n",
      "Epoch: 96/100... Step: 98160... Loss: 0.000049... Val Loss: 0.001160\n",
      "Epoch: 96/100... Step: 98165... Loss: 0.000139... Val Loss: 0.001109\n",
      "Epoch: 96/100... Step: 98170... Loss: 0.000265... Val Loss: 0.001086\n",
      "Epoch: 96/100... Step: 98175... Loss: 0.000324... Val Loss: 0.001086\n",
      "Epoch: 96/100... Step: 98180... Loss: 0.000379... Val Loss: 0.001104\n",
      "Epoch: 96/100... Step: 98185... Loss: 0.000522... Val Loss: 0.001134\n",
      "Epoch: 96/100... Step: 98190... Loss: 0.000530... Val Loss: 0.001173\n",
      "Epoch: 96/100... Step: 98195... Loss: 0.000377... Val Loss: 0.001221\n",
      "Epoch: 96/100... Step: 98200... Loss: 0.000225... Val Loss: 0.001278\n",
      "Epoch: 96/100... Step: 98205... Loss: 0.000088... Val Loss: 0.001341\n",
      "Epoch: 96/100... Step: 98210... Loss: 0.000052... Val Loss: 0.001402\n",
      "Epoch: 96/100... Step: 98215... Loss: 0.000077... Val Loss: 0.001440\n",
      "Epoch: 96/100... Step: 98220... Loss: 0.000121... Val Loss: 0.001454\n",
      "Epoch: 96/100... Step: 98225... Loss: 0.000199... Val Loss: 0.001444\n",
      "Epoch: 96/100... Step: 98230... Loss: 0.000280... Val Loss: 0.001418\n",
      "Epoch: 96/100... Step: 98235... Loss: 0.000226... Val Loss: 0.001380\n",
      "Epoch: 96/100... Step: 98240... Loss: 0.000192... Val Loss: 0.001333\n",
      "Epoch: 96/100... Step: 98245... Loss: 0.000100... Val Loss: 0.001281\n",
      "Epoch: 96/100... Step: 98250... Loss: 0.000038... Val Loss: 0.001230\n",
      "Epoch: 96/100... Step: 98255... Loss: 0.000084... Val Loss: 0.001199\n",
      "Epoch: 96/100... Step: 98260... Loss: 0.000064... Val Loss: 0.001187\n",
      "Epoch: 96/100... Step: 98265... Loss: 0.000065... Val Loss: 0.001183\n",
      "Epoch: 96/100... Step: 98270... Loss: 0.000109... Val Loss: 0.001179\n",
      "Epoch: 96/100... Step: 98275... Loss: 0.000103... Val Loss: 0.001168\n",
      "Epoch: 96/100... Step: 98280... Loss: 0.000057... Val Loss: 0.001147\n",
      "Epoch: 96/100... Step: 98285... Loss: 0.000068... Val Loss: 0.001133\n",
      "Epoch: 96/100... Step: 98290... Loss: 0.000058... Val Loss: 0.001122\n",
      "Epoch: 96/100... Step: 98295... Loss: 0.000065... Val Loss: 0.001123\n",
      "Epoch: 96/100... Step: 98300... Loss: 0.000129... Val Loss: 0.001130\n",
      "Epoch: 96/100... Step: 98305... Loss: 0.000207... Val Loss: 0.001125\n",
      "Epoch: 96/100... Step: 98310... Loss: 0.000205... Val Loss: 0.001104\n",
      "Epoch: 96/100... Step: 98315... Loss: 0.000212... Val Loss: 0.001071\n",
      "Epoch: 96/100... Step: 98320... Loss: 0.000119... Val Loss: 0.001029\n",
      "Epoch: 96/100... Step: 98325... Loss: 0.000079... Val Loss: 0.000981\n",
      "Epoch: 96/100... Step: 98330... Loss: 0.000217... Val Loss: 0.000930\n",
      "Epoch: 96/100... Step: 98335... Loss: 0.000285... Val Loss: 0.000876\n",
      "Epoch: 96/100... Step: 98340... Loss: 0.000297... Val Loss: 0.000818\n",
      "Epoch: 96/100... Step: 98345... Loss: 0.000106... Val Loss: 0.000758\n",
      "Epoch: 96/100... Step: 98350... Loss: 0.000137... Val Loss: 0.000702\n",
      "Epoch: 96/100... Step: 98355... Loss: 0.000093... Val Loss: 0.000649\n",
      "Epoch: 96/100... Step: 98360... Loss: 0.000039... Val Loss: 0.000594\n",
      "Epoch: 96/100... Step: 98365... Loss: 0.000093... Val Loss: 0.000560\n",
      "Epoch: 96/100... Step: 98370... Loss: 0.000107... Val Loss: 0.000529\n",
      "Epoch: 96/100... Step: 98375... Loss: 0.000050... Val Loss: 0.000486\n",
      "Epoch: 96/100... Step: 98380... Loss: 0.000177... Val Loss: 0.000453\n",
      "Epoch: 96/100... Step: 98385... Loss: 0.000188... Val Loss: 0.000444\n",
      "Epoch: 96/100... Step: 98390... Loss: 0.000113... Val Loss: 0.000431\n",
      "Epoch: 96/100... Step: 98395... Loss: 0.000224... Val Loss: 0.000409\n",
      "Epoch: 96/100... Step: 98400... Loss: 0.000400... Val Loss: 0.000403\n",
      "Epoch: 96/100... Step: 98405... Loss: 0.000584... Val Loss: 0.000418\n",
      "Epoch: 96/100... Step: 98410... Loss: 0.000611... Val Loss: 0.000450\n",
      "Epoch: 96/100... Step: 98415... Loss: 0.000560... Val Loss: 0.000493\n",
      "Epoch: 96/100... Step: 98420... Loss: 0.000584... Val Loss: 0.000544\n",
      "Epoch: 96/100... Step: 98425... Loss: 0.000466... Val Loss: 0.000599\n",
      "Epoch: 96/100... Step: 98430... Loss: 0.000350... Val Loss: 0.000658\n",
      "Epoch: 96/100... Step: 98435... Loss: 0.000345... Val Loss: 0.000716\n",
      "Epoch: 96/100... Step: 98440... Loss: 0.000349... Val Loss: 0.000775\n",
      "Epoch: 96/100... Step: 98445... Loss: 0.000305... Val Loss: 0.000830\n",
      "Epoch: 96/100... Step: 98450... Loss: 0.000484... Val Loss: 0.000868\n",
      "Epoch: 96/100... Step: 98455... Loss: 0.001417... Val Loss: 0.000884\n",
      "Epoch: 96/100... Step: 98460... Loss: 0.002021... Val Loss: 0.000880\n",
      "Epoch: 96/100... Step: 98465... Loss: 0.001926... Val Loss: 0.000864\n",
      "Epoch: 96/100... Step: 98470... Loss: 0.001926... Val Loss: 0.000837\n",
      "Epoch: 96/100... Step: 98475... Loss: 0.001985... Val Loss: 0.000801\n",
      "Epoch: 96/100... Step: 98480... Loss: 0.001926... Val Loss: 0.000759\n",
      "Epoch: 96/100... Step: 98485... Loss: 0.001860... Val Loss: 0.000712\n",
      "Epoch: 96/100... Step: 98490... Loss: 0.001925... Val Loss: 0.000661\n",
      "Epoch: 96/100... Step: 98495... Loss: 0.001932... Val Loss: 0.000607\n",
      "Epoch: 96/100... Step: 98500... Loss: 0.001885... Val Loss: 0.000548\n",
      "Epoch: 96/100... Step: 98505... Loss: 0.001890... Val Loss: 0.000488\n",
      "Epoch: 96/100... Step: 98510... Loss: 0.001624... Val Loss: 0.000428\n",
      "Epoch: 96/100... Step: 98515... Loss: 0.001419... Val Loss: 0.000368\n",
      "Epoch: 96/100... Step: 98520... Loss: 0.001422... Val Loss: 0.000313\n",
      "Epoch: 96/100... Step: 98525... Loss: 0.001131... Val Loss: 0.000277\n",
      "Epoch: 96/100... Step: 98530... Loss: 0.000934... Val Loss: 0.000260\n",
      "Epoch: 96/100... Step: 98535... Loss: 0.000745... Val Loss: 0.000266\n",
      "Epoch: 96/100... Step: 98540... Loss: 0.000495... Val Loss: 0.000314\n",
      "Epoch: 96/100... Step: 98545... Loss: 0.000183... Val Loss: 0.000418\n",
      "Epoch: 96/100... Step: 98550... Loss: 0.000227... Val Loss: 0.000533\n",
      "Epoch: 96/100... Step: 98555... Loss: 0.000212... Val Loss: 0.000574\n",
      "Epoch: 96/100... Step: 98560... Loss: 0.000103... Val Loss: 0.000554\n",
      "Epoch: 96/100... Step: 98565... Loss: 0.000110... Val Loss: 0.000522\n",
      "Epoch: 96/100... Step: 98570... Loss: 0.000166... Val Loss: 0.000535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96/100... Step: 98575... Loss: 0.000093... Val Loss: 0.000583\n",
      "Epoch: 96/100... Step: 98580... Loss: 0.000082... Val Loss: 0.000637\n",
      "Epoch: 96/100... Step: 98585... Loss: 0.000156... Val Loss: 0.000697\n",
      "Epoch: 96/100... Step: 98590... Loss: 0.000112... Val Loss: 0.000782\n",
      "Epoch: 96/100... Step: 98595... Loss: 0.000096... Val Loss: 0.000880\n",
      "Epoch: 96/100... Step: 98600... Loss: 0.000179... Val Loss: 0.000927\n",
      "Epoch: 96/100... Step: 98605... Loss: 0.000142... Val Loss: 0.000910\n",
      "Epoch: 96/100... Step: 98610... Loss: 0.000059... Val Loss: 0.000855\n",
      "Epoch: 96/100... Step: 98615... Loss: 0.000137... Val Loss: 0.000811\n",
      "Epoch: 96/100... Step: 98620... Loss: 0.000101... Val Loss: 0.000799\n",
      "Epoch: 96/100... Step: 98625... Loss: 0.000069... Val Loss: 0.000805\n",
      "Epoch: 96/100... Step: 98630... Loss: 0.000050... Val Loss: 0.000837\n",
      "Epoch: 96/100... Step: 98635... Loss: 0.000072... Val Loss: 0.000885\n",
      "Epoch: 96/100... Step: 98640... Loss: 0.000085... Val Loss: 0.000931\n",
      "Epoch: 96/100... Step: 98645... Loss: 0.000055... Val Loss: 0.000967\n",
      "Epoch: 96/100... Step: 98650... Loss: 0.000074... Val Loss: 0.000981\n",
      "Epoch: 96/100... Step: 98655... Loss: 0.000087... Val Loss: 0.000952\n",
      "Epoch: 96/100... Step: 98660... Loss: 0.000073... Val Loss: 0.000896\n",
      "Epoch: 96/100... Step: 98665... Loss: 0.000075... Val Loss: 0.000855\n",
      "Epoch: 96/100... Step: 98670... Loss: 0.000059... Val Loss: 0.000855\n",
      "Epoch: 96/100... Step: 98675... Loss: 0.000062... Val Loss: 0.000870\n",
      "Epoch: 96/100... Step: 98680... Loss: 0.000143... Val Loss: 0.000873\n",
      "Epoch: 96/100... Step: 98685... Loss: 0.000237... Val Loss: 0.000835\n",
      "Epoch: 96/100... Step: 98690... Loss: 0.000162... Val Loss: 0.000785\n",
      "Epoch: 96/100... Step: 98695... Loss: 0.000163... Val Loss: 0.000755\n",
      "Epoch: 96/100... Step: 98700... Loss: 0.000093... Val Loss: 0.000764\n",
      "Epoch: 96/100... Step: 98705... Loss: 0.000191... Val Loss: 0.000773\n",
      "Epoch: 96/100... Step: 98710... Loss: 0.000376... Val Loss: 0.000748\n",
      "Epoch: 96/100... Step: 98715... Loss: 0.000328... Val Loss: 0.000691\n",
      "Epoch: 96/100... Step: 98720... Loss: 0.000358... Val Loss: 0.000619\n",
      "Epoch: 96/100... Step: 98725... Loss: 0.000414... Val Loss: 0.000533\n",
      "Epoch: 96/100... Step: 98730... Loss: 0.000207... Val Loss: 0.000446\n",
      "Epoch: 96/100... Step: 98735... Loss: 0.000107... Val Loss: 0.000367\n",
      "Epoch: 96/100... Step: 98740... Loss: 0.000055... Val Loss: 0.000309\n",
      "Epoch: 96/100... Step: 98745... Loss: 0.000072... Val Loss: 0.000279\n",
      "Epoch: 96/100... Step: 98750... Loss: 0.000134... Val Loss: 0.000269\n",
      "Epoch: 96/100... Step: 98755... Loss: 0.000235... Val Loss: 0.000268\n",
      "Epoch: 96/100... Step: 98760... Loss: 0.000263... Val Loss: 0.000273\n",
      "Epoch: 96/100... Step: 98765... Loss: 0.000177... Val Loss: 0.000287\n",
      "Epoch: 96/100... Step: 98770... Loss: 0.000118... Val Loss: 0.000312\n",
      "Epoch: 96/100... Step: 98775... Loss: 0.000092... Val Loss: 0.000347\n",
      "Epoch: 96/100... Step: 98780... Loss: 0.000367... Val Loss: 0.000378\n",
      "Epoch: 96/100... Step: 98785... Loss: 0.000589... Val Loss: 0.000388\n",
      "Epoch: 96/100... Step: 98790... Loss: 0.000611... Val Loss: 0.000373\n",
      "Epoch: 96/100... Step: 98795... Loss: 0.000577... Val Loss: 0.000343\n",
      "Epoch: 96/100... Step: 98800... Loss: 0.000548... Val Loss: 0.000309\n",
      "Epoch: 96/100... Step: 98805... Loss: 0.000451... Val Loss: 0.000282\n",
      "Epoch: 96/100... Step: 98810... Loss: 0.000492... Val Loss: 0.000265\n",
      "Epoch: 96/100... Step: 98815... Loss: 0.000410... Val Loss: 0.000258\n",
      "Epoch: 96/100... Step: 98820... Loss: 0.000461... Val Loss: 0.000262\n",
      "Epoch: 96/100... Step: 98825... Loss: 0.000458... Val Loss: 0.000273\n",
      "Epoch: 96/100... Step: 98830... Loss: 0.000320... Val Loss: 0.000294\n",
      "Epoch: 96/100... Step: 98835... Loss: 0.000113... Val Loss: 0.000328\n",
      "Epoch: 96/100... Step: 98840... Loss: 0.000081... Val Loss: 0.000370\n",
      "Epoch: 96/100... Step: 98845... Loss: 0.000250... Val Loss: 0.000414\n",
      "Epoch: 96/100... Step: 98850... Loss: 0.000340... Val Loss: 0.000459\n",
      "Epoch: 96/100... Step: 98855... Loss: 0.000212... Val Loss: 0.000507\n",
      "Epoch: 96/100... Step: 98860... Loss: 0.000098... Val Loss: 0.000559\n",
      "Epoch: 96/100... Step: 98865... Loss: 0.000026... Val Loss: 0.000612\n",
      "Epoch: 96/100... Step: 98870... Loss: 0.000028... Val Loss: 0.000654\n",
      "Epoch: 96/100... Step: 98875... Loss: 0.000075... Val Loss: 0.000677\n",
      "Epoch: 96/100... Step: 98880... Loss: 0.000121... Val Loss: 0.000685\n",
      "Epoch: 96/100... Step: 98885... Loss: 0.000092... Val Loss: 0.000678\n",
      "Epoch: 96/100... Step: 98890... Loss: 0.000053... Val Loss: 0.000659\n",
      "Epoch: 96/100... Step: 98895... Loss: 0.000039... Val Loss: 0.000634\n",
      "Epoch: 96/100... Step: 98900... Loss: 0.000120... Val Loss: 0.000621\n",
      "Epoch: 96/100... Step: 98905... Loss: 0.000169... Val Loss: 0.000623\n",
      "Epoch: 96/100... Step: 98910... Loss: 0.000144... Val Loss: 0.000637\n",
      "Epoch: 96/100... Step: 98915... Loss: 0.000128... Val Loss: 0.000660\n",
      "Epoch: 96/100... Step: 98920... Loss: 0.000133... Val Loss: 0.000691\n",
      "Epoch: 96/100... Step: 98925... Loss: 0.000063... Val Loss: 0.000727\n",
      "Epoch: 96/100... Step: 98930... Loss: 0.000073... Val Loss: 0.000753\n",
      "Epoch: 96/100... Step: 98935... Loss: 0.000064... Val Loss: 0.000762\n",
      "Epoch: 96/100... Step: 98940... Loss: 0.000089... Val Loss: 0.000760\n",
      "Epoch: 96/100... Step: 98945... Loss: 0.000138... Val Loss: 0.000746\n",
      "Epoch: 96/100... Step: 98950... Loss: 0.000188... Val Loss: 0.000723\n",
      "Epoch: 96/100... Step: 98955... Loss: 0.000118... Val Loss: 0.000693\n",
      "Epoch: 96/100... Step: 98960... Loss: 0.000029... Val Loss: 0.000660\n",
      "Epoch: 96/100... Step: 98965... Loss: 0.000089... Val Loss: 0.000640\n",
      "Epoch: 96/100... Step: 98970... Loss: 0.000161... Val Loss: 0.000636\n",
      "Epoch: 96/100... Step: 98975... Loss: 0.000232... Val Loss: 0.000646\n",
      "Epoch: 96/100... Step: 98980... Loss: 0.000242... Val Loss: 0.000665\n",
      "Epoch: 96/100... Step: 98985... Loss: 0.000115... Val Loss: 0.000691\n",
      "Epoch: 96/100... Step: 98990... Loss: 0.000110... Val Loss: 0.000724\n",
      "Epoch: 96/100... Step: 98995... Loss: 0.000194... Val Loss: 0.000761\n",
      "Epoch: 96/100... Step: 99000... Loss: 0.000197... Val Loss: 0.000802\n",
      "Epoch: 96/100... Step: 99005... Loss: 0.000132... Val Loss: 0.000847\n",
      "Epoch: 96/100... Step: 99010... Loss: 0.000103... Val Loss: 0.000895\n",
      "Epoch: 96/100... Step: 99015... Loss: 0.000043... Val Loss: 0.000945\n",
      "Epoch: 96/100... Step: 99020... Loss: 0.000069... Val Loss: 0.000989\n",
      "Epoch: 96/100... Step: 99025... Loss: 0.000090... Val Loss: 0.001014\n",
      "Epoch: 96/100... Step: 99030... Loss: 0.000029... Val Loss: 0.001040\n",
      "Epoch: 96/100... Step: 99035... Loss: 0.000021... Val Loss: 0.001066\n",
      "Epoch: 96/100... Step: 99040... Loss: 0.000063... Val Loss: 0.001079\n",
      "Epoch: 96/100... Step: 99045... Loss: 0.000152... Val Loss: 0.001075\n",
      "Epoch: 96/100... Step: 99050... Loss: 0.000199... Val Loss: 0.001057\n",
      "Epoch: 96/100... Step: 99055... Loss: 0.000073... Val Loss: 0.001029\n",
      "Epoch: 96/100... Step: 99060... Loss: 0.000063... Val Loss: 0.001000\n",
      "Epoch: 96/100... Step: 99065... Loss: 0.000054... Val Loss: 0.000982\n",
      "Epoch: 96/100... Step: 99070... Loss: 0.000049... Val Loss: 0.000975\n",
      "Epoch: 97/100... Step: 99075... Loss: 0.000354... Val Loss: 0.000976\n",
      "Epoch: 97/100... Step: 99080... Loss: 0.000482... Val Loss: 0.000991\n",
      "Epoch: 97/100... Step: 99085... Loss: 0.000538... Val Loss: 0.001016\n",
      "Epoch: 97/100... Step: 99090... Loss: 0.000722... Val Loss: 0.001050\n",
      "Epoch: 97/100... Step: 99095... Loss: 0.000687... Val Loss: 0.001090\n",
      "Epoch: 97/100... Step: 99100... Loss: 0.000633... Val Loss: 0.001136\n",
      "Epoch: 97/100... Step: 99105... Loss: 0.000695... Val Loss: 0.001187\n",
      "Epoch: 97/100... Step: 99110... Loss: 0.000453... Val Loss: 0.001242\n",
      "Epoch: 97/100... Step: 99115... Loss: 0.000417... Val Loss: 0.001302\n",
      "Epoch: 97/100... Step: 99120... Loss: 0.000434... Val Loss: 0.001367\n",
      "Epoch: 97/100... Step: 99125... Loss: 0.000302... Val Loss: 0.001436\n",
      "Epoch: 97/100... Step: 99130... Loss: 0.000177... Val Loss: 0.001510\n",
      "Epoch: 97/100... Step: 99135... Loss: 0.000038... Val Loss: 0.001588\n",
      "Epoch: 97/100... Step: 99140... Loss: 0.000183... Val Loss: 0.001642\n",
      "Epoch: 97/100... Step: 99145... Loss: 0.000476... Val Loss: 0.001661\n",
      "Epoch: 97/100... Step: 99150... Loss: 0.000428... Val Loss: 0.001652\n",
      "Epoch: 97/100... Step: 99155... Loss: 0.000255... Val Loss: 0.001623\n",
      "Epoch: 97/100... Step: 99160... Loss: 0.000233... Val Loss: 0.001581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97/100... Step: 99165... Loss: 0.000271... Val Loss: 0.001530\n",
      "Epoch: 97/100... Step: 99170... Loss: 0.000393... Val Loss: 0.001474\n",
      "Epoch: 97/100... Step: 99175... Loss: 0.000330... Val Loss: 0.001416\n",
      "Epoch: 97/100... Step: 99180... Loss: 0.000225... Val Loss: 0.001358\n",
      "Epoch: 97/100... Step: 99185... Loss: 0.000159... Val Loss: 0.001300\n",
      "Epoch: 97/100... Step: 99190... Loss: 0.000113... Val Loss: 0.001244\n",
      "Epoch: 97/100... Step: 99195... Loss: 0.000058... Val Loss: 0.001192\n",
      "Epoch: 97/100... Step: 99200... Loss: 0.000153... Val Loss: 0.001159\n",
      "Epoch: 97/100... Step: 99205... Loss: 0.000240... Val Loss: 0.001146\n",
      "Epoch: 97/100... Step: 99210... Loss: 0.000265... Val Loss: 0.001149\n",
      "Epoch: 97/100... Step: 99215... Loss: 0.000426... Val Loss: 0.001163\n",
      "Epoch: 97/100... Step: 99220... Loss: 0.000512... Val Loss: 0.001185\n",
      "Epoch: 97/100... Step: 99225... Loss: 0.000433... Val Loss: 0.001214\n",
      "Epoch: 97/100... Step: 99230... Loss: 0.000276... Val Loss: 0.001248\n",
      "Epoch: 97/100... Step: 99235... Loss: 0.000158... Val Loss: 0.001288\n",
      "Epoch: 97/100... Step: 99240... Loss: 0.000053... Val Loss: 0.001330\n",
      "Epoch: 97/100... Step: 99245... Loss: 0.000050... Val Loss: 0.001370\n",
      "Epoch: 97/100... Step: 99250... Loss: 0.000054... Val Loss: 0.001399\n",
      "Epoch: 97/100... Step: 99255... Loss: 0.000122... Val Loss: 0.001410\n",
      "Epoch: 97/100... Step: 99260... Loss: 0.000224... Val Loss: 0.001406\n",
      "Epoch: 97/100... Step: 99265... Loss: 0.000245... Val Loss: 0.001390\n",
      "Epoch: 97/100... Step: 99270... Loss: 0.000210... Val Loss: 0.001365\n",
      "Epoch: 97/100... Step: 99275... Loss: 0.000161... Val Loss: 0.001334\n",
      "Epoch: 97/100... Step: 99280... Loss: 0.000068... Val Loss: 0.001299\n",
      "Epoch: 97/100... Step: 99285... Loss: 0.000033... Val Loss: 0.001264\n",
      "Epoch: 97/100... Step: 99290... Loss: 0.000042... Val Loss: 0.001242\n",
      "Epoch: 97/100... Step: 99295... Loss: 0.000054... Val Loss: 0.001228\n",
      "Epoch: 97/100... Step: 99300... Loss: 0.000094... Val Loss: 0.001215\n",
      "Epoch: 97/100... Step: 99305... Loss: 0.000143... Val Loss: 0.001200\n",
      "Epoch: 97/100... Step: 99310... Loss: 0.000091... Val Loss: 0.001181\n",
      "Epoch: 97/100... Step: 99315... Loss: 0.000065... Val Loss: 0.001161\n",
      "Epoch: 97/100... Step: 99320... Loss: 0.000057... Val Loss: 0.001142\n",
      "Epoch: 97/100... Step: 99325... Loss: 0.000052... Val Loss: 0.001129\n",
      "Epoch: 97/100... Step: 99330... Loss: 0.000088... Val Loss: 0.001126\n",
      "Epoch: 97/100... Step: 99335... Loss: 0.000184... Val Loss: 0.001121\n",
      "Epoch: 97/100... Step: 99340... Loss: 0.000202... Val Loss: 0.001107\n",
      "Epoch: 97/100... Step: 99345... Loss: 0.000203... Val Loss: 0.001085\n",
      "Epoch: 97/100... Step: 99350... Loss: 0.000164... Val Loss: 0.001058\n",
      "Epoch: 97/100... Step: 99355... Loss: 0.000085... Val Loss: 0.001026\n",
      "Epoch: 97/100... Step: 99360... Loss: 0.000197... Val Loss: 0.000992\n",
      "Epoch: 97/100... Step: 99365... Loss: 0.000348... Val Loss: 0.000955\n",
      "Epoch: 97/100... Step: 99370... Loss: 0.000391... Val Loss: 0.000918\n",
      "Epoch: 97/100... Step: 99375... Loss: 0.000289... Val Loss: 0.000881\n",
      "Epoch: 97/100... Step: 99380... Loss: 0.000210... Val Loss: 0.000843\n",
      "Epoch: 97/100... Step: 99385... Loss: 0.000291... Val Loss: 0.000805\n",
      "Epoch: 97/100... Step: 99390... Loss: 0.000178... Val Loss: 0.000766\n",
      "Epoch: 97/100... Step: 99395... Loss: 0.000141... Val Loss: 0.000727\n",
      "Epoch: 97/100... Step: 99400... Loss: 0.000260... Val Loss: 0.000688\n",
      "Epoch: 97/100... Step: 99405... Loss: 0.000237... Val Loss: 0.000649\n",
      "Epoch: 97/100... Step: 99410... Loss: 0.000111... Val Loss: 0.000609\n",
      "Epoch: 97/100... Step: 99415... Loss: 0.000135... Val Loss: 0.000577\n",
      "Epoch: 97/100... Step: 99420... Loss: 0.000241... Val Loss: 0.000555\n",
      "Epoch: 97/100... Step: 99425... Loss: 0.000213... Val Loss: 0.000531\n",
      "Epoch: 97/100... Step: 99430... Loss: 0.000215... Val Loss: 0.000512\n",
      "Epoch: 97/100... Step: 99435... Loss: 0.000381... Val Loss: 0.000507\n",
      "Epoch: 97/100... Step: 99440... Loss: 0.000520... Val Loss: 0.000512\n",
      "Epoch: 97/100... Step: 99445... Loss: 0.000495... Val Loss: 0.000526\n",
      "Epoch: 97/100... Step: 99450... Loss: 0.000528... Val Loss: 0.000546\n",
      "Epoch: 97/100... Step: 99455... Loss: 0.000544... Val Loss: 0.000571\n",
      "Epoch: 97/100... Step: 99460... Loss: 0.000408... Val Loss: 0.000599\n",
      "Epoch: 97/100... Step: 99465... Loss: 0.000402... Val Loss: 0.000630\n",
      "Epoch: 97/100... Step: 99470... Loss: 0.000440... Val Loss: 0.000662\n",
      "Epoch: 97/100... Step: 99475... Loss: 0.000340... Val Loss: 0.000695\n",
      "Epoch: 97/100... Step: 99480... Loss: 0.000272... Val Loss: 0.000722\n",
      "Epoch: 97/100... Step: 99485... Loss: 0.000871... Val Loss: 0.000736\n",
      "Epoch: 97/100... Step: 99490... Loss: 0.001776... Val Loss: 0.000738\n",
      "Epoch: 97/100... Step: 99495... Loss: 0.001805... Val Loss: 0.000731\n",
      "Epoch: 97/100... Step: 99500... Loss: 0.001771... Val Loss: 0.000717\n",
      "Epoch: 97/100... Step: 99505... Loss: 0.001837... Val Loss: 0.000699\n",
      "Epoch: 97/100... Step: 99510... Loss: 0.001842... Val Loss: 0.000676\n",
      "Epoch: 97/100... Step: 99515... Loss: 0.001771... Val Loss: 0.000650\n",
      "Epoch: 97/100... Step: 99520... Loss: 0.001823... Val Loss: 0.000621\n",
      "Epoch: 97/100... Step: 99525... Loss: 0.001899... Val Loss: 0.000591\n",
      "Epoch: 97/100... Step: 99530... Loss: 0.001864... Val Loss: 0.000559\n",
      "Epoch: 97/100... Step: 99535... Loss: 0.001913... Val Loss: 0.000527\n",
      "Epoch: 97/100... Step: 99540... Loss: 0.001804... Val Loss: 0.000495\n",
      "Epoch: 97/100... Step: 99545... Loss: 0.001551... Val Loss: 0.000463\n",
      "Epoch: 97/100... Step: 99550... Loss: 0.001553... Val Loss: 0.000431\n",
      "Epoch: 97/100... Step: 99555... Loss: 0.001492... Val Loss: 0.000400\n",
      "Epoch: 97/100... Step: 99560... Loss: 0.001218... Val Loss: 0.000369\n",
      "Epoch: 97/100... Step: 99565... Loss: 0.001164... Val Loss: 0.000339\n",
      "Epoch: 97/100... Step: 99570... Loss: 0.000996... Val Loss: 0.000313\n",
      "Epoch: 97/100... Step: 99575... Loss: 0.000773... Val Loss: 0.000292\n",
      "Epoch: 97/100... Step: 99580... Loss: 0.000493... Val Loss: 0.000277\n",
      "Epoch: 97/100... Step: 99585... Loss: 0.000390... Val Loss: 0.000268\n",
      "Epoch: 97/100... Step: 99590... Loss: 0.000448... Val Loss: 0.000261\n",
      "Epoch: 97/100... Step: 99595... Loss: 0.000486... Val Loss: 0.000258\n",
      "Epoch: 97/100... Step: 99600... Loss: 0.000571... Val Loss: 0.000262\n",
      "Epoch: 97/100... Step: 99605... Loss: 0.000499... Val Loss: 0.000273\n",
      "Epoch: 97/100... Step: 99610... Loss: 0.000375... Val Loss: 0.000298\n",
      "Epoch: 97/100... Step: 99615... Loss: 0.000468... Val Loss: 0.000334\n",
      "Epoch: 97/100... Step: 99620... Loss: 0.000519... Val Loss: 0.000383\n",
      "Epoch: 97/100... Step: 99625... Loss: 0.000441... Val Loss: 0.000445\n",
      "Epoch: 97/100... Step: 99630... Loss: 0.000242... Val Loss: 0.000517\n",
      "Epoch: 97/100... Step: 99635... Loss: 0.000166... Val Loss: 0.000597\n",
      "Epoch: 97/100... Step: 99640... Loss: 0.000136... Val Loss: 0.000684\n",
      "Epoch: 97/100... Step: 99645... Loss: 0.000170... Val Loss: 0.000776\n",
      "Epoch: 97/100... Step: 99650... Loss: 0.000106... Val Loss: 0.000867\n",
      "Epoch: 97/100... Step: 99655... Loss: 0.000067... Val Loss: 0.000934\n",
      "Epoch: 97/100... Step: 99660... Loss: 0.000101... Val Loss: 0.000965\n",
      "Epoch: 97/100... Step: 99665... Loss: 0.000068... Val Loss: 0.000966\n",
      "Epoch: 97/100... Step: 99670... Loss: 0.000060... Val Loss: 0.000953\n",
      "Epoch: 97/100... Step: 99675... Loss: 0.000059... Val Loss: 0.000936\n",
      "Epoch: 97/100... Step: 99680... Loss: 0.000077... Val Loss: 0.000929\n",
      "Epoch: 97/100... Step: 99685... Loss: 0.000067... Val Loss: 0.000923\n",
      "Epoch: 97/100... Step: 99690... Loss: 0.000055... Val Loss: 0.000901\n",
      "Epoch: 97/100... Step: 99695... Loss: 0.000066... Val Loss: 0.000879\n",
      "Epoch: 97/100... Step: 99700... Loss: 0.000058... Val Loss: 0.000877\n",
      "Epoch: 97/100... Step: 99705... Loss: 0.000044... Val Loss: 0.000886\n",
      "Epoch: 97/100... Step: 99710... Loss: 0.000062... Val Loss: 0.000890\n",
      "Epoch: 97/100... Step: 99715... Loss: 0.000198... Val Loss: 0.000875\n",
      "Epoch: 97/100... Step: 99720... Loss: 0.000209... Val Loss: 0.000838\n",
      "Epoch: 97/100... Step: 99725... Loss: 0.000106... Val Loss: 0.000815\n",
      "Epoch: 97/100... Step: 99730... Loss: 0.000067... Val Loss: 0.000806\n",
      "Epoch: 97/100... Step: 99735... Loss: 0.000111... Val Loss: 0.000805\n",
      "Epoch: 97/100... Step: 99740... Loss: 0.000306... Val Loss: 0.000788\n",
      "Epoch: 97/100... Step: 99745... Loss: 0.000425... Val Loss: 0.000754\n",
      "Epoch: 97/100... Step: 99750... Loss: 0.000374... Val Loss: 0.000710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97/100... Step: 99755... Loss: 0.000506... Val Loss: 0.000658\n",
      "Epoch: 97/100... Step: 99760... Loss: 0.000393... Val Loss: 0.000603\n",
      "Epoch: 97/100... Step: 99765... Loss: 0.000262... Val Loss: 0.000546\n",
      "Epoch: 97/100... Step: 99770... Loss: 0.000228... Val Loss: 0.000488\n",
      "Epoch: 97/100... Step: 99775... Loss: 0.000187... Val Loss: 0.000433\n",
      "Epoch: 97/100... Step: 99780... Loss: 0.000091... Val Loss: 0.000384\n",
      "Epoch: 97/100... Step: 99785... Loss: 0.000048... Val Loss: 0.000346\n",
      "Epoch: 97/100... Step: 99790... Loss: 0.000114... Val Loss: 0.000326\n",
      "Epoch: 97/100... Step: 99795... Loss: 0.000118... Val Loss: 0.000320\n",
      "Epoch: 97/100... Step: 99800... Loss: 0.000075... Val Loss: 0.000321\n",
      "Epoch: 97/100... Step: 99805... Loss: 0.000096... Val Loss: 0.000330\n",
      "Epoch: 97/100... Step: 99810... Loss: 0.000222... Val Loss: 0.000338\n",
      "Epoch: 97/100... Step: 99815... Loss: 0.000529... Val Loss: 0.000338\n",
      "Epoch: 97/100... Step: 99820... Loss: 0.000555... Val Loss: 0.000330\n",
      "Epoch: 97/100... Step: 99825... Loss: 0.000533... Val Loss: 0.000318\n",
      "Epoch: 97/100... Step: 99830... Loss: 0.000539... Val Loss: 0.000304\n",
      "Epoch: 97/100... Step: 99835... Loss: 0.000458... Val Loss: 0.000289\n",
      "Epoch: 97/100... Step: 99840... Loss: 0.000498... Val Loss: 0.000276\n",
      "Epoch: 97/100... Step: 99845... Loss: 0.000504... Val Loss: 0.000266\n",
      "Epoch: 97/100... Step: 99850... Loss: 0.000528... Val Loss: 0.000260\n",
      "Epoch: 97/100... Step: 99855... Loss: 0.000584... Val Loss: 0.000258\n",
      "Epoch: 97/100... Step: 99860... Loss: 0.000504... Val Loss: 0.000259\n",
      "Epoch: 97/100... Step: 99865... Loss: 0.000361... Val Loss: 0.000263\n",
      "Epoch: 97/100... Step: 99870... Loss: 0.000241... Val Loss: 0.000269\n",
      "Epoch: 97/100... Step: 99875... Loss: 0.000372... Val Loss: 0.000278\n",
      "Epoch: 97/100... Step: 99880... Loss: 0.000563... Val Loss: 0.000290\n",
      "Epoch: 97/100... Step: 99885... Loss: 0.000518... Val Loss: 0.000305\n",
      "Epoch: 97/100... Step: 99890... Loss: 0.000410... Val Loss: 0.000325\n",
      "Epoch: 97/100... Step: 99895... Loss: 0.000345... Val Loss: 0.000347\n",
      "Epoch: 97/100... Step: 99900... Loss: 0.000314... Val Loss: 0.000372\n",
      "Epoch: 97/100... Step: 99905... Loss: 0.000280... Val Loss: 0.000398\n",
      "Epoch: 97/100... Step: 99910... Loss: 0.000228... Val Loss: 0.000425\n",
      "Epoch: 97/100... Step: 99915... Loss: 0.000172... Val Loss: 0.000453\n",
      "Epoch: 97/100... Step: 99920... Loss: 0.000157... Val Loss: 0.000482\n",
      "Epoch: 97/100... Step: 99925... Loss: 0.000153... Val Loss: 0.000511\n",
      "Epoch: 97/100... Step: 99930... Loss: 0.000180... Val Loss: 0.000542\n",
      "Epoch: 97/100... Step: 99935... Loss: 0.000200... Val Loss: 0.000574\n",
      "Epoch: 97/100... Step: 99940... Loss: 0.000186... Val Loss: 0.000608\n",
      "Epoch: 97/100... Step: 99945... Loss: 0.000140... Val Loss: 0.000641\n",
      "Epoch: 97/100... Step: 99950... Loss: 0.000142... Val Loss: 0.000675\n",
      "Epoch: 97/100... Step: 99955... Loss: 0.000096... Val Loss: 0.000710\n",
      "Epoch: 97/100... Step: 99960... Loss: 0.000063... Val Loss: 0.000740\n",
      "Epoch: 97/100... Step: 99965... Loss: 0.000067... Val Loss: 0.000758\n",
      "Epoch: 97/100... Step: 99970... Loss: 0.000080... Val Loss: 0.000766\n",
      "Epoch: 97/100... Step: 99975... Loss: 0.000115... Val Loss: 0.000764\n",
      "Epoch: 97/100... Step: 99980... Loss: 0.000202... Val Loss: 0.000753\n",
      "Epoch: 97/100... Step: 99985... Loss: 0.000187... Val Loss: 0.000737\n",
      "Epoch: 97/100... Step: 99990... Loss: 0.000096... Val Loss: 0.000717\n",
      "Epoch: 97/100... Step: 99995... Loss: 0.000033... Val Loss: 0.000695\n",
      "Epoch: 97/100... Step: 100000... Loss: 0.000085... Val Loss: 0.000679\n",
      "Epoch: 97/100... Step: 100005... Loss: 0.000171... Val Loss: 0.000673\n",
      "Epoch: 97/100... Step: 100010... Loss: 0.000215... Val Loss: 0.000677\n",
      "Epoch: 97/100... Step: 100015... Loss: 0.000170... Val Loss: 0.000686\n",
      "Epoch: 97/100... Step: 100020... Loss: 0.000091... Val Loss: 0.000700\n",
      "Epoch: 97/100... Step: 100025... Loss: 0.000188... Val Loss: 0.000719\n",
      "Epoch: 97/100... Step: 100030... Loss: 0.000256... Val Loss: 0.000740\n",
      "Epoch: 97/100... Step: 100035... Loss: 0.000221... Val Loss: 0.000765\n",
      "Epoch: 97/100... Step: 100040... Loss: 0.000190... Val Loss: 0.000792\n",
      "Epoch: 97/100... Step: 100045... Loss: 0.000168... Val Loss: 0.000822\n",
      "Epoch: 97/100... Step: 100050... Loss: 0.000092... Val Loss: 0.000854\n",
      "Epoch: 97/100... Step: 100055... Loss: 0.000101... Val Loss: 0.000887\n",
      "Epoch: 97/100... Step: 100060... Loss: 0.000160... Val Loss: 0.000919\n",
      "Epoch: 97/100... Step: 100065... Loss: 0.000115... Val Loss: 0.000954\n",
      "Epoch: 97/100... Step: 100070... Loss: 0.000058... Val Loss: 0.000993\n",
      "Epoch: 97/100... Step: 100075... Loss: 0.000070... Val Loss: 0.001028\n",
      "Epoch: 97/100... Step: 100080... Loss: 0.000174... Val Loss: 0.001048\n",
      "Epoch: 97/100... Step: 100085... Loss: 0.000142... Val Loss: 0.001053\n",
      "Epoch: 97/100... Step: 100090... Loss: 0.000058... Val Loss: 0.001046\n",
      "Epoch: 97/100... Step: 100095... Loss: 0.000056... Val Loss: 0.001039\n",
      "Epoch: 97/100... Step: 100100... Loss: 0.000038... Val Loss: 0.001033\n",
      "Epoch: 98/100... Step: 100105... Loss: 0.000243... Val Loss: 0.001023\n",
      "Epoch: 98/100... Step: 100110... Loss: 0.000392... Val Loss: 0.001021\n",
      "Epoch: 98/100... Step: 100115... Loss: 0.000475... Val Loss: 0.001031\n",
      "Epoch: 98/100... Step: 100120... Loss: 0.000645... Val Loss: 0.001048\n",
      "Epoch: 98/100... Step: 100125... Loss: 0.000697... Val Loss: 0.001072\n",
      "Epoch: 98/100... Step: 100130... Loss: 0.000613... Val Loss: 0.001103\n",
      "Epoch: 98/100... Step: 100135... Loss: 0.000711... Val Loss: 0.001138\n",
      "Epoch: 98/100... Step: 100140... Loss: 0.000595... Val Loss: 0.001176\n",
      "Epoch: 98/100... Step: 100145... Loss: 0.000453... Val Loss: 0.001219\n",
      "Epoch: 98/100... Step: 100150... Loss: 0.000538... Val Loss: 0.001265\n",
      "Epoch: 98/100... Step: 100155... Loss: 0.000426... Val Loss: 0.001314\n",
      "Epoch: 98/100... Step: 100160... Loss: 0.000362... Val Loss: 0.001366\n",
      "Epoch: 98/100... Step: 100165... Loss: 0.000239... Val Loss: 0.001421\n",
      "Epoch: 98/100... Step: 100170... Loss: 0.000092... Val Loss: 0.001480\n",
      "Epoch: 98/100... Step: 100175... Loss: 0.000258... Val Loss: 0.001533\n",
      "Epoch: 98/100... Step: 100180... Loss: 0.000398... Val Loss: 0.001561\n",
      "Epoch: 98/100... Step: 100185... Loss: 0.000258... Val Loss: 0.001565\n",
      "Epoch: 98/100... Step: 100190... Loss: 0.000201... Val Loss: 0.001552\n",
      "Epoch: 98/100... Step: 100195... Loss: 0.000222... Val Loss: 0.001527\n",
      "Epoch: 98/100... Step: 100200... Loss: 0.000348... Val Loss: 0.001493\n",
      "Epoch: 98/100... Step: 100205... Loss: 0.000404... Val Loss: 0.001454\n",
      "Epoch: 98/100... Step: 100210... Loss: 0.000283... Val Loss: 0.001412\n",
      "Epoch: 98/100... Step: 100215... Loss: 0.000233... Val Loss: 0.001368\n",
      "Epoch: 98/100... Step: 100220... Loss: 0.000181... Val Loss: 0.001322\n",
      "Epoch: 98/100... Step: 100225... Loss: 0.000117... Val Loss: 0.001277\n",
      "Epoch: 98/100... Step: 100230... Loss: 0.000051... Val Loss: 0.001236\n",
      "Epoch: 98/100... Step: 100235... Loss: 0.000163... Val Loss: 0.001209\n",
      "Epoch: 98/100... Step: 100240... Loss: 0.000218... Val Loss: 0.001199\n",
      "Epoch: 98/100... Step: 100245... Loss: 0.000303... Val Loss: 0.001200\n",
      "Epoch: 98/100... Step: 100250... Loss: 0.000477... Val Loss: 0.001211\n",
      "Epoch: 98/100... Step: 100255... Loss: 0.000440... Val Loss: 0.001229\n",
      "Epoch: 98/100... Step: 100260... Loss: 0.000305... Val Loss: 0.001251\n",
      "Epoch: 98/100... Step: 100265... Loss: 0.000172... Val Loss: 0.001278\n",
      "Epoch: 98/100... Step: 100270... Loss: 0.000081... Val Loss: 0.001310\n",
      "Epoch: 98/100... Step: 100275... Loss: 0.000061... Val Loss: 0.001342\n",
      "Epoch: 98/100... Step: 100280... Loss: 0.000037... Val Loss: 0.001370\n",
      "Epoch: 98/100... Step: 100285... Loss: 0.000057... Val Loss: 0.001388\n",
      "Epoch: 98/100... Step: 100290... Loss: 0.000161... Val Loss: 0.001392\n",
      "Epoch: 98/100... Step: 100295... Loss: 0.000242... Val Loss: 0.001384\n",
      "Epoch: 98/100... Step: 100300... Loss: 0.000211... Val Loss: 0.001368\n",
      "Epoch: 98/100... Step: 100305... Loss: 0.000190... Val Loss: 0.001345\n",
      "Epoch: 98/100... Step: 100310... Loss: 0.000124... Val Loss: 0.001318\n",
      "Epoch: 98/100... Step: 100315... Loss: 0.000038... Val Loss: 0.001287\n",
      "Epoch: 98/100... Step: 100320... Loss: 0.000042... Val Loss: 0.001262\n",
      "Epoch: 98/100... Step: 100325... Loss: 0.000039... Val Loss: 0.001246\n",
      "Epoch: 98/100... Step: 100330... Loss: 0.000064... Val Loss: 0.001230\n",
      "Epoch: 98/100... Step: 100335... Loss: 0.000137... Val Loss: 0.001215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98/100... Step: 100340... Loss: 0.000130... Val Loss: 0.001197\n",
      "Epoch: 98/100... Step: 100345... Loss: 0.000053... Val Loss: 0.001178\n",
      "Epoch: 98/100... Step: 100350... Loss: 0.000070... Val Loss: 0.001161\n",
      "Epoch: 98/100... Step: 100355... Loss: 0.000048... Val Loss: 0.001146\n",
      "Epoch: 98/100... Step: 100360... Loss: 0.000064... Val Loss: 0.001138\n",
      "Epoch: 98/100... Step: 100365... Loss: 0.000160... Val Loss: 0.001133\n",
      "Epoch: 98/100... Step: 100370... Loss: 0.000219... Val Loss: 0.001122\n",
      "Epoch: 98/100... Step: 100375... Loss: 0.000203... Val Loss: 0.001106\n",
      "Epoch: 98/100... Step: 100380... Loss: 0.000208... Val Loss: 0.001086\n",
      "Epoch: 98/100... Step: 100385... Loss: 0.000138... Val Loss: 0.001063\n",
      "Epoch: 98/100... Step: 100390... Loss: 0.000156... Val Loss: 0.001039\n",
      "Epoch: 98/100... Step: 100395... Loss: 0.000345... Val Loss: 0.001013\n",
      "Epoch: 98/100... Step: 100400... Loss: 0.000416... Val Loss: 0.000986\n",
      "Epoch: 98/100... Step: 100405... Loss: 0.000422... Val Loss: 0.000959\n",
      "Epoch: 98/100... Step: 100410... Loss: 0.000288... Val Loss: 0.000931\n",
      "Epoch: 98/100... Step: 100415... Loss: 0.000356... Val Loss: 0.000906\n",
      "Epoch: 98/100... Step: 100420... Loss: 0.000327... Val Loss: 0.000881\n",
      "Epoch: 98/100... Step: 100425... Loss: 0.000255... Val Loss: 0.000856\n",
      "Epoch: 98/100... Step: 100430... Loss: 0.000360... Val Loss: 0.000831\n",
      "Epoch: 98/100... Step: 100435... Loss: 0.000430... Val Loss: 0.000807\n",
      "Epoch: 98/100... Step: 100440... Loss: 0.000322... Val Loss: 0.000783\n",
      "Epoch: 98/100... Step: 100445... Loss: 0.000156... Val Loss: 0.000759\n",
      "Epoch: 98/100... Step: 100450... Loss: 0.000324... Val Loss: 0.000736\n",
      "Epoch: 98/100... Step: 100455... Loss: 0.000411... Val Loss: 0.000714\n",
      "Epoch: 98/100... Step: 100460... Loss: 0.000135... Val Loss: 0.000691\n",
      "Epoch: 98/100... Step: 100465... Loss: 0.000097... Val Loss: 0.000673\n",
      "Epoch: 98/100... Step: 100470... Loss: 0.000291... Val Loss: 0.000662\n",
      "Epoch: 98/100... Step: 100475... Loss: 0.000345... Val Loss: 0.000659\n",
      "Epoch: 98/100... Step: 100480... Loss: 0.000363... Val Loss: 0.000662\n",
      "Epoch: 98/100... Step: 100485... Loss: 0.000437... Val Loss: 0.000670\n",
      "Epoch: 98/100... Step: 100490... Loss: 0.000356... Val Loss: 0.000680\n",
      "Epoch: 98/100... Step: 100495... Loss: 0.000314... Val Loss: 0.000693\n",
      "Epoch: 98/100... Step: 100500... Loss: 0.000370... Val Loss: 0.000708\n",
      "Epoch: 98/100... Step: 100505... Loss: 0.000404... Val Loss: 0.000724\n",
      "Epoch: 98/100... Step: 100510... Loss: 0.000305... Val Loss: 0.000741\n",
      "Epoch: 98/100... Step: 100515... Loss: 0.000468... Val Loss: 0.000751\n",
      "Epoch: 98/100... Step: 100520... Loss: 0.001451... Val Loss: 0.000753\n",
      "Epoch: 98/100... Step: 100525... Loss: 0.001853... Val Loss: 0.000751\n",
      "Epoch: 98/100... Step: 100530... Loss: 0.001777... Val Loss: 0.000744\n",
      "Epoch: 98/100... Step: 100535... Loss: 0.001801... Val Loss: 0.000733\n",
      "Epoch: 98/100... Step: 100540... Loss: 0.001875... Val Loss: 0.000720\n",
      "Epoch: 98/100... Step: 100545... Loss: 0.001833... Val Loss: 0.000706\n",
      "Epoch: 98/100... Step: 100550... Loss: 0.001842... Val Loss: 0.000691\n",
      "Epoch: 98/100... Step: 100555... Loss: 0.001970... Val Loss: 0.000674\n",
      "Epoch: 98/100... Step: 100560... Loss: 0.001974... Val Loss: 0.000656\n",
      "Epoch: 98/100... Step: 100565... Loss: 0.002011... Val Loss: 0.000638\n",
      "Epoch: 98/100... Step: 100570... Loss: 0.002024... Val Loss: 0.000620\n",
      "Epoch: 98/100... Step: 100575... Loss: 0.001766... Val Loss: 0.000601\n",
      "Epoch: 98/100... Step: 100580... Loss: 0.001690... Val Loss: 0.000582\n",
      "Epoch: 98/100... Step: 100585... Loss: 0.001738... Val Loss: 0.000562\n",
      "Epoch: 98/100... Step: 100590... Loss: 0.001486... Val Loss: 0.000543\n",
      "Epoch: 98/100... Step: 100595... Loss: 0.001394... Val Loss: 0.000524\n",
      "Epoch: 98/100... Step: 100600... Loss: 0.001313... Val Loss: 0.000505\n",
      "Epoch: 98/100... Step: 100605... Loss: 0.001167... Val Loss: 0.000487\n",
      "Epoch: 98/100... Step: 100610... Loss: 0.000900... Val Loss: 0.000469\n",
      "Epoch: 98/100... Step: 100615... Loss: 0.000715... Val Loss: 0.000454\n",
      "Epoch: 98/100... Step: 100620... Loss: 0.000734... Val Loss: 0.000439\n",
      "Epoch: 98/100... Step: 100625... Loss: 0.000859... Val Loss: 0.000423\n",
      "Epoch: 98/100... Step: 100630... Loss: 0.000959... Val Loss: 0.000407\n",
      "Epoch: 98/100... Step: 100635... Loss: 0.000997... Val Loss: 0.000391\n",
      "Epoch: 98/100... Step: 100640... Loss: 0.000913... Val Loss: 0.000375\n",
      "Epoch: 98/100... Step: 100645... Loss: 0.000957... Val Loss: 0.000360\n",
      "Epoch: 98/100... Step: 100650... Loss: 0.001100... Val Loss: 0.000345\n",
      "Epoch: 98/100... Step: 100655... Loss: 0.001110... Val Loss: 0.000331\n",
      "Epoch: 98/100... Step: 100660... Loss: 0.000985... Val Loss: 0.000318\n",
      "Epoch: 98/100... Step: 100665... Loss: 0.000907... Val Loss: 0.000306\n",
      "Epoch: 98/100... Step: 100670... Loss: 0.000916... Val Loss: 0.000296\n",
      "Epoch: 98/100... Step: 100675... Loss: 0.000989... Val Loss: 0.000288\n",
      "Epoch: 98/100... Step: 100680... Loss: 0.000976... Val Loss: 0.000281\n",
      "Epoch: 98/100... Step: 100685... Loss: 0.000906... Val Loss: 0.000275\n",
      "Epoch: 98/100... Step: 100690... Loss: 0.000915... Val Loss: 0.000270\n",
      "Epoch: 98/100... Step: 100695... Loss: 0.000924... Val Loss: 0.000266\n",
      "Epoch: 98/100... Step: 100700... Loss: 0.000915... Val Loss: 0.000263\n",
      "Epoch: 98/100... Step: 100705... Loss: 0.000909... Val Loss: 0.000260\n",
      "Epoch: 98/100... Step: 100710... Loss: 0.000928... Val Loss: 0.000258\n",
      "Epoch: 98/100... Step: 100715... Loss: 0.000838... Val Loss: 0.000258\n",
      "Epoch: 98/100... Step: 100720... Loss: 0.000764... Val Loss: 0.000260\n",
      "Epoch: 98/100... Step: 100725... Loss: 0.000768... Val Loss: 0.000263\n",
      "Epoch: 98/100... Step: 100730... Loss: 0.000773... Val Loss: 0.000269\n",
      "Epoch: 98/100... Step: 100735... Loss: 0.000720... Val Loss: 0.000277\n",
      "Epoch: 98/100... Step: 100740... Loss: 0.000658... Val Loss: 0.000288\n",
      "Epoch: 98/100... Step: 100745... Loss: 0.000504... Val Loss: 0.000303\n",
      "Epoch: 98/100... Step: 100750... Loss: 0.000324... Val Loss: 0.000325\n",
      "Epoch: 98/100... Step: 100755... Loss: 0.000486... Val Loss: 0.000337\n",
      "Epoch: 98/100... Step: 100760... Loss: 0.000547... Val Loss: 0.000357\n",
      "Epoch: 98/100... Step: 100765... Loss: 0.000410... Val Loss: 0.000382\n",
      "Epoch: 98/100... Step: 100770... Loss: 0.000188... Val Loss: 0.000412\n",
      "Epoch: 98/100... Step: 100775... Loss: 0.000069... Val Loss: 0.000442\n",
      "Epoch: 98/100... Step: 100780... Loss: 0.000069... Val Loss: 0.000456\n",
      "Epoch: 98/100... Step: 100785... Loss: 0.000214... Val Loss: 0.000458\n",
      "Epoch: 98/100... Step: 100790... Loss: 0.000284... Val Loss: 0.000452\n",
      "Epoch: 98/100... Step: 100795... Loss: 0.000142... Val Loss: 0.000438\n",
      "Epoch: 98/100... Step: 100800... Loss: 0.000155... Val Loss: 0.000422\n",
      "Epoch: 98/100... Step: 100805... Loss: 0.000142... Val Loss: 0.000404\n",
      "Epoch: 98/100... Step: 100810... Loss: 0.000101... Val Loss: 0.000385\n",
      "Epoch: 98/100... Step: 100815... Loss: 0.000059... Val Loss: 0.000367\n",
      "Epoch: 98/100... Step: 100820... Loss: 0.000069... Val Loss: 0.000353\n",
      "Epoch: 98/100... Step: 100825... Loss: 0.000113... Val Loss: 0.000348\n",
      "Epoch: 98/100... Step: 100830... Loss: 0.000046... Val Loss: 0.000348\n",
      "Epoch: 98/100... Step: 100835... Loss: 0.000058... Val Loss: 0.000351\n",
      "Epoch: 98/100... Step: 100840... Loss: 0.000127... Val Loss: 0.000354\n",
      "Epoch: 98/100... Step: 100845... Loss: 0.000425... Val Loss: 0.000355\n",
      "Epoch: 98/100... Step: 100850... Loss: 0.000569... Val Loss: 0.000350\n",
      "Epoch: 98/100... Step: 100855... Loss: 0.000570... Val Loss: 0.000341\n",
      "Epoch: 98/100... Step: 100860... Loss: 0.000586... Val Loss: 0.000331\n",
      "Epoch: 98/100... Step: 100865... Loss: 0.000537... Val Loss: 0.000320\n",
      "Epoch: 98/100... Step: 100870... Loss: 0.000543... Val Loss: 0.000310\n",
      "Epoch: 98/100... Step: 100875... Loss: 0.000593... Val Loss: 0.000299\n",
      "Epoch: 98/100... Step: 100880... Loss: 0.000598... Val Loss: 0.000289\n",
      "Epoch: 98/100... Step: 100885... Loss: 0.000687... Val Loss: 0.000280\n",
      "Epoch: 98/100... Step: 100890... Loss: 0.000698... Val Loss: 0.000272\n",
      "Epoch: 98/100... Step: 100895... Loss: 0.000577... Val Loss: 0.000267\n",
      "Epoch: 98/100... Step: 100900... Loss: 0.000431... Val Loss: 0.000264\n",
      "Epoch: 98/100... Step: 100905... Loss: 0.000488... Val Loss: 0.000261\n",
      "Epoch: 98/100... Step: 100910... Loss: 0.000710... Val Loss: 0.000258\n",
      "Epoch: 98/100... Step: 100915... Loss: 0.000786... Val Loss: 0.000257\n",
      "Epoch: 98/100... Step: 100920... Loss: 0.000694... Val Loss: 0.000258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98/100... Step: 100925... Loss: 0.000634... Val Loss: 0.000259\n",
      "Epoch: 98/100... Step: 100930... Loss: 0.000589... Val Loss: 0.000261\n",
      "Epoch: 98/100... Step: 100935... Loss: 0.000588... Val Loss: 0.000263\n",
      "Epoch: 98/100... Step: 100940... Loss: 0.000555... Val Loss: 0.000266\n",
      "Epoch: 98/100... Step: 100945... Loss: 0.000510... Val Loss: 0.000269\n",
      "Epoch: 98/100... Step: 100950... Loss: 0.000513... Val Loss: 0.000273\n",
      "Epoch: 98/100... Step: 100955... Loss: 0.000493... Val Loss: 0.000278\n",
      "Epoch: 98/100... Step: 100960... Loss: 0.000538... Val Loss: 0.000282\n",
      "Epoch: 98/100... Step: 100965... Loss: 0.000595... Val Loss: 0.000288\n",
      "Epoch: 98/100... Step: 100970... Loss: 0.000616... Val Loss: 0.000295\n",
      "Epoch: 98/100... Step: 100975... Loss: 0.000581... Val Loss: 0.000302\n",
      "Epoch: 98/100... Step: 100980... Loss: 0.000597... Val Loss: 0.000310\n",
      "Epoch: 98/100... Step: 100985... Loss: 0.000592... Val Loss: 0.000319\n",
      "Epoch: 98/100... Step: 100990... Loss: 0.000495... Val Loss: 0.000327\n",
      "Epoch: 98/100... Step: 100995... Loss: 0.000443... Val Loss: 0.000337\n",
      "Epoch: 98/100... Step: 101000... Loss: 0.000447... Val Loss: 0.000348\n",
      "Epoch: 98/100... Step: 101005... Loss: 0.000388... Val Loss: 0.000358\n",
      "Epoch: 98/100... Step: 101010... Loss: 0.000310... Val Loss: 0.000369\n",
      "Epoch: 98/100... Step: 101015... Loss: 0.000240... Val Loss: 0.000380\n",
      "Epoch: 98/100... Step: 101020... Loss: 0.000273... Val Loss: 0.000391\n",
      "Epoch: 98/100... Step: 101025... Loss: 0.000318... Val Loss: 0.000403\n",
      "Epoch: 98/100... Step: 101030... Loss: 0.000377... Val Loss: 0.000415\n",
      "Epoch: 98/100... Step: 101035... Loss: 0.000433... Val Loss: 0.000429\n",
      "Epoch: 98/100... Step: 101040... Loss: 0.000501... Val Loss: 0.000443\n",
      "Epoch: 98/100... Step: 101045... Loss: 0.000480... Val Loss: 0.000454\n",
      "Epoch: 98/100... Step: 101050... Loss: 0.000388... Val Loss: 0.000464\n",
      "Epoch: 98/100... Step: 101055... Loss: 0.000422... Val Loss: 0.000474\n",
      "Epoch: 98/100... Step: 101060... Loss: 0.000524... Val Loss: 0.000486\n",
      "Epoch: 98/100... Step: 101065... Loss: 0.000526... Val Loss: 0.000498\n",
      "Epoch: 98/100... Step: 101070... Loss: 0.000502... Val Loss: 0.000507\n",
      "Epoch: 98/100... Step: 101075... Loss: 0.000513... Val Loss: 0.000519\n",
      "Epoch: 98/100... Step: 101080... Loss: 0.000484... Val Loss: 0.000532\n",
      "Epoch: 98/100... Step: 101085... Loss: 0.000427... Val Loss: 0.000544\n",
      "Epoch: 98/100... Step: 101090... Loss: 0.000519... Val Loss: 0.000557\n",
      "Epoch: 98/100... Step: 101095... Loss: 0.000531... Val Loss: 0.000573\n",
      "Epoch: 98/100... Step: 101100... Loss: 0.000515... Val Loss: 0.000586\n",
      "Epoch: 98/100... Step: 101105... Loss: 0.000452... Val Loss: 0.000596\n",
      "Epoch: 98/100... Step: 101110... Loss: 0.000337... Val Loss: 0.000607\n",
      "Epoch: 98/100... Step: 101115... Loss: 0.000290... Val Loss: 0.000618\n",
      "Epoch: 98/100... Step: 101120... Loss: 0.000390... Val Loss: 0.000631\n",
      "Epoch: 98/100... Step: 101125... Loss: 0.000437... Val Loss: 0.000643\n",
      "Epoch: 98/100... Step: 101130... Loss: 0.000393... Val Loss: 0.000657\n",
      "Epoch: 98/100... Step: 101135... Loss: 0.000332... Val Loss: 0.000667\n",
      "Epoch: 99/100... Step: 101140... Loss: 0.000700... Val Loss: 0.000678\n",
      "Epoch: 99/100... Step: 101145... Loss: 0.000820... Val Loss: 0.000691\n",
      "Epoch: 99/100... Step: 101150... Loss: 0.000909... Val Loss: 0.000703\n",
      "Epoch: 99/100... Step: 101155... Loss: 0.001098... Val Loss: 0.000721\n",
      "Epoch: 99/100... Step: 101160... Loss: 0.000999... Val Loss: 0.000726\n",
      "Epoch: 99/100... Step: 101165... Loss: 0.001069... Val Loss: 0.000738\n",
      "Epoch: 99/100... Step: 101170... Loss: 0.001117... Val Loss: 0.000750\n",
      "Epoch: 99/100... Step: 101175... Loss: 0.000909... Val Loss: 0.000763\n",
      "Epoch: 99/100... Step: 101180... Loss: 0.000967... Val Loss: 0.000775\n",
      "Epoch: 99/100... Step: 101185... Loss: 0.000994... Val Loss: 0.000789\n",
      "Epoch: 99/100... Step: 101190... Loss: 0.000943... Val Loss: 0.000802\n",
      "Epoch: 99/100... Step: 101195... Loss: 0.000865... Val Loss: 0.000816\n",
      "Epoch: 99/100... Step: 101200... Loss: 0.000752... Val Loss: 0.000828\n",
      "Epoch: 99/100... Step: 101205... Loss: 0.000544... Val Loss: 0.000840\n",
      "Epoch: 99/100... Step: 101210... Loss: 0.000283... Val Loss: 0.000853\n",
      "Epoch: 99/100... Step: 101215... Loss: 0.000393... Val Loss: 0.000866\n",
      "Epoch: 99/100... Step: 101220... Loss: 0.000481... Val Loss: 0.000881\n",
      "Epoch: 99/100... Step: 101225... Loss: 0.000437... Val Loss: 0.000894\n",
      "Epoch: 99/100... Step: 101230... Loss: 0.000318... Val Loss: 0.000911\n",
      "Epoch: 99/100... Step: 101235... Loss: 0.000140... Val Loss: 0.000923\n",
      "Epoch: 99/100... Step: 101240... Loss: 0.000176... Val Loss: 0.000938\n",
      "Epoch: 99/100... Step: 101245... Loss: 0.000183... Val Loss: 0.000953\n",
      "Epoch: 99/100... Step: 101250... Loss: 0.000194... Val Loss: 0.000969\n",
      "Epoch: 99/100... Step: 101255... Loss: 0.000160... Val Loss: 0.000989\n",
      "Epoch: 99/100... Step: 101260... Loss: 0.000214... Val Loss: 0.001009\n",
      "Epoch: 99/100... Step: 101265... Loss: 0.000324... Val Loss: 0.001027\n",
      "Epoch: 99/100... Step: 101270... Loss: 0.000365... Val Loss: 0.001048\n",
      "Epoch: 99/100... Step: 101275... Loss: 0.000399... Val Loss: 0.001070\n",
      "Epoch: 99/100... Step: 101280... Loss: 0.000541... Val Loss: 0.001087\n",
      "Epoch: 99/100... Step: 101285... Loss: 0.000616... Val Loss: 0.001095\n",
      "Epoch: 99/100... Step: 101290... Loss: 0.000510... Val Loss: 0.001097\n",
      "Epoch: 99/100... Step: 101295... Loss: 0.000404... Val Loss: 0.001109\n",
      "Epoch: 99/100... Step: 101300... Loss: 0.000307... Val Loss: 0.001125\n",
      "Epoch: 99/100... Step: 101305... Loss: 0.000196... Val Loss: 0.001141\n",
      "Epoch: 99/100... Step: 101310... Loss: 0.000188... Val Loss: 0.001158\n",
      "Epoch: 99/100... Step: 101315... Loss: 0.000156... Val Loss: 0.001178\n",
      "Epoch: 99/100... Step: 101320... Loss: 0.000101... Val Loss: 0.001196\n",
      "Epoch: 99/100... Step: 101325... Loss: 0.000077... Val Loss: 0.001208\n",
      "Epoch: 99/100... Step: 101330... Loss: 0.000070... Val Loss: 0.001215\n",
      "Epoch: 99/100... Step: 101335... Loss: 0.000067... Val Loss: 0.001218\n",
      "Epoch: 99/100... Step: 101340... Loss: 0.000072... Val Loss: 0.001220\n",
      "Epoch: 99/100... Step: 101345... Loss: 0.000051... Val Loss: 0.001224\n",
      "Epoch: 99/100... Step: 101350... Loss: 0.000083... Val Loss: 0.001231\n",
      "Epoch: 99/100... Step: 101355... Loss: 0.000057... Val Loss: 0.001227\n",
      "Epoch: 99/100... Step: 101360... Loss: 0.000057... Val Loss: 0.001224\n",
      "Epoch: 99/100... Step: 101365... Loss: 0.000113... Val Loss: 0.001222\n",
      "Epoch: 99/100... Step: 101370... Loss: 0.000174... Val Loss: 0.001219\n",
      "Epoch: 99/100... Step: 101375... Loss: 0.000091... Val Loss: 0.001216\n",
      "Epoch: 99/100... Step: 101380... Loss: 0.000082... Val Loss: 0.001212\n",
      "Epoch: 99/100... Step: 101385... Loss: 0.000097... Val Loss: 0.001206\n",
      "Epoch: 99/100... Step: 101390... Loss: 0.000044... Val Loss: 0.001199\n",
      "Epoch: 99/100... Step: 101395... Loss: 0.000148... Val Loss: 0.001190\n",
      "Epoch: 99/100... Step: 101400... Loss: 0.000244... Val Loss: 0.001180\n",
      "Epoch: 99/100... Step: 101405... Loss: 0.000267... Val Loss: 0.001169\n",
      "Epoch: 99/100... Step: 101410... Loss: 0.000282... Val Loss: 0.001157\n",
      "Epoch: 99/100... Step: 101415... Loss: 0.000242... Val Loss: 0.001144\n",
      "Epoch: 99/100... Step: 101420... Loss: 0.000205... Val Loss: 0.001131\n",
      "Epoch: 99/100... Step: 101425... Loss: 0.000360... Val Loss: 0.001118\n",
      "Epoch: 99/100... Step: 101430... Loss: 0.000501... Val Loss: 0.001104\n",
      "Epoch: 99/100... Step: 101435... Loss: 0.000566... Val Loss: 0.001091\n",
      "Epoch: 99/100... Step: 101440... Loss: 0.000449... Val Loss: 0.001076\n",
      "Epoch: 99/100... Step: 101445... Loss: 0.000454... Val Loss: 0.001063\n",
      "Epoch: 99/100... Step: 101450... Loss: 0.000519... Val Loss: 0.001052\n",
      "Epoch: 99/100... Step: 101455... Loss: 0.000443... Val Loss: 0.001039\n",
      "Epoch: 99/100... Step: 101460... Loss: 0.000494... Val Loss: 0.001026\n",
      "Epoch: 99/100... Step: 101465... Loss: 0.000630... Val Loss: 0.001013\n",
      "Epoch: 99/100... Step: 101470... Loss: 0.000628... Val Loss: 0.001002\n",
      "Epoch: 99/100... Step: 101475... Loss: 0.000453... Val Loss: 0.000990\n",
      "Epoch: 99/100... Step: 101480... Loss: 0.000480... Val Loss: 0.000979\n",
      "Epoch: 99/100... Step: 101485... Loss: 0.000709... Val Loss: 0.000968\n",
      "Epoch: 99/100... Step: 101490... Loss: 0.000520... Val Loss: 0.000956\n",
      "Epoch: 99/100... Step: 101495... Loss: 0.000245... Val Loss: 0.000944\n",
      "Epoch: 99/100... Step: 101500... Loss: 0.000114... Val Loss: 0.000933\n",
      "Epoch: 99/100... Step: 101505... Loss: 0.000072... Val Loss: 0.000923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99/100... Step: 101510... Loss: 0.000068... Val Loss: 0.000919\n",
      "Epoch: 99/100... Step: 101515... Loss: 0.000135... Val Loss: 0.000918\n",
      "Epoch: 99/100... Step: 101520... Loss: 0.000135... Val Loss: 0.000921\n",
      "Epoch: 99/100... Step: 101525... Loss: 0.000068... Val Loss: 0.000924\n",
      "Epoch: 99/100... Step: 101530... Loss: 0.000098... Val Loss: 0.000929\n",
      "Epoch: 99/100... Step: 101535... Loss: 0.000155... Val Loss: 0.000934\n",
      "Epoch: 99/100... Step: 101540... Loss: 0.000245... Val Loss: 0.000942\n",
      "Epoch: 99/100... Step: 101545... Loss: 0.000530... Val Loss: 0.000947\n",
      "Epoch: 99/100... Step: 101550... Loss: 0.001294... Val Loss: 0.000948\n",
      "Epoch: 99/100... Step: 101555... Loss: 0.002121... Val Loss: 0.000947\n",
      "Epoch: 99/100... Step: 101560... Loss: 0.002008... Val Loss: 0.000944\n",
      "Epoch: 99/100... Step: 101565... Loss: 0.002017... Val Loss: 0.000940\n",
      "Epoch: 99/100... Step: 101570... Loss: 0.002117... Val Loss: 0.000933\n",
      "Epoch: 99/100... Step: 101575... Loss: 0.002133... Val Loss: 0.000927\n",
      "Epoch: 99/100... Step: 101580... Loss: 0.002057... Val Loss: 0.000920\n",
      "Epoch: 99/100... Step: 101585... Loss: 0.002166... Val Loss: 0.000913\n",
      "Epoch: 99/100... Step: 101590... Loss: 0.002263... Val Loss: 0.000904\n",
      "Epoch: 99/100... Step: 101595... Loss: 0.002226... Val Loss: 0.000897\n",
      "Epoch: 99/100... Step: 101600... Loss: 0.002339... Val Loss: 0.000889\n",
      "Epoch: 99/100... Step: 101605... Loss: 0.002189... Val Loss: 0.000882\n",
      "Epoch: 99/100... Step: 101610... Loss: 0.001984... Val Loss: 0.000873\n",
      "Epoch: 99/100... Step: 101615... Loss: 0.002059... Val Loss: 0.000862\n",
      "Epoch: 99/100... Step: 101620... Loss: 0.001936... Val Loss: 0.000853\n",
      "Epoch: 99/100... Step: 101625... Loss: 0.001752... Val Loss: 0.000842\n",
      "Epoch: 99/100... Step: 101630... Loss: 0.001745... Val Loss: 0.000829\n",
      "Epoch: 99/100... Step: 101635... Loss: 0.001601... Val Loss: 0.000819\n",
      "Epoch: 99/100... Step: 101640... Loss: 0.001397... Val Loss: 0.000812\n",
      "Epoch: 99/100... Step: 101645... Loss: 0.001132... Val Loss: 0.000804\n",
      "Epoch: 99/100... Step: 101650... Loss: 0.001138... Val Loss: 0.000804\n",
      "Epoch: 99/100... Step: 101655... Loss: 0.001249... Val Loss: 0.000801\n",
      "Epoch: 99/100... Step: 101660... Loss: 0.001375... Val Loss: 0.000796\n",
      "Epoch: 99/100... Step: 101665... Loss: 0.001479... Val Loss: 0.000789\n",
      "Epoch: 99/100... Step: 101670... Loss: 0.001424... Val Loss: 0.000782\n",
      "Epoch: 99/100... Step: 101675... Loss: 0.001390... Val Loss: 0.000772\n",
      "Epoch: 99/100... Step: 101680... Loss: 0.001535... Val Loss: 0.000765\n",
      "Epoch: 99/100... Step: 101685... Loss: 0.001630... Val Loss: 0.000758\n",
      "Epoch: 99/100... Step: 101690... Loss: 0.001572... Val Loss: 0.000752\n",
      "Epoch: 99/100... Step: 101695... Loss: 0.001459... Val Loss: 0.000746\n",
      "Epoch: 99/100... Step: 101700... Loss: 0.001477... Val Loss: 0.000739\n",
      "Epoch: 99/100... Step: 101705... Loss: 0.001530... Val Loss: 0.000729\n",
      "Epoch: 99/100... Step: 101710... Loss: 0.001624... Val Loss: 0.000721\n",
      "Epoch: 99/100... Step: 101715... Loss: 0.001540... Val Loss: 0.000717\n",
      "Epoch: 99/100... Step: 101720... Loss: 0.001544... Val Loss: 0.000712\n",
      "Epoch: 99/100... Step: 101725... Loss: 0.001543... Val Loss: 0.000702\n",
      "Epoch: 99/100... Step: 101730... Loss: 0.001609... Val Loss: 0.000693\n",
      "Epoch: 99/100... Step: 101735... Loss: 0.001588... Val Loss: 0.000686\n",
      "Epoch: 99/100... Step: 101740... Loss: 0.001617... Val Loss: 0.000682\n",
      "Epoch: 99/100... Step: 101745... Loss: 0.001574... Val Loss: 0.000679\n",
      "Epoch: 99/100... Step: 101750... Loss: 0.001503... Val Loss: 0.000668\n",
      "Epoch: 99/100... Step: 101755... Loss: 0.001507... Val Loss: 0.000653\n",
      "Epoch: 99/100... Step: 101760... Loss: 0.001568... Val Loss: 0.000650\n",
      "Epoch: 99/100... Step: 101765... Loss: 0.001523... Val Loss: 0.000650\n",
      "Epoch: 99/100... Step: 101770... Loss: 0.001499... Val Loss: 0.000641\n",
      "Epoch: 99/100... Step: 101775... Loss: 0.001423... Val Loss: 0.000631\n",
      "Epoch: 99/100... Step: 101780... Loss: 0.001245... Val Loss: 0.000610\n",
      "Epoch: 99/100... Step: 101785... Loss: 0.001299... Val Loss: 0.000610\n",
      "Epoch: 99/100... Step: 101790... Loss: 0.001496... Val Loss: 0.000628\n",
      "Epoch: 99/100... Step: 101795... Loss: 0.001425... Val Loss: 0.000622\n",
      "Epoch: 99/100... Step: 101800... Loss: 0.001248... Val Loss: 0.000613\n",
      "Epoch: 99/100... Step: 101805... Loss: 0.000995... Val Loss: 0.000598\n",
      "Epoch: 99/100... Step: 101810... Loss: 0.000925... Val Loss: 0.000584\n",
      "Epoch: 99/100... Step: 101815... Loss: 0.000877... Val Loss: 0.000584\n",
      "Epoch: 99/100... Step: 101820... Loss: 0.000715... Val Loss: 0.000579\n",
      "Epoch: 99/100... Step: 101825... Loss: 0.000801... Val Loss: 0.000583\n",
      "Epoch: 99/100... Step: 101830... Loss: 0.000821... Val Loss: 0.000580\n",
      "Epoch: 99/100... Step: 101835... Loss: 0.000780... Val Loss: 0.000577\n",
      "Epoch: 99/100... Step: 101840... Loss: 0.000783... Val Loss: 0.000572\n",
      "Epoch: 99/100... Step: 101845... Loss: 0.000825... Val Loss: 0.000568\n",
      "Epoch: 99/100... Step: 101850... Loss: 0.000891... Val Loss: 0.000561\n",
      "Epoch: 99/100... Step: 101855... Loss: 0.000925... Val Loss: 0.000556\n",
      "Epoch: 99/100... Step: 101860... Loss: 0.000897... Val Loss: 0.000551\n",
      "Epoch: 99/100... Step: 101865... Loss: 0.000865... Val Loss: 0.000543\n",
      "Epoch: 99/100... Step: 101870... Loss: 0.000838... Val Loss: 0.000533\n",
      "Epoch: 99/100... Step: 101875... Loss: 0.000510... Val Loss: 0.000520\n",
      "Epoch: 99/100... Step: 101880... Loss: 0.000250... Val Loss: 0.000484\n",
      "Epoch: 99/100... Step: 101885... Loss: 0.000204... Val Loss: 0.000459\n",
      "Epoch: 99/100... Step: 101890... Loss: 0.000203... Val Loss: 0.000449\n",
      "Epoch: 99/100... Step: 101895... Loss: 0.000192... Val Loss: 0.000484\n",
      "Epoch: 99/100... Step: 101900... Loss: 0.000211... Val Loss: 0.000507\n",
      "Epoch: 99/100... Step: 101905... Loss: 0.000133... Val Loss: 0.000506\n",
      "Epoch: 99/100... Step: 101910... Loss: 0.000111... Val Loss: 0.000501\n",
      "Epoch: 99/100... Step: 101915... Loss: 0.000102... Val Loss: 0.000499\n",
      "Epoch: 99/100... Step: 101920... Loss: 0.000072... Val Loss: 0.000492\n",
      "Epoch: 99/100... Step: 101925... Loss: 0.000070... Val Loss: 0.000493\n",
      "Epoch: 99/100... Step: 101930... Loss: 0.000118... Val Loss: 0.000496\n",
      "Epoch: 99/100... Step: 101935... Loss: 0.000145... Val Loss: 0.000495\n",
      "Epoch: 99/100... Step: 101940... Loss: 0.000147... Val Loss: 0.000492\n",
      "Epoch: 99/100... Step: 101945... Loss: 0.000276... Val Loss: 0.000490\n",
      "Epoch: 99/100... Step: 101950... Loss: 0.000243... Val Loss: 0.000489\n",
      "Epoch: 99/100... Step: 101955... Loss: 0.000172... Val Loss: 0.000489\n",
      "Epoch: 99/100... Step: 101960... Loss: 0.000144... Val Loss: 0.000493\n",
      "Epoch: 99/100... Step: 101965... Loss: 0.000144... Val Loss: 0.000496\n",
      "Epoch: 99/100... Step: 101970... Loss: 0.000160... Val Loss: 0.000499\n",
      "Epoch: 99/100... Step: 101975... Loss: 0.000108... Val Loss: 0.000502\n",
      "Epoch: 99/100... Step: 101980... Loss: 0.000110... Val Loss: 0.000506\n",
      "Epoch: 99/100... Step: 101985... Loss: 0.000122... Val Loss: 0.000510\n",
      "Epoch: 99/100... Step: 101990... Loss: 0.000175... Val Loss: 0.000512\n",
      "Epoch: 99/100... Step: 101995... Loss: 0.000231... Val Loss: 0.000514\n",
      "Epoch: 99/100... Step: 102000... Loss: 0.000279... Val Loss: 0.000513\n",
      "Epoch: 99/100... Step: 102005... Loss: 0.000273... Val Loss: 0.000513\n",
      "Epoch: 99/100... Step: 102010... Loss: 0.000270... Val Loss: 0.000518\n",
      "Epoch: 99/100... Step: 102015... Loss: 0.000307... Val Loss: 0.000523\n",
      "Epoch: 99/100... Step: 102020... Loss: 0.000268... Val Loss: 0.000533\n",
      "Epoch: 99/100... Step: 102025... Loss: 0.000176... Val Loss: 0.000539\n",
      "Epoch: 99/100... Step: 102030... Loss: 0.000190... Val Loss: 0.000544\n",
      "Epoch: 99/100... Step: 102035... Loss: 0.000172... Val Loss: 0.000548\n",
      "Epoch: 99/100... Step: 102040... Loss: 0.000117... Val Loss: 0.000552\n",
      "Epoch: 99/100... Step: 102045... Loss: 0.000041... Val Loss: 0.000555\n",
      "Epoch: 99/100... Step: 102050... Loss: 0.000037... Val Loss: 0.000558\n",
      "Epoch: 99/100... Step: 102055... Loss: 0.000103... Val Loss: 0.000560\n",
      "Epoch: 99/100... Step: 102060... Loss: 0.000173... Val Loss: 0.000561\n",
      "Epoch: 99/100... Step: 102065... Loss: 0.000243... Val Loss: 0.000561\n",
      "Epoch: 99/100... Step: 102070... Loss: 0.000314... Val Loss: 0.000557\n",
      "Epoch: 99/100... Step: 102075... Loss: 0.000341... Val Loss: 0.000557\n",
      "Epoch: 99/100... Step: 102080... Loss: 0.000271... Val Loss: 0.000566\n",
      "Epoch: 99/100... Step: 102085... Loss: 0.000234... Val Loss: 0.000572\n",
      "Epoch: 99/100... Step: 102090... Loss: 0.000369... Val Loss: 0.000577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99/100... Step: 102095... Loss: 0.000420... Val Loss: 0.000580\n",
      "Epoch: 99/100... Step: 102100... Loss: 0.000424... Val Loss: 0.000586\n",
      "Epoch: 99/100... Step: 102105... Loss: 0.000420... Val Loss: 0.000592\n",
      "Epoch: 99/100... Step: 102110... Loss: 0.000415... Val Loss: 0.000595\n",
      "Epoch: 99/100... Step: 102115... Loss: 0.000356... Val Loss: 0.000597\n",
      "Epoch: 99/100... Step: 102120... Loss: 0.000423... Val Loss: 0.000600\n",
      "Epoch: 99/100... Step: 102125... Loss: 0.000490... Val Loss: 0.000602\n",
      "Epoch: 99/100... Step: 102130... Loss: 0.000467... Val Loss: 0.000601\n",
      "Epoch: 99/100... Step: 102135... Loss: 0.000454... Val Loss: 0.000606\n",
      "Epoch: 99/100... Step: 102140... Loss: 0.000384... Val Loss: 0.000614\n",
      "Epoch: 99/100... Step: 102145... Loss: 0.000280... Val Loss: 0.000618\n",
      "Epoch: 99/100... Step: 102150... Loss: 0.000348... Val Loss: 0.000622\n",
      "Epoch: 99/100... Step: 102155... Loss: 0.000459... Val Loss: 0.000625\n",
      "Epoch: 99/100... Step: 102160... Loss: 0.000451... Val Loss: 0.000627\n",
      "Epoch: 99/100... Step: 102165... Loss: 0.000381... Val Loss: 0.000630\n",
      "Epoch: 100/100... Step: 102170... Loss: 0.000700... Val Loss: 0.000636\n",
      "Epoch: 100/100... Step: 102175... Loss: 0.000837... Val Loss: 0.000638\n",
      "Epoch: 100/100... Step: 102180... Loss: 0.000921... Val Loss: 0.000642\n",
      "Epoch: 100/100... Step: 102185... Loss: 0.001137... Val Loss: 0.000643\n",
      "Epoch: 100/100... Step: 102190... Loss: 0.001138... Val Loss: 0.000647\n",
      "Epoch: 100/100... Step: 102195... Loss: 0.001108... Val Loss: 0.000651\n",
      "Epoch: 100/100... Step: 102200... Loss: 0.001254... Val Loss: 0.000654\n",
      "Epoch: 100/100... Step: 102205... Loss: 0.001104... Val Loss: 0.000657\n",
      "Epoch: 100/100... Step: 102210... Loss: 0.001056... Val Loss: 0.000660\n",
      "Epoch: 100/100... Step: 102215... Loss: 0.001169... Val Loss: 0.000662\n",
      "Epoch: 100/100... Step: 102220... Loss: 0.001099... Val Loss: 0.000665\n",
      "Epoch: 100/100... Step: 102225... Loss: 0.001060... Val Loss: 0.000668\n",
      "Epoch: 100/100... Step: 102230... Loss: 0.000972... Val Loss: 0.000671\n",
      "Epoch: 100/100... Step: 102235... Loss: 0.000855... Val Loss: 0.000674\n",
      "Epoch: 100/100... Step: 102240... Loss: 0.000569... Val Loss: 0.000677\n",
      "Epoch: 100/100... Step: 102245... Loss: 0.000553... Val Loss: 0.000680\n",
      "Epoch: 100/100... Step: 102250... Loss: 0.000700... Val Loss: 0.000682\n",
      "Epoch: 100/100... Step: 102255... Loss: 0.000706... Val Loss: 0.000685\n",
      "Epoch: 100/100... Step: 102260... Loss: 0.000638... Val Loss: 0.000687\n",
      "Epoch: 100/100... Step: 102265... Loss: 0.000460... Val Loss: 0.000689\n",
      "Epoch: 100/100... Step: 102270... Loss: 0.000412... Val Loss: 0.000692\n",
      "Epoch: 100/100... Step: 102275... Loss: 0.000483... Val Loss: 0.000694\n",
      "Epoch: 100/100... Step: 102280... Loss: 0.000477... Val Loss: 0.000697\n",
      "Epoch: 100/100... Step: 102285... Loss: 0.000488... Val Loss: 0.000698\n",
      "Epoch: 100/100... Step: 102290... Loss: 0.000495... Val Loss: 0.000698\n",
      "Epoch: 100/100... Step: 102295... Loss: 0.000616... Val Loss: 0.000700\n",
      "Epoch: 100/100... Step: 102300... Loss: 0.000705... Val Loss: 0.000702\n",
      "Epoch: 100/100... Step: 102305... Loss: 0.000729... Val Loss: 0.000704\n",
      "Epoch: 100/100... Step: 102310... Loss: 0.000848... Val Loss: 0.000706\n",
      "Epoch: 100/100... Step: 102315... Loss: 0.001001... Val Loss: 0.000709\n",
      "Epoch: 100/100... Step: 102320... Loss: 0.000956... Val Loss: 0.000713\n",
      "Epoch: 100/100... Step: 102325... Loss: 0.000830... Val Loss: 0.000718\n",
      "Epoch: 100/100... Step: 102330... Loss: 0.000751... Val Loss: 0.000720\n",
      "Epoch: 100/100... Step: 102335... Loss: 0.000677... Val Loss: 0.000722\n",
      "Epoch: 100/100... Step: 102340... Loss: 0.000671... Val Loss: 0.000725\n",
      "Epoch: 100/100... Step: 102345... Loss: 0.000656... Val Loss: 0.000727\n",
      "Epoch: 100/100... Step: 102350... Loss: 0.000608... Val Loss: 0.000728\n",
      "Epoch: 100/100... Step: 102355... Loss: 0.000492... Val Loss: 0.000731\n",
      "Epoch: 100/100... Step: 102360... Loss: 0.000433... Val Loss: 0.000733\n",
      "Epoch: 100/100... Step: 102365... Loss: 0.000453... Val Loss: 0.000735\n",
      "Epoch: 100/100... Step: 102370... Loss: 0.000444... Val Loss: 0.000737\n",
      "Epoch: 100/100... Step: 102375... Loss: 0.000500... Val Loss: 0.000737\n",
      "Epoch: 100/100... Step: 102380... Loss: 0.000537... Val Loss: 0.000738\n",
      "Epoch: 100/100... Step: 102385... Loss: 0.000541... Val Loss: 0.000739\n",
      "Epoch: 100/100... Step: 102390... Loss: 0.000486... Val Loss: 0.000743\n",
      "Epoch: 100/100... Step: 102395... Loss: 0.000458... Val Loss: 0.000745\n",
      "Epoch: 100/100... Step: 102400... Loss: 0.000360... Val Loss: 0.000749\n",
      "Epoch: 100/100... Step: 102405... Loss: 0.000359... Val Loss: 0.000751\n",
      "Epoch: 100/100... Step: 102410... Loss: 0.000437... Val Loss: 0.000753\n",
      "Epoch: 100/100... Step: 102415... Loss: 0.000379... Val Loss: 0.000755\n",
      "Epoch: 100/100... Step: 102420... Loss: 0.000435... Val Loss: 0.000756\n",
      "Epoch: 100/100... Step: 102425... Loss: 0.000366... Val Loss: 0.000758\n",
      "Epoch: 100/100... Step: 102430... Loss: 0.000202... Val Loss: 0.000760\n",
      "Epoch: 100/100... Step: 102435... Loss: 0.000179... Val Loss: 0.000762\n",
      "Epoch: 100/100... Step: 102440... Loss: 0.000146... Val Loss: 0.000764\n",
      "Epoch: 100/100... Step: 102445... Loss: 0.000143... Val Loss: 0.000765\n",
      "Epoch: 100/100... Step: 102450... Loss: 0.000197... Val Loss: 0.000767\n",
      "Epoch: 100/100... Step: 102455... Loss: 0.000150... Val Loss: 0.000768\n",
      "Epoch: 100/100... Step: 102460... Loss: 0.000119... Val Loss: 0.000770\n",
      "Epoch: 100/100... Step: 102465... Loss: 0.000205... Val Loss: 0.000771\n",
      "Epoch: 100/100... Step: 102470... Loss: 0.000187... Val Loss: 0.000771\n",
      "Epoch: 100/100... Step: 102475... Loss: 0.000119... Val Loss: 0.000769\n",
      "Epoch: 100/100... Step: 102480... Loss: 0.000224... Val Loss: 0.000770\n",
      "Epoch: 100/100... Step: 102485... Loss: 0.000179... Val Loss: 0.000770\n",
      "Epoch: 100/100... Step: 102490... Loss: 0.000171... Val Loss: 0.000769\n",
      "Epoch: 100/100... Step: 102495... Loss: 0.000321... Val Loss: 0.000768\n",
      "Epoch: 100/100... Step: 102500... Loss: 0.000376... Val Loss: 0.000767\n",
      "Epoch: 100/100... Step: 102505... Loss: 0.000271... Val Loss: 0.000766\n",
      "Epoch: 100/100... Step: 102510... Loss: 0.000177... Val Loss: 0.000764\n",
      "Epoch: 100/100... Step: 102515... Loss: 0.000408... Val Loss: 0.000764\n",
      "Epoch: 100/100... Step: 102520... Loss: 0.000416... Val Loss: 0.000763\n",
      "Epoch: 100/100... Step: 102525... Loss: 0.000143... Val Loss: 0.000761\n",
      "Epoch: 100/100... Step: 102530... Loss: 0.000095... Val Loss: 0.000759\n",
      "Epoch: 100/100... Step: 102535... Loss: 0.000215... Val Loss: 0.000758\n",
      "Epoch: 100/100... Step: 102540... Loss: 0.000235... Val Loss: 0.000757\n",
      "Epoch: 100/100... Step: 102545... Loss: 0.000270... Val Loss: 0.000757\n",
      "Epoch: 100/100... Step: 102550... Loss: 0.000322... Val Loss: 0.000758\n",
      "Epoch: 100/100... Step: 102555... Loss: 0.000247... Val Loss: 0.000759\n",
      "Epoch: 100/100... Step: 102560... Loss: 0.000243... Val Loss: 0.000760\n",
      "Epoch: 100/100... Step: 102565... Loss: 0.000305... Val Loss: 0.000761\n",
      "Epoch: 100/100... Step: 102570... Loss: 0.000330... Val Loss: 0.000762\n",
      "Epoch: 100/100... Step: 102575... Loss: 0.000305... Val Loss: 0.000763\n",
      "Epoch: 100/100... Step: 102580... Loss: 0.000711... Val Loss: 0.000764\n",
      "Epoch: 100/100... Step: 102585... Loss: 0.001641... Val Loss: 0.000764\n",
      "Epoch: 100/100... Step: 102590... Loss: 0.001829... Val Loss: 0.000763\n",
      "Epoch: 100/100... Step: 102595... Loss: 0.001782... Val Loss: 0.000763\n",
      "Epoch: 100/100... Step: 102600... Loss: 0.001872... Val Loss: 0.000762\n",
      "Epoch: 100/100... Step: 102605... Loss: 0.001926... Val Loss: 0.000761\n",
      "Epoch: 100/100... Step: 102610... Loss: 0.001885... Val Loss: 0.000761\n",
      "Epoch: 100/100... Step: 102615... Loss: 0.001927... Val Loss: 0.000760\n",
      "Epoch: 100/100... Step: 102620... Loss: 0.002083... Val Loss: 0.000759\n",
      "Epoch: 100/100... Step: 102625... Loss: 0.002090... Val Loss: 0.000758\n",
      "Epoch: 100/100... Step: 102630... Loss: 0.002140... Val Loss: 0.000757\n",
      "Epoch: 100/100... Step: 102635... Loss: 0.002141... Val Loss: 0.000756\n",
      "Epoch: 100/100... Step: 102640... Loss: 0.001891... Val Loss: 0.000755\n",
      "Epoch: 100/100... Step: 102645... Loss: 0.001898... Val Loss: 0.000753\n",
      "Epoch: 100/100... Step: 102650... Loss: 0.001922... Val Loss: 0.000752\n",
      "Epoch: 100/100... Step: 102655... Loss: 0.001697... Val Loss: 0.000751\n",
      "Epoch: 100/100... Step: 102660... Loss: 0.001652... Val Loss: 0.000748\n",
      "Epoch: 100/100... Step: 102665... Loss: 0.001557... Val Loss: 0.000747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100/100... Step: 102670... Loss: 0.001401... Val Loss: 0.000746\n",
      "Epoch: 100/100... Step: 102675... Loss: 0.001162... Val Loss: 0.000744\n",
      "Epoch: 100/100... Step: 102680... Loss: 0.001048... Val Loss: 0.000745\n",
      "Epoch: 100/100... Step: 102685... Loss: 0.001124... Val Loss: 0.000746\n",
      "Epoch: 100/100... Step: 102690... Loss: 0.001256... Val Loss: 0.000747\n",
      "Epoch: 100/100... Step: 102695... Loss: 0.001387... Val Loss: 0.000746\n",
      "Epoch: 100/100... Step: 102700... Loss: 0.001408... Val Loss: 0.000745\n",
      "Epoch: 100/100... Step: 102705... Loss: 0.001336... Val Loss: 0.000744\n",
      "Epoch: 100/100... Step: 102710... Loss: 0.001445... Val Loss: 0.000743\n",
      "Epoch: 100/100... Step: 102715... Loss: 0.001602... Val Loss: 0.000742\n",
      "Epoch: 100/100... Step: 102720... Loss: 0.001591... Val Loss: 0.000741\n",
      "Epoch: 100/100... Step: 102725... Loss: 0.001493... Val Loss: 0.000742\n",
      "Epoch: 100/100... Step: 102730... Loss: 0.001466... Val Loss: 0.000741\n",
      "Epoch: 100/100... Step: 102735... Loss: 0.001516... Val Loss: 0.000740\n",
      "Epoch: 100/100... Step: 102740... Loss: 0.001612... Val Loss: 0.000738\n",
      "Epoch: 100/100... Step: 102745... Loss: 0.001592... Val Loss: 0.000738\n",
      "Epoch: 100/100... Step: 102750... Loss: 0.001542... Val Loss: 0.000738\n",
      "Epoch: 100/100... Step: 102755... Loss: 0.001586... Val Loss: 0.000737\n",
      "Epoch: 100/100... Step: 102760... Loss: 0.001611... Val Loss: 0.000735\n",
      "Epoch: 100/100... Step: 102765... Loss: 0.001638... Val Loss: 0.000735\n",
      "Epoch: 100/100... Step: 102770... Loss: 0.001653... Val Loss: 0.000734\n",
      "Epoch: 100/100... Step: 102775... Loss: 0.001669... Val Loss: 0.000735\n",
      "Epoch: 100/100... Step: 102780... Loss: 0.001606... Val Loss: 0.000734\n",
      "Epoch: 100/100... Step: 102785... Loss: 0.001575... Val Loss: 0.000731\n",
      "Epoch: 100/100... Step: 102790... Loss: 0.001635... Val Loss: 0.000730\n",
      "Epoch: 100/100... Step: 102795... Loss: 0.001639... Val Loss: 0.000731\n",
      "Epoch: 100/100... Step: 102800... Loss: 0.001596... Val Loss: 0.000731\n",
      "Epoch: 100/100... Step: 102805... Loss: 0.001573... Val Loss: 0.000730\n",
      "Epoch: 100/100... Step: 102810... Loss: 0.001421... Val Loss: 0.000728\n",
      "Epoch: 100/100... Step: 102815... Loss: 0.001350... Val Loss: 0.000722\n",
      "Epoch: 100/100... Step: 102820... Loss: 0.001524... Val Loss: 0.000731\n",
      "Epoch: 100/100... Step: 102825... Loss: 0.001581... Val Loss: 0.000731\n",
      "Epoch: 100/100... Step: 102830... Loss: 0.001464... Val Loss: 0.000730\n",
      "Epoch: 100/100... Step: 102835... Loss: 0.001214... Val Loss: 0.000728\n",
      "Epoch: 100/100... Step: 102840... Loss: 0.001059... Val Loss: 0.000725\n",
      "Epoch: 100/100... Step: 102845... Loss: 0.001062... Val Loss: 0.000725\n",
      "Epoch: 100/100... Step: 102850... Loss: 0.000911... Val Loss: 0.000725\n",
      "Epoch: 100/100... Step: 102855... Loss: 0.000901... Val Loss: 0.000726\n",
      "Epoch: 100/100... Step: 102860... Loss: 0.000994... Val Loss: 0.000727\n",
      "Epoch: 100/100... Step: 102865... Loss: 0.000973... Val Loss: 0.000727\n",
      "Epoch: 100/100... Step: 102870... Loss: 0.000966... Val Loss: 0.000727\n",
      "Epoch: 100/100... Step: 102875... Loss: 0.000997... Val Loss: 0.000727\n",
      "Epoch: 100/100... Step: 102880... Loss: 0.001036... Val Loss: 0.000726\n",
      "Epoch: 100/100... Step: 102885... Loss: 0.001115... Val Loss: 0.000726\n",
      "Epoch: 100/100... Step: 102890... Loss: 0.001118... Val Loss: 0.000726\n",
      "Epoch: 100/100... Step: 102895... Loss: 0.001057... Val Loss: 0.000725\n",
      "Epoch: 100/100... Step: 102900... Loss: 0.001067... Val Loss: 0.000724\n",
      "Epoch: 100/100... Step: 102905... Loss: 0.000885... Val Loss: 0.000722\n",
      "Epoch: 100/100... Step: 102910... Loss: 0.000565... Val Loss: 0.000717\n",
      "Epoch: 100/100... Step: 102915... Loss: 0.000454... Val Loss: 0.000711\n",
      "Epoch: 100/100... Step: 102920... Loss: 0.000445... Val Loss: 0.000706\n",
      "Epoch: 100/100... Step: 102925... Loss: 0.000434... Val Loss: 0.000714\n",
      "Epoch: 100/100... Step: 102930... Loss: 0.000451... Val Loss: 0.000721\n",
      "Epoch: 100/100... Step: 102935... Loss: 0.000417... Val Loss: 0.000722\n",
      "Epoch: 100/100... Step: 102940... Loss: 0.000358... Val Loss: 0.000723\n",
      "Epoch: 100/100... Step: 102945... Loss: 0.000310... Val Loss: 0.000723\n",
      "Epoch: 100/100... Step: 102950... Loss: 0.000199... Val Loss: 0.000721\n",
      "Epoch: 100/100... Step: 102955... Loss: 0.000199... Val Loss: 0.000720\n",
      "Epoch: 100/100... Step: 102960... Loss: 0.000307... Val Loss: 0.000723\n",
      "Epoch: 100/100... Step: 102965... Loss: 0.000412... Val Loss: 0.000722\n",
      "Epoch: 100/100... Step: 102970... Loss: 0.000289... Val Loss: 0.000722\n",
      "Epoch: 100/100... Step: 102975... Loss: 0.000095... Val Loss: 0.000721\n",
      "Epoch: 100/100... Step: 102980... Loss: 0.000049... Val Loss: 0.000720\n",
      "Epoch: 100/100... Step: 102985... Loss: 0.000064... Val Loss: 0.000720\n",
      "Epoch: 100/100... Step: 102990... Loss: 0.000119... Val Loss: 0.000720\n",
      "Epoch: 100/100... Step: 102995... Loss: 0.000112... Val Loss: 0.000721\n",
      "Epoch: 100/100... Step: 103000... Loss: 0.000091... Val Loss: 0.000721\n",
      "Epoch: 100/100... Step: 103005... Loss: 0.000116... Val Loss: 0.000721\n",
      "Epoch: 100/100... Step: 103010... Loss: 0.000144... Val Loss: 0.000721\n",
      "Epoch: 100/100... Step: 103015... Loss: 0.000127... Val Loss: 0.000721\n",
      "Epoch: 100/100... Step: 103020... Loss: 0.000107... Val Loss: 0.000721\n",
      "Epoch: 100/100... Step: 103025... Loss: 0.000047... Val Loss: 0.000720\n",
      "Epoch: 100/100... Step: 103030... Loss: 0.000047... Val Loss: 0.000718\n",
      "Epoch: 100/100... Step: 103035... Loss: 0.000056... Val Loss: 0.000717\n",
      "Epoch: 100/100... Step: 103040... Loss: 0.000039... Val Loss: 0.000717\n",
      "Epoch: 100/100... Step: 103045... Loss: 0.000076... Val Loss: 0.000718\n",
      "Epoch: 100/100... Step: 103050... Loss: 0.000090... Val Loss: 0.000718\n",
      "Epoch: 100/100... Step: 103055... Loss: 0.000062... Val Loss: 0.000720\n",
      "Epoch: 100/100... Step: 103060... Loss: 0.000055... Val Loss: 0.000721\n",
      "Epoch: 100/100... Step: 103065... Loss: 0.000052... Val Loss: 0.000721\n",
      "Epoch: 100/100... Step: 103070... Loss: 0.000054... Val Loss: 0.000721\n",
      "Epoch: 100/100... Step: 103075... Loss: 0.000135... Val Loss: 0.000721\n",
      "Epoch: 100/100... Step: 103080... Loss: 0.000168... Val Loss: 0.000721\n",
      "Epoch: 100/100... Step: 103085... Loss: 0.000107... Val Loss: 0.000720\n",
      "Epoch: 100/100... Step: 103090... Loss: 0.000052... Val Loss: 0.000720\n",
      "Epoch: 100/100... Step: 103095... Loss: 0.000045... Val Loss: 0.000719\n",
      "Epoch: 100/100... Step: 103100... Loss: 0.000096... Val Loss: 0.000717\n",
      "Epoch: 100/100... Step: 103105... Loss: 0.000170... Val Loss: 0.000714\n",
      "Epoch: 100/100... Step: 103110... Loss: 0.000148... Val Loss: 0.000715\n",
      "Epoch: 100/100... Step: 103115... Loss: 0.000068... Val Loss: 0.000717\n",
      "Epoch: 100/100... Step: 103120... Loss: 0.000167... Val Loss: 0.000718\n",
      "Epoch: 100/100... Step: 103125... Loss: 0.000278... Val Loss: 0.000718\n",
      "Epoch: 100/100... Step: 103130... Loss: 0.000279... Val Loss: 0.000718\n",
      "Epoch: 100/100... Step: 103135... Loss: 0.000257... Val Loss: 0.000720\n",
      "Epoch: 100/100... Step: 103140... Loss: 0.000278... Val Loss: 0.000720\n",
      "Epoch: 100/100... Step: 103145... Loss: 0.000262... Val Loss: 0.000719\n",
      "Epoch: 100/100... Step: 103150... Loss: 0.000269... Val Loss: 0.000720\n",
      "Epoch: 100/100... Step: 103155... Loss: 0.000345... Val Loss: 0.000719\n",
      "Epoch: 100/100... Step: 103160... Loss: 0.000361... Val Loss: 0.000718\n",
      "Epoch: 100/100... Step: 103165... Loss: 0.000349... Val Loss: 0.000717\n",
      "Epoch: 100/100... Step: 103170... Loss: 0.000291... Val Loss: 0.000719\n",
      "Epoch: 100/100... Step: 103175... Loss: 0.000183... Val Loss: 0.000719\n",
      "Epoch: 100/100... Step: 103180... Loss: 0.000200... Val Loss: 0.000720\n",
      "Epoch: 100/100... Step: 103185... Loss: 0.000309... Val Loss: 0.000720\n",
      "Epoch: 100/100... Step: 103190... Loss: 0.000357... Val Loss: 0.000720\n",
      "Epoch: 100/100... Step: 103195... Loss: 0.000308... Val Loss: 0.000719\n",
      "Epoch: 100/100... Step: 103200... Loss: 0.000266... Val Loss: 0.000720\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for i in range(epochs):\n",
    "    for inputs, labels in dataset.train_loader:\n",
    "        h = model.init_hidden(BATCH_SIZE)\n",
    "        counter += 1\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        h = tuple([e.data for e in h])\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = tuple([each.data for each in h])#model.init_hidden(BATCH_SIZE)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in dataset.val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(DEVICE), lab.to(DEVICE)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) < valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Visualizer:\n",
    "    colors=['blue','black', 'red']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.date_pred_targ_dict: dict = dict()\n",
    "        \n",
    "    def add(self, timestamp, pred, targ, color='red'):\n",
    "        self.date_pred_targ_dict[color] = pd.concat([self.date_pred_targ_dict.get(color, pd.DataFrame()),\n",
    "                                                     pd.concat([pd.DataFrame(timestamp),\n",
    "                                                       pd.DataFrame(pred),\n",
    "                                                       pd.DataFrame(targ)], axis=1)])\n",
    "    \n",
    "    def plot(self):\n",
    "        for color in self.colors:\n",
    "            color_df = visualizer.date_pred_targ_dict.get(color, pd.DataFrame())\n",
    "            plt.scatter(color_df.iloc[:,0], color_df.iloc[:, -2]*10, color=color)\n",
    "            plt.plot(color_df.iloc[:,0], color_df.iloc[:, -2]*10, color=color)\n",
    "            plt.scatter(color_df.iloc[:,0], color_df.iloc[:, -1]*10, color='g')\n",
    "            plt.plot(color_df.iloc[:,0], color_df.iloc[:, -1]*10, color='g')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(dataset.val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE loss: 0.00000017\n",
      "MAE loss: 0.00035921\n",
      "KLDiv loss: -0.07918944\n"
     ]
    }
   ],
   "source": [
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('./state_dict.pt'))\n",
    "visualizer = Visualizer()\n",
    "\n",
    "mse_losses = []\n",
    "mae_losses = []\n",
    "kldiv_losses = []\n",
    "totalIndexes = 0\n",
    "custom_batch = BATCH_SIZE\n",
    "# index_of_elapsed = dataset.train_data.columns.tolist().index(dataset.ELAPSED)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# #TRAIN\n",
    "# for inputs, labels in tqdm(dataset.train_loader):\n",
    "#     if inputs.shape[0] != custom_batch:\n",
    "#         continue\n",
    "#       totalIndexes += 1\n",
    "#     inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "#     output, h = model(inputs, h)\n",
    "#     visualizer.add(np.arange(len(inputs)), output, labels[:,-1], color='black')\n",
    "    \n",
    "#VALIDATE\n",
    "for index, (inputs, labels) in enumerate(dataset.val_loader):\n",
    "#     if inputs.shape[0] != custom_batch:\n",
    "#         continue\n",
    "    h = model.init_hidden(inputs.shape[0])\n",
    "    totalIndexes += 1\n",
    "    inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "    output, h = model(inputs, h)\n",
    "#     print(output[-1].shape, labels[:,-1].shape)\n",
    "    visualizer.add(np.array([totalIndexes]), np.array(output[-1]), np.array(labels[-1]), color='blue')\n",
    "    \n",
    "#TEST\n",
    "for index, (inputs, labels) in enumerate(dataset.test_loader):#tqdm(dataset.test_loader):\n",
    "#     if inputs.shape[0] != custom_batch:\n",
    "#         continue\n",
    "    h = model.init_hidden(inputs.shape[0])\n",
    "    totalIndexes += 1\n",
    "    inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "    output, h = model(inputs, h)\n",
    "    visualizer.add(np.array([totalIndexes]), np.array(output[-1]), np.array(labels[-1]))\n",
    "    \n",
    "    mse_losses.append(nn.MSELoss()(output.squeeze(), labels.float()).item())\n",
    "    mae_losses.append(nn.L1Loss()(output.squeeze(), labels.float()).item())\n",
    "    kldiv_losses.append(nn.KLDivLoss()(output.squeeze(), labels.float()).item())\n",
    "    \n",
    "\n",
    "print(\"MSE loss: {:.8f}\".format(np.mean(mse_losses)))\n",
    "print(\"MAE loss: {:.8f}\".format(np.mean(mae_losses)))\n",
    "print(\"KLDiv loss: {:.8f}\".format(np.mean(kldiv_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BS10 maxLR==lr 2lstms\n",
    "lr=0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.009417</td>\n",
       "      <td>0.021561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.008226</td>\n",
       "      <td>0.021566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.001924</td>\n",
       "      <td>0.021657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.006680</td>\n",
       "      <td>0.021619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.048801</td>\n",
       "      <td>0.021592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.020556</td>\n",
       "      <td>0.021378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.015731</td>\n",
       "      <td>0.021419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.002783</td>\n",
       "      <td>0.021392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>-0.000198</td>\n",
       "      <td>0.021166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>-0.003927</td>\n",
       "      <td>0.021159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>-0.001303</td>\n",
       "      <td>0.021091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>-0.000701</td>\n",
       "      <td>0.020957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>-0.001529</td>\n",
       "      <td>0.020914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>-0.001350</td>\n",
       "      <td>0.021026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.020925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>0.008879</td>\n",
       "      <td>0.021010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>-0.002485</td>\n",
       "      <td>0.021084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>0.002426</td>\n",
       "      <td>0.021110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>0.007645</td>\n",
       "      <td>0.021147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.021176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>-0.002775</td>\n",
       "      <td>0.021160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>-0.000994</td>\n",
       "      <td>0.021117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>-0.009009</td>\n",
       "      <td>0.021073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>-0.009836</td>\n",
       "      <td>0.021060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>-0.009922</td>\n",
       "      <td>0.020830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>-0.010269</td>\n",
       "      <td>0.020888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>-0.005811</td>\n",
       "      <td>0.020939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>-0.000950</td>\n",
       "      <td>0.021002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>-0.005978</td>\n",
       "      <td>0.020867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.010038</td>\n",
       "      <td>0.020943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171</td>\n",
       "      <td>-0.005473</td>\n",
       "      <td>0.020376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>172</td>\n",
       "      <td>-0.005481</td>\n",
       "      <td>0.020280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173</td>\n",
       "      <td>-0.007005</td>\n",
       "      <td>0.020199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>-0.004673</td>\n",
       "      <td>0.020243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175</td>\n",
       "      <td>-0.003590</td>\n",
       "      <td>0.020233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176</td>\n",
       "      <td>-0.003822</td>\n",
       "      <td>0.020251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>177</td>\n",
       "      <td>-0.003684</td>\n",
       "      <td>0.020216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>178</td>\n",
       "      <td>-0.004382</td>\n",
       "      <td>0.020253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>179</td>\n",
       "      <td>-0.003566</td>\n",
       "      <td>0.020241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180</td>\n",
       "      <td>-0.004998</td>\n",
       "      <td>0.020250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181</td>\n",
       "      <td>-0.004696</td>\n",
       "      <td>0.020228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>182</td>\n",
       "      <td>-0.003982</td>\n",
       "      <td>0.020198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>183</td>\n",
       "      <td>-0.003547</td>\n",
       "      <td>0.020090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>184</td>\n",
       "      <td>-0.002894</td>\n",
       "      <td>0.020127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>185</td>\n",
       "      <td>-0.001602</td>\n",
       "      <td>0.020338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>186</td>\n",
       "      <td>-0.002606</td>\n",
       "      <td>0.020321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>187</td>\n",
       "      <td>0.018795</td>\n",
       "      <td>0.020365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188</td>\n",
       "      <td>-0.008169</td>\n",
       "      <td>0.020406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>189</td>\n",
       "      <td>-0.018036</td>\n",
       "      <td>0.020369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190</td>\n",
       "      <td>-0.019138</td>\n",
       "      <td>0.020343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>191</td>\n",
       "      <td>-0.017679</td>\n",
       "      <td>0.020342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192</td>\n",
       "      <td>-0.016002</td>\n",
       "      <td>0.020324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>193</td>\n",
       "      <td>-0.014368</td>\n",
       "      <td>0.020350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194</td>\n",
       "      <td>-0.013140</td>\n",
       "      <td>0.020326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>195</td>\n",
       "      <td>-0.012583</td>\n",
       "      <td>0.020330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>-0.013096</td>\n",
       "      <td>0.020448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>197</td>\n",
       "      <td>-0.012173</td>\n",
       "      <td>0.020433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>198</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.020439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>199</td>\n",
       "      <td>-0.016555</td>\n",
       "      <td>0.020369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>-0.016039</td>\n",
       "      <td>0.020163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         0         0\n",
       "0     1 -0.009417  0.021561\n",
       "0     2 -0.008226  0.021566\n",
       "0     3 -0.001924  0.021657\n",
       "0     4 -0.006680  0.021619\n",
       "0     5  0.048801  0.021592\n",
       "0     6 -0.020556  0.021378\n",
       "0     7 -0.015731  0.021419\n",
       "0     8 -0.002783  0.021392\n",
       "0     9 -0.000198  0.021166\n",
       "0    10 -0.003927  0.021159\n",
       "0    11 -0.001303  0.021091\n",
       "0    12 -0.000701  0.020957\n",
       "0    13 -0.001529  0.020914\n",
       "0    14 -0.001350  0.021026\n",
       "0    15  0.000286  0.020925\n",
       "0    16  0.008879  0.021010\n",
       "0    17 -0.002485  0.021084\n",
       "0    18  0.002426  0.021110\n",
       "0    19  0.007645  0.021147\n",
       "0    20  0.001136  0.021176\n",
       "0    21 -0.002775  0.021160\n",
       "0    22 -0.000994  0.021117\n",
       "0    23 -0.009009  0.021073\n",
       "0    24 -0.009836  0.021060\n",
       "0    25 -0.009922  0.020830\n",
       "0    26 -0.010269  0.020888\n",
       "0    27 -0.005811  0.020939\n",
       "0    28 -0.000950  0.021002\n",
       "0    29 -0.005978  0.020867\n",
       "0    30  0.010038  0.020943\n",
       "..  ...       ...       ...\n",
       "0   171 -0.005473  0.020376\n",
       "0   172 -0.005481  0.020280\n",
       "0   173 -0.007005  0.020199\n",
       "0   174 -0.004673  0.020243\n",
       "0   175 -0.003590  0.020233\n",
       "0   176 -0.003822  0.020251\n",
       "0   177 -0.003684  0.020216\n",
       "0   178 -0.004382  0.020253\n",
       "0   179 -0.003566  0.020241\n",
       "0   180 -0.004998  0.020250\n",
       "0   181 -0.004696  0.020228\n",
       "0   182 -0.003982  0.020198\n",
       "0   183 -0.003547  0.020090\n",
       "0   184 -0.002894  0.020127\n",
       "0   185 -0.001602  0.020338\n",
       "0   186 -0.002606  0.020321\n",
       "0   187  0.018795  0.020365\n",
       "0   188 -0.008169  0.020406\n",
       "0   189 -0.018036  0.020369\n",
       "0   190 -0.019138  0.020343\n",
       "0   191 -0.017679  0.020342\n",
       "0   192 -0.016002  0.020324\n",
       "0   193 -0.014368  0.020350\n",
       "0   194 -0.013140  0.020326\n",
       "0   195 -0.012583  0.020330\n",
       "0   196 -0.013096  0.020448\n",
       "0   197 -0.012173  0.020433\n",
       "0   198  0.002503  0.020439\n",
       "0   199 -0.016555  0.020369\n",
       "0   200 -0.016039  0.020163\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualizer.date_pred_targ_dict['blue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "# visualizer.plot()\n",
    "# for color in ['blue', 'red']:\n",
    "for color in ['red']:\n",
    "    color_df = visualizer.date_pred_targ_dict.get(color, pd.DataFrame())\n",
    "    plt.scatter(color_df.iloc[:,0], color_df.iloc[:, -2]*10, color=color)\n",
    "    plt.plot(color_df.iloc[:,0], color_df.iloc[:, -2]*10, color=color)\n",
    "    plt.scatter(color_df.iloc[:,0], color_df.iloc[:, -1]*10, color='g')\n",
    "    plt.plot(color_df.iloc[:,0], color_df.iloc[:, -1]*10, color='g')\n",
    "#     plt.scatter([color_df.iloc[0,0]], [color_df.iloc[0, -2]*10], color=color)\n",
    "#     plt.plot([color_df.iloc[0,0]], [color_df.iloc[0, -2]*10], color=color)\n",
    "#     plt.scatter([color_df.iloc[0,0]], [color_df.iloc[0, -1]*10], color='g')\n",
    "#     plt.plot([color_df.iloc[0,0]], [color_df.iloc[0, -1]*10], color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BS10 maxLR==0.01 3lstms\n",
    "Epoch: 85/100... Step: 8738... Loss: 0.000911... Val Loss: 0.000262\n",
    "Validation loss decreased (0.000265 --> 0.000262).  Saving model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "# visualizer.plot()\n",
    "# for color in ['blue', 'red']:\n",
    "for color in ['red']:\n",
    "    color_df = visualizer.date_pred_targ_dict.get(color, pd.DataFrame())\n",
    "    plt.scatter(color_df.iloc[:,0], color_df.iloc[:, -2]*10, color=color)\n",
    "    plt.plot(color_df.iloc[:,0], color_df.iloc[:, -2]*10, color=color)\n",
    "    plt.scatter(color_df.iloc[:,0], color_df.iloc[:, -1]*10, color='g')\n",
    "    plt.plot(color_df.iloc[:,0], color_df.iloc[:, -1]*10, color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BS10 maxLR==0.01 2lstms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "# visualizer.plot()\n",
    "for color in ['blue', 'red']:\n",
    "# for color in ['red']:\n",
    "    color_df = visualizer.date_pred_targ_dict.get(color, pd.DataFrame())\n",
    "    plt.scatter(color_df.iloc[:,0], color_df.iloc[:, -2]*10, color=color)\n",
    "    plt.plot(color_df.iloc[:,0], color_df.iloc[:, -2]*10, color=color)\n",
    "    plt.scatter(color_df.iloc[:,0], color_df.iloc[:, -1]*10, color='g')\n",
    "    plt.plot(color_df.iloc[:,0], color_df.iloc[:, -1]*10, color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BS10 maxLR==0.001 2lstms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "# visualizer.plot()\n",
    "for color in ['blue', 'red']:\n",
    "# for color in ['red']:\n",
    "    color_df = visualizer.date_pred_targ_dict.get(color, pd.DataFrame())\n",
    "    plt.scatter(color_df.iloc[:,0], color_df.iloc[:, -2]*10, color=color)\n",
    "    plt.plot(color_df.iloc[:,0], color_df.iloc[:, -2]*10, color=color)\n",
    "    plt.scatter(color_df.iloc[:,0], color_df.iloc[:, -1]*10, color='g')\n",
    "    plt.plot(color_df.iloc[:,0], color_df.iloc[:, -1]*10, color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "# visualizer.plot()\n",
    "for color in ['black', 'blue', 'red']:\n",
    "# for color in ['red']:\n",
    "    color_df = visualizer.date_pred_targ_dict.get(color, pd.DataFrame())\n",
    "    plt.scatter(color_df.iloc[:,0], color_df.iloc[:, -2]*10, color=color)\n",
    "    plt.plot(color_df.iloc[:,0], color_df.iloc[:, -2]*10, color=color)\n",
    "    plt.scatter(color_df.iloc[:,0], color_df.iloc[:, -1]*10, color='g')\n",
    "    plt.plot(color_df.iloc[:,0], color_df.iloc[:, -1]*10, color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters:\n",
    "\n",
    "- model:\n",
    "    - embedding_dim=features*2   \n",
    "    - hidden_dim=features*16  \n",
    "- learning rate  \n",
    "- batch size (window size?)\n",
    "- DONE - DID IMPROVE criterion = nn.MSELoss()   \n",
    "    - try a loss which scales through time. \n",
    "- optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "- add polynomial features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
